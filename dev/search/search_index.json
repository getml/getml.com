{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p> <p>What's new? | getML 1.4 release \u2192</p> <p></p>"},{"location":"#your-ml-suite-for-relational-and-time-series-data","title":"Your ML Suite for relational and time-series data.","text":"<p>getML Relational Learning unlocks a 10x speed-up potential and superior model performance.  A game changer in predictive applications for enterprise applications.</p> <p> <p>Request a demo API Reference</p> <p></p> <p> <p>Powering the world\u2019s best machine learning pipelines. From next-gen startups to established enterprises.</p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p></p>"},{"location":"#machine-learning-for-enterprise-data","title":"Machine Learning for enterprise data","text":""},{"location":"#why-you-should-consider-getml","title":"Why you should consider getML","text":"<p>getML introduces new ML algorithms that empower data scientists to achieve superior model performance without the burden of manual feature engineering and building complex feature pipelines.</p> <p>By generalizing gradient boosting to multi-relational decision trees, getML brings supervised learning to raw relational data, enabling end-to-end prediction pipelines. The getML Suite provides an easy to use Python API, adhering to modern standards.</p> <p>getML is developed by Code17 GmbH from Leipzig, and is used across industries, from finance and manufacturing to healthcare and beyond.</p>"},{"location":"#the-better-solution-for-predictive-analytics-on-your-enterprise-data","title":"The better solution for predictive analytics on your enterprise data","text":"<ul> <li> <p>getML is easy to use helping you deliverbetter models faster</p> </li> <li> <p>Delivered 10x speedup incustomer projects</p> </li> <li> <p>Up to 65% gain in prediction accuracyover baseline models</p> </li> </ul>"},{"location":"#whats-under-the-hood","title":"What's under the hood?","text":""},{"location":"#billions-of-features-with-a-few-lines-of-code","title":"Billions of Features with a few lines of code","text":"<p>getML is a high-performance machine learning software for predictive analytics on relational and time series data. At the heart of getML's innovation are five novel feature learning algorithms that automate manual feature engineering using supervised learning. getML enables the creation of end-to-end prediction pipelines capable of learning from terabytes of raw relational data, achieving unparalleled model accuracy within days not months.</p> <p>Three magic lines of  Python <code>code</code> to learn from billions of features.  Press  for more details.</p> <pre><code>import getml\n\n# Staging of data\ndata_container = ... # (6)!\n\n# Defintion of data model\nstar_schema = ... # (7)!\n\n# Define the feature learners\nfast_prop = getml.feature_learning.FastProp() # (1)!\nrelboost = getml.feature_learning.Relboost() # (2)!\n\n# Define the predictor\nxgb_predictor = getml.predictors.ScaleGBMRegressor() # (3)!\n\n# Define the pipeline\npipeline = getml.pipeline.Pipeline( # (4)!\n    data_model = star_schema.data_model,\n    feature_learners = [fast_prop, relboost],\n    predictors = xgb_predictor\n)\n\n# Train the feature learnings and the predictor\npipeline.fit(data_container.train) # (5)!\n\npipeline.predict(data_container.test)\n</code></pre> <ol> <li> <p><code>FastProp</code> comes with our Community edition. It is fast and generates a substantial number of important features based on simple aggregations.</p> </li> <li> <p><code>Relboost</code> is part our Enterprise edition. A generalization of the gradient boosting algorithm, Relboost can learn really complex interdependencies.</p> </li> <li> <p><code>ScaleGBMRegressor</code> is our memory-mapped predictor that can handle big datasets that do not fit into memory.</p> </li> <li> <p><code>Pipeline</code> bundles together the data model, feature learners and predictors. Just with this line of code, getML takes care of generation and selection of features, and training of the predictor when the pipeline's <code>fit</code> is called next.</p> </li> <li> <p>Inspired by libraries like <code>scikit-learn</code>, the <code>fit()</code>, <code>score()</code>, and <code>predict()</code> methods of a pipeline make the machine learning process a breeze.</p> </li> <li> <p><code>Container</code> holds the actual data of the population and peripheral tables as well as the train-test-validation split. It smoothly ensures the reproducibility of results. </p> </li> <li> <p><code>StarSchema</code> captures the relationships between population and peripheral tables and is our go-to data model abstraction.  In contrast, unlimited relationship complexity such as the snowflake schema can be modelled with <code>DataModel</code>.</p> </li> </ol> <p>To find the best set of aggregation functions and conditions, getML\u2019s supervised learning algorithms perform an iterative, tree-based search inside relational data. This allows for the automatic generation of complex features for a given target variable on a scale and accuracy that no manual or brute-force approach can match.</p> <ul> <li> </li> <li> </li> </ul>"},{"location":"#mission-of-getml","title":"Mission of getML","text":""},{"location":"#making-data-science-fun-again","title":"Making Data Science Fun Again:","text":"<ul> <li> No complex infrastructure</li> <li> No feature code</li> <li> No long waiting times</li> </ul>"},{"location":"#getting-started-is-easy","title":"Getting started is easy","text":"<ol> <li>Check out our user guide.</li> <li>Install getML Community <code>pip install getml</code>.</li> <li>Clone getml-demo and explore our example notebooks.</li> </ol> <p>Need help along the way? Here are our support options!</p>"},{"location":"#the-right-getml-flavor-for-your-ml-application","title":"The right getML flavor for your ML application","text":"<p>Both editions share the same  Python API.</p> <p>Request a meeting to explore the potential of getML Relational Learning for your business application. Or check out one of our code examples before.</p> <p>Talk to sales Get started</p>"},{"location":"#fa-secondaryopacity4-community-edition","title":"Community edition","text":"<p>For anyone who worked with Prophet, tsfresh or FeatureTools and is looking for a more memory and run-time efficient solution. getML Community is the leading open source implementation of the propositionalization framework.</p> <p>Get started </p>"},{"location":"#fa-secondaryopacity4-enterprise-edition","title":"Enterprise edition","text":"<p>This is your choice if shorter development cycles and unprecedented model accuracy provide a competitive edge to your business. getML Enterprise gives you access to the most advanced Relational Learning algorithms.</p> <p>Learn more </p>"},{"location":"#interested-in-getml-relational-learning","title":"Interested in getML Relational Learning?","text":""},{"location":"about/careers/","title":"Careers at getML","text":"<p>Are you ready to shape the future of predictive AI?</p> <p>If you\u2019re looking for somewhere you can learn quickly, make a meaningful impact in a fast-paced domain, and grow your career, you\u2019re in the right place.</p>"},{"location":"about/careers/#putting-people-first","title":"Putting People First","text":""},{"location":"about/careers/#benefit-package","title":"Benefit Package","text":"<p>\ud83c\udf74 Subsidized lunches at the office</p> <p>\ud83e\udd38 Sports membership (Urban Sports)</p> <p>\ud83d\ude8c Job ticket (Deutschland Ticket)</p> <p>\u2615 Free drinks &amp; coffee at the office</p>"},{"location":"about/careers/#life-family","title":"Life &amp; family","text":"<p>\ud83c\udfd6\ufe0f 30 days paid vacation</p> <p>\ud83d\udd52 Flexible working hours</p> <p>\ud83c\udfe2 Monthly team on-sites in Leipzig</p> <p>\ud83c\udfe1 Home office equipment</p>"},{"location":"about/careers/#values-that-guide-us","title":"Values that guide us","text":"<p>\ud83e\udd1d Trust each other and be respectful</p> <p>\ud83d\ude80 Innovate fearlessly</p> <p>\ud83d\udc51 Take ownership and be proactive</p> <p>\ud83d\udcda Keep learning</p>"},{"location":"about/careers/#our-open-positions","title":"Our open positions","text":""},{"location":"about/company/","title":"About","text":""},{"location":"about/company/#about_1","title":"About","text":"<p>Established in 2017 by Dr. Patrick Urbanke and Alexander Uhlig, getML is dedicated to transforming predictive AI for relational and time-series data. Our mission is to make advanced machine learning algorithms accessible, enhancing data scientists' productivity with automated feature engineering and improving model accuracy.</p> <p>Part of Code17 GmbH, our team of 10+ engineers in Leipzig and Munich is committed to delivering exceptional software solutions. From concept to deployment, we guide business leaders through the process, offering continuous support to ensure sustained performance. The unique getML framework, tailored for relational enterprise data and relying on reflect-cpp enables faster project completions and better outcomes.</p>"},{"location":"about/imprint/","title":"Imprint","text":"<p>Code17 GmbH  Philipp-Reis-Str. 11b  04179 Leipzig  \u200dhello@getml.com</p> <p>Managing Directors: Alexander Uhlig, Dr. Patrick Urbanke  Registration Court: Amtsgericht Leipzig  HRB 34030  VAT ID: DE313712523</p>"},{"location":"contact/","title":"Index","text":""},{"location":"contact/#how-can-we-help-you","title":"How can we help you?","text":"<p>Want to get in touch with us? Choose an option below and we\u2019ll be happy to show you how getML can transform your company\u2019s approach to machine learning on enterprise data.</p>"},{"location":"contact/#fa-secondaryopacity4-book-a-30-min-demo","title":"Book a 30-min demo","text":"<p>Learn how getML Relational Learning is working and details about successful applications in many cross-industry ML applications.</p> <p> Request a demo </p>"},{"location":"contact/#fa-secondaryopacity4-talk-to-engineering","title":"Talk to engineering","text":"<p>Need support in building ML enabled data products or service? Connect with our full-service team to discuss how we can help.</p> <p> Get in touch</p>"},{"location":"contact/#fa-secondaryopacity4-for-agencies","title":"For agencies","text":"<p>Have an agency? Partner with getML for technical or sales support or to discuss co-marketing opportunities.</p> <p> Tell us more</p>"},{"location":"contact/#from-the-engineers-of-getml","title":"From the engineers of getML","text":""},{"location":"contact/#support-options","title":"Support Options","text":"<p>We provide enterprise support packages for companies that rely on getML in production, ensuring 100% product coverage and timely responses.</p> <p>Request a demo or contact us for more information.</p>"},{"location":"contact/#free","title":"Free","text":""},{"location":"contact/#community-support","title":"Community Support","text":"<ul> <li> Support of getML Community edition</li> <li> No guaranteed response time</li> <li> GitHub Issue tracker</li> <li> GitHub Discussions</li> </ul> <p>Issue Tracker Discussions</p>"},{"location":"contact/#negotiable","title":"Negotiable","text":""},{"location":"contact/#enterprise-support","title":"Enterprise Support","text":"<ul> <li> Support of all getML products</li> <li> Guaranteed response time</li> <li> Private issue tracker and datasets</li> <li> Negotiable support volume</li> </ul> <p>Get in touch</p>"},{"location":"contact/#flexible-scope","title":"Flexible scope:","text":"<ul> <li> Deployment &amp; MLOps advice</li> <li> Hands-on data science support</li> <li> getML API training</li> </ul>"},{"location":"contact/message-us/","title":"Message us","text":""},{"location":"contact/message-us/#talk-to-an-expert","title":"Talk to an expert","text":"<p>Please let us know how we can help and our team will get back to you as soon as possible. You can also reach us directly via email at team@getml.com.</p>"},{"location":"enterprise/benefits/","title":"Why choose getML Enterprise?","text":"<p>ML teams working enterprise data, be it relational or time series, face a significant challenge. Building ML applications on top of traditional open-source ML libraries and algorithms takes months of work, and results in hard to maintain model code and feature store infrastructure. getML changes that.</p>"},{"location":"enterprise/benefits/#unprecedented-value-at-a-fraction-of-the-cost","title":"Unprecedented value at a fraction of the cost","text":"<p>getML Enterprise is your choice if your business application demands high model accuracies and short development cycles. Using getML, Data Science Teams will experience:</p> <ul> <li> <p>No Coding Overhead: getML Relational Learning eliminates the need for writing and maintaining extensive feature logic code, allowing your data science teams to accelerate development timelines and focus on business impact.</p> </li> <li> <p>Fast Time to Market: Streamline the MLOps process with getML Relational Learning reducing project timelines from months to days and cutting model training times from hours to minutes.</p> </li> <li> <p>Efficient Project Management: Minimize delays and simplify workflows by reducing the need for intensive collaboration between data scientists and domain experts, enhancing overall team efficiency.</p> </li> </ul>"},{"location":"enterprise/benefits/#an-edge-for-ml-driven-enterprises","title":"An Edge for ML-Driven Enterprises","text":"<p>getML Enterprise offers a Suite of cutting-edge ML algorithms that surpass both the getML Community edition and other propositionalization libraries. The unique capabilities of getML Enterprise algorithms provide several key advantages:</p> <ul> <li>More Accurate Models: Achieve double-digit accuracy gains across various use cases.</li> <li>Robust Against Feature Drift: Ensure consistent performance despite data pattern changes.</li> <li>Captures Smallest Nuances: Built for the complexity of real-world data where alternative solutions fail.</li> </ul>"},{"location":"enterprise/benefits/#talk-to-sales","title":"Talk to sales","text":"<p>Request a meeting to explore the potential of getML Relational Learning for your business application.</p> <p>Let's talk</p>"},{"location":"enterprise/book-demo/","title":"Book a Demo","text":""},{"location":"enterprise/book-demo/#free-demo-from-the-engineers-of-getml","title":"Free demo from the engineers of getML","text":"<p>Join us for a personalized walkthrough of getML.</p> <p>We will demonstrate getML\u2019s core functionality, touch on the benefits of our novel feature learning algorithms and answer all your questions.</p> <p>Book an appointment with our engineering team today.</p> <ul> <li> Discover if getML is the right fit for you</li> <li> No obligations, just valuable insights</li> </ul>"},{"location":"enterprise/feature-list/","title":"Feature List","text":""},{"location":"enterprise/feature-list/#community-vs-enterprise-edition","title":"Community vs Enterprise edition","text":"<p>Here are the highlights of the open-source getML Community edition and full-featured getML Enterprise edition:</p> Community edition Enterprise edition* License Elastic Licence v2 Proprietary Platform Linux &amp; Docker macOS Preprocessors EmailDomain, Imputation, Seasonal, Substring, TextFieldSplitter Mapping Feature learners FastProp Multirel, Relboost, RelMT, Fastboost Predictors LinearRegression, LogisticRegression, XGBoostClassifier, XGBoostRegressor ScaleGBMClassifier, ScaleGBMRegressor Productionization Transpilation to human-readable SQL Transpilation to MySQL, PostgreSQL, SQLite, Spark SQL, SAP HANA, BigQuery, TSQL. Built-in HTTP Endpoints. Hyperparameter optimization Not supported RandomSearch, LatinHypercube, GaussianOptimization, Customized tuning routines Database connectors SQLite, MySQL, MariaDB, PostgreSQL Greenplum, ODBC, SAP HANA, BigQuery Other data sources CSV, Parquet, Pandas, Arrow, Pyspark, JSON S3 Other functionalities Memory mapping Web Frontend <p>*The Enterprise edition contains all features of the Community edition plus the features listed in the column.</p>"},{"location":"enterprise/request-trial/","title":"getML Enterprise trial download","text":"<p>Test our most advanced algorithms first hand.</p> <p>We invite you to a free test of getML Enterprise. Experience the impact of your data and our most advanced algorithms for relational learning.</p> <p>This is best for data scientists and engineers that are already familiar with our API from using the getML Community edition.</p> <p>What you will get:</p> <ul> <li> Access to getML Enterprise binaries</li> <li> Chance to test drive relational learning</li> <li> Build any model for non-productive use (see terms)</li> </ul>"},{"location":"enterprise/trial/terms/","title":"trial terms","text":""},{"location":"examples/","title":"Examples","text":"<p>This section showcases practical demonstrations of getML's capabilities across various domains and use cases, including integrations with other tools and frameworks.</p>"},{"location":"examples/#community-edition-notebooks","title":"Community Edition Notebooks","text":"<p>A collection of Jupyter notebooks published on the getml-community repository, showcasing the features of the getML Community edition across various data types, prediction tasks, and application domains. These notebooks can be run locally or on Google Colab without any restrictions (certain features in the Enterprise edition require a license).</p>"},{"location":"examples/#fastprop-benchmarks","title":"FastProp Benchmarks","text":"<p>Showcase the performance of getML's FastProp algorithm, designed for efficient feature engineering and typically outperforming competing tools in runtime and resource requirements. These benchmarks help you understand how getML handles large datasets and complex feature engineering tasks.</p> <p>Key Examples of FastProp Benchmarks:</p> <p>Air Pollution Prediction: Demonstrates the superiority of FastProp over featuretools and tsfresh in runtime and predictive accuracy.</p> <p>Dodgers Traffic Volume Prediction: Showcases FastProp's handling of high-frequency time series data, outperforming Prophet and tsfresh.</p> <p>Interstate 94 Traffic Volume Prediction: Highlights FastProp's efficiency and predictive power compared to traditional methods.</p>"},{"location":"examples/#enterprise-edition-notebooks","title":"Enterprise Edition Notebooks","text":"<p>Published on the getml-demo repository, these notebooks demonstrate the advanced feature engineering capabilities available in the Enterprise edition, including sophisticated algorithms like Multirel, Relboost, and RelMT. They provide insights into achieving superior predictive performance on real-world data. Here are some interesting sections to get you started:</p> <p>Key Examples of Enterprise Features:</p> <p>AdventureWorks Customer Churn Prediction: Utilizes Multirel to predict customer churn, demonstrating the benefits of relational learning algorithms.</p> <p>Atherosclerosis Disease Lethality Prediction: Applies Relboost to complex medical datasets, showcasing its ability to manage high-dimensional data efficiently.</p> <p>Baseball Salary Prediction: Uses RelMT to predict baseball player salaries, demonstrating advanced feature learners in sports analytics.</p>"},{"location":"examples/#integrations","title":"Integrations","text":"<p>Demonstrations of how to connect getML with other tools and frameworks to enhance its functionality. Currently, we have an example showcasing integration with FastAPI, with more integrations coming soon.</p>"},{"location":"examples/#fastapi","title":"FastAPI","text":"<p>How to integrate getML with FastAPI to create a REST API for your machine learning models. This guide shows how to set up a generic prediction endpoint, making your getML pipelines accessible via web APIs.</p>"},{"location":"examples/#vertexai","title":"VertexAI","text":"<p>A step-by-step guide on how to deploy getML models on Google Cloud Vertex AI, enabling scalable and powerful machine learning workflows on Google Cloud Platform.</p>"},{"location":"examples/community-notebooks/","title":"Community Edition Notebooks","text":"<p>A collection of Jupyter notebooks that demonstrate the extensive features and  capabilities of the getML Community Edition. These notebooks are designed to cover a  wide range of data types, prediction tasks, difficulty levels, and application domains,  providing an excellent foundation for your own projects.</p> <p>Note</p> <p>Community Edition notebooks can be run locally or on Google Colab without restrictions,  while some features in the Enterprise Edition notebooks require a license for full  functionality.</p>","boost":0.9},{"location":"examples/community-notebooks/#overview","title":"Overview","text":"Notebook Prediction Type Population Size Data Type Target Domain Difficulty Comments adventure_works.ipynb Classification 19,704 Relational Churn Customer loyalty Hard Good reference for a complex data model formula1.ipynb Classification 31,578 Relational Win Sports Medium interstate94.ipynb Regression 24,096 Time Series Traffic Transportation Easy Good notebook to get started on time series loans.ipynb Classification 682 Relational Default Finance Easy Good notebook to get started on relational data robot.ipynb Regression 15,001 Time Series Force Robotics Medium seznam.ipynb Regression 1,462,078 Relational Volume E-commerce Medium","boost":0.9},{"location":"examples/community-notebooks/#source","title":"Source","text":"<p>These notebooks are published on the getml-community repository.</p>","boost":0.9},{"location":"examples/community-notebooks/adventure_works/","title":"<span class=\"ntitle\">adventure_works.ipynb</span> <span class=\"ndesc\">Predicting customer churn</span>","text":"<p>Let's get started with the analysis and set up your session:</p> In\u00a0[1]: Copied! <pre>%pip install -q \"getml==1.4.0\" \"numpy&lt;2.0.0\" \"pandas~=2.2\" \n\nimport numpy as np\nimport pandas as pd\n\nimport getml\n\ngetml.engine.launch()\ngetml.set_project('adventure_works')\n</pre> %pip install -q \"getml==1.4.0\" \"numpy&lt;2.0.0\" \"pandas~=2.2\"   import numpy as np import pandas as pd  import getml  getml.engine.launch() getml.set_project('adventure_works') <pre>Note: you may need to restart the kernel to use updated packages.\nLaunching ./getML --allow-push-notifications=true --allow-remote-ips=false --home-directory=/home/alex/.local/lib/python3.10/site-packages/getml --in-memory=true --install=false --launch-browser=true --log=false in /home/alex/.local/lib/python3.10/site-packages/getml/.getML/getml-1.4.0-x64-community-edition-linux...\nLaunched the getML engine. The log output will be stored in /home/alex/.getML/logs/20240807162542.log.\n\nConnected to project 'adventure_works'\n</pre> In\u00a0[2]: Copied! <pre>conn = getml.database.connect_mariadb(\n    host=\"db.relational-data.org\",\n    dbname=\"AdventureWorks2014\",\n    port=3306,\n    user=\"guest\",\n    password=\"relational\"\n)\n\nconn\n</pre> conn = getml.database.connect_mariadb(     host=\"db.relational-data.org\",     dbname=\"AdventureWorks2014\",     port=3306,     user=\"guest\",     password=\"relational\" )  conn Out[2]: <pre>Connection(dbname='AdventureWorks2014',\n           dialect='mysql',\n           host='db.relational-data.org',\n           port=3306)</pre> In\u00a0[3]: Copied! <pre>def load_if_needed(name):\n    \"\"\"\n    Loads the data from the relational learning\n    repository, if the data frame has not already\n    been loaded.\n    \"\"\"\n    if not getml.data.exists(name):\n        data_frame = getml.DataFrame.from_db(\n            name=name,\n            table_name=name,\n            conn=conn\n        )\n        data_frame.save()\n    else:\n        data_frame = getml.data.load_data_frame(name)\n    return data_frame\n</pre> def load_if_needed(name):     \"\"\"     Loads the data from the relational learning     repository, if the data frame has not already     been loaded.     \"\"\"     if not getml.data.exists(name):         data_frame = getml.DataFrame.from_db(             name=name,             table_name=name,             conn=conn         )         data_frame.save()     else:         data_frame = getml.data.load_data_frame(name)     return data_frame In\u00a0[4]: Copied! <pre>product = load_if_needed(\"Product\")\nsales_order_detail = load_if_needed(\"SalesOrderDetail\")\nsales_order_header = load_if_needed(\"SalesOrderHeader\")\nsales_order_reason = load_if_needed(\"SalesOrderHeaderSalesReason\")\nspecial_offer = load_if_needed(\"SpecialOffer\")\nstore = load_if_needed(\"Store\")\n</pre> product = load_if_needed(\"Product\") sales_order_detail = load_if_needed(\"SalesOrderDetail\") sales_order_header = load_if_needed(\"SalesOrderHeader\") sales_order_reason = load_if_needed(\"SalesOrderHeaderSalesReason\") special_offer = load_if_needed(\"SpecialOffer\") store = load_if_needed(\"Store\") In\u00a0[5]: Copied! <pre>product\n</pre> product Out[5]: name    ProductID     MakeFlag FinishedGoodsFlag SafetyStockLevel ReorderPoint DaysToManufacture ProductSubcategoryID ProductModelID Name                  ProductNumber Color         StandardCost  ListPrice     Size          SizeUnitMeasureCode WeightUnitMeasureCode Weight        ProductLine   Class         Style         SellStartDate       SellEndDate   DiscontinuedDate rowguid                          ModifiedDate        role unused_float unused_float      unused_float     unused_float unused_float      unused_float         unused_float   unused_float unused_string         unused_string unused_string unused_string unused_string unused_string unused_string       unused_string         unused_string unused_string unused_string unused_string unused_string       unused_string unused_string    unused_string                    unused_string       0 1 0 0 1000 750 0 nan nan Adjustable Race AR-5381 NULL 0.0000 0.0000 NULL NULL NULL NULL NULL NULL NULL 2008-04-30 00:00:00 NULL NULL 694215B7-08F7-4C0D-ACB1-D734BA44... 2014-02-08 09:01:36 1 2 0 0 1000 750 0 nan nan Bearing Ball BA-8327 NULL 0.0000 0.0000 NULL NULL NULL NULL NULL NULL NULL 2008-04-30 00:00:00 NULL NULL 58AE3C20-4F3A-4749-A7D4-D568806C... 2014-02-08 09:01:36 2 3 1 0 800 600 1 nan nan BB Ball Bearing BE-2349 NULL 0.0000 0.0000 NULL NULL NULL NULL NULL NULL NULL 2008-04-30 00:00:00 NULL NULL 9C21AED2-5BFA-4F18-BCB8-F11638DC... 2014-02-08 09:01:36 3 4 0 0 800 600 0 nan nan Headset Ball Bearings BE-2908 NULL 0.0000 0.0000 NULL NULL NULL NULL NULL NULL NULL 2008-04-30 00:00:00 NULL NULL ECFED6CB-51FF-49B5-B06C-7D8AC834... 2014-02-08 09:01:36 4 316 1 0 800 600 1 nan nan Blade BL-2036 NULL 0.0000 0.0000 NULL NULL NULL NULL NULL NULL NULL 2008-04-30 00:00:00 NULL NULL E73E9750-603B-4131-89F5-3DD15ED5... 2014-02-08 09:01:36 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 499 995 1 1 500 375 1 5 96 ML Bottom Bracket BB-8107 NULL 44.9506 101.2400 NULL NULL G 168.00 NULL M NULL 2013-05-30 00:00:00 NULL NULL 71AB847F-D091-42D6-B735-7B0C2D82... 2014-02-08 09:01:36 500 996 1 1 500 375 1 5 97 HL Bottom Bracket BB-9108 NULL 53.9416 121.4900 NULL NULL G 170.00 NULL H NULL 2013-05-30 00:00:00 NULL NULL 230C47C5-08B2-4CE3-B706-69C0BDD6... 2014-02-08 09:01:36 501 997 1 1 100 75 4 2 31 Road-750 Black, 44 BK-R19B-44 Black 343.6496 539.9900 44 CM LB 19.77 R L U 2013-05-30 00:00:00 NULL NULL 44CE4802-409F-43AB-9B27-CA534218... 2014-02-08 09:01:36 502 998 1 1 100 75 4 2 31 Road-750 Black, 48 BK-R19B-48 Black 343.6496 539.9900 48 CM LB 20.13 R L U 2013-05-30 00:00:00 NULL NULL 3DE9A212-1D49-40B6-B10A-F564D981... 2014-02-08 09:01:36 503 999 1 1 100 75 4 2 31 Road-750 Black, 52 BK-R19B-52 Black 343.6496 539.9900 52 CM LB 20.42 R L U 2013-05-30 00:00:00 NULL NULL AE638923-2B67-4679-B90E-ABBAB17D... 2014-02-08 09:01:36 <p>     504 rows x 25 columns     memory usage: 0.17 MB     name: Product     type: getml.DataFrame </p> In\u00a0[6]: Copied! <pre>sales_order_detail\n</pre> sales_order_detail Out[6]:   name SalesOrderID SalesOrderDetailID     OrderQty    ProductID SpecialOfferID CarrierTrackingNumber UnitPrice     UnitPriceDiscount LineTotal     rowguid                          ModifiedDate          role unused_float       unused_float unused_float unused_float   unused_float unused_string         unused_string unused_string     unused_string unused_string                    unused_string       0 43659 1 1 776 1 4911-403C-98 2024.9940 0.0000 2024.994000 B207C96D-D9E6-402B-8470-2CC176C4... 2011-05-30 22:00:00 1 43659 2 3 777 1 4911-403C-98 2024.9940 0.0000 6074.982000 7ABB600D-1E77-41BE-9FE5-B9142CFC... 2011-05-30 22:00:00 2 43659 3 1 778 1 4911-403C-98 2024.9940 0.0000 2024.994000 475CF8C6-49F6-486E-B0AD-AFC6A50C... 2011-05-30 22:00:00 3 43659 4 1 771 1 4911-403C-98 2039.9940 0.0000 2039.994000 04C4DE91-5815-45D6-8670-F462719F... 2011-05-30 22:00:00 4 43659 5 1 772 1 4911-403C-98 2039.9940 0.0000 2039.994000 5A74C7D2-E641-438E-A7AC-37BF2328... 2011-05-30 22:00:00 ... ... ... ... ... ... ... ... ... ... ... 121312 75122 121313 1 878 1 NULL 21.9800 0.0000 21.980000 8CAD6675-18CC-4F47-8287-97B41A8E... 2014-06-29 22:00:00 121313 75122 121314 1 712 1 NULL 8.9900 0.0000 8.990000 84F1C363-1C50-4442-BE16-541C59B6... 2014-06-29 22:00:00 121314 75123 121315 1 878 1 NULL 21.9800 0.0000 21.980000 C18B6476-429F-4BB1-828E-2BE5F82A... 2014-06-29 22:00:00 121315 75123 121316 1 879 1 NULL 159.0000 0.0000 159.000000 75A89C6A-C60A-47EA-8A52-B52A9C43... 2014-06-29 22:00:00 121316 75123 121317 1 712 1 NULL 8.9900 0.0000 8.990000 73646D26-0461-450D-8019-2C6C8586... 2014-06-29 22:00:00 <p>     121317 rows x 11 columns     memory usage: 21.60 MB     name: SalesOrderDetail     type: getml.DataFrame </p> In\u00a0[7]: Copied! <pre>sales_order_header\n</pre> sales_order_header Out[7]:  name SalesOrderID RevisionNumber       Status OnlineOrderFlag   CustomerID SalesPersonID  TerritoryID BillToAddressID ShipToAddressID ShipMethodID CreditCardID CurrencyRateID OrderDate           DueDate             ShipDate            SalesOrderNumber PurchaseOrderNumber AccountNumber  CreditCardApprovalCode SubTotal      TaxAmt        Freight       TotalDue      Comment       rowguid                          ModifiedDate         role unused_float   unused_float unused_float    unused_float unused_float  unused_float unused_float    unused_float    unused_float unused_float unused_float   unused_float unused_string       unused_string       unused_string       unused_string    unused_string       unused_string  unused_string          unused_string unused_string unused_string unused_string unused_string unused_string                    unused_string       0 43659 8 5 0 29825 279 5 985 985 5 16281 nan 2011-05-30 22:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 SO43659 PO522145787 10-4020-000676 105041Vi84182 20565.6206 1971.5149 616.0984 23153.2339 NULL 79B65321-39CA-4115-9CBA-8FE0903E... 2011-06-06 22:00:00 1 43660 8 5 0 29672 279 5 921 921 5 5618 nan 2011-05-30 22:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 SO43660 PO18850127500 10-4020-000117 115213Vi29411 1294.2529 124.2483 38.8276 1457.3288 NULL 738DC42D-D03B-48A1-9822-F95A67EA... 2011-06-06 22:00:00 2 43661 8 5 0 29734 282 6 517 517 5 1346 4 2011-05-30 22:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 SO43661 PO18473189620 10-4020-000442 85274Vi6854 32726.4786 3153.7696 985.5530 36865.8012 NULL D91B9131-18A4-4A11-BC3A-90B6F53E... 2011-06-06 22:00:00 3 43662 8 5 0 29994 282 6 482 482 5 10456 4 2011-05-30 22:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 SO43662 PO18444174044 10-4020-000227 125295Vi53935 28832.5289 2775.1646 867.2389 32474.9324 NULL 4A1ECFC0-CC3A-4740-B028-1C50BB48... 2011-06-06 22:00:00 4 43663 8 5 0 29565 276 4 1073 1073 5 4322 nan 2011-05-30 22:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 SO43663 PO18009186470 10-4020-000510 45303Vi22691 419.4589 40.2681 12.5838 472.3108 NULL 9B1E7A40-6AE0-4AD3-811C-A6495185... 2011-06-06 22:00:00 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 31460 75119 8 5 1 11981 nan 1 17649 17649 1 6761 nan 2014-06-29 22:00:00 2014-07-12 00:00:00 2014-07-07 00:00:00 SO75119 NULL 10-4030-011981 429826Vi35166 42.2800 3.3824 1.0570 46.7194 NULL 9382F1C9-383A-435F-9449-0EECEA21... 2014-07-06 22:00:00 31461 75120 8 5 1 18749 nan 6 28374 28374 1 8925 nan 2014-06-29 22:00:00 2014-07-12 00:00:00 2014-07-07 00:00:00 SO75120 NULL 10-4030-018749 929849Vi46003 84.9600 6.7968 2.1240 93.8808 NULL AE6A4FCF-FF73-4CD4-AF2C-5993D00D... 2014-07-06 22:00:00 31462 75121 8 5 1 15251 nan 6 26553 26553 1 14220 nan 2014-06-29 22:00:00 2014-07-12 00:00:00 2014-07-07 00:00:00 SO75121 NULL 10-4030-015251 529864Vi73738 74.9800 5.9984 1.8745 82.8529 NULL D7395C0E-00CB-4BFA-A238-0D6A9F49... 2014-07-06 22:00:00 31463 75122 8 5 1 15868 nan 6 14616 14616 1 18719 nan 2014-06-29 22:00:00 2014-07-12 00:00:00 2014-07-07 00:00:00 SO75122 NULL 10-4030-015868 330022Vi97312 30.9700 2.4776 0.7743 34.2219 NULL 4221035A-4159-492F-AF40-4363A64F... 2014-07-06 22:00:00 31464 75123 8 5 1 18759 nan 6 14024 14024 1 10084 nan 2014-06-29 22:00:00 2014-07-12 00:00:00 2014-07-07 00:00:00 SO75123 NULL 10-4030-018759 230370Vi51970 189.9700 15.1976 4.7493 209.9169 NULL D54752FF-2B54-4BE5-95EA-3B72289C... 2014-07-06 22:00:00 <p>     31465 rows x 26 columns     memory usage: 12.56 MB     name: SalesOrderHeader     type: getml.DataFrame </p> In\u00a0[8]: Copied! <pre>sales_order_reason\n</pre> sales_order_reason Out[8]:  name SalesOrderID SalesReasonID ModifiedDate         role unused_float  unused_float unused_string       0 43697 5 2011-05-30 22:00:00 1 43697 9 2011-05-30 22:00:00 2 43702 5 2011-05-31 22:00:00 3 43702 9 2011-05-31 22:00:00 4 43703 5 2011-05-31 22:00:00 ... ... ... 27642 75119 1 2014-06-29 22:00:00 27643 75120 1 2014-06-29 22:00:00 27644 75121 1 2014-06-29 22:00:00 27645 75122 1 2014-06-29 22:00:00 27646 75123 1 2014-06-29 22:00:00 <p>     27647 rows x 3 columns     memory usage: 1.22 MB     name: SalesOrderHeaderSalesReason     type: getml.DataFrame </p> In\u00a0[9]: Copied! <pre>special_offer\n</pre> special_offer Out[9]: name SpecialOfferID       MinQty       MaxQty Description                      DiscountPct   Type                 Category      StartDate           EndDate             rowguid                          ModifiedDate        role   unused_float unused_float unused_float unused_string                    unused_string unused_string        unused_string unused_string       unused_string       unused_string                    unused_string       0 1 0 nan No Discount 0.0000 No Discount No Discount 2011-05-01 00:00:00 2014-11-30 00:00:00 0290C4F5-191F-4337-AB6B-0A2DDE03... 2011-03-31 22:00:00 1 2 11 14 Volume Discount 11 to 14 0.0200 Volume Discount Reseller 2011-05-31 00:00:00 2014-05-30 00:00:00 D7542EE7-15DB-4541-985C-5CC27AEF... 2011-04-30 22:00:00 2 3 15 24 Volume Discount 15 to 24 0.0500 Volume Discount Reseller 2011-05-31 00:00:00 2014-05-30 00:00:00 4BDBCC01-8CF7-40A9-B643-40EC5B71... 2011-04-30 22:00:00 3 4 25 40 Volume Discount 25 to 40 0.1000 Volume Discount Reseller 2011-05-31 00:00:00 2014-05-30 00:00:00 504B5E85-8F3F-4EBC-9E1D-C1BC5DEA... 2011-04-30 22:00:00 4 5 41 60 Volume Discount 41 to 60 0.1500 Volume Discount Reseller 2011-05-31 00:00:00 2014-05-30 00:00:00 677E1D9D-944F-4E81-90E8-47EB0A82... 2011-04-30 22:00:00 ... ... ... ... ... ... ... ... ... ... ... 11 12 0 nan LL Road Frame Sale 0.3500 Excess Inventory Reseller 2013-05-30 00:00:00 2013-07-14 00:00:00 C0AF1C89-9722-4235-9248-3FBA4D9E... 2013-04-29 22:00:00 12 13 0 nan Touring-3000 Promotion 0.1500 New Product Reseller 2013-05-30 00:00:00 2013-08-29 00:00:00 5061CCE4-E021-45A8-9A75-DFB36CBB... 2013-04-29 22:00:00 13 14 0 nan Touring-1000 Promotion 0.2000 New Product Reseller 2013-05-30 00:00:00 2013-08-29 00:00:00 1AF84A9E-A98C-4BD9-B48F-DC2B8B6B... 2013-04-29 22:00:00 14 15 0 nan Half-Price Pedal Sale 0.5000 Seasonal Discount Customer 2013-07-14 00:00:00 2013-08-14 00:00:00 03E3594D-6EBB-46A6-B8EE-A9289C0C... 2013-06-13 22:00:00 15 16 0 nan Mountain-500 Silver Clearance Sa... 0.4000 Discontinued Product Reseller 2014-03-31 00:00:00 2014-05-30 00:00:00 EB7CB484-BCCF-4D2D-BF73-521B2001... 2014-02-28 23:00:00 <p>     16 rows x 11 columns     memory usage: 0.00 MB     name: SpecialOffer     type: getml.DataFrame </p> In\u00a0[10]: Copied! <pre>store\n</pre> store Out[10]: name BusinessEntityID SalesPersonID Name                           Demographics                     rowguid                          ModifiedDate        role     unused_float  unused_float unused_string                  unused_string                    unused_string                    unused_string       0 292 279 Next-Door Bike Store &lt;StoreSurvey xmlns=\"http://schem... A22517E3-848D-4EBE-B9D9-7437F343... 2014-09-12 09:15:07 1 294 276 Professional Sales and Service &lt;StoreSurvey xmlns=\"http://schem... B50CA50B-C601-4A13-B07E-2C63862D... 2014-09-12 09:15:07 2 296 277 Riders Company &lt;StoreSurvey xmlns=\"http://schem... 337C3688-1339-4E1A-A08A-B54B2356... 2014-09-12 09:15:07 3 298 275 The Bike Mechanics &lt;StoreSurvey xmlns=\"http://schem... 7894F278-F0C8-4D16-BD75-213FDBF1... 2014-09-12 09:15:07 4 300 286 Nationwide Supply &lt;StoreSurvey xmlns=\"http://schem... C3FC9705-A8C4-4F3A-9550-EB2FA4B7... 2014-09-12 09:15:07 ... ... ... ... ... ... 696 1988 282 Retreat Inn &lt;StoreSurvey xmlns=\"http://schem... EA21EC81-1BFA-4A07-9B4D-73D9852A... 2014-09-12 09:15:07 697 1990 281 Technical Parts Manufacturing &lt;StoreSurvey xmlns=\"http://schem... C8E3C4ED-8F58-4DB2-B600-E0CD11D9... 2014-09-12 09:15:07 698 1992 277 Totes &amp; Baskets Company &lt;StoreSurvey xmlns=\"http://schem... CE860B58-643C-4567-BFD8-06E97969... 2014-09-12 09:15:07 699 1994 277 World of Bikes &lt;StoreSurvey xmlns=\"http://schem... 0C10F2B6-A13A-440C-9C25-5B28D482... 2014-09-12 09:15:07 700 2051 275 A Bicycle Association &lt;StoreSurvey xmlns=\"http://schem... 82237172-D3FE-4A95-82EF-636F6552... 2014-09-12 09:15:07 <p>     701 rows x 6 columns     memory usage: 0.38 MB     name: Store     type: getml.DataFrame </p> <p>getML requires that we define roles for each of the columns.</p> In\u00a0[11]: Copied! <pre>product.set_role(\"ProductID\", getml.data.roles.join_key)\nproduct.set_role([\"MakeFlag\", \"ProductSubcategoryID\", \"ProductModelID\"], getml.data.roles.categorical)\nproduct.set_role([\"SafetyStockLevel\", \"ReorderPoint\", \"StandardCost\", \"ListPrice\"], getml.data.roles.numerical)\n\nproduct\n</pre> product.set_role(\"ProductID\", getml.data.roles.join_key) product.set_role([\"MakeFlag\", \"ProductSubcategoryID\", \"ProductModelID\"], getml.data.roles.categorical) product.set_role([\"SafetyStockLevel\", \"ReorderPoint\", \"StandardCost\", \"ListPrice\"], getml.data.roles.numerical)  product Out[11]: name ProductID MakeFlag    ProductSubcategoryID ProductModelID SafetyStockLevel ReorderPoint StandardCost ListPrice FinishedGoodsFlag DaysToManufacture Name                  ProductNumber Color         Size          SizeUnitMeasureCode WeightUnitMeasureCode Weight        ProductLine   Class         Style         SellStartDate       SellEndDate   DiscontinuedDate rowguid                          ModifiedDate        role  join_key categorical categorical          categorical           numerical    numerical    numerical numerical      unused_float      unused_float unused_string         unused_string unused_string unused_string unused_string       unused_string         unused_string unused_string unused_string unused_string unused_string       unused_string unused_string    unused_string                    unused_string       0 1 0 nan nan 1000 750 0 0 0 0 Adjustable Race AR-5381 NULL NULL NULL NULL NULL NULL NULL NULL 2008-04-30 00:00:00 NULL NULL 694215B7-08F7-4C0D-ACB1-D734BA44... 2014-02-08 09:01:36 1 2 0 nan nan 1000 750 0 0 0 0 Bearing Ball BA-8327 NULL NULL NULL NULL NULL NULL NULL NULL 2008-04-30 00:00:00 NULL NULL 58AE3C20-4F3A-4749-A7D4-D568806C... 2014-02-08 09:01:36 2 3 1 nan nan 800 600 0 0 0 1 BB Ball Bearing BE-2349 NULL NULL NULL NULL NULL NULL NULL NULL 2008-04-30 00:00:00 NULL NULL 9C21AED2-5BFA-4F18-BCB8-F11638DC... 2014-02-08 09:01:36 3 4 0 nan nan 800 600 0 0 0 0 Headset Ball Bearings BE-2908 NULL NULL NULL NULL NULL NULL NULL NULL 2008-04-30 00:00:00 NULL NULL ECFED6CB-51FF-49B5-B06C-7D8AC834... 2014-02-08 09:01:36 4 316 1 nan nan 800 600 0 0 0 1 Blade BL-2036 NULL NULL NULL NULL NULL NULL NULL NULL 2008-04-30 00:00:00 NULL NULL E73E9750-603B-4131-89F5-3DD15ED5... 2014-02-08 09:01:36 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 499 995 1 5 96 500 375 44.9506 101.24 1 1 ML Bottom Bracket BB-8107 NULL NULL NULL G 168.00 NULL M NULL 2013-05-30 00:00:00 NULL NULL 71AB847F-D091-42D6-B735-7B0C2D82... 2014-02-08 09:01:36 500 996 1 5 97 500 375 53.9416 121.49 1 1 HL Bottom Bracket BB-9108 NULL NULL NULL G 170.00 NULL H NULL 2013-05-30 00:00:00 NULL NULL 230C47C5-08B2-4CE3-B706-69C0BDD6... 2014-02-08 09:01:36 501 997 1 2 31 100 75 343.6496 539.99 1 4 Road-750 Black, 44 BK-R19B-44 Black 44 CM LB 19.77 R L U 2013-05-30 00:00:00 NULL NULL 44CE4802-409F-43AB-9B27-CA534218... 2014-02-08 09:01:36 502 998 1 2 31 100 75 343.6496 539.99 1 4 Road-750 Black, 48 BK-R19B-48 Black 48 CM LB 20.13 R L U 2013-05-30 00:00:00 NULL NULL 3DE9A212-1D49-40B6-B10A-F564D981... 2014-02-08 09:01:36 503 999 1 2 31 100 75 343.6496 539.99 1 4 Road-750 Black, 52 BK-R19B-52 Black 52 CM LB 20.42 R L U 2013-05-30 00:00:00 NULL NULL AE638923-2B67-4679-B90E-ABBAB17D... 2014-02-08 09:01:36 <p>     504 rows x 25 columns     memory usage: 0.16 MB     name: Product     type: getml.DataFrame </p> In\u00a0[12]: Copied! <pre>sales_order_detail.set_role([\"SalesOrderID\", \"SalesOrderDetailID\", \"ProductID\", \"SpecialOfferID\"], getml.data.roles.join_key)\nsales_order_detail.set_role([\"OrderQty\", \"UnitPrice\", \"UnitPriceDiscount\", \"LineTotal\"], getml.data.roles.numerical)\nsales_order_detail.set_role(\"ModifiedDate\", getml.data.roles.time_stamp)\n\nsales_order_detail\n</pre> sales_order_detail.set_role([\"SalesOrderID\", \"SalesOrderDetailID\", \"ProductID\", \"SpecialOfferID\"], getml.data.roles.join_key) sales_order_detail.set_role([\"OrderQty\", \"UnitPrice\", \"UnitPriceDiscount\", \"LineTotal\"], getml.data.roles.numerical) sales_order_detail.set_role(\"ModifiedDate\", getml.data.roles.time_stamp)  sales_order_detail Out[12]:   name                ModifiedDate SalesOrderID SalesOrderDetailID ProductID SpecialOfferID  OrderQty UnitPrice UnitPriceDiscount LineTotal CarrierTrackingNumber rowguid                            role                  time_stamp     join_key           join_key  join_key       join_key numerical numerical         numerical numerical unused_string         unused_string                      unit time stamp, comparison only 0 2011-05-30 22:00:00 43659 1 776 1 1 2024.994 0 2024.994 4911-403C-98 B207C96D-D9E6-402B-8470-2CC176C4... 1 2011-05-30 22:00:00 43659 2 777 1 3 2024.994 0 6074.982 4911-403C-98 7ABB600D-1E77-41BE-9FE5-B9142CFC... 2 2011-05-30 22:00:00 43659 3 778 1 1 2024.994 0 2024.994 4911-403C-98 475CF8C6-49F6-486E-B0AD-AFC6A50C... 3 2011-05-30 22:00:00 43659 4 771 1 1 2039.994 0 2039.994 4911-403C-98 04C4DE91-5815-45D6-8670-F462719F... 4 2011-05-30 22:00:00 43659 5 772 1 1 2039.994 0 2039.994 4911-403C-98 5A74C7D2-E641-438E-A7AC-37BF2328... ... ... ... ... ... ... ... ... ... ... ... 121312 2014-06-29 22:00:00 75122 121313 878 1 1 21.98 0 21.98 NULL 8CAD6675-18CC-4F47-8287-97B41A8E... 121313 2014-06-29 22:00:00 75122 121314 712 1 1 8.99 0 8.99 NULL 84F1C363-1C50-4442-BE16-541C59B6... 121314 2014-06-29 22:00:00 75123 121315 878 1 1 21.98 0 21.98 NULL C18B6476-429F-4BB1-828E-2BE5F82A... 121315 2014-06-29 22:00:00 75123 121316 879 1 1 159 0 159 NULL 75A89C6A-C60A-47EA-8A52-B52A9C43... 121316 2014-06-29 22:00:00 75123 121317 712 1 1 8.99 0 8.99 NULL 73646D26-0461-450D-8019-2C6C8586... <p>     121317 rows x 11 columns     memory usage: 14.08 MB     name: SalesOrderDetail     type: getml.DataFrame </p> In\u00a0[13]: Copied! <pre>sales_order_reason.set_role(\"SalesOrderID\", getml.data.roles.join_key)\nsales_order_reason.set_role(\"SalesReasonID\", getml.data.roles.categorical)\n\nsales_order_reason\n</pre> sales_order_reason.set_role(\"SalesOrderID\", getml.data.roles.join_key) sales_order_reason.set_role(\"SalesReasonID\", getml.data.roles.categorical)  sales_order_reason Out[13]:  name SalesOrderID SalesReasonID ModifiedDate         role     join_key categorical   unused_string       0 43697 5 2011-05-30 22:00:00 1 43697 9 2011-05-30 22:00:00 2 43702 5 2011-05-31 22:00:00 3 43702 9 2011-05-31 22:00:00 4 43703 5 2011-05-31 22:00:00 ... ... ... 27642 75119 1 2014-06-29 22:00:00 27643 75120 1 2014-06-29 22:00:00 27644 75121 1 2014-06-29 22:00:00 27645 75122 1 2014-06-29 22:00:00 27646 75123 1 2014-06-29 22:00:00 <p>     27647 rows x 3 columns     memory usage: 1.00 MB     name: SalesOrderHeaderSalesReason     type: getml.DataFrame </p> In\u00a0[14]: Copied! <pre>special_offer.set_role([\"SpecialOfferID\"], getml.data.roles.join_key)\nspecial_offer.set_role([\"MinQty\", \"DiscountPct\"], getml.data.roles.numerical)\nspecial_offer.set_role([\"Category\", \"Description\", \"Type\"], getml.data.roles.categorical)\nspecial_offer.set_role([\"StartDate\", \"EndDate\"], getml.data.roles.time_stamp)\n\nspecial_offer\n</pre> special_offer.set_role([\"SpecialOfferID\"], getml.data.roles.join_key) special_offer.set_role([\"MinQty\", \"DiscountPct\"], getml.data.roles.numerical) special_offer.set_role([\"Category\", \"Description\", \"Type\"], getml.data.roles.categorical) special_offer.set_role([\"StartDate\", \"EndDate\"], getml.data.roles.time_stamp)  special_offer Out[14]: name                   StartDate                     EndDate SpecialOfferID Category    Description                      Type                    MinQty DiscountPct       MaxQty rowguid                          ModifiedDate        role                  time_stamp                  time_stamp       join_key categorical categorical                      categorical          numerical   numerical unused_float unused_string                    unused_string       unit time stamp, comparison only time stamp, comparison only 0 2011-05-01 2014-11-30 1 No Discount No Discount No Discount 0 0 nan 0290C4F5-191F-4337-AB6B-0A2DDE03... 2011-03-31 22:00:00 1 2011-05-31 2014-05-30 2 Reseller Volume Discount 11 to 14 Volume Discount 11 0.02 14 D7542EE7-15DB-4541-985C-5CC27AEF... 2011-04-30 22:00:00 2 2011-05-31 2014-05-30 3 Reseller Volume Discount 15 to 24 Volume Discount 15 0.05 24 4BDBCC01-8CF7-40A9-B643-40EC5B71... 2011-04-30 22:00:00 3 2011-05-31 2014-05-30 4 Reseller Volume Discount 25 to 40 Volume Discount 25 0.1 40 504B5E85-8F3F-4EBC-9E1D-C1BC5DEA... 2011-04-30 22:00:00 4 2011-05-31 2014-05-30 5 Reseller Volume Discount 41 to 60 Volume Discount 41 0.15 60 677E1D9D-944F-4E81-90E8-47EB0A82... 2011-04-30 22:00:00 ... ... ... ... ... ... ... ... ... ... ... 11 2013-05-30 2013-07-14 12 Reseller LL Road Frame Sale Excess Inventory 0 0.35 nan C0AF1C89-9722-4235-9248-3FBA4D9E... 2013-04-29 22:00:00 12 2013-05-30 2013-08-29 13 Reseller Touring-3000 Promotion New Product 0 0.15 nan 5061CCE4-E021-45A8-9A75-DFB36CBB... 2013-04-29 22:00:00 13 2013-05-30 2013-08-29 14 Reseller Touring-1000 Promotion New Product 0 0.2 nan 1AF84A9E-A98C-4BD9-B48F-DC2B8B6B... 2013-04-29 22:00:00 14 2013-07-14 2013-08-14 15 Customer Half-Price Pedal Sale Seasonal Discount 0 0.5 nan 03E3594D-6EBB-46A6-B8EE-A9289C0C... 2013-06-13 22:00:00 15 2014-03-31 2014-05-30 16 Reseller Mountain-500 Silver Clearance Sa... Discontinued Product 0 0.4 nan EB7CB484-BCCF-4D2D-BF73-521B2001... 2014-02-28 23:00:00 <p>     16 rows x 11 columns     memory usage: 0.00 MB     name: SpecialOffer     type: getml.DataFrame </p> In\u00a0[15]: Copied! <pre>store.set_role([\"SalesPersonID\"], getml.data.roles.join_key)\n\nstore[\"test\"] = store[\"ModifiedDate\"].update(getml.data.random() &gt; 0.5, \"NULL\")\n\nstore.set_role([\"SalesPersonID\"], getml.data.roles.join_key)\nstore.set_role([\"test\"], getml.data.roles.time_stamp)\n\nstore\n</pre> store.set_role([\"SalesPersonID\"], getml.data.roles.join_key)  store[\"test\"] = store[\"ModifiedDate\"].update(getml.data.random() &gt; 0.5, \"NULL\")  store.set_role([\"SalesPersonID\"], getml.data.roles.join_key) store.set_role([\"test\"], getml.data.roles.time_stamp)  store Out[15]: name                        test SalesPersonID BusinessEntityID Name                           Demographics                     rowguid                          ModifiedDate        role                  time_stamp      join_key     unused_float unused_string                  unused_string                    unused_string                    unused_string       unit time stamp, comparison only 0 2014-09-12 09:15:07 279 292 Next-Door Bike Store &lt;StoreSurvey xmlns=\"http://schem... A22517E3-848D-4EBE-B9D9-7437F343... 2014-09-12 09:15:07 1 NULL 276 294 Professional Sales and Service &lt;StoreSurvey xmlns=\"http://schem... B50CA50B-C601-4A13-B07E-2C63862D... 2014-09-12 09:15:07 2 2014-09-12 09:15:07 277 296 Riders Company &lt;StoreSurvey xmlns=\"http://schem... 337C3688-1339-4E1A-A08A-B54B2356... 2014-09-12 09:15:07 3 NULL 275 298 The Bike Mechanics &lt;StoreSurvey xmlns=\"http://schem... 7894F278-F0C8-4D16-BD75-213FDBF1... 2014-09-12 09:15:07 4 NULL 286 300 Nationwide Supply &lt;StoreSurvey xmlns=\"http://schem... C3FC9705-A8C4-4F3A-9550-EB2FA4B7... 2014-09-12 09:15:07 ... ... ... ... ... ... ... 696 NULL 282 1988 Retreat Inn &lt;StoreSurvey xmlns=\"http://schem... EA21EC81-1BFA-4A07-9B4D-73D9852A... 2014-09-12 09:15:07 697 2014-09-12 09:15:07 281 1990 Technical Parts Manufacturing &lt;StoreSurvey xmlns=\"http://schem... C8E3C4ED-8F58-4DB2-B600-E0CD11D9... 2014-09-12 09:15:07 698 NULL 277 1992 Totes &amp; Baskets Company &lt;StoreSurvey xmlns=\"http://schem... CE860B58-643C-4567-BFD8-06E97969... 2014-09-12 09:15:07 699 NULL 277 1994 World of Bikes &lt;StoreSurvey xmlns=\"http://schem... 0C10F2B6-A13A-440C-9C25-5B28D482... 2014-09-12 09:15:07 700 2014-09-12 09:15:07 275 2051 A Bicycle Association &lt;StoreSurvey xmlns=\"http://schem... 82237172-D3FE-4A95-82EF-636F6552... 2014-09-12 09:15:07 <p>     701 rows x 7 columns     memory usage: 0.38 MB     name: Store     type: getml.DataFrame </p> In\u00a0[16]: Copied! <pre>sales_order_header[\"SalesPersonIDCat\"] = sales_order_header[\"SalesPersonID\"]\nsales_order_header[\"TerritoryIDCat\"] = sales_order_header[\"TerritoryID\"]\n\nsales_order_header.set_role([\"CustomerID\", \"SalesOrderID\", \"SalesPersonID\", \"TerritoryID\"], getml.data.roles.join_key)\nsales_order_header.set_role(\n    [\"RevisionNumber\", \"OnlineOrderFlag\", \"SalesPersonIDCat\", \"TerritoryIDCat\", \"ShipMethodID\"], \n    getml.data.roles.categorical)\nsales_order_header.set_role([\"SubTotal\", \"TaxAmt\", \"Freight\", \"TotalDue\"], getml.data.roles.numerical)\nsales_order_header.set_role([\"OrderDate\", \"DueDate\", \"ShipDate\", \"ModifiedDate\"], getml.data.roles.time_stamp)\n\nsales_order_header\n</pre> sales_order_header[\"SalesPersonIDCat\"] = sales_order_header[\"SalesPersonID\"] sales_order_header[\"TerritoryIDCat\"] = sales_order_header[\"TerritoryID\"]  sales_order_header.set_role([\"CustomerID\", \"SalesOrderID\", \"SalesPersonID\", \"TerritoryID\"], getml.data.roles.join_key) sales_order_header.set_role(     [\"RevisionNumber\", \"OnlineOrderFlag\", \"SalesPersonIDCat\", \"TerritoryIDCat\", \"ShipMethodID\"],      getml.data.roles.categorical) sales_order_header.set_role([\"SubTotal\", \"TaxAmt\", \"Freight\", \"TotalDue\"], getml.data.roles.numerical) sales_order_header.set_role([\"OrderDate\", \"DueDate\", \"ShipDate\", \"ModifiedDate\"], getml.data.roles.time_stamp)  sales_order_header Out[16]:  name                   OrderDate                     DueDate                    ShipDate                ModifiedDate CustomerID SalesOrderID SalesPersonID TerritoryID RevisionNumber OnlineOrderFlag SalesPersonIDCat TerritoryIDCat ShipMethodID   SubTotal    TaxAmt   Freight   TotalDue       Status BillToAddressID ShipToAddressID CreditCardID CurrencyRateID SalesOrderNumber PurchaseOrderNumber AccountNumber  CreditCardApprovalCode Comment       rowguid                           role                  time_stamp                  time_stamp                  time_stamp                  time_stamp   join_key     join_key      join_key    join_key categorical    categorical     categorical      categorical    categorical   numerical numerical numerical  numerical unused_float    unused_float    unused_float unused_float   unused_float unused_string    unused_string       unused_string  unused_string          unused_string unused_string                     unit time stamp, comparison only time stamp, comparison only time stamp, comparison only time stamp, comparison only 0 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 29825 43659 279 5 8 0 279 5 5 20565.6206 1971.5149 616.0984 23153.2339 5 985 985 16281 nan SO43659 PO522145787 10-4020-000676 105041Vi84182 NULL 79B65321-39CA-4115-9CBA-8FE0903E... 1 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 29672 43660 279 5 8 0 279 5 5 1294.2529 124.2483 38.8276 1457.3288 5 921 921 5618 nan SO43660 PO18850127500 10-4020-000117 115213Vi29411 NULL 738DC42D-D03B-48A1-9822-F95A67EA... 2 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 29734 43661 282 6 8 0 282 6 5 32726.4786 3153.7696 985.553 36865.8012 5 517 517 1346 4 SO43661 PO18473189620 10-4020-000442 85274Vi6854 NULL D91B9131-18A4-4A11-BC3A-90B6F53E... 3 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 29994 43662 282 6 8 0 282 6 5 28832.5289 2775.1646 867.2389 32474.9324 5 482 482 10456 4 SO43662 PO18444174044 10-4020-000227 125295Vi53935 NULL 4A1ECFC0-CC3A-4740-B028-1C50BB48... 4 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 29565 43663 276 4 8 0 276 4 5 419.4589 40.2681 12.5838 472.3108 5 1073 1073 4322 nan SO43663 PO18009186470 10-4020-000510 45303Vi22691 NULL 9B1E7A40-6AE0-4AD3-811C-A6495185... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 31460 2014-06-29 22:00:00 2014-07-12 2014-07-07 2014-07-06 22:00:00 11981 75119 nan 1 8 1 nan 1 1 42.28 3.3824 1.057 46.7194 5 17649 17649 6761 nan SO75119 NULL 10-4030-011981 429826Vi35166 NULL 9382F1C9-383A-435F-9449-0EECEA21... 31461 2014-06-29 22:00:00 2014-07-12 2014-07-07 2014-07-06 22:00:00 18749 75120 nan 6 8 1 nan 6 1 84.96 6.7968 2.124 93.8808 5 28374 28374 8925 nan SO75120 NULL 10-4030-018749 929849Vi46003 NULL AE6A4FCF-FF73-4CD4-AF2C-5993D00D... 31462 2014-06-29 22:00:00 2014-07-12 2014-07-07 2014-07-06 22:00:00 15251 75121 nan 6 8 1 nan 6 1 74.98 5.9984 1.8745 82.8529 5 26553 26553 14220 nan SO75121 NULL 10-4030-015251 529864Vi73738 NULL D7395C0E-00CB-4BFA-A238-0D6A9F49... 31463 2014-06-29 22:00:00 2014-07-12 2014-07-07 2014-07-06 22:00:00 15868 75122 nan 6 8 1 nan 6 1 30.97 2.4776 0.7743 34.2219 5 14616 14616 18719 nan SO75122 NULL 10-4030-015868 330022Vi97312 NULL 4221035A-4159-492F-AF40-4363A64F... 31464 2014-06-29 22:00:00 2014-07-12 2014-07-07 2014-07-06 22:00:00 18759 75123 nan 6 8 1 nan 6 1 189.97 15.1976 4.7493 209.9169 5 14024 14024 10084 nan SO75123 NULL 10-4030-018759 230370Vi51970 NULL D54752FF-2B54-4BE5-95EA-3B72289C... <p>     31465 rows x 28 columns     memory usage: 8.34 MB     name: SalesOrderHeader     type: getml.DataFrame </p> <p>We must also define customer churn. In this case, we define customer churn as a customer not making another purchase within 180 days of his or her last purchase.</p> <p>Thus, the churn variable is defined as follows:</p> <ul> <li>0, if another purchase by the same customer has been made within 180 days after <code>OrderDate</code></li> <li>1, if no purchase by the same customer has been made within 180 days after <code>OrderDate</code></li> <li>NULL, if <code>max(OrderDate) - OrderDate &lt;= 180 days</code></li> </ul> <p>NULL targets can not be used in our analysis.</p> In\u00a0[17]: Copied! <pre>sales_order_header_pd = sales_order_header[[\"OrderDate\", \"CustomerID\", \"SalesOrderID\"]].to_pandas()\n\nrepeat_purchases = sales_order_header_pd.merge(\n    sales_order_header_pd[[\"OrderDate\", \"CustomerID\"]],\n    on=\"CustomerID\",\n    how=\"left\",\n)\n\nrepeat_purchases = repeat_purchases[\n    repeat_purchases[\"OrderDate_y\"] &gt; repeat_purchases[\"OrderDate_x\"]\n]\n\nrepeat_purchases = repeat_purchases[\n    repeat_purchases[\"OrderDate_y\"] - repeat_purchases[\"OrderDate_x\"] &gt; pd.Timedelta('180 days')\n]\n\nrepeat_purchases = repeat_purchases.groupby(\"SalesOrderID\", as_index=False).aggregate({\"CustomerID\": \"max\"})\n\nrepeat_purchase_ids = {sid: True for sid in repeat_purchases[\"SalesOrderID\"]}\n\ncut_off_date = max(sales_order_header_pd[\"OrderDate\"]) - pd.Timedelta('180 days')\n\nchurn = np.asarray([\n    np.nan if order_date &gt;= cut_off_date else 0 if order_id in repeat_purchase_ids else 1 \n    for (order_date, order_id) in zip(sales_order_header_pd[\"OrderDate\"], sales_order_header_pd[\"SalesOrderID\"])\n])\n\nsales_order_header[\"churn\"] = churn\n\nsales_order_header = sales_order_header[~sales_order_header.churn.is_nan()].to_df(\"SalesOrderHeaderRefined\")\n\nsales_order_header.set_role(\"churn\", getml.data.roles.target)\n\nsales_order_header\n</pre> sales_order_header_pd = sales_order_header[[\"OrderDate\", \"CustomerID\", \"SalesOrderID\"]].to_pandas()  repeat_purchases = sales_order_header_pd.merge(     sales_order_header_pd[[\"OrderDate\", \"CustomerID\"]],     on=\"CustomerID\",     how=\"left\", )  repeat_purchases = repeat_purchases[     repeat_purchases[\"OrderDate_y\"] &gt; repeat_purchases[\"OrderDate_x\"] ]  repeat_purchases = repeat_purchases[     repeat_purchases[\"OrderDate_y\"] - repeat_purchases[\"OrderDate_x\"] &gt; pd.Timedelta('180 days') ]  repeat_purchases = repeat_purchases.groupby(\"SalesOrderID\", as_index=False).aggregate({\"CustomerID\": \"max\"})  repeat_purchase_ids = {sid: True for sid in repeat_purchases[\"SalesOrderID\"]}  cut_off_date = max(sales_order_header_pd[\"OrderDate\"]) - pd.Timedelta('180 days')  churn = np.asarray([     np.nan if order_date &gt;= cut_off_date else 0 if order_id in repeat_purchase_ids else 1      for (order_date, order_id) in zip(sales_order_header_pd[\"OrderDate\"], sales_order_header_pd[\"SalesOrderID\"]) ])  sales_order_header[\"churn\"] = churn  sales_order_header = sales_order_header[~sales_order_header.churn.is_nan()].to_df(\"SalesOrderHeaderRefined\")  sales_order_header.set_role(\"churn\", getml.data.roles.target)  sales_order_header <pre>/home/alex/.local/lib/python3.10/site-packages/getml/data/data_frame.py:463: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  if _is_numerical_type(temp_df.dtypes[0]):\n</pre> Out[17]:  name                   OrderDate                     DueDate                    ShipDate                ModifiedDate CustomerID SalesOrderID SalesPersonID TerritoryID  churn RevisionNumber OnlineOrderFlag SalesPersonIDCat TerritoryIDCat ShipMethodID   SubTotal    TaxAmt   Freight   TotalDue       Status BillToAddressID ShipToAddressID CreditCardID CurrencyRateID SalesOrderNumber PurchaseOrderNumber AccountNumber  CreditCardApprovalCode Comment       rowguid                           role                  time_stamp                  time_stamp                  time_stamp                  time_stamp   join_key     join_key      join_key    join_key target categorical    categorical     categorical      categorical    categorical   numerical numerical numerical  numerical unused_float    unused_float    unused_float unused_float   unused_float unused_string    unused_string       unused_string  unused_string          unused_string unused_string                     unit time stamp, comparison only time stamp, comparison only time stamp, comparison only time stamp, comparison only 0 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 29825 43659 279 5 0 8 0 279 5 5 20565.6206 1971.5149 616.0984 23153.2339 5 985 985 16281 nan SO43659 PO522145787 10-4020-000676 105041Vi84182 NULL 79B65321-39CA-4115-9CBA-8FE0903E... 1 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 29672 43660 279 5 0 8 0 279 5 5 1294.2529 124.2483 38.8276 1457.3288 5 921 921 5618 nan SO43660 PO18850127500 10-4020-000117 115213Vi29411 NULL 738DC42D-D03B-48A1-9822-F95A67EA... 2 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 29734 43661 282 6 0 8 0 282 6 5 32726.4786 3153.7696 985.553 36865.8012 5 517 517 1346 4 SO43661 PO18473189620 10-4020-000442 85274Vi6854 NULL D91B9131-18A4-4A11-BC3A-90B6F53E... 3 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 29994 43662 282 6 0 8 0 282 6 5 28832.5289 2775.1646 867.2389 32474.9324 5 482 482 10456 4 SO43662 PO18444174044 10-4020-000227 125295Vi53935 NULL 4A1ECFC0-CC3A-4740-B028-1C50BB48... 4 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 29565 43663 276 4 0 8 0 276 4 5 419.4589 40.2681 12.5838 472.3108 5 1073 1073 4322 nan SO43663 PO18009186470 10-4020-000510 45303Vi22691 NULL 9B1E7A40-6AE0-4AD3-811C-A6495185... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 19699 2013-12-30 23:00:00 2014-01-12 2014-01-07 2014-01-06 23:00:00 20826 63358 nan 7 1 8 1 nan 7 1 1173.96 93.9168 29.349 1297.2258 5 24387 24387 3239 nan SO63358 NULL 10-4030-020826 1142084Vi17039 NULL 41278FBB-3DD8-488B-AEA5-8BF4A6F1... 19700 2013-12-30 23:00:00 2014-01-12 2014-01-07 2014-01-06 23:00:00 24114 63359 nan 10 1 8 1 nan 10 1 1179.47 94.3576 29.4868 1303.3144 5 29682 29682 nan 10770 SO63359 NULL 10-4030-024114 NULL NULL 749532D4-BFF7-4FEC-9F77-396C4A96... 19701 2013-12-30 23:00:00 2014-01-12 2014-01-07 2014-01-06 23:00:00 11417 63360 nan 7 1 8 1 nan 7 1 548.98 43.9184 13.7245 606.6229 5 21465 21465 6582 nan SO63360 NULL 10-4030-011417 242387Vi34223 NULL F92FA2A3-73E1-4DD0-987C-99D1C87E... 19702 2013-12-30 23:00:00 2014-01-12 2014-01-07 2014-01-06 23:00:00 18125 63361 nan 8 1 8 1 nan 8 1 2384.07 190.7256 59.6018 2634.3974 5 26562 26562 11994 nan SO63361 NULL 10-4030-018125 1242859Vi61993 NULL 3A3758BF-CDFA-4740-9104-87BD5A08... 19703 2013-12-30 23:00:00 2014-01-12 2014-01-07 2014-01-06 23:00:00 15692 63362 nan 10 1 8 1 nan 10 1 2419.06 193.5248 60.4765 2673.0613 5 27090 27090 16083 10770 SO63362 NULL 10-4030-015692 242864Vi83167 NULL 3151D31F-4020-41B2-87F3-B6732869... <p>     19704 rows x 29 columns     memory usage: 5.39 MB     name: SalesOrderHeaderRefined     type: getml.DataFrame </p> In\u00a0[18]: Copied! <pre>split = getml.data.split.random(train=0.8, test=0.2)\nsplit\n</pre> split = getml.data.split.random(train=0.8, test=0.2) split Out[18]: 0 train 1 train 2 train 3 test 4 train ... <p>     infinite number of  rows          type: StringColumnView </p> In\u00a0[19]: Copied! <pre>container = getml.data.Container(population=sales_order_header, split=split)\n\ncontainer.add(\n    product=product,\n    sales_order_detail=sales_order_detail, \n    sales_order_header=sales_order_header, \n    sales_order_reason=sales_order_reason,\n    special_offer=special_offer,\n    store=store,\n)\n\ncontainer\n</pre> container = getml.data.Container(population=sales_order_header, split=split)  container.add(     product=product,     sales_order_detail=sales_order_detail,      sales_order_header=sales_order_header,      sales_order_reason=sales_order_reason,     special_offer=special_offer,     store=store, )  container Out[19]: population subset name                     rows type 0 test SalesOrderHeaderRefined 3879 View 1 train SalesOrderHeaderRefined 15825 View peripheral alias              name                          rows type      0 product Product 504 DataFrame 1 sales_order_detail SalesOrderDetail 121317 DataFrame 2 sales_order_header SalesOrderHeaderRefined 19704 DataFrame 3 sales_order_reason SalesOrderHeaderSalesReason 27647 DataFrame 4 special_offer SpecialOffer 16 DataFrame 5 store Store 701 DataFrame In\u00a0[20]: Copied! <pre>dm = getml.data.DataModel(sales_order_header.to_placeholder(\"population\"))\n\ndm.add(getml.data.to_placeholder(\n    product=product,\n    sales_order_detail=sales_order_detail, \n    sales_order_header=sales_order_header, \n    sales_order_reason=sales_order_reason,\n    special_offer=special_offer,\n    store=store,\n))\n\n\ndm.population.join(\n    dm.sales_order_header,\n    on=\"CustomerID\",\n    time_stamps=\"OrderDate\",\n    lagged_targets=True,\n    horizon=getml.data.time.days(1),\n)\n\ndm.population.join(\n    dm.sales_order_detail,\n    on=\"SalesOrderID\",\n)\n\ndm.population.join(\n    dm.sales_order_reason,\n    on=\"SalesOrderID\", \n)\n\ndm.population.join(\n    dm.store,\n    on=\"SalesPersonID\", \n)\n\ndm.sales_order_detail.join(\n    dm.product,\n    on=\"ProductID\",\n    relationship=getml.data.relationship.many_to_one,\n)\n\ndm.sales_order_detail.join(\n    dm.special_offer,\n    on=\"SpecialOfferID\",\n    relationship=getml.data.relationship.many_to_one,\n)\n\ndm\n</pre> dm = getml.data.DataModel(sales_order_header.to_placeholder(\"population\"))  dm.add(getml.data.to_placeholder(     product=product,     sales_order_detail=sales_order_detail,      sales_order_header=sales_order_header,      sales_order_reason=sales_order_reason,     special_offer=special_offer,     store=store, ))   dm.population.join(     dm.sales_order_header,     on=\"CustomerID\",     time_stamps=\"OrderDate\",     lagged_targets=True,     horizon=getml.data.time.days(1), )  dm.population.join(     dm.sales_order_detail,     on=\"SalesOrderID\", )  dm.population.join(     dm.sales_order_reason,     on=\"SalesOrderID\",  )  dm.population.join(     dm.store,     on=\"SalesPersonID\",  )  dm.sales_order_detail.join(     dm.product,     on=\"ProductID\",     relationship=getml.data.relationship.many_to_one, )  dm.sales_order_detail.join(     dm.special_offer,     on=\"SpecialOfferID\",     relationship=getml.data.relationship.many_to_one, )  dm    Out[20]: diagram sales_order_headerproductspecial_offersales_order_detailsales_order_reasonstorepopulationProductID = ProductIDRelationship: many-to-oneSpecialOfferID = SpecialOfferIDRelationship: many-to-oneCustomerID = CustomerIDOrderDate &lt;= OrderDateHorizon: 1.0 daysLagged targets allowedSalesOrderID = SalesOrderIDSalesOrderID = SalesOrderIDSalesPersonID = SalesPersonID staging data frames                      staging table                    0 population POPULATION__STAGING_TABLE_1 1 sales_order_detail, product, special_offer SALES_ORDER_DETAIL__STAGING_TABLE_2 2 sales_order_header SALES_ORDER_HEADER__STAGING_TABLE_3 3 sales_order_reason SALES_ORDER_REASON__STAGING_TABLE_4 4 store STORE__STAGING_TABLE_5 <p>Set-up the feature learner &amp; predictor</p> In\u00a0[21]: Copied! <pre>fast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    num_threads=1,    \n    num_features=400,\n)\n\npredictor = getml.predictors.XGBoostClassifier(n_jobs=1)\n</pre> fast_prop = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     num_threads=1,         num_features=400, )  predictor = getml.predictors.XGBoostClassifier(n_jobs=1) <p>Build the pipeline</p> In\u00a0[22]: Copied! <pre>pipe1 = getml.Pipeline(\n    tags=['fast_prop'],\n    data_model=dm,\n    feature_learners=[fast_prop],\n    predictors=[predictor],\n    include_categorical=True,\n)\n\npipe1\n</pre> pipe1 = getml.Pipeline(     tags=['fast_prop'],     data_model=dm,     feature_learners=[fast_prop],     predictors=[predictor],     include_categorical=True, )  pipe1 Out[22]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='CrossEntropyLoss',\n         peripheral=['product', 'sales_order_detail', 'sales_order_header',\n                     'sales_order_reason', 'special_offer', 'store'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['fast_prop'])</pre> In\u00a0[23]: Copied! <pre>pipe1.check(container.train)\n</pre> pipe1.check(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 2 issues labeled INFO and 0 issues labeled WARNING.\n</pre> Out[23]: type label                  message                          0 INFO FOREIGN KEYS NOT FOUND When joining POPULATION__STAGING_TABLE_1 and SALES_ORDER_REASON__STAGING_TABLE_4 over 'SalesOrderID' and 'SalesOrderID', there are no corresponding entries for 33.769352% of entries in 'SalesOrderID' in 'POPULATION__STAGING_TABLE_1'. You might want to double-check your join keys. 1 INFO FOREIGN KEYS NOT FOUND When joining POPULATION__STAGING_TABLE_1 and STORE__STAGING_TABLE_5 over 'SalesPersonID' and 'SalesPersonID', there are no corresponding entries for 84.941548% of entries in 'SalesPersonID' in 'POPULATION__STAGING_TABLE_1'. You might want to double-check your join keys. In\u00a0[24]: Copied! <pre>pipe1.fit(container.train)\n</pre> pipe1.fit(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 2 issues labeled INFO and 0 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 563 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:14, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:19.952771\n\n</pre> Out[24]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='CrossEntropyLoss',\n         peripheral=['product', 'sales_order_detail', 'sales_order_header',\n                     'sales_order_reason', 'special_offer', 'store'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['fast_prop', 'container-1l2C0q'])</pre> In\u00a0[25]: Copied! <pre>pipe1.score(container.test)\n</pre> pipe1.score(container.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[25]: date time           set used target accuracy     auc cross entropy 0 2024-08-07 16:26:10 train churn 0.9152 0.9723 0.2187 1 2024-08-07 16:26:11 test churn 0.9162 0.9699 0.2265 In\u00a0[26]: Copied! <pre>pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name]\n</pre> pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name] Out[26]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_307\";\n\nCREATE TABLE \"FEATURE_1_307\" AS\nSELECT AVG( t1.\"orderdate\" - t2.\"t4__startdate\" ) AS \"feature_1_307\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"SALES_ORDER_DETAIL__STAGING_TABLE_2\" t2\nON t1.\"salesorderid\" = t2.\"salesorderid\"\nGROUP BY t1.rowid;\n</pre> In\u00a0[27]: Copied! <pre>getml.engine.shutdown()\n</pre> getml.engine.shutdown()","boost":0.9},{"location":"examples/community-notebooks/adventure_works/#adventureworks-predicting-customer-churn","title":"AdventureWorks - Predicting customer churn\u00b6","text":"<p>In this notebook, we will demonstrate how getML can be used for a customer churn project using a synthetic dataset of a fictional company.</p> <p>Summary:</p> <ul> <li>Prediction type: Classification model</li> <li>Domain: Customer loyalty</li> <li>Prediction target: churn</li> <li>Population size: 19704</li> </ul> <p>Author: Dr. Patrick Urbanke</p>","boost":0.9},{"location":"examples/community-notebooks/adventure_works/#background","title":"Background\u00b6","text":"<p>AdventureWorks is a fictional company that sells bicycles. It is used by Microsoft to showcase how its MS SQL Server can be used to manage business data. Since this dataset resembles a real-world customer database and it is open-source, we will use it to showcase how getML can be used for a classic customer churn project (real customer databases are not easily available for the purposes of showcasing and benchmarking, for reasons of data privacy).</p> <p>The dataset has been downloaded from the CTU Prague relational learning repository (Motl and Schulte, 2015).</p>","boost":0.9},{"location":"examples/community-notebooks/adventure_works/#analysis","title":"Analysis\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/adventure_works/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/adventure_works/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>We begin by downloading the data:</p>","boost":0.9},{"location":"examples/community-notebooks/adventure_works/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/adventure_works/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"<p>We loaded the data and defined the roles and units. Next, we create a getML pipeline for relational learning.</p>","boost":0.9},{"location":"examples/community-notebooks/adventure_works/#21-define-relational-model","title":"2.1 Define relational model\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/adventure_works/#22-getml-pipeline","title":"2.2 getML pipeline\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/adventure_works/#23-model-training","title":"2.3 Model training\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/adventure_works/#24-model-evaluation","title":"2.4 Model evaluation\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/adventure_works/#25-features","title":"2.5 Features\u00b6","text":"<p>The most important features look as follows:</p>","boost":0.9},{"location":"examples/community-notebooks/adventure_works/#3-conclusion","title":"3. Conclusion\u00b6","text":"<p>In this notebook, we successfully demonstrated the process of predicting customer churn for AdventureWorks, a fictional company that sells bicycles, using the getML library. The key steps we covered include:</p> <ol> <li><p>Background and Data Preparation:</p> <ul> <li>Introduced AdventureWorks and the purpose of the analysis.</li> <li>Loaded and prepared the data from the AdventureWorks database, including various tables such as Product, SalesOrderDetail, SalesOrderHeader, and more.</li> </ul> </li> <li><p>Data Visualization and Preparation:</p> <ul> <li>Defined roles for the dataset columns to prepare them for modeling.</li> <li>Ensured the data was in the correct format for getML to process.</li> </ul> </li> <li><p>Predictive Modeling:</p> <ul> <li>Created a relational model using getML to capture the relationships between different tables.</li> <li>Built a getML pipeline for feature learning and prediction.</li> </ul> </li> <li><p>Model Training and Evaluation:</p> <ul> <li>Trained the model on the prepared data.</li> <li>Evaluated the model's performance to predict customer churn.</li> </ul> </li> </ol> <p>By leveraging getML's capabilities, we efficiently handled the complexities of relational data and built a robust classification model to predict customer churn. This approach can be extended to other customer databases and prediction tasks, providing valuable insights and accurate forecasts.</p>","boost":0.9},{"location":"examples/community-notebooks/adventure_works/#references","title":"References\u00b6","text":"<p>Motl, Jan, and Oliver Schulte. \"The CTU prague relational learning repository.\" arXiv preprint arXiv:1511.03086 (2015).</p>","boost":0.9},{"location":"examples/community-notebooks/formula1/","title":"<span class=\"ntitle\">formula1.ipynb</span> <span class=\"ndesc\">Predicting the winner of a race</span>","text":"<p>Let's get started with the analysis and set up your session:</p> In\u00a0[1]: Copied! <pre>%pip install -q \"getml==1.4.0\" \"numpy&lt;2.0.0\" \"matplotlib~=3.9\"\n\nimport matplotlib.pyplot as plt\n%matplotlib inline  \n\nimport getml\n\ngetml.engine.launch()\ngetml.engine.set_project('formula1')\n</pre> %pip install -q \"getml==1.4.0\" \"numpy&lt;2.0.0\" \"matplotlib~=3.9\"  import matplotlib.pyplot as plt %matplotlib inline    import getml  getml.engine.launch() getml.engine.set_project('formula1') <pre>Note: you may need to restart the kernel to use updated packages.\nLaunching ./getML --allow-push-notifications=true --allow-remote-ips=false --home-directory=/home/alex/.local/lib/python3.10/site-packages/getml --in-memory=true --install=false --launch-browser=true --log=false in /home/alex/.local/lib/python3.10/site-packages/getml/.getML/getml-1.4.0-x64-community-edition-linux...\nLaunched the getML engine. The log output will be stored in /home/alex/.getML/logs/20240807162935.log.\n\nConnected to project 'formula1'\n</pre> In\u00a0[2]: Copied! <pre>conn = getml.database.connect_mariadb(\n    host=\"db.relational-data.org\",\n    dbname=\"ErgastF1\",\n    port=3306,\n    user=\"guest\",\n    password=\"relational\"\n)\n\nconn\n</pre> conn = getml.database.connect_mariadb(     host=\"db.relational-data.org\",     dbname=\"ErgastF1\",     port=3306,     user=\"guest\",     password=\"relational\" )  conn Out[2]: <pre>Connection(dbname='ErgastF1',\n           dialect='mysql',\n           host='db.relational-data.org',\n           port=3306)</pre> In\u00a0[3]: Copied! <pre>def load_if_needed(name):\n    \"\"\"\n    Loads the data from the relational learning\n    repository, if the data frame has not already\n    been loaded.\n    \"\"\"\n    if not getml.data.exists(name):\n        data_frame = getml.data.DataFrame.from_db(\n            name=name,\n            table_name=name,\n            conn=conn\n        )\n        data_frame.save()\n    else:\n        data_frame = getml.data.load_data_frame(name)\n    return data_frame\n</pre> def load_if_needed(name):     \"\"\"     Loads the data from the relational learning     repository, if the data frame has not already     been loaded.     \"\"\"     if not getml.data.exists(name):         data_frame = getml.data.DataFrame.from_db(             name=name,             table_name=name,             conn=conn         )         data_frame.save()     else:         data_frame = getml.data.load_data_frame(name)     return data_frame In\u00a0[4]: Copied! <pre>driverStandings = load_if_needed(\"driverStandings\")\ndrivers = load_if_needed(\"drivers\")\nlapTimes = load_if_needed(\"lapTimes\")\npitStops = load_if_needed(\"pitStops\")\nraces = load_if_needed(\"races\")\nqualifying = load_if_needed(\"qualifying\")\n</pre> driverStandings = load_if_needed(\"driverStandings\") drivers = load_if_needed(\"drivers\") lapTimes = load_if_needed(\"lapTimes\") pitStops = load_if_needed(\"pitStops\") races = load_if_needed(\"races\") qualifying = load_if_needed(\"qualifying\") In\u00a0[5]: Copied! <pre>driverStandings\n</pre> driverStandings Out[5]:  name driverStandingsId       raceId     driverId       points     position         wins positionText   role      unused_float unused_float unused_float unused_float unused_float unused_float unused_string 0 1 18 1 10 1 1 1 1 2 18 2 8 2 0 2 2 3 18 3 6 3 0 3 3 4 18 4 5 4 0 4 4 5 18 5 4 5 0 5 ... ... ... ... ... ... ... 31573 68456 982 835 8 16 0 16 31574 68457 982 154 26 13 0 13 31575 68458 982 836 5 18 0 18 31576 68459 982 18 0 22 0 22 31577 68460 982 814 0 23 0 23 <p>     31578 rows x 7 columns     memory usage: 1.85 MB     name: driverStandings     type: getml.DataFrame </p> In\u00a0[6]: Copied! <pre>drivers\n</pre> drivers Out[6]: name     driverId       number driverRef     code          forename      surname       dob           nationality   url                              role unused_float unused_float unused_string unused_string unused_string unused_string unused_string unused_string unused_string                    0 1 44 hamilton HAM Lewis Hamilton 1985-01-07 British http://en.wikipedia.org/wiki/Lew... 1 2 nan heidfeld HEI Nick Heidfeld 1977-05-10 German http://en.wikipedia.org/wiki/Nic... 2 3 6 rosberg ROS Nico Rosberg 1985-06-27 German http://en.wikipedia.org/wiki/Nic... 3 4 14 alonso ALO Fernando Alonso 1981-07-29 Spanish http://en.wikipedia.org/wiki/Fer... 4 5 nan kovalainen KOV Heikki Kovalainen 1981-10-19 Finnish http://en.wikipedia.org/wiki/Hei... ... ... ... ... ... ... ... ... ... 835 837 88 haryanto HAR Rio Haryanto 1993-01-22 Indonesian http://en.wikipedia.org/wiki/Rio... 836 838 2 vandoorne VAN Stoffel Vandoorne 1992-03-26 Belgian http://en.wikipedia.org/wiki/Sto... 837 839 31 ocon OCO Esteban Ocon 1996-09-17 French http://en.wikipedia.org/wiki/Est... 838 840 18 stroll STR Lance Stroll 1998-10-29 Canadian http://en.wikipedia.org/wiki/Lan... 839 841 36 giovinazzi GIO Antonio Giovinazzi 1993-12-14 Italian http://en.wikipedia.org/wiki/Ant... <p>     840 rows x 9 columns     memory usage: 0.13 MB     name: drivers     type: getml.DataFrame </p> In\u00a0[7]: Copied! <pre>lapTimes\n</pre> lapTimes Out[7]:   name       raceId     driverId          lap     position milliseconds time            role unused_float unused_float unused_float unused_float unused_float unused_string 0 1 1 1 13 109088 1:49.088 1 1 1 2 12 93740 1:33.740 2 1 1 3 11 91600 1:31.600 3 1 1 4 10 91067 1:31.067 4 1 1 5 10 92129 1:32.129 ... ... ... ... ... ... 420364 982 840 54 8 107528 1:47.528 420365 982 840 55 8 107512 1:47.512 420366 982 840 56 8 108143 1:48.143 420367 982 840 57 8 107848 1:47.848 420368 982 840 58 8 108699 1:48.699 <p>     420369 rows x 6 columns     memory usage: 23.96 MB     name: lapTimes     type: getml.DataFrame </p> In\u00a0[8]: Copied! <pre>pitStops\n</pre> pitStops Out[8]: name       raceId     driverId         stop          lap milliseconds time          duration      role unused_float unused_float unused_float unused_float unused_float unused_string unused_string 0 841 1 1 16 23227 17:28:24 23.227 1 841 1 2 36 23199 17:59:29 23.199 2 841 2 1 15 22994 17:27:41 22.994 3 841 2 2 30 25098 17:51:32 25.098 4 841 3 1 16 23716 17:29:00 23.716 ... ... ... ... ... ... ... 6065 982 839 6 38 29134 21:29:07 29.134 6066 982 840 1 1 37403 20:06:43 37.403 6067 982 840 2 2 29294 20:10:07 29.294 6068 982 840 3 3 25584 20:13:16 25.584 6069 982 840 4 26 29412 21:05:07 29.412 <p>     6070 rows x 7 columns     memory usage: 0.44 MB     name: pitStops     type: getml.DataFrame </p> In\u00a0[9]: Copied! <pre>races\n</pre> races Out[9]: name       raceId         year        round    circuitId name                     date          time          url                              role unused_float unused_float unused_float unused_float unused_string            unused_string unused_string unused_string                    0 1 2009 1 1 Australian Grand Prix 2009-03-29 06:00:00 http://en.wikipedia.org/wiki/200... 1 2 2009 2 2 Malaysian Grand Prix 2009-04-05 09:00:00 http://en.wikipedia.org/wiki/200... 2 3 2009 3 17 Chinese Grand Prix 2009-04-19 07:00:00 http://en.wikipedia.org/wiki/200... 3 4 2009 4 3 Bahrain Grand Prix 2009-04-26 12:00:00 http://en.wikipedia.org/wiki/200... 4 5 2009 5 4 Spanish Grand Prix 2009-05-10 12:00:00 http://en.wikipedia.org/wiki/200... ... ... ... ... ... ... ... ... 971 984 2017 16 22 Japanese Grand Prix 2017-10-08 05:00:00 https://en.wikipedia.org/wiki/20... 972 985 2017 17 69 United States Grand Prix 2017-10-22 19:00:00 https://en.wikipedia.org/wiki/20... 973 986 2017 18 32 Mexican Grand Prix 2017-10-29 19:00:00 https://en.wikipedia.org/wiki/20... 974 987 2017 19 18 Brazilian Grand Prix 2017-11-12 16:00:00 https://en.wikipedia.org/wiki/20... 975 988 2017 20 24 Abu Dhabi Grand Prix 2017-11-26 17:00:00 https://en.wikipedia.org/wiki/20... <p>     976 rows x 8 columns     memory usage: 0.15 MB     name: races     type: getml.DataFrame </p> In\u00a0[10]: Copied! <pre>qualifying\n</pre> qualifying Out[10]: name    qualifyId       raceId     driverId constructorId       number     position q1            q2            q3            role unused_float unused_float unused_float  unused_float unused_float unused_float unused_string unused_string unused_string 0 1 18 1 1 22 1 1:26.572 1:25.187 1:26.714 1 2 18 9 2 4 2 1:26.103 1:25.315 1:26.869 2 3 18 5 1 23 3 1:25.664 1:25.452 1:27.079 3 4 18 13 6 2 4 1:25.994 1:25.691 1:27.178 4 5 18 2 2 3 5 1:25.960 1:25.518 1:27.236 ... ... ... ... ... ... ... ... ... 7392 7415 982 825 210 20 16 1:43.756 NULL NULL 7393 7416 982 13 3 19 17 1:44.014 NULL NULL 7394 7417 982 840 3 18 18 1:44.728 NULL NULL 7395 7418 982 836 15 94 19 1:45.059 NULL NULL 7396 7419 982 828 15 9 20 1:45.570 NULL NULL <p>     7397 rows x 9 columns     memory usage: 0.66 MB     name: qualifying     type: getml.DataFrame </p> In\u00a0[11]: Copied! <pre>racesPd = races.to_pandas()\nracesPd\n</pre> racesPd = races.to_pandas() racesPd Out[11]: raceId year round circuitId name date time url 0 1.0 2009.0 1.0 1.0 Australian Grand Prix 2009-03-29 06:00:00 http://en.wikipedia.org/wiki/2009_Australian_G... 1 2.0 2009.0 2.0 2.0 Malaysian Grand Prix 2009-04-05 09:00:00 http://en.wikipedia.org/wiki/2009_Malaysian_Gr... 2 3.0 2009.0 3.0 17.0 Chinese Grand Prix 2009-04-19 07:00:00 http://en.wikipedia.org/wiki/2009_Chinese_Gran... 3 4.0 2009.0 4.0 3.0 Bahrain Grand Prix 2009-04-26 12:00:00 http://en.wikipedia.org/wiki/2009_Bahrain_Gran... 4 5.0 2009.0 5.0 4.0 Spanish Grand Prix 2009-05-10 12:00:00 http://en.wikipedia.org/wiki/2009_Spanish_Gran... ... ... ... ... ... ... ... ... ... 971 984.0 2017.0 16.0 22.0 Japanese Grand Prix 2017-10-08 05:00:00 https://en.wikipedia.org/wiki/2017_Japanese_Gr... 972 985.0 2017.0 17.0 69.0 United States Grand Prix 2017-10-22 19:00:00 https://en.wikipedia.org/wiki/2017_United_Stat... 973 986.0 2017.0 18.0 32.0 Mexican Grand Prix 2017-10-29 19:00:00 https://en.wikipedia.org/wiki/2017_Mexican_Gra... 974 987.0 2017.0 19.0 18.0 Brazilian Grand Prix 2017-11-12 16:00:00 https://en.wikipedia.org/wiki/2017_Brazilian_G... 975 988.0 2017.0 20.0 24.0 Abu Dhabi Grand Prix 2017-11-26 17:00:00 https://en.wikipedia.org/wiki/2017_Abu_Dhabi_G... <p>976 rows \u00d7 8 columns</p> <p>We actually need some set-up, because the target variable is not readily available. The <code>wins</code> column in <code>driverStandings</code> is actually the accumulated number of wins over a year, but what we want is a boolean variable indicated whether someone has one a particular race or not.</p> In\u00a0[12]: Copied! <pre>driverStandingsPd = driverStandings.to_pandas()\n\ndriverStandingsPd = driverStandingsPd.merge(\n    racesPd[[\"raceId\", \"year\", \"date\", \"round\"]],\n    on=\"raceId\"\n)\n\npreviousStanding = driverStandingsPd.merge(\n    driverStandingsPd[[\"driverId\", \"year\", \"wins\", \"round\"]],\n    on=[\"driverId\", \"year\"],\n)\n\nisPreviousRound = (previousStanding[\"round_x\"] - previousStanding[\"round_y\"] == 1.0)\n\npreviousStanding = previousStanding[isPreviousRound]\n\npreviousStanding[\"win\"] = previousStanding[\"wins_x\"] - previousStanding[\"wins_y\"]\n\ndriverStandingsPd = driverStandingsPd.merge(\n    previousStanding[[\"raceId\", \"driverId\", \"win\"]],\n    on=[\"raceId\", \"driverId\"],\n    how=\"left\",\n)\n\ndriverStandingsPd[\"win\"] = [win if win == win else wins for win, wins in zip(driverStandingsPd[\"win\"], driverStandingsPd[\"wins\"])]\n\ndriver_standings = getml.data.DataFrame.from_pandas(driverStandingsPd, \"driver_standings\")\n\ndriver_standings\n</pre> driverStandingsPd = driverStandings.to_pandas()  driverStandingsPd = driverStandingsPd.merge(     racesPd[[\"raceId\", \"year\", \"date\", \"round\"]],     on=\"raceId\" )  previousStanding = driverStandingsPd.merge(     driverStandingsPd[[\"driverId\", \"year\", \"wins\", \"round\"]],     on=[\"driverId\", \"year\"], )  isPreviousRound = (previousStanding[\"round_x\"] - previousStanding[\"round_y\"] == 1.0)  previousStanding = previousStanding[isPreviousRound]  previousStanding[\"win\"] = previousStanding[\"wins_x\"] - previousStanding[\"wins_y\"]  driverStandingsPd = driverStandingsPd.merge(     previousStanding[[\"raceId\", \"driverId\", \"win\"]],     on=[\"raceId\", \"driverId\"],     how=\"left\", )  driverStandingsPd[\"win\"] = [win if win == win else wins for win, wins in zip(driverStandingsPd[\"win\"], driverStandingsPd[\"wins\"])]  driver_standings = getml.data.DataFrame.from_pandas(driverStandingsPd, \"driver_standings\")  driver_standings Out[12]:  name driverStandingsId       raceId     driverId       points     position         wins         year        round          win positionText  date           role      unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_string unused_string 0 1 18 1 10 1 1 2008 1 1 1 2008-03-16 1 2 18 2 8 2 0 2008 1 0 2 2008-03-16 2 3 18 3 6 3 0 2008 1 0 3 2008-03-16 3 4 18 4 5 4 0 2008 1 0 4 2008-03-16 4 5 18 5 4 5 0 2008 1 0 5 2008-03-16 ... ... ... ... ... ... ... ... ... ... ... 31573 68456 982 835 8 16 0 2017 14 0 16 2017-09-17 31574 68457 982 154 26 13 0 2017 14 0 13 2017-09-17 31575 68458 982 836 5 18 0 2017 14 0 18 2017-09-17 31576 68459 982 18 0 22 0 2017 14 0 22 2017-09-17 31577 68460 982 814 0 23 0 2017 14 0 23 2017-09-17 <p>     31578 rows x 11 columns     memory usage: 3.21 MB     name: driver_standings     type: getml.DataFrame </p> <p>We also need to include the date of the race to <code>lapTimes</code> and <code>pitStops</code>, because we cannot use this data for the race we would like to predict. We can only take lap times and pit stops from previous races.</p> In\u00a0[13]: Copied! <pre>lapTimesPd = lapTimes.to_pandas()\n\nlapTimesPd = lapTimesPd.merge(\n    racesPd[[\"raceId\", \"date\", \"year\"]],\n    on=\"raceId\"\n)\n\nlap_times = getml.data.DataFrame.from_pandas(lapTimesPd, \"lap_times\")\n\nlap_times\n</pre> lapTimesPd = lapTimes.to_pandas()  lapTimesPd = lapTimesPd.merge(     racesPd[[\"raceId\", \"date\", \"year\"]],     on=\"raceId\" )  lap_times = getml.data.DataFrame.from_pandas(lapTimesPd, \"lap_times\")  lap_times Out[13]:   name       raceId     driverId          lap     position milliseconds         year time          date            role unused_float unused_float unused_float unused_float unused_float unused_float unused_string unused_string 0 1 1 1 13 109088 2009 1:49.088 2009-03-29 1 1 1 2 12 93740 2009 1:33.740 2009-03-29 2 1 1 3 11 91600 2009 1:31.600 2009-03-29 3 1 1 4 10 91067 2009 1:31.067 2009-03-29 4 1 1 5 10 92129 2009 1:32.129 2009-03-29 ... ... ... ... ... ... ... ... 420364 982 840 54 8 107528 2017 1:47.528 2017-09-17 420365 982 840 55 8 107512 2017 1:47.512 2017-09-17 420366 982 840 56 8 108143 2017 1:48.143 2017-09-17 420367 982 840 57 8 107848 2017 1:47.848 2017-09-17 420368 982 840 58 8 108699 2017 1:48.699 2017-09-17 <p>     420369 rows x 8 columns     memory usage: 35.31 MB     name: lap_times     type: getml.DataFrame </p> In\u00a0[14]: Copied! <pre>pitStopsPd = pitStops.to_pandas()\n\npitStopsPd = pitStopsPd.merge(\n    racesPd[[\"raceId\", \"date\", \"year\"]],\n    on=\"raceId\"\n)\n\npit_stops = getml.data.DataFrame.from_pandas(pitStopsPd, \"pit_stops\")\n\npit_stops\n</pre> pitStopsPd = pitStops.to_pandas()  pitStopsPd = pitStopsPd.merge(     racesPd[[\"raceId\", \"date\", \"year\"]],     on=\"raceId\" )  pit_stops = getml.data.DataFrame.from_pandas(pitStopsPd, \"pit_stops\")  pit_stops Out[14]: name       raceId     driverId         stop          lap milliseconds         year time          duration      date          role unused_float unused_float unused_float unused_float unused_float unused_float unused_string unused_string unused_string 0 841 1 1 16 23227 2011 17:28:24 23.227 2011-03-27 1 841 1 2 36 23199 2011 17:59:29 23.199 2011-03-27 2 841 2 1 15 22994 2011 17:27:41 22.994 2011-03-27 3 841 2 2 30 25098 2011 17:51:32 25.098 2011-03-27 4 841 3 1 16 23716 2011 17:29:00 23.716 2011-03-27 ... ... ... ... ... ... ... ... ... 6065 982 839 6 38 29134 2017 21:29:07 29.134 2017-09-17 6066 982 840 1 1 37403 2017 20:06:43 37.403 2017-09-17 6067 982 840 2 2 29294 2017 20:10:07 29.294 2017-09-17 6068 982 840 3 3 25584 2017 20:13:16 25.584 2017-09-17 6069 982 840 4 26 29412 2017 21:05:07 29.412 2017-09-17 <p>     6070 rows x 9 columns     memory usage: 0.60 MB     name: pit_stops     type: getml.DataFrame </p> <p>getML requires that we define roles for each of the columns.</p> In\u00a0[15]: Copied! <pre>driver_standings.set_role(\"win\", getml.data.roles.target)\ndriver_standings.set_role([\"raceId\", \"driverId\", \"year\"], getml.data.roles.join_key)\ndriver_standings.set_role(\"position\", getml.data.roles.numerical)\ndriver_standings.set_role(\"date\", getml.data.roles.time_stamp)\n\ndriver_standings\n</pre> driver_standings.set_role(\"win\", getml.data.roles.target) driver_standings.set_role([\"raceId\", \"driverId\", \"year\"], getml.data.roles.join_key) driver_standings.set_role(\"position\", getml.data.roles.numerical) driver_standings.set_role(\"date\", getml.data.roles.time_stamp)  driver_standings Out[15]:  name                        date   raceId driverId     year    win  position driverStandingsId       points         wins        round positionText   role                  time_stamp join_key join_key join_key target numerical      unused_float unused_float unused_float unused_float unused_string  unit time stamp, comparison only 0 2008-03-16 18 1 2008 1 1 1 10 1 1 1 1 2008-03-16 18 2 2008 0 2 2 8 0 1 2 2 2008-03-16 18 3 2008 0 3 3 6 0 1 3 3 2008-03-16 18 4 2008 0 4 4 5 0 1 4 4 2008-03-16 18 5 2008 0 5 5 4 0 1 5 ... ... ... ... ... ... ... ... ... ... ... 31573 2017-09-17 982 835 2017 0 16 68456 8 0 14 16 31574 2017-09-17 982 154 2017 0 13 68457 26 0 14 13 31575 2017-09-17 982 836 2017 0 18 68458 5 0 14 18 31576 2017-09-17 982 18 2017 0 22 68459 0 0 14 22 31577 2017-09-17 982 814 2017 0 23 68460 0 0 14 23 <p>     31578 rows x 11 columns     memory usage: 2.49 MB     name: driver_standings     type: getml.DataFrame </p> In\u00a0[16]: Copied! <pre>drivers.set_role(\"driverId\", getml.data.roles.join_key)\ndrivers.set_role([\"nationality\", \"driverRef\"], getml.data.roles.categorical)\n\ndrivers\n</pre> drivers.set_role(\"driverId\", getml.data.roles.join_key) drivers.set_role([\"nationality\", \"driverRef\"], getml.data.roles.categorical)  drivers Out[16]: name driverId nationality driverRef         number code          forename      surname       dob           url                              role join_key categorical categorical unused_float unused_string unused_string unused_string unused_string unused_string                    0 1 British hamilton 44 HAM Lewis Hamilton 1985-01-07 http://en.wikipedia.org/wiki/Lew... 1 2 German heidfeld nan HEI Nick Heidfeld 1977-05-10 http://en.wikipedia.org/wiki/Nic... 2 3 German rosberg 6 ROS Nico Rosberg 1985-06-27 http://en.wikipedia.org/wiki/Nic... 3 4 Spanish alonso 14 ALO Fernando Alonso 1981-07-29 http://en.wikipedia.org/wiki/Fer... 4 5 Finnish kovalainen nan KOV Heikki Kovalainen 1981-10-19 http://en.wikipedia.org/wiki/Hei... ... ... ... ... ... ... ... ... ... 835 837 Indonesian haryanto 88 HAR Rio Haryanto 1993-01-22 http://en.wikipedia.org/wiki/Rio... 836 838 Belgian vandoorne 2 VAN Stoffel Vandoorne 1992-03-26 http://en.wikipedia.org/wiki/Sto... 837 839 French ocon 31 OCO Esteban Ocon 1996-09-17 http://en.wikipedia.org/wiki/Est... 838 840 Canadian stroll 18 STR Lance Stroll 1998-10-29 http://en.wikipedia.org/wiki/Lan... 839 841 Italian giovinazzi 36 GIO Antonio Giovinazzi 1993-12-14 http://en.wikipedia.org/wiki/Ant... <p>     840 rows x 9 columns     memory usage: 0.11 MB     name: drivers     type: getml.DataFrame </p> In\u00a0[17]: Copied! <pre>lap_times.set_role([\"raceId\", \"driverId\", \"year\"], getml.data.roles.join_key)\nlap_times.set_role([\"lap\", \"milliseconds\", \"position\"], getml.data.roles.numerical)\nlap_times.set_role(\"date\", getml.data.roles.time_stamp)\n\nlap_times\n</pre> lap_times.set_role([\"raceId\", \"driverId\", \"year\"], getml.data.roles.join_key) lap_times.set_role([\"lap\", \"milliseconds\", \"position\"], getml.data.roles.numerical) lap_times.set_role(\"date\", getml.data.roles.time_stamp)  lap_times Out[17]:   name                        date   raceId driverId     year       lap milliseconds  position time            role                  time_stamp join_key join_key join_key numerical    numerical numerical unused_string   unit time stamp, comparison only 0 2009-03-29 1 1 2009 1 109088 13 1:49.088 1 2009-03-29 1 1 2009 2 93740 12 1:33.740 2 2009-03-29 1 1 2009 3 91600 11 1:31.600 3 2009-03-29 1 1 2009 4 91067 10 1:31.067 4 2009-03-29 1 1 2009 5 92129 10 1:32.129 ... ... ... ... ... ... ... ... 420364 2017-09-17 982 840 2017 54 107528 8 1:47.528 420365 2017-09-17 982 840 2017 55 107512 8 1:47.512 420366 2017-09-17 982 840 2017 56 108143 8 1:48.143 420367 2017-09-17 982 840 2017 57 107848 8 1:47.848 420368 2017-09-17 982 840 2017 58 108699 8 1:48.699 <p>     420369 rows x 8 columns     memory usage: 25.64 MB     name: lap_times     type: getml.DataFrame </p> In\u00a0[18]: Copied! <pre>pit_stops.set_role([\"raceId\", \"driverId\", \"year\"], getml.data.roles.join_key)\npit_stops.set_role([\"lap\", \"milliseconds\", \"stop\"], getml.data.roles.numerical)\npit_stops.set_role(\"date\", getml.data.roles.time_stamp)\n\npit_stops\n</pre> pit_stops.set_role([\"raceId\", \"driverId\", \"year\"], getml.data.roles.join_key) pit_stops.set_role([\"lap\", \"milliseconds\", \"stop\"], getml.data.roles.numerical) pit_stops.set_role(\"date\", getml.data.roles.time_stamp)  pit_stops Out[18]: name                        date   raceId driverId     year       lap milliseconds      stop time          duration      role                  time_stamp join_key join_key join_key numerical    numerical numerical unused_string unused_string unit time stamp, comparison only 0 2011-03-27 841 1 2011 16 23227 1 17:28:24 23.227 1 2011-03-27 841 1 2011 36 23199 2 17:59:29 23.199 2 2011-03-27 841 2 2011 15 22994 1 17:27:41 22.994 3 2011-03-27 841 2 2011 30 25098 2 17:51:32 25.098 4 2011-03-27 841 3 2011 16 23716 1 17:29:00 23.716 ... ... ... ... ... ... ... ... ... 6065 2017-09-17 982 839 2017 38 29134 6 21:29:07 29.134 6066 2017-09-17 982 840 2017 1 37403 1 20:06:43 37.403 6067 2017-09-17 982 840 2017 2 29294 2 20:10:07 29.294 6068 2017-09-17 982 840 2017 3 25584 3 20:13:16 25.584 6069 2017-09-17 982 840 2017 26 29412 4 21:05:07 29.412 <p>     6070 rows x 9 columns     memory usage: 0.46 MB     name: pit_stops     type: getml.DataFrame </p> In\u00a0[19]: Copied! <pre>qualifying.set_role([\"raceId\", \"driverId\", \"qualifyId\"], getml.data.roles.join_key)\nqualifying.set_role([\"position\", \"number\"], getml.data.roles.numerical)\n\nqualifying\n</pre> qualifying.set_role([\"raceId\", \"driverId\", \"qualifyId\"], getml.data.roles.join_key) qualifying.set_role([\"position\", \"number\"], getml.data.roles.numerical)  qualifying Out[19]: name   raceId driverId qualifyId  position    number constructorId q1            q2            q3            role join_key join_key  join_key numerical numerical  unused_float unused_string unused_string unused_string 0 18 1 1 1 22 1 1:26.572 1:25.187 1:26.714 1 18 9 2 2 4 2 1:26.103 1:25.315 1:26.869 2 18 5 3 3 23 1 1:25.664 1:25.452 1:27.079 3 18 13 4 4 2 6 1:25.994 1:25.691 1:27.178 4 18 2 5 5 3 2 1:25.960 1:25.518 1:27.236 ... ... ... ... ... ... ... ... ... 7392 982 825 7415 16 20 210 1:43.756 NULL NULL 7393 982 13 7416 17 19 3 1:44.014 NULL NULL 7394 982 840 7417 18 18 3 1:44.728 NULL NULL 7395 982 836 7418 19 94 15 1:45.059 NULL NULL 7396 982 828 7419 20 9 15 1:45.570 NULL NULL <p>     7397 rows x 9 columns     memory usage: 0.57 MB     name: qualifying     type: getml.DataFrame </p> In\u00a0[20]: Copied! <pre>split = getml.data.split.random(train=0.8, test=0.2)\nsplit\n</pre> split = getml.data.split.random(train=0.8, test=0.2) split Out[20]: 0 train 1 train 2 train 3 test 4 train ... <p>     infinite number of  rows          type: StringColumnView </p> In\u00a0[21]: Copied! <pre>star_schema = getml.data.StarSchema(population=driver_standings.drop([\"position\"]), alias=\"population\", split=split)\n\nstar_schema.join(\n    driver_standings,\n    on=[\"driverId\"],\n    time_stamps=\"date\",\n    horizon=getml.data.time.days(1),\n    lagged_targets=True,\n)\n\n# We cannot use lap times for the race\n# we would like to predict, so we set\n# a non-zero horizon.\nstar_schema.join(\n    lap_times,\n    on=[\"driverId\"],\n    time_stamps=\"date\",\n    horizon=getml.data.time.days(1),\n)\n\n# We cannot use pit stops for the race\n# we would like to predict, so we set\n# a non-zero horizon.\nstar_schema.join(\n    pit_stops,\n    on=[\"driverId\"],\n    time_stamps=\"date\",\n    horizon=getml.data.time.days(1),\n)\n\nstar_schema.join(\n    qualifying,\n    on=[\"driverId\", \"raceId\"],\n    relationship=getml.data.relationship.many_to_one,\n)\n\nstar_schema.join(\n    drivers,\n    on=[\"driverId\"],\n    relationship=getml.data.relationship.many_to_one,\n)\n\nstar_schema\n</pre> star_schema = getml.data.StarSchema(population=driver_standings.drop([\"position\"]), alias=\"population\", split=split)  star_schema.join(     driver_standings,     on=[\"driverId\"],     time_stamps=\"date\",     horizon=getml.data.time.days(1),     lagged_targets=True, )  # We cannot use lap times for the race # we would like to predict, so we set # a non-zero horizon. star_schema.join(     lap_times,     on=[\"driverId\"],     time_stamps=\"date\",     horizon=getml.data.time.days(1), )  # We cannot use pit stops for the race # we would like to predict, so we set # a non-zero horizon. star_schema.join(     pit_stops,     on=[\"driverId\"],     time_stamps=\"date\",     horizon=getml.data.time.days(1), )  star_schema.join(     qualifying,     on=[\"driverId\", \"raceId\"],     relationship=getml.data.relationship.many_to_one, )  star_schema.join(     drivers,     on=[\"driverId\"],     relationship=getml.data.relationship.many_to_one, )  star_schema Out[21]: data model diagram driver_standingslap_timespit_stopsqualifyingdriverspopulationdriverId = driverIddate &lt;= dateHorizon: 1.0 daysLagged targets alloweddriverId = driverIddate &lt;= dateHorizon: 1.0 daysdriverId = driverIddate &lt;= dateHorizon: 1.0 daysdriverId = driverIdraceId = raceIdRelationship: many-to-onedriverId = driverIdRelationship: many-to-one staging data frames                     staging table                    0 population, qualifying, drivers POPULATION__STAGING_TABLE_1 1 driver_standings DRIVER_STANDINGS__STAGING_TABLE_2 2 lap_times LAP_TIMES__STAGING_TABLE_3 3 pit_stops PIT_STOPS__STAGING_TABLE_4 container population subset name              rows type 0 test driver_standings 6229 View 1 train driver_standings 25349 View peripheral name               rows type      0 driver_standings 31578 DataFrame 1 lap_times 420369 DataFrame 2 pit_stops 6070 DataFrame 3 qualifying 7397 DataFrame 4 drivers 840 DataFrame <p>Set-up the feature learner &amp; predictor</p> <p>We use the relboost algorithms for this problem. Because of the large number of keywords, we regularize the model a bit by requiring a minimum support for the keywords (<code>min_num_samples</code>).</p> In\u00a0[22]: Copied! <pre>fast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    aggregation=getml.feature_learning.FastProp.agg_sets.All,\n    num_threads=1,\n)\n\npredictor = getml.predictors.XGBoostClassifier(n_jobs=1)\n</pre> fast_prop = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     aggregation=getml.feature_learning.FastProp.agg_sets.All,     num_threads=1, )  predictor = getml.predictors.XGBoostClassifier(n_jobs=1) <p>Build the pipeline</p> In\u00a0[23]: Copied! <pre>pipe1 = getml.pipeline.Pipeline(\n    tags=['fast_prop'],\n    data_model=star_schema.data_model,\n    feature_learners=[fast_prop],\n    predictors=[predictor],\n    include_categorical=True,\n)\n\npipe1\n</pre> pipe1 = getml.pipeline.Pipeline(     tags=['fast_prop'],     data_model=star_schema.data_model,     feature_learners=[fast_prop],     predictors=[predictor],     include_categorical=True, )  pipe1 Out[23]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='CrossEntropyLoss',\n         peripheral=['driver_standings', 'drivers', 'lap_times', 'pit_stops', 'qualifying'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['fast_prop'])</pre> In\u00a0[24]: Copied! <pre>pipe1.check(star_schema.train)\n</pre> pipe1.check(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 2 issues labeled INFO and 0 issues labeled WARNING.\n</pre> Out[24]: type label                  message                          0 INFO FOREIGN KEYS NOT FOUND When joining POPULATION__STAGING_TABLE_1 and LAP_TIMES__STAGING_TABLE_3 over 'driverId' and 'driverId', there are no corresponding entries for 68.551028% of entries in 'driverId' in 'POPULATION__STAGING_TABLE_1'. You might want to double-check your join keys. 1 INFO FOREIGN KEYS NOT FOUND When joining POPULATION__STAGING_TABLE_1 and PIT_STOPS__STAGING_TABLE_4 over 'driverId' and 'driverId', there are no corresponding entries for 82.527910% of entries in 'driverId' in 'POPULATION__STAGING_TABLE_1'. You might want to double-check your join keys. In\u00a0[25]: Copied! <pre>pipe1.fit(star_schema.train)\n</pre> pipe1.fit(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 2 issues labeled INFO and 0 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 664 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 01:19, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:22, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:11, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:1m:54.191233\n\n</pre> Out[25]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='CrossEntropyLoss',\n         peripheral=['driver_standings', 'drivers', 'lap_times', 'pit_stops', 'qualifying'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['fast_prop', 'container-Ml8asS'])</pre> In\u00a0[26]: Copied! <pre>pipe1.score(star_schema.test)\n</pre> pipe1.score(star_schema.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:05, remaining: 00:00]          \n\n</pre> Out[26]: date time           set used target accuracy     auc cross entropy 0 2024-08-07 16:31:54 train win 0.9714 0.9448 0.0825 1 2024-08-07 16:31:59 test win 0.9714 0.9165 0.08801 In\u00a0[27]: Copied! <pre>names, importances = pipe1.features.importances(target_num=0)\n\nplt.subplots(figsize=(20, 10))\n\nplt.bar(names[:30], importances[:30], color='#6829c2')\n\nplt.title(\"feature importances\")\nplt.grid(True)\nplt.xlabel(\"column\")\nplt.ylabel(\"importance\")\nplt.xticks(rotation='vertical')\n\nplt.show()\n</pre> names, importances = pipe1.features.importances(target_num=0)  plt.subplots(figsize=(20, 10))  plt.bar(names[:30], importances[:30], color='#6829c2')  plt.title(\"feature importances\") plt.grid(True) plt.xlabel(\"column\") plt.ylabel(\"importance\") plt.xticks(rotation='vertical')  plt.show() <p>We take a look at the most important features, to get an idea where the predictive power comes from:</p> In\u00a0[28]: Copied! <pre>pipe1.features.to_sql().find(names[0])[0]\n</pre> pipe1.features.to_sql().find(names[0])[0] Out[28]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_50\";\n\nCREATE TABLE \"FEATURE_1_50\" AS\nSELECT EWMA_90D( t2.\"win\" ORDER BY t1.\"date\" - t2.\"date, '+1.000000 days'\" ) AS \"feature_1_50\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"DRIVER_STANDINGS__STAGING_TABLE_2\" t2\nON t1.\"driverid\" = t2.\"driverid\"\nWHERE t2.\"date, '+1.000000 days'\" &lt;= t1.\"date\"\nGROUP BY t1.rowid;\n</pre> In\u00a0[29]: Copied! <pre>pipe1.features.to_sql()[names[1]]\n</pre> pipe1.features.to_sql()[names[1]] Out[29]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_51\";\n\nCREATE TABLE \"FEATURE_1_51\" AS\nSELECT EWMA_365D( t2.\"win\" ORDER BY t1.\"date\" - t2.\"date, '+1.000000 days'\" ) AS \"feature_1_51\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"DRIVER_STANDINGS__STAGING_TABLE_2\" t2\nON t1.\"driverid\" = t2.\"driverid\"\nWHERE t2.\"date, '+1.000000 days'\" &lt;= t1.\"date\"\nGROUP BY t1.rowid;\n</pre> In\u00a0[30]: Copied! <pre>pipe1.features.to_sql()[names[2]]\n</pre> pipe1.features.to_sql()[names[2]] Out[30]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_9\";\n\nCREATE TABLE \"FEATURE_1_9\" AS\nSELECT EWMA_7D( t2.\"position\" ORDER BY t1.\"date\" - t2.\"date, '+1.000000 days'\" ) AS \"feature_1_9\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"DRIVER_STANDINGS__STAGING_TABLE_2\" t2\nON t1.\"driverid\" = t2.\"driverid\"\nWHERE t2.\"date, '+1.000000 days'\" &lt;= t1.\"date\"\nGROUP BY t1.rowid;\n</pre> In\u00a0[31]: Copied! <pre>getml.engine.shutdown()\n</pre> getml.engine.shutdown()","boost":0.9},{"location":"examples/community-notebooks/formula1/#formula-1-predicting-the-winner-of-a-race","title":"Formula 1 - Predicting the winner of a race\u00b6","text":"<p>In this notebook we will use getML to predict the winner of a Formula 1 race.</p> <p>Summary:</p> <ul> <li>Prediction type: Classification model</li> <li>Domain: Sports</li> <li>Prediction target: Win</li> <li>Population size: 31578</li> </ul> <p>Author: Dr. Patrick Urbanke</p>","boost":0.9},{"location":"examples/community-notebooks/formula1/#background","title":"Background\u00b6","text":"<p>We would like to develop a prediction model for Formula 1 races, that would allow us to predict the winner of a race before the race has started.</p> <p>We use dataset of all Formula 1 races from 1950 to 2017. The dataset includes information such as the time taken in each lap, the time taken for pit stops, the performance in the qualifying rounds etc.</p> <p>The dataset has been downloaded from the CTU Prague relational learning repository (Motl and Schulte, 2015).</p>","boost":0.9},{"location":"examples/community-notebooks/formula1/#analysis","title":"Analysis\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/formula1/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/formula1/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>We begin by downloading the data:</p>","boost":0.9},{"location":"examples/community-notebooks/formula1/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/formula1/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"<p>We loaded the data and defined the roles and units. Next, we create a getML pipeline for relational learning.</p>","boost":0.9},{"location":"examples/community-notebooks/formula1/#21-define-relational-model","title":"2.1 Define relational model\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/formula1/#22-getml-pipeline","title":"2.2 getML pipeline\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/formula1/#23-model-training","title":"2.3 Model training\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/formula1/#24-model-evaluation","title":"2.4 Model evaluation\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/formula1/#25-studying-features","title":"2.5 Studying features\u00b6","text":"<p>We take a look at the importance of the features FastProp has learned:</p>","boost":0.9},{"location":"examples/community-notebooks/formula1/#3-conclusion","title":"3. Conclusion\u00b6","text":"<p>What we can learn from these features is the following: Knowing which driver we are talking about and who won the most recent races is the best predictor for whether a driver will win this race.</p>","boost":0.9},{"location":"examples/community-notebooks/formula1/#references","title":"References\u00b6","text":"<p>Motl, Jan, and Oliver Schulte. \"The CTU prague relational learning repository.\" arXiv preprint arXiv:1511.03086 (2015).</p>","boost":0.9},{"location":"examples/community-notebooks/interstate94/","title":"<span class=\"ntitle\">interstate94.ipynb</span> <span class=\"ndesc\">Multivariate time series prediction</span>","text":"<p>Let's get started with the analysis and set-up your session:</p> In\u00a0[1]: Copied! <pre>%pip install -q \"getml==1.4.0\" \"numpy&lt;2.0.0\" \"matplotlib~=3.9\"\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport getml\n\nprint(f\"getML API version: {getml.__version__}\\n\")\n\ngetml.engine.launch()\ngetml.engine.set_project(\"interstate94\")\n</pre> %pip install -q \"getml==1.4.0\" \"numpy&lt;2.0.0\" \"matplotlib~=3.9\"  import matplotlib.pyplot as plt %matplotlib inline  import getml  print(f\"getML API version: {getml.__version__}\\n\")  getml.engine.launch() getml.engine.set_project(\"interstate94\") <pre>Note: you may need to restart the kernel to use updated packages.\ngetML API version: 1.4.0\n\nLaunching ./getML --allow-push-notifications=true --allow-remote-ips=false --home-directory=/home/alex/.local/lib/python3.10/site-packages/getml --in-memory=true --install=false --launch-browser=true --log=false in /home/alex/.local/lib/python3.10/site-packages/getml/.getML/getml-1.4.0-x64-community-edition-linux...\nLaunched the getML engine. The log output will be stored in /home/alex/.getML/logs/20240807163727.log.\n\nConnected to project 'interstate94'\n</pre> In\u00a0[2]: Copied! <pre>traffic = getml.datasets.load_interstate94(roles=False, units=False)\n</pre> traffic = getml.datasets.load_interstate94(roles=False, units=False) <pre>\nLoading traffic...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n</pre> <p>The dataset comes with its own seasonal components. However, we choose not to use them, because we want to demonstrate our seasonal preprocessor.</p> In\u00a0[3]: Copied! <pre>traffic.set_role(\"ds\", getml.data.roles.time_stamp)\ntraffic.set_role(\"holiday\", getml.data.roles.categorical)\ntraffic.set_role(\"traffic_volume\", getml.data.roles.target)\n\ntraffic\n</pre> traffic.set_role(\"ds\", getml.data.roles.time_stamp) traffic.set_role(\"holiday\", getml.data.roles.categorical) traffic.set_role(\"traffic_volume\", getml.data.roles.target)  traffic Out[3]:  name                          ds traffic_volume holiday               hour      weekday          day        month         year  role                  time_stamp         target categorical   unused_float unused_float unused_float unused_float unused_float  unit time stamp, comparison only 0 2016-01-01 1513 New Years Day 0 4 1 1 2016 1 2016-01-01 01:00:00 1550 New Years Day 1 4 1 1 2016 2 2016-01-01 02:00:00 993 New Years Day 2 4 1 1 2016 3 2016-01-01 03:00:00 719 New Years Day 3 4 1 1 2016 4 2016-01-01 04:00:00 533 New Years Day 4 4 1 1 2016 ... ... ... ... ... ... ... ... 24091 2018-09-30 19:00:00 3543 No holiday 19 6 30 9 2018 24092 2018-09-30 20:00:00 2781 No holiday 20 6 30 9 2018 24093 2018-09-30 21:00:00 2159 No holiday 21 6 30 9 2018 24094 2018-09-30 22:00:00 1450 No holiday 22 6 30 9 2018 24095 2018-09-30 23:00:00 954 No holiday 23 6 30 9 2018 <p>     24096 rows x 8 columns     memory usage: 1.45 MB     name: traffic     type: getml.DataFrame </p> <p>Data visualization</p> <p>The first week of the original traffic time series is plotted below.</p> In\u00a0[4]: Copied! <pre>col_data = \"black\"\ncol_getml = \"darkviolet\"\n</pre> col_data = \"black\" col_getml = \"darkviolet\" In\u00a0[5]: Copied! <pre>fig, ax = plt.subplots(figsize=(20, 10))\n\n# 2016/01/01 was a friday, we'd like to start the visualizations on a monday\nstart = 72\nend = 72 + 168\n\nfig.suptitle(\n    \"Traffic volume for first full week of the training set\",\n    fontsize=14,\n    fontweight=\"bold\",\n)\nax.plot(\n    traffic[\"ds\"].to_numpy()[start:end],\n    traffic[\"traffic_volume\"].to_numpy()[start:end],\n    color=col_data,\n)\n</pre> fig, ax = plt.subplots(figsize=(20, 10))  # 2016/01/01 was a friday, we'd like to start the visualizations on a monday start = 72 end = 72 + 168  fig.suptitle(     \"Traffic volume for first full week of the training set\",     fontsize=14,     fontweight=\"bold\", ) ax.plot(     traffic[\"ds\"].to_numpy()[start:end],     traffic[\"traffic_volume\"].to_numpy()[start:end],     color=col_data, ) Out[5]: <pre>[&lt;matplotlib.lines.Line2D at 0x7fe3f2e911b0&gt;]</pre> <p>Traffic: population table</p> <p>To allow the algorithm to capture seasonal information, we include time components (such as the day of the week) as categorical variables. Note that we could have also used getML's Seasonal preprocessor (<code>getml.prepreprocessors.Seasonal()</code>), but in this case the information was already included in the dataset.</p> <p>Train/test split</p> <p>We use getML's split functionality to retrieve a lazily evaluated split column, that we can supply to the time series api below.</p> In\u00a0[6]: Copied! <pre>split = getml.data.split.time(traffic, \"ds\", test=getml.data.time.datetime(2018, 3, 15))\n</pre> split = getml.data.split.time(traffic, \"ds\", test=getml.data.time.datetime(2018, 3, 15)) <p>Split columns are columns of mere strings that can be used to subset the data by forming bolean conditions over them:</p> In\u00a0[7]: Copied! <pre>traffic[split == \"test\"]\n</pre> traffic[split == \"test\"] Out[7]: name                          ds traffic_volume holiday             hour      weekday          day        month         year role                  time_stamp         target categorical unused_float unused_float unused_float unused_float unused_float unit time stamp, comparison only 0 2018-03-15 577 No holiday 0 3 15 3 2018 1 2018-03-15 01:00:00 354 No holiday 1 3 15 3 2018 2 2018-03-15 02:00:00 259 No holiday 2 3 15 3 2018 3 2018-03-15 03:00:00 360 No holiday 3 3 15 3 2018 4 2018-03-15 04:00:00 910 No holiday 4 3 15 3 2018 ... ... ... ... ... ... ... ... ... <p>     unknown number of rows          type: getml.data.View </p> In\u00a0[8]: Copied! <pre>time_series = getml.data.TimeSeries(\n    population=traffic,\n    split=split,\n    time_stamps=\"ds\",\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.days(7),\n    lagged_targets=True,\n)\n\ntime_series\n</pre> time_series = getml.data.TimeSeries(     population=traffic,     split=split,     time_stamps=\"ds\",     horizon=getml.data.time.hours(1),     memory=getml.data.time.days(7),     lagged_targets=True, )  time_series Out[8]: data model diagram trafficpopulationds &lt;= dsMemory: 7.0 daysHorizon: 1.0 hoursLagged targets allowed staging data frames staging table               0 population POPULATION__STAGING_TABLE_1 1 traffic TRAFFIC__STAGING_TABLE_2 container population subset name     rows type 0 test traffic 4800 View 1 train traffic 19296 View peripheral name     rows type      0 traffic 24096 DataFrame <p>Set-up of feature learners, selectors &amp; predictor</p> In\u00a0[9]: Copied! <pre># The Seasonal preprocessor extracts seasonal\n# components from the time stamps.\nseasonal = getml.preprocessors.Seasonal()\n\nfast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    num_threads=1,    \n    num_features=20,\n)\n\npredictor = getml.predictors.XGBoostRegressor()\n</pre> # The Seasonal preprocessor extracts seasonal # components from the time stamps. seasonal = getml.preprocessors.Seasonal()  fast_prop = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.SquareLoss,     num_threads=1,         num_features=20, )  predictor = getml.predictors.XGBoostRegressor() <p>Build the pipeline</p> In\u00a0[10]: Copied! <pre>pipe = getml.pipeline.Pipeline(\n    tags=[\"memory: 7d\", \"horizon: 1h\", \"fast_prop\"],\n    data_model=time_series.data_model,\n    preprocessors=[seasonal],\n    feature_learners=[fast_prop],\n    predictors=[predictor],\n)\n</pre> pipe = getml.pipeline.Pipeline(     tags=[\"memory: 7d\", \"horizon: 1h\", \"fast_prop\"],     data_model=time_series.data_model,     preprocessors=[seasonal],     feature_learners=[fast_prop],     predictors=[predictor], ) In\u00a0[11]: Copied! <pre>pipe.fit(time_series.train)\n</pre> pipe.fit(time_series.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 373 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:10, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:11.394631\n\n</pre> Out[11]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=['traffic'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=['Seasonal'],\n         share_selected_features=0.5,\n         tags=['memory: 7d', 'horizon: 1h', 'fast_prop', 'container-DNncpJ'])</pre> In\u00a0[12]: Copied! <pre>pipe.score(time_series.test)\n</pre> pipe.score(time_series.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[12]: date time           set used target               mae      rmse rsquared 0 2024-08-07 16:37:41 train traffic_volume 200.4302 299.2045 0.9768 1 2024-08-07 16:37:41 test traffic_volume 179.9515 269.631 0.9816 <p>Feature correlations</p> <p>Correlations of the calculated features with the target</p> In\u00a0[13]: Copied! <pre>names, correlations = pipe.features.correlations()\n\nplt.subplots(figsize=(20, 10))\n\nplt.bar(names, correlations, color=col_getml)\nplt.title(\"Feature Correlations\")\nplt.xlabel(\"Features\")\nplt.ylabel(\"Correlations\")\nplt.xticks(rotation=\"vertical\")\nplt.show()\n</pre> names, correlations = pipe.features.correlations()  plt.subplots(figsize=(20, 10))  plt.bar(names, correlations, color=col_getml) plt.title(\"Feature Correlations\") plt.xlabel(\"Features\") plt.ylabel(\"Correlations\") plt.xticks(rotation=\"vertical\") plt.show() <p>Feature importances</p> In\u00a0[14]: Copied! <pre>names, importances = pipe.features.importances()\n\nplt.subplots(figsize=(20, 10))\n\nplt.bar(names, importances, color=col_getml)\nplt.title(\"Feature Importances\")\nplt.xlabel(\"Features\")\nplt.ylabel(\"Importances\")\nplt.xticks(rotation=\"vertical\")\nplt.show()\n</pre> names, importances = pipe.features.importances()  plt.subplots(figsize=(20, 10))  plt.bar(names, importances, color=col_getml) plt.title(\"Feature Importances\") plt.xlabel(\"Features\") plt.ylabel(\"Importances\") plt.xticks(rotation=\"vertical\") plt.show() <p>Visualizing the learned features</p> <p>We can also transpile the features as SQL code. Here, we show the most important feature.</p> In\u00a0[15]: Copied! <pre>by_importance = pipe.features.sort(by=\"importance\")\nby_importance[0].sql\n</pre> by_importance = pipe.features.sort(by=\"importance\") by_importance[0].sql Out[15]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_4\";\n\nCREATE TABLE \"FEATURE_1_4\" AS\nSELECT FIRST( t2.\"traffic_volume\" ORDER BY t2.\"ds, '+1.000000 hours'\" ) AS \"feature_1_4\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"TRAFFIC__STAGING_TABLE_2\" t2\nON 1 = 1\nWHERE t2.\"ds, '+1.000000 hours'\" &lt;= t1.\"ds\"\nAND ( t2.\"ds, '+7.041667 days'\" &gt; t1.\"ds\" OR t2.\"ds, '+7.041667 days'\" IS NULL )\nAND t1.\"hour( ds )\" = t2.\"hour( ds )\"\nGROUP BY t1.rowid;\n</pre> <p>Plot predictions &amp; traffic volume vs. time</p> <p>We now plot the predictions against the observed values of the target for the first 7 days of the testing set. You can see that the predictions closely follows the original series. FastProp was able to identify certain patterns in the series, including:</p> <ul> <li>Day and night separation</li> <li>The daily commuting peeks (on weekdays)</li> <li>The decline on weekends</li> </ul> In\u00a0[16]: Copied! <pre>predictions = pipe.predict(time_series.test)\n</pre> predictions = pipe.predict(time_series.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> In\u00a0[17]: Copied! <pre>fig, ax = plt.subplots(figsize=(20, 10))\n\n# the test set starts at 2018/03/15 \u2013 a thursday; we introduce an offset to, once again, start on a monday\nstart = 96\nend = 96 + 168\n\nactual = time_series.test.population[start:end].to_pandas()\npredicted = predictions[start:end]\n\nax.plot(actual[\"ds\"], actual[\"traffic_volume\"], color=col_data, label=\"Actual\")\nax.plot(actual[\"ds\"], predicted, color=col_getml, label=\"Predicted\")\nfig.suptitle(\n    \"Predicted vs. actual traffic volume for first full week of testing set\",\n    fontsize=14,\n    fontweight=\"bold\",\n)\nfig.legend()\n</pre> fig, ax = plt.subplots(figsize=(20, 10))  # the test set starts at 2018/03/15 \u2013 a thursday; we introduce an offset to, once again, start on a monday start = 96 end = 96 + 168  actual = time_series.test.population[start:end].to_pandas() predicted = predictions[start:end]  ax.plot(actual[\"ds\"], actual[\"traffic_volume\"], color=col_data, label=\"Actual\") ax.plot(actual[\"ds\"], predicted, color=col_getml, label=\"Predicted\") fig.suptitle(     \"Predicted vs. actual traffic volume for first full week of testing set\",     fontsize=14,     fontweight=\"bold\", ) fig.legend() Out[17]: <pre>&lt;matplotlib.legend.Legend at 0x7fe3f29a40d0&gt;</pre> In\u00a0[18]: Copied! <pre>pipe.features.to_sql()[pipe.features.sort(by=\"importances\")[0].name]\n</pre> pipe.features.to_sql()[pipe.features.sort(by=\"importances\")[0].name] Out[18]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_4\";\n\nCREATE TABLE \"FEATURE_1_4\" AS\nSELECT FIRST( t2.\"traffic_volume\" ORDER BY t2.\"ds, '+1.000000 hours'\" ) AS \"feature_1_4\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"TRAFFIC__STAGING_TABLE_2\" t2\nON 1 = 1\nWHERE t2.\"ds, '+1.000000 hours'\" &lt;= t1.\"ds\"\nAND ( t2.\"ds, '+7.041667 days'\" &gt; t1.\"ds\" OR t2.\"ds, '+7.041667 days'\" IS NULL )\nAND t1.\"hour( ds )\" = t2.\"hour( ds )\"\nGROUP BY t1.rowid;\n</pre> In\u00a0[19]: Copied! <pre>getml.engine.shutdown()\n</pre> getml.engine.shutdown()","boost":0.9},{"location":"examples/community-notebooks/interstate94/#interstate-94-multivariate-time-series-prediction","title":"Interstate 94 - Multivariate time series prediction\u00b6","text":"<p>In this tutorial, we demonstrate a time series application of getML. We predict the hourly traffic volume on I-94 westbound from Minneapolis-St Paul.</p> <p>Summary:</p> <ul> <li>Prediction type: Regression model</li> <li>Domain: Transportation</li> <li>Prediction target: Hourly traffic volume</li> <li>Source data: Multivariate time series, 5 components</li> <li>Population size: 24096</li> </ul> <p>Author: S\u00f6ren Nikolaus</p>","boost":0.9},{"location":"examples/community-notebooks/interstate94/#background","title":"Background\u00b6","text":"<p>The dataset features some particularly interesting characteristics common for time series, which classical models may struggle to deal with appropriately. Such characteristics are:</p> <ul> <li>High frequency (hourly)</li> <li>Dependence on irregular events (holidays)</li> <li>Strong and overlapping cycles (daily, weekly)</li> <li>Anomalies</li> <li>Multiple seasonalities</li> </ul> <p>The analysis is built on top of a dataset provided by the MN Department of Transportation, with some data preparation done by John Hogue.</p>","boost":0.9},{"location":"examples/community-notebooks/interstate94/#analysis","title":"Analysis\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/interstate94/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/interstate94/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>Downloading the raw data and convert it into a prediction ready format takes time. To get to the getML model building as fast as possible, we prepared the data for you and excluded the code from this notebook. It is made available in the example notebook featuring the full analysis. We only include data after 2016 and introduced a fixed train/test split at 80% of the available data.</p>","boost":0.9},{"location":"examples/community-notebooks/interstate94/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"<p>The <code>getml.datasets.load_interstate94</code> method took care of the entire data preparation:</p> <ul> <li>Downloads csv's from our servers into python</li> <li>Converts csv's to getML DataFrames</li> <li>Sets roles &amp; units to columns inside getML DataFrames</li> </ul>","boost":0.9},{"location":"examples/community-notebooks/interstate94/#13-define-relational-model","title":"1.3 Define relational model\u00b6","text":"<p>To start with relational learning, we need to specify the data model. We manually replicate the appropriate time series structure by setting time series related join conditions (<code>horizon</code>, <code>memory</code> and <code>allow_lagged_targets</code>). We use the high-level time series api for this.</p> <p>Under the hood, the time series api abstracts away a self cross join of the population table (<code>traffic</code>) that allows getML's feature learning algorithms to learn patterns from past observations.</p>","boost":0.9},{"location":"examples/community-notebooks/interstate94/#2predictive-modeling","title":"2.Predictive modeling\u00b6","text":"<p>We loaded the data, defined the roles, units and the abstract data model. Next, we create a getML pipeline for relational learning.</p>","boost":0.9},{"location":"examples/community-notebooks/interstate94/#21-getml-pipeline","title":"2.1 getML Pipeline\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/interstate94/#22-model-training","title":"2.2 Model training\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/interstate94/#23-model-evaluation","title":"2.3 Model evaluation\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/interstate94/#24-studying-features","title":"2.4 Studying features\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/interstate94/#25-features","title":"2.5 Features\u00b6","text":"<p>The most important feature looks as follows:</p>","boost":0.9},{"location":"examples/community-notebooks/interstate94/#3-conclusion","title":"3. Conclusion\u00b6","text":"<p>In this notebook, we demonstrated a comprehensive approach to predicting hourly traffic volume on Interstate 94 westbound from Minneapolis-St Paul using the getML library. We covered the following steps:</p> <ol> <li><p>Background and Data Preparation:</p> <ul> <li>Introduced the dataset and its characteristics.</li> <li>Loaded and prepared the data using <code>getml.datasets.load_interstate94</code>.</li> </ul> </li> <li><p>Data Visualization:</p> <ul> <li>Visualized the first week of traffic volume data to understand the patterns and trends.</li> </ul> </li> <li><p>Data Modeling:</p> <ul> <li>Defined roles and units for the dataset columns.</li> <li>Created a time series model using <code>getml.data.TimeSeries</code> to capture temporal dependencies.</li> </ul> </li> <li><p>Predictive Modeling:</p> <ul> <li>Built a getML pipeline for relational learning to predict traffic volume.</li> </ul> </li> </ol> <p>By leveraging getML's capabilities, we efficiently handled the complexities of time series data, including high frequency, irregular events, and multiple seasonalities. This approach can be extended to other time series prediction tasks in various domains.</p>","boost":0.9},{"location":"examples/community-notebooks/loans/","title":"<span class=\"ntitle\">loans.ipynb</span> <span class=\"ndesc\">Predicting loan default risk</span>","text":"<p>Let's get started with the analysis and set-up your session:</p> In\u00a0[1]: Copied! <pre>%pip install -q \"getml==1.4.0\" \"numpy&lt;2.0.0\" \"matplotlib~=3.9\"\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport getml\n\ngetml.engine.launch()\ngetml.engine.set_project(\"loans\")\n</pre> %pip install -q \"getml==1.4.0\" \"numpy&lt;2.0.0\" \"matplotlib~=3.9\"  import matplotlib.pyplot as plt %matplotlib inline  import getml  getml.engine.launch() getml.engine.set_project(\"loans\") <pre>Note: you may need to restart the kernel to use updated packages.\nLaunching ./getML --allow-push-notifications=true --allow-remote-ips=false --home-directory=/home/alex/.local/lib/python3.10/site-packages/getml --in-memory=true --install=false --launch-browser=true --log=false in /home/alex/.local/lib/python3.10/site-packages/getml/.getML/getml-1.4.0-x64-community-edition-linux...\nLaunched the getML engine. The log output will be stored in /home/alex/.getML/logs/20240807164035.log.\n\nConnected to project 'loans'\n</pre> In\u00a0[2]: Copied! <pre>population_train, population_test, order, trans, meta = getml.datasets.load_loans(roles=True, units=True)\n</pre> population_train, population_test, order, trans, meta = getml.datasets.load_loans(roles=True, units=True) <pre>\nLoading population_train...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nLoading population_test...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nLoading order...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nLoading trans...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:03, remaining: 00:00]          \n\nLoading meta...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n</pre> In\u00a0[3]: Copied! <pre>population_train.set_role(\"date_loan\", \"time_stamp\")\npopulation_test.set_role(\"date_loan\", \"time_stamp\")\n</pre> population_train.set_role(\"date_loan\", \"time_stamp\") population_test.set_role(\"date_loan\", \"time_stamp\") In\u00a0[4]: Copied! <pre>population_test\n</pre> population_test Out[4]: name                   date_loan account_id default frequency           duration  payments    amount      loan_id  district_id date_account status        role                  time_stamp   join_key  target categorical        numerical numerical numerical unused_float unused_float unused_float unused_string unit time stamp, comparison only     money 0 1996-04-29 19 1 POPLATEK MESICNE 12 2523 30276 4961 21 1995 B 1 1998-10-14 37 1 POPLATEK MESICNE 60 5308 318480 4967 20 1997 D 2 1998-04-19 38 0 POPLATEK TYDNE 48 2307 110736 4968 19 1997 C 3 1997-08-10 97 0 POPLATEK MESICNE 12 8573 102876 4986 74 1996 A 4 1996-11-06 132 0 POPLATEK PO OBRATU 12 7370 88440 4996 40 1996 A ... ... ... ... ... ... ... ... ... ... ... 218 1995-12-04 11042 0 POPLATEK MESICNE 36 6032 217152 7243 72 1995 A 219 1996-08-20 11054 0 POPLATEK TYDNE 60 2482 148920 7246 59 1996 C 220 1994-01-31 11111 0 POPLATEK MESICNE 36 3004 108144 7259 1 1993 A 221 1998-11-22 11317 0 POPLATEK MESICNE 60 5291 317460 7292 50 1997 C 222 1996-12-27 11362 0 POPLATEK MESICNE 24 5392 129408 7308 67 1995 A <p>     223 rows x 11 columns     memory usage: 0.02 MB     name: population_test     type: getml.DataFrame </p> In\u00a0[5]: Copied! <pre>order\n</pre> order Out[5]: name account_id bank_to     k_symbol       amount   account_to     order_id role   join_key categorical categorical numerical unused_float unused_float unit     money 0 1 YZ SIPO 2452 87144583 29401 1 2 ST UVER 3372.7 89597016 29402 2 2 QR SIPO 7266 13943797 29403 3 3 WX SIPO 1135 83084338 29404 4 3 CD NULL 327 24485939 29405 ... ... ... ... ... ... 6466 11362 YZ SIPO 4780 70641225 46334 6467 11362 MN NULL 56 78507822 46335 6468 11362 ST POJISTNE 330 40799850 46336 6469 11362 KL NULL 129 20009470 46337 6470 11362 MN UVER 5392 61540514 46338 <p>     6471 rows x 6 columns     memory usage: 0.23 MB     name: order     type: getml.DataFrame </p> In\u00a0[6]: Copied! <pre>trans\n</pre> trans Out[6]:    name                        date account_id type        k_symbol    bank        operation        amount   balance     trans_id account          role                  time_stamp   join_key categorical categorical categorical categorical   numerical numerical unused_float unused_string    unit time stamp, comparison only     money 0 1995-03-24 1 PRIJEM NULL NULL VKLAD 1000 1000 1 NULL 1 1995-04-13 1 PRIJEM NULL AB PREVOD Z UCTU 3679 4679 5 41403269.0 2 1995-05-13 1 PRIJEM NULL AB PREVOD Z UCTU 3679 20977 6 41403269.0 3 1995-06-13 1 PRIJEM NULL AB PREVOD Z UCTU 3679 26835 7 41403269.0 4 1995-07-13 1 PRIJEM NULL AB PREVOD Z UCTU 3679 30415 8 41403269.0 ... ... ... ... ... ... ... ... ... ... 1056315 1998-08-31 10451 PRIJEM UROK NULL NULL 62 17300 3682983 NULL 1056316 1998-09-30 10451 PRIJEM UROK NULL NULL 49 13442 3682984 NULL 1056317 1998-10-31 10451 PRIJEM UROK NULL NULL 34 10118 3682985 NULL 1056318 1998-11-30 10451 PRIJEM UROK NULL NULL 26 8398 3682986 NULL 1056319 1998-12-31 10451 PRIJEM UROK NULL NULL 42 13695 3682987 NULL <p>     1056320 rows x 10 columns     memory usage: 67.20 MB     name: trans     type: getml.DataFrame </p> In\u00a0[7]: Copied! <pre>meta\n</pre> meta Out[7]: name account_id type_disp   type_card   gender      A3                     A4        A5        A6        A7        A8        A9       A10       A11       A12       A13       A14       A15       A16      disp_id    client_id   birth_date  district_id card_id       issued        A2              role   join_key categorical categorical categorical categorical     numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical unused_float unused_float unused_float unused_float unused_string unused_string unused_string   0 1 OWNER F south Bohemia 70699 60 13 2 1 4 65.3 8968 2.8 3.35 131 1740 1910 1 1 1970 18 NULL NULL Pisek 1 2 OWNER M Prague 1204953 0 0 0 1 1 100 12541 0.2 0.43 167 85677 99107 2 2 1945 1 NULL NULL Hl.m. Praha 2 2 DISPONENT F Prague 1204953 0 0 0 1 1 100 12541 0.2 0.43 167 85677 99107 3 3 1940 1 NULL NULL Hl.m. Praha 3 3 OWNER M central Bohemia 95616 65 30 4 1 6 51.4 9307 3.8 4.43 118 2616 3040 4 4 1956 5 NULL NULL Kolin 4 3 DISPONENT F central Bohemia 95616 65 30 4 1 6 51.4 9307 3.8 4.43 118 2616 3040 5 5 1960 5 NULL NULL Kolin ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 5364 11349 OWNER F Prague 1204953 0 0 0 1 1 100 12541 0.2 0.43 167 85677 99107 13647 13955 1945 1 NULL NULL Hl.m. Praha 5365 11349 DISPONENT M Prague 1204953 0 0 0 1 1 100 12541 0.2 0.43 167 85677 99107 13648 13956 1943 1 NULL NULL Hl.m. Praha 5366 11359 OWNER classic M south Moravia 117897 139 28 5 1 6 53.8 8814 4.7 5.74 107 2112 2059 13660 13968 1968 61 1247.0 1995-06-13 Trebic 5367 11362 OWNER F north Moravia 106054 38 25 6 2 6 63.1 8110 5.7 6.55 109 3244 3079 13663 13971 1962 67 NULL NULL Bruntal 5368 11382 OWNER F north Moravia 323870 0 0 0 1 1 100 10673 4.7 5.44 100 18782 18347 13690 13998 1953 74 NULL NULL Ostrava - mesto <p>     5369 rows x 25 columns     memory usage: 1.05 MB     name: meta     type: getml.DataFrame </p> In\u00a0[8]: Copied! <pre>star_schema = getml.data.StarSchema(\n    train=population_train, test=population_test, alias=\"population\"\n)\n\nstar_schema.join(\n    trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\"),\n)\n\nstar_schema.join(\n    order,\n    on=\"account_id\",\n)\n\nstar_schema.join(\n    meta,\n    on=\"account_id\",\n)\n\nstar_schema\n</pre> star_schema = getml.data.StarSchema(     train=population_train, test=population_test, alias=\"population\" )  star_schema.join(     trans,     on=\"account_id\",     time_stamps=(\"date_loan\", \"date\"), )  star_schema.join(     order,     on=\"account_id\", )  star_schema.join(     meta,     on=\"account_id\", )  star_schema Out[8]: data model diagram transordermetapopulationaccount_id = account_iddate &lt;= date_loanaccount_id = account_idaccount_id = account_id staging data frames staging table               0 population POPULATION__STAGING_TABLE_1 1 meta META__STAGING_TABLE_2 2 order ORDER__STAGING_TABLE_3 3 trans TRANS__STAGING_TABLE_4 container population subset name             rows type      0 train population_train 459 DataFrame 1 test population_test 223 DataFrame peripheral name     rows type      0 trans 1056320 DataFrame 1 order 6471 DataFrame 2 meta 5369 DataFrame <p>Set-up of feature learners, selectors &amp; predictor</p> In\u00a0[9]: Copied! <pre>fast_prop = getml.feature_learning.FastProp(\n    aggregation=getml.feature_learning.FastProp.agg_sets.All,\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    num_threads=1,\n)\n\nfeature_selector = getml.predictors.XGBoostClassifier(n_jobs=1)\n\n# the population is really small, so we set gamma to mitigate overfitting\npredictor = getml.predictors.XGBoostClassifier(gamma=2, n_jobs=1,)\n</pre> fast_prop = getml.feature_learning.FastProp(     aggregation=getml.feature_learning.FastProp.agg_sets.All,     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     num_threads=1, )  feature_selector = getml.predictors.XGBoostClassifier(n_jobs=1)  # the population is really small, so we set gamma to mitigate overfitting predictor = getml.predictors.XGBoostClassifier(gamma=2, n_jobs=1,) <p>Build the pipeline</p> In\u00a0[10]: Copied! <pre>pipe = getml.pipeline.Pipeline(\n    data_model=star_schema.data_model,\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=predictor,\n)\n</pre> pipe = getml.pipeline.Pipeline(     data_model=star_schema.data_model,     feature_learners=[fast_prop],     feature_selectors=[feature_selector],     predictors=predictor, ) In\u00a0[11]: Copied! <pre>pipe.fit(star_schema.train)\n</pre> pipe.fit(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 548 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as feature selector... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:0.701545\n\n</pre> Out[11]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=['XGBoostClassifier'],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['meta', 'order', 'trans'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['container-2C6u9O'])</pre> In\u00a0[12]: Copied! <pre>pipe.score(star_schema.test)\n</pre> pipe.score(star_schema.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[12]: date time           set used target  accuracy     auc cross entropy 0 2024-08-07 16:40:56 train default 1.0 1. 0.06515 1 2024-08-07 16:40:56 test default 0.9641 0.9298 0.15548 <p>Visualizing the learned features</p> <p>The feature with the highest importance is:</p> In\u00a0[13]: Copied! <pre>by_importances = pipe.features.sort(by=\"importances\")\nby_importances[0].sql\n</pre> by_importances = pipe.features.sort(by=\"importances\") by_importances[0].sql Out[13]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_39\";\n\nCREATE TABLE \"FEATURE_1_39\" AS\nSELECT Q1( t2.\"balance\" ) AS \"feature_1_39\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"TRANS__STAGING_TABLE_4\" t2\nON t1.\"account_id\" = t2.\"account_id\"\nWHERE t2.\"date\" &lt;= t1.\"date_loan\"\nGROUP BY t1.rowid;\n</pre> <p>Feature correlations</p> <p>We want to analyze how the features are correlated with the target variable.</p> In\u00a0[14]: Copied! <pre>names, correlations = pipe.features[:50].correlations()\n\nfig, ax = plt.subplots(figsize=(20, 10))\n\nax.bar(names, correlations, color=\"#6829c2\")\n\nax.set_title(\"feature correlations\")\nax.set_xlabel(\"feature\")\nax.set_ylabel(\"correlation\")\nax.tick_params(axis=\"x\", rotation=90)\n</pre> names, correlations = pipe.features[:50].correlations()  fig, ax = plt.subplots(figsize=(20, 10))  ax.bar(names, correlations, color=\"#6829c2\")  ax.set_title(\"feature correlations\") ax.set_xlabel(\"feature\") ax.set_ylabel(\"correlation\") ax.tick_params(axis=\"x\", rotation=90) <p>Feature importances</p> <p>Feature importances are calculated by analyzing the improvement in predictive accuracy on each node of the trees in the XGBoost predictor. They are then normalized, so that all importances add up to 100%.</p> In\u00a0[15]: Copied! <pre>names, importances = pipe.features[:50].importances()\n\nfig, ax = plt.subplots(figsize=(20, 10))\n\nax.bar(names, importances, color='#6829c2')\n\nax.set_title(\"feature importances\")\nax.set_xlabel(\"feature\")\nax.set_ylabel(\"importance\")\nax.tick_params(axis=\"x\", rotation=90)\n</pre> names, importances = pipe.features[:50].importances()  fig, ax = plt.subplots(figsize=(20, 10))  ax.bar(names, importances, color='#6829c2')  ax.set_title(\"feature importances\") ax.set_xlabel(\"feature\") ax.set_ylabel(\"importance\") ax.tick_params(axis=\"x\", rotation=90) <p>Column importances</p> <p>Because getML uses relational learning, we can apply the principles we used to calculate the feature importances to individual columns as well.</p> <p>As we can see, a lot of the predictive power stems from the account balance. This is unsurprising: People with less money on their bank accounts are more likely to default on their loans.</p> In\u00a0[16]: Copied! <pre>names, importances = pipe.columns.importances()\n\nfig, ax = plt.subplots(figsize=(20, 10))\n\nax.bar(names, importances, color=\"#6829c2\")\n\nax.set_title(\"column importances\")\nax.set_xlabel(\"column\")\nax.set_ylabel(\"importance\")\nax.tick_params(axis=\"x\", rotation=90)\n</pre> names, importances = pipe.columns.importances()  fig, ax = plt.subplots(figsize=(20, 10))  ax.bar(names, importances, color=\"#6829c2\")  ax.set_title(\"column importances\") ax.set_xlabel(\"column\") ax.set_ylabel(\"importance\") ax.tick_params(axis=\"x\", rotation=90) <p>The most important feature looks as follows:</p> In\u00a0[17]: Copied! <pre>pipe.features.to_sql()[pipe.features.sort(by=\"importances\")[0].name]\n</pre> pipe.features.to_sql()[pipe.features.sort(by=\"importances\")[0].name] Out[17]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_39\";\n\nCREATE TABLE \"FEATURE_1_39\" AS\nSELECT Q1( t2.\"balance\" ) AS \"feature_1_39\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"TRANS__STAGING_TABLE_4\" t2\nON t1.\"account_id\" = t2.\"account_id\"\nWHERE t2.\"date\" &lt;= t1.\"date_loan\"\nGROUP BY t1.rowid;\n</pre> In\u00a0[18]: Copied! <pre>getml.engine.shutdown()\n</pre> getml.engine.shutdown() <p>By applying getML to the PKDD'99 Financial dataset, we were able to show the power and relevance of Relational Learning on a real-world data set. Within a training time below 2 seconds, we outperformed almost all approaches based on manually generated features. This makes getML the prime choice when dealing with complex relational data schemes. This result holds independent of the problem domain since no expertise in the financial sector was used in this analysis.</p> <p>The present analysis could be improved in two directions. By performing an extensive hyperparameter optimization, the out of sample AUC could be further improved. On the other hand, the hyperparameters could be tuned to produce less complex features that result in worse performance (in terms of AUC) but are better interpretable by humans.</p>","boost":0.9},{"location":"examples/community-notebooks/loans/#loans-predicting-loan-default-risk","title":"Loans - Predicting loan default risk\u00b6","text":"<p>This notebook demonstrates the application of our relational learning algorithm to predict if a customer of a bank will default on his loan. We train the predictor on customer metadata, transaction history, as well as other successful and unsuccessful loans.</p> <p>Summary:</p> <ul> <li>Prediction type: Binary classification</li> <li>Domain: Finance</li> <li>Prediction target: Loan default</li> <li>Source data: 8 tables, 78.8 MB</li> <li>Population size: 682</li> </ul> <p>Author: Dr. Johannes King, Dr. Patrick Urbanke</p>","boost":0.9},{"location":"examples/community-notebooks/loans/#background","title":"Background\u00b6","text":"<p>This notebook features a textbook example of predictive analytics applied to the financial sector. A loan is the lending of money to companies or individuals. Banks grant loans in exchange for the promise of repayment. Loan default is defined as the failure to meet this legal obligation, for example, when a home buyer fails to make a mortgage payment. A bank needs to estimate the risk it carries when granting loans to potentially non-performing customers.</p> <p>The analysis is based on the financial dataset from the the CTU Prague Relational Learning Repository (Motl and Schulte, 2015).</p>","boost":0.9},{"location":"examples/community-notebooks/loans/#analysis","title":"Analysis\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/loans/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/loans/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>Downloading the raw data from the CTU Prague Relational Learning Repository into a prediction ready format takes time. To get to the getML model building as fast as possible, we prepared the data for you and excluded the code from this notebook. It will be made available in a future version.</p>","boost":0.9},{"location":"examples/community-notebooks/loans/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"<p>The <code>getml.datasets.load_loans</code> method took care of the entire data lifting:</p> <ul> <li>Downloads csv's from our servers in python</li> <li>Converts csv's to getML DataFrames</li> <li>Sets roles to columns inside getML DataFrames</li> </ul> <p>The only thing left is to set units to columns that the relational learning algorithm is allowed to compare to each other.</p>","boost":0.9},{"location":"examples/community-notebooks/loans/#population-table","title":"Population table\u00b6","text":"<ul> <li>Information on the loan itself (duration, amount, date, ...)</li> <li>Geo-information about the branch where the loans was granted (A**)</li> <li>Column <code>status</code> contains binary target. Levels [A, C] := loan paid back and [B, D] := loan default; we recoded status to our binary target: <code>default</code></li> </ul>","boost":0.9},{"location":"examples/community-notebooks/loans/#peripheral-tables","title":"Peripheral tables\u00b6","text":"<ul> <li><code>meta</code><ul> <li>Meta info about the client (card_type, gender, ...)</li> <li>Geo-information about the client</li> </ul> </li> <li><code>order</code><ul> <li>Permanent orders related to a loan (amount, balance, ...)</li> </ul> </li> <li><code>trans</code><ul> <li>Transactions related to a given loan (amount, ...)</li> </ul> </li> </ul> <p>While the contents of <code>meta</code> and <code>order</code> are omitted for brevity, here are contents of <code>trans</code>:</p>","boost":0.9},{"location":"examples/community-notebooks/loans/#13-define-relational-model","title":"1.3 Define relational model\u00b6","text":"<p>To start with relational learning, we need to specify an abstract data model. Here, we use the high-level star schema API that allows us to define the abstract data model and construct a container with the concrete data at one-go. While a simple <code>StarSchema</code> indeed works in many cases, it is not sufficient for more complex data models like schoflake schemas, where you would have to define the data model and construct the container in separate steps, by utilzing getML's full-fledged data model and container APIs respectively.</p>","boost":0.9},{"location":"examples/community-notebooks/loans/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"<p>We loaded the data, defined the roles, units and the abstract data model. Next, we create a getML pipeline for relational learning.</p>","boost":0.9},{"location":"examples/community-notebooks/loans/#21-getml-pipeline","title":"2.1 getML Pipeline\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/loans/#22-model-training","title":"2.2 Model training\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/loans/#23-model-evaluation","title":"2.3 Model evaluation\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/loans/#24-studying-features","title":"2.4 Studying features\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/loans/#3-conclusion","title":"3. Conclusion\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/loans/#references","title":"References\u00b6","text":"<p>Schulte, Oliver, et al. \"A hierarchy of independence assumptions for multi-relational Bayes net classifiers.\" 2013 IEEE Symposium on Computational Intelligence and Data Mining (CIDM). IEEE, 2013.</p>","boost":0.9},{"location":"examples/community-notebooks/robot/","title":"<span class=\"ntitle\">robot.ipynb</span> <span class=\"ndesc\">Feature engineering on sensor data</span>","text":"In\u00a0[1]: Copied! <pre>%pip install -q \"getml==1.4.0\" \"numpy&lt;2.0.0\" \"matplotlib~=3.9\"\n\nimport getml\nimport matplotlib.pyplot as plt\n%matplotlib inline  \n \ngetml.engine.launch()\ngetml.engine.set_project('robot')\n</pre> %pip install -q \"getml==1.4.0\" \"numpy&lt;2.0.0\" \"matplotlib~=3.9\"  import getml import matplotlib.pyplot as plt %matplotlib inline     getml.engine.launch() getml.engine.set_project('robot') <pre>Note: you may need to restart the kernel to use updated packages.\nLaunching ./getML --allow-push-notifications=true --allow-remote-ips=false --home-directory=/home/alex/.local/lib/python3.10/site-packages/getml --in-memory=true --install=false --launch-browser=true --log=false in /home/alex/.local/lib/python3.10/site-packages/getml/.getML/getml-1.4.0-x64-community-edition-linux...\nLaunched the getML engine. The log output will be stored in /home/alex/.getML/logs/20240807164517.log.\n\nConnected to project 'robot'\n</pre> In\u00a0[2]: Copied! <pre>data_all = getml.data.DataFrame.from_csv(\n    \"https://static.getml.com/datasets/robotarm/robot-demo.csv\", \n    \"data_all\"\n)\n</pre> data_all = getml.data.DataFrame.from_csv(     \"https://static.getml.com/datasets/robotarm/robot-demo.csv\",      \"data_all\" ) <pre>Downloading robot-demo.csv...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n</pre> In\u00a0[3]: Copied! <pre>data_all\n</pre> data_all Out[3]:  name            3            4            5            6            7            8            9           10           11           12           13           14           15           16           17           18           19           20           21           22           23           24           25           26           27           28           29           30           31           32           33           34           35           36           37           38           39           40           41           42           43           44           45           46           47           48           49           50           51           52           53           54           55           56           57           58           59           60           61           62           63           64           65           66           67           68           69           70           71           72           73           74           75           76           77           78           79           80           81           82           83           84           85           86           98           99          100          101          102          103          104          105          106          f_x          f_y          f_z  role unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float 0 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3274 0.9605 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1233 -6.5483 -2.8045 -0.8296 0.07625 -0.1906 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1983 0.7699 0.41 0.08279 -1.4094 0.786 -0.3682 0 0 0 0 0 0 -22.654 -11.503 -18.673 -3.5155 5.8354 -2.05 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.069 48.009 0.9668 47.834 47.925 47.818 47.834 47.955 47.971 -11.03 6.9 -7.33 1 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3274 0.9604 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1188 -6.5506 -2.8404 -0.8281 0.06405 -0.1998 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1983 0.7699 0.41 0.0828 -1.4094 0.7859 -0.3682 0 0 0 0 0 0 -21.627 -11.046 -18.66 -3.5395 5.7577 -1.9805 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.009 48.009 0.8594 47.834 47.925 47.818 47.834 47.955 47.971 -10.848 6.7218 -7.4427 2 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3274 0.9605 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1099 -6.5438 -2.8 -0.8205 0.07473 -0.183 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1922 0.7699 0.41 0.08279 -1.4094 0.7859 -0.3682 0 0 0 0 0 0 -23.843 -12.127 -18.393 -3.6453 5.978 -1.9978 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.009 48.069 0.931 47.879 47.925 47.818 47.834 47.955 47.971 -10.666 6.5436 -7.5555 3 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3273 0.9604 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1233 -6.5483 -2.8224 -0.8266 0.07168 -0.1998 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1967 0.7699 0.41 0.08275 -1.4094 0.786 -0.3681 0 0 0 0 0 0 -21.772 -10.872 -18.691 -3.5512 5.6648 -1.9976 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.069 48.069 0.931 47.879 47.925 47.818 47.834 47.955 47.971 -10.507 6.4533 -7.65 4 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3274 0.9604 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1255 -6.5394 -2.8 -0.8327 0.07473 -0.1952 0.1211 -6.5483 -2.8157 -0.8327 0.07015 -0.1922 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 -22.823 -11.645 -18.524 -3.5305 5.8712 -2.0096 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.069 48.069 0.8952 47.879 47.925 47.818 47.834 47.955 47.971 -10.413 6.6267 -7.69 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 14996 3.0837 -0.8836 1.4501 -2.2102 -1.559 -5.3265 -0.03151 -0.05375 0.04732 0.1482 -0.05218 0.06706 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3694 -4.1879 -1.1847 -0.09441 -0.1568 0.1898 1.1605 -42.951 -19.023 -2.6343 0.1551 -0.1338 3.0836 -0.8836 1.4503 -2.2101 -1.5591 -5.3263 -0.03347 -0.05585 0.04805 0.151 -0.05513 0.07114 -0.3564 -6.0394 -2.3001 -0.2181 -0.1159 0.09608 -0.3632 -6.0394 -2.3023 -0.212 -0.125 0.1113 0.7116 0.06957 0.06036 -0.8506 2.9515 -0.03352 -0.03558 -0.03029 0.002444 -0.04208 0.1458 -0.1098 -0.8784 -0.07291 -37.584 0.0001132 -2.1031 0.03318 0.7117 0.0697 0.06044 -0.8511 2.951 -0.03356 -0.03508 -0.02849 0.001571 -0.03951 0.1442 -0.1036 48.069 48.009 0.8952 47.818 47.834 47.818 47.803 47.94 47.94 10.84 -1.41 16.14 14997 3.0835 -0.884 1.4505 -2.2091 -1.5594 -5.326 -0.02913 -0.0497 0.04376 0.137 -0.04825 0.062 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3677 -4.1837 -1.1874 -0.09682 -0.1562 0.189 1.1592 -42.937 -19.023 -2.6331 0.1545 -0.1338 3.0833 -0.8841 1.4507 -2.209 -1.5596 -5.3258 -0.02909 -0.04989 0.04198 0.1481 -0.05465 0.06249 -0.3161 -6.1179 -2.253 -0.3752 -0.03965 0.08693 -0.3273 -6.1022 -2.2597 -0.366 -0.05033 0.0915 0.7114 0.06932 0.06039 -0.8497 2.953 -0.03359 -0.0335 -0.02723 0.001208 -0.04242 0.1428 -0.0967 -2.7137 0.8552 -38.514 -0.6088 -3.2383 -0.9666 0.7114 0.06948 0.06045 -0.8503 2.9525 -0.03359 -0.03246 -0.02633 0.001469 -0.03657 0.1333 -0.09571 48.009 48.009 0.8594 47.818 47.834 47.818 47.803 47.94 47.94 10.857 -1.52 15.943 14998 3.0833 -0.8844 1.4508 -2.208 -1.5598 -5.3256 -0.02676 -0.04565 0.04019 0.1258 -0.04431 0.05695 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3659 -4.1797 -1.1901 -0.09922 -0.1555 0.1881 1.1579 -42.924 -19.023 -2.6321 0.154 -0.1338 3.0831 -0.8844 1.451 -2.2078 -1.56 -5.3253 -0.02776 -0.04382 0.03652 0.1295 -0.05064 0.04818 -0.343 -6.2569 -2.1566 -0.3035 0.00305 0.1434 -0.3385 -6.2322 -2.1589 -0.302 -0.00915 0.1571 0.7111 0.06912 0.06039 -0.849 2.9544 -0.0337 -0.02911 -0.02589 0.001292 -0.04046 0.1246 -0.08058 4.2749 1.0128 -36.412 -1.2811 -0.4296 -1.1013 0.7112 0.06928 0.06046 -0.8495 2.9538 -0.03362 -0.02984 -0.02417 0.001364 -0.03362 0.1224 -0.08786 48.009 48.009 0.931 47.818 47.834 47.818 47.803 47.94 47.94 10.89 -1.74 15.55 14999 3.0831 -0.8847 1.4511 -2.2071 -1.5602 -5.3251 -0.02438 -0.0416 0.03662 0.1147 -0.04038 0.0519 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3642 -4.1758 -1.1928 -0.1016 -0.1548 0.1873 1.1568 -42.912 -19.023 -2.6311 0.1535 -0.1338 3.0829 -0.8848 1.4513 -2.2068 -1.5604 -5.3249 -0.02149 -0.04059 0.03417 0.1202 -0.0395 0.04178 -0.4237 -6.2703 -2.0939 -0.302 -0.01372 0.1739 -0.4125 -6.2569 -2.0916 -0.2943 -0.02898 0.1891 0.7109 0.06894 0.06039 -0.8484 2.9557 -0.03384 -0.02738 -0.01982 0.001031 -0.03028 0.1157 -0.06702 11.518 1.5002 -39.314 -1.8671 -0.3734 -0.5733 0.7109 0.06909 0.06047 -0.8488 2.955 -0.03364 -0.02721 -0.02201 0.001255 -0.03067 0.1115 -0.08003 48.009 48.009 0.931 47.818 47.834 47.818 47.803 47.94 47.94 11.29 -1.4601 15.743 15000 3.0829 -0.885 1.4514 -2.2062 -1.5605 -5.3247 -0.02201 -0.03755 0.03305 0.1035 -0.03645 0.04684 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3624 -4.172 -1.1955 -0.1041 -0.1542 0.1864 1.1558 -42.901 -19.023 -2.6302 0.1531 -0.1338 3.0827 -0.8851 1.4516 -2.2059 -1.5607 -5.3246 -0.02096 -0.03808 0.02958 0.1171 -0.03289 0.03883 -0.417 -6.2434 -2.058 -0.4102 -0.04728 0.1967 -0.4237 -6.2367 -2.0714 -0.4163 -0.0671 0.2059 0.7107 0.06878 0.06041 -0.8478 2.9567 -0.03382 -0.02535 -0.01854 0.001614 -0.02421 0.11 -0.06304 15.099 2.936 -39.068 -1.9402 0.139 -0.2674 0.7107 0.06893 0.06048 -0.8482 2.9561 -0.03367 -0.02458 -0.01986 0.001142 -0.0277 0.1007 -0.07221 48.009 48.069 0.8952 47.818 47.834 47.818 47.803 47.94 47.955 11.69 -1.1801 15.937 <p>     15001 rows x 96 columns     memory usage: 11.52 MB     name: data_all     type: getml.DataFrame </p> In\u00a0[4]: Copied! <pre>data_all.set_role([\"f_x\", \"f_y\", \"f_z\"], getml.data.roles.target)\ndata_all.set_role(data_all.roles.unused, getml.data.roles.numerical)\n</pre> data_all.set_role([\"f_x\", \"f_y\", \"f_z\"], getml.data.roles.target) data_all.set_role(data_all.roles.unused, getml.data.roles.numerical) <p>This is what the data set looks like:</p> In\u00a0[5]: Copied! <pre>data_all\n</pre> data_all Out[5]:  name     f_x     f_y     f_z         3         4         5         6         7         8         9        10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32        33        34        35        36        37        38        39        40        41        42        43        44        45        46        47        48        49        50        51        52        53        54        55        56        57        58        59        60        61        62        63        64        65        66        67        68        69        70        71        72        73        74        75        76        77        78        79        80        81        82        83        84        85        86        98        99       100       101       102       103       104       105       106  role  target  target  target numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical 0 -11.03 6.9 -7.33 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3274 0.9605 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1233 -6.5483 -2.8045 -0.8296 0.07625 -0.1906 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1983 0.7699 0.41 0.08279 -1.4094 0.786 -0.3682 0 0 0 0 0 0 -22.654 -11.503 -18.673 -3.5155 5.8354 -2.05 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.069 48.009 0.9668 47.834 47.925 47.818 47.834 47.955 47.971 1 -10.848 6.7218 -7.4427 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3274 0.9604 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1188 -6.5506 -2.8404 -0.8281 0.06405 -0.1998 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1983 0.7699 0.41 0.0828 -1.4094 0.7859 -0.3682 0 0 0 0 0 0 -21.627 -11.046 -18.66 -3.5395 5.7577 -1.9805 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.009 48.009 0.8594 47.834 47.925 47.818 47.834 47.955 47.971 2 -10.666 6.5436 -7.5555 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3274 0.9605 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1099 -6.5438 -2.8 -0.8205 0.07473 -0.183 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1922 0.7699 0.41 0.08279 -1.4094 0.7859 -0.3682 0 0 0 0 0 0 -23.843 -12.127 -18.393 -3.6453 5.978 -1.9978 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.009 48.069 0.931 47.879 47.925 47.818 47.834 47.955 47.971 3 -10.507 6.4533 -7.65 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3273 0.9604 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1233 -6.5483 -2.8224 -0.8266 0.07168 -0.1998 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1967 0.7699 0.41 0.08275 -1.4094 0.786 -0.3681 0 0 0 0 0 0 -21.772 -10.872 -18.691 -3.5512 5.6648 -1.9976 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.069 48.069 0.931 47.879 47.925 47.818 47.834 47.955 47.971 4 -10.413 6.6267 -7.69 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3274 0.9604 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1255 -6.5394 -2.8 -0.8327 0.07473 -0.1952 0.1211 -6.5483 -2.8157 -0.8327 0.07015 -0.1922 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 -22.823 -11.645 -18.524 -3.5305 5.8712 -2.0096 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.069 48.069 0.8952 47.879 47.925 47.818 47.834 47.955 47.971 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 14996 10.84 -1.41 16.14 3.0837 -0.8836 1.4501 -2.2102 -1.559 -5.3265 -0.03151 -0.05375 0.04732 0.1482 -0.05218 0.06706 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3694 -4.1879 -1.1847 -0.09441 -0.1568 0.1898 1.1605 -42.951 -19.023 -2.6343 0.1551 -0.1338 3.0836 -0.8836 1.4503 -2.2101 -1.5591 -5.3263 -0.03347 -0.05585 0.04805 0.151 -0.05513 0.07114 -0.3564 -6.0394 -2.3001 -0.2181 -0.1159 0.09608 -0.3632 -6.0394 -2.3023 -0.212 -0.125 0.1113 0.7116 0.06957 0.06036 -0.8506 2.9515 -0.03352 -0.03558 -0.03029 0.002444 -0.04208 0.1458 -0.1098 -0.8784 -0.07291 -37.584 0.0001132 -2.1031 0.03318 0.7117 0.0697 0.06044 -0.8511 2.951 -0.03356 -0.03508 -0.02849 0.001571 -0.03951 0.1442 -0.1036 48.069 48.009 0.8952 47.818 47.834 47.818 47.803 47.94 47.94 14997 10.857 -1.52 15.943 3.0835 -0.884 1.4505 -2.2091 -1.5594 -5.326 -0.02913 -0.0497 0.04376 0.137 -0.04825 0.062 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3677 -4.1837 -1.1874 -0.09682 -0.1562 0.189 1.1592 -42.937 -19.023 -2.6331 0.1545 -0.1338 3.0833 -0.8841 1.4507 -2.209 -1.5596 -5.3258 -0.02909 -0.04989 0.04198 0.1481 -0.05465 0.06249 -0.3161 -6.1179 -2.253 -0.3752 -0.03965 0.08693 -0.3273 -6.1022 -2.2597 -0.366 -0.05033 0.0915 0.7114 0.06932 0.06039 -0.8497 2.953 -0.03359 -0.0335 -0.02723 0.001208 -0.04242 0.1428 -0.0967 -2.7137 0.8552 -38.514 -0.6088 -3.2383 -0.9666 0.7114 0.06948 0.06045 -0.8503 2.9525 -0.03359 -0.03246 -0.02633 0.001469 -0.03657 0.1333 -0.09571 48.009 48.009 0.8594 47.818 47.834 47.818 47.803 47.94 47.94 14998 10.89 -1.74 15.55 3.0833 -0.8844 1.4508 -2.208 -1.5598 -5.3256 -0.02676 -0.04565 0.04019 0.1258 -0.04431 0.05695 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3659 -4.1797 -1.1901 -0.09922 -0.1555 0.1881 1.1579 -42.924 -19.023 -2.6321 0.154 -0.1338 3.0831 -0.8844 1.451 -2.2078 -1.56 -5.3253 -0.02776 -0.04382 0.03652 0.1295 -0.05064 0.04818 -0.343 -6.2569 -2.1566 -0.3035 0.00305 0.1434 -0.3385 -6.2322 -2.1589 -0.302 -0.00915 0.1571 0.7111 0.06912 0.06039 -0.849 2.9544 -0.0337 -0.02911 -0.02589 0.001292 -0.04046 0.1246 -0.08058 4.2749 1.0128 -36.412 -1.2811 -0.4296 -1.1013 0.7112 0.06928 0.06046 -0.8495 2.9538 -0.03362 -0.02984 -0.02417 0.001364 -0.03362 0.1224 -0.08786 48.009 48.009 0.931 47.818 47.834 47.818 47.803 47.94 47.94 14999 11.29 -1.4601 15.743 3.0831 -0.8847 1.4511 -2.2071 -1.5602 -5.3251 -0.02438 -0.0416 0.03662 0.1147 -0.04038 0.0519 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3642 -4.1758 -1.1928 -0.1016 -0.1548 0.1873 1.1568 -42.912 -19.023 -2.6311 0.1535 -0.1338 3.0829 -0.8848 1.4513 -2.2068 -1.5604 -5.3249 -0.02149 -0.04059 0.03417 0.1202 -0.0395 0.04178 -0.4237 -6.2703 -2.0939 -0.302 -0.01372 0.1739 -0.4125 -6.2569 -2.0916 -0.2943 -0.02898 0.1891 0.7109 0.06894 0.06039 -0.8484 2.9557 -0.03384 -0.02738 -0.01982 0.001031 -0.03028 0.1157 -0.06702 11.518 1.5002 -39.314 -1.8671 -0.3734 -0.5733 0.7109 0.06909 0.06047 -0.8488 2.955 -0.03364 -0.02721 -0.02201 0.001255 -0.03067 0.1115 -0.08003 48.009 48.009 0.931 47.818 47.834 47.818 47.803 47.94 47.94 15000 11.69 -1.1801 15.937 3.0829 -0.885 1.4514 -2.2062 -1.5605 -5.3247 -0.02201 -0.03755 0.03305 0.1035 -0.03645 0.04684 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3624 -4.172 -1.1955 -0.1041 -0.1542 0.1864 1.1558 -42.901 -19.023 -2.6302 0.1531 -0.1338 3.0827 -0.8851 1.4516 -2.2059 -1.5607 -5.3246 -0.02096 -0.03808 0.02958 0.1171 -0.03289 0.03883 -0.417 -6.2434 -2.058 -0.4102 -0.04728 0.1967 -0.4237 -6.2367 -2.0714 -0.4163 -0.0671 0.2059 0.7107 0.06878 0.06041 -0.8478 2.9567 -0.03382 -0.02535 -0.01854 0.001614 -0.02421 0.11 -0.06304 15.099 2.936 -39.068 -1.9402 0.139 -0.2674 0.7107 0.06893 0.06048 -0.8482 2.9561 -0.03367 -0.02458 -0.01986 0.001142 -0.0277 0.1007 -0.07221 48.009 48.069 0.8952 47.818 47.834 47.818 47.803 47.94 47.955 <p>     15001 rows x 96 columns     memory usage: 11.52 MB     name: data_all     type: getml.DataFrame </p> In\u00a0[6]: Copied! <pre>split = getml.data.split.time(data_all, \"rowid\", test=10500)\nsplit\n</pre> split = getml.data.split.time(data_all, \"rowid\", test=10500) split Out[6]: 0 train 1 train 2 train 3 train 4 train ... <p>     15001 rows          type: StringColumnView </p> In\u00a0[7]: Copied! <pre>time_series = getml.data.TimeSeries(\n    population=data_all,\n    split=split,\n    time_stamps=\"rowid\",\n    lagged_targets=False,\n    memory=30,\n)\n\ntime_series\n</pre> time_series = getml.data.TimeSeries(     population=data_all,     split=split,     time_stamps=\"rowid\",     lagged_targets=False,     memory=30, )  time_series Out[7]: data model diagram data_allpopulationrowid &lt;= rowidMemory: 30 time steps staging data frames staging table               0 population POPULATION__STAGING_TABLE_1 1 data_all DATA_ALL__STAGING_TABLE_2 container population subset name      rows type 0 test data_all 4501 View 1 train data_all 10500 View peripheral name      rows type 0 data_all 15001 View In\u00a0[8]: Copied! <pre>fast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    num_features=10,\n)\n\nxgboost = getml.predictors.XGBoostRegressor()\n\npipe1 = getml.pipeline.Pipeline(\n    data_model=time_series.data_model,\n    feature_learners=[fast_prop],\n    predictors=xgboost\n)\n</pre> fast_prop = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.SquareLoss,     num_features=10, )  xgboost = getml.predictors.XGBoostRegressor()  pipe1 = getml.pipeline.Pipeline(     data_model=time_series.data_model,     feature_learners=[fast_prop],     predictors=xgboost ) <p>It is always a good idea to check the pipeline for any potential issues.</p> In\u00a0[9]: Copied! <pre>pipe1.check(time_series.train)\n</pre> pipe1.check(time_series.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\n</pre> In\u00a0[10]: Copied! <pre>pipe1.fit(time_series.train)\n</pre> pipe1.fit(time_series.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 1130 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:04, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:04, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:03, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:13.011683\n\n</pre> Out[10]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=['data_all'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['container-pcWY0M'])</pre> In\u00a0[11]: Copied! <pre>pipe1.score(time_series.test)\n</pre> pipe1.score(time_series.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[11]: date time           set used target     mae    rmse rsquared 0 2024-08-07 16:45:34 train f_x 0.4403 0.58 0.9962 1 2024-08-07 16:45:34 train f_y 0.5168 0.6813 0.9893 2 2024-08-07 16:45:34 train f_z 0.2918 0.385 0.9986 3 2024-08-07 16:45:34 test f_x 0.5605 0.7319 0.995 4 2024-08-07 16:45:34 test f_y 0.5653 0.7532 0.9871 5 2024-08-07 16:45:34 test f_z 0.3131 0.4071 0.9984 In\u00a0[12]: Copied! <pre>plt.subplots(figsize=(20, 10))\n\nnames, importances = pipe1.features.importances(target_num=0)\n\nplt.bar(names[0:30], importances[0:30], color='#6829c2')\n\nplt.title(\"feature importances for the x-component\", size=20)\nplt.grid(True)\nplt.xlabel(\"features\")\nplt.ylabel(\"importance\")\nplt.xticks(rotation='vertical')\n\nplt.show()\n</pre> plt.subplots(figsize=(20, 10))  names, importances = pipe1.features.importances(target_num=0)  plt.bar(names[0:30], importances[0:30], color='#6829c2')  plt.title(\"feature importances for the x-component\", size=20) plt.grid(True) plt.xlabel(\"features\") plt.ylabel(\"importance\") plt.xticks(rotation='vertical')  plt.show() In\u00a0[13]: Copied! <pre>plt.subplots(figsize=(20, 10))\n\nnames, importances = pipe1.features.importances(target_num=1)\n\nplt.bar(names[0:30], importances[0:30], color='#6829c2')\n\nplt.title(\"feature importances for the y-component\", size=20)\nplt.grid(True)\nplt.xlabel(\"features\")\nplt.ylabel(\"importance\")\nplt.xticks(rotation='vertical')\n\nplt.show()\n</pre> plt.subplots(figsize=(20, 10))  names, importances = pipe1.features.importances(target_num=1)  plt.bar(names[0:30], importances[0:30], color='#6829c2')  plt.title(\"feature importances for the y-component\", size=20) plt.grid(True) plt.xlabel(\"features\") plt.ylabel(\"importance\") plt.xticks(rotation='vertical')  plt.show() In\u00a0[14]: Copied! <pre>plt.subplots(figsize=(20, 10))\n\nnames, importances = pipe1.features.importances(target_num=2)\n\nplt.bar(names[0:30], importances[0:30], color='#6829c2')\n\nplt.title(\"feature importances for the z-component\", size=20)\nplt.grid(True)\nplt.xlabel(\"features\")\nplt.ylabel(\"importance\")\nplt.xticks(rotation='vertical')\n\nplt.show()\n</pre> plt.subplots(figsize=(20, 10))  names, importances = pipe1.features.importances(target_num=2)  plt.bar(names[0:30], importances[0:30], color='#6829c2')  plt.title(\"feature importances for the z-component\", size=20) plt.grid(True) plt.xlabel(\"features\") plt.ylabel(\"importance\") plt.xticks(rotation='vertical')  plt.show() In\u00a0[15]: Copied! <pre>plt.subplots(figsize=(20, 10))\n\nnames, importances = pipe1.columns.importances(target_num=0)\n\nplt.bar(names[0:30], importances[0:30], color='#6829c2')\n\nplt.title(\"column importances for the x-component\", size=20)\nplt.grid(True)\nplt.xlabel(\"column\")\nplt.ylabel(\"importance\")\nplt.xticks(rotation='vertical')\n\nplt.show()\n</pre> plt.subplots(figsize=(20, 10))  names, importances = pipe1.columns.importances(target_num=0)  plt.bar(names[0:30], importances[0:30], color='#6829c2')  plt.title(\"column importances for the x-component\", size=20) plt.grid(True) plt.xlabel(\"column\") plt.ylabel(\"importance\") plt.xticks(rotation='vertical')  plt.show() In\u00a0[16]: Copied! <pre>plt.subplots(figsize=(20, 10))\n\nnames, importances = pipe1.columns.importances(target_num=1)\n\nplt.bar(names[0:30], importances[0:30], color='#6829c2')\n\nplt.title(\"column importances for the y-component\", size=20)\nplt.grid(True)\nplt.xlabel(\"column\")\nplt.ylabel(\"importance\")\nplt.xticks(rotation='vertical')\n\nplt.show()\n</pre> plt.subplots(figsize=(20, 10))  names, importances = pipe1.columns.importances(target_num=1)  plt.bar(names[0:30], importances[0:30], color='#6829c2')  plt.title(\"column importances for the y-component\", size=20) plt.grid(True) plt.xlabel(\"column\") plt.ylabel(\"importance\") plt.xticks(rotation='vertical')  plt.show() In\u00a0[17]: Copied! <pre>plt.subplots(figsize=(20, 10))\n\nnames, importances = pipe1.columns.importances(target_num=2)\n\nplt.bar(names[0:30], importances[0:30], color='#6829c2')\n\nplt.title(\"column importances for the z-component\", size=20)\nplt.grid(True)\nplt.xlabel(\"column\")\nplt.ylabel(\"importance\")\nplt.xticks(rotation='vertical')\n\nplt.show()\n</pre> plt.subplots(figsize=(20, 10))  names, importances = pipe1.columns.importances(target_num=2)  plt.bar(names[0:30], importances[0:30], color='#6829c2')  plt.title(\"column importances for the z-component\", size=20) plt.grid(True) plt.xlabel(\"column\") plt.ylabel(\"importance\") plt.xticks(rotation='vertical')  plt.show() In\u00a0[18]: Copied! <pre>f_x = time_series.test.population[\"f_x\"].to_numpy()\nf_y = time_series.test.population[\"f_y\"].to_numpy()\nf_z = time_series.test.population[\"f_z\"].to_numpy()\n</pre> f_x = time_series.test.population[\"f_x\"].to_numpy() f_y = time_series.test.population[\"f_y\"].to_numpy() f_z = time_series.test.population[\"f_z\"].to_numpy() In\u00a0[19]: Copied! <pre>predictions = pipe1.predict(time_series.test)\n</pre> predictions = pipe1.predict(time_series.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> In\u00a0[20]: Copied! <pre>col_data = 'black'\ncol_getml = 'darkviolet'\ncol_getml_alt = 'coral'\n</pre> col_data = 'black' col_getml = 'darkviolet' col_getml_alt = 'coral' In\u00a0[21]: Copied! <pre>plt.subplots(figsize=(20, 10))\n\nplt.title(\"x-component of the force vector\", size=20)\n\nplt.plot(f_x, label=\"ground truth\", color=col_data)\nplt.plot(predictions[:,0], label=\"prediction\",color=col_getml)\n\nplt.legend(loc=\"upper right\", fontsize=16)\n</pre> plt.subplots(figsize=(20, 10))  plt.title(\"x-component of the force vector\", size=20)  plt.plot(f_x, label=\"ground truth\", color=col_data) plt.plot(predictions[:,0], label=\"prediction\",color=col_getml)  plt.legend(loc=\"upper right\", fontsize=16) Out[21]: <pre>&lt;matplotlib.legend.Legend at 0x7a52f1751e70&gt;</pre> In\u00a0[22]: Copied! <pre>plt.subplots(figsize=(20, 10))\n\nplt.title(\"y-component of the force vector\", size=20)\n\nplt.plot(f_y, label=\"ground truth\", color=col_data)\nplt.plot(predictions[:,1], label=\"prediction\",color=col_getml)\n\nplt.legend(loc=\"upper right\", fontsize=16)\n</pre> plt.subplots(figsize=(20, 10))  plt.title(\"y-component of the force vector\", size=20)  plt.plot(f_y, label=\"ground truth\", color=col_data) plt.plot(predictions[:,1], label=\"prediction\",color=col_getml)  plt.legend(loc=\"upper right\", fontsize=16) Out[22]: <pre>&lt;matplotlib.legend.Legend at 0x7a52f17d7e80&gt;</pre> In\u00a0[23]: Copied! <pre>plt.subplots(figsize=(20, 10))\n\nplt.title(\"z-component of the force vector\", size=20)\n\nplt.plot(f_z, label=\"ground truth\", color=col_data)\nplt.plot(predictions[:,2], label=\"prediction\",color=col_getml)\n\nplt.legend(loc=\"upper right\", fontsize=16)\n</pre> plt.subplots(figsize=(20, 10))  plt.title(\"z-component of the force vector\", size=20)  plt.plot(f_z, label=\"ground truth\", color=col_data) plt.plot(predictions[:,2], label=\"prediction\",color=col_getml)  plt.legend(loc=\"upper right\", fontsize=16) Out[23]: <pre>&lt;matplotlib.legend.Legend at 0x7a52f16a4fa0&gt;</pre> In\u00a0[24]: Copied! <pre>pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name]\n</pre> pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name] Out[24]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_1\";\n\nCREATE TABLE \"FEATURE_1_1\" AS\nSELECT AVG( t2.\"7\" ) AS \"feature_1_1\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"DATA_ALL__STAGING_TABLE_2\" t2\nON 1 = 1\nWHERE t2.\"rowid\" &lt;= t1.\"rowid\"\nAND ( t2.\"( rowid + 30.000000 )\" &gt; t1.\"rowid\" OR t2.\"( rowid + 30.000000 )\" IS NULL )\nGROUP BY t1.rowid;\n</pre> In\u00a0[25]: Copied! <pre>getml.engine.shutdown()\n</pre> getml.engine.shutdown() <p>As we can see, the predictions are very accurate. This suggests that it is very feasible to predict the force vector based on other sensor data.</p>","boost":0.9},{"location":"examples/community-notebooks/robot/#robot-feature-engineering-on-sensor-data","title":"Robot - Feature engineering on sensor data\u00b6","text":"<p>The purpose of this notebook is to illustrate how we can overcome the feature explosion problem based on an example dataset involving sensor data.</p> <p>Summary:</p> <ul> <li>Prediction type: Regression</li> <li>Domain: Robotics</li> <li>Prediction target: The force vector on the robot's arm</li> <li>Population size: 15001</li> </ul> <p>Author: Dr. Patrick Urbanke</p>","boost":0.9},{"location":"examples/community-notebooks/robot/#the-data-set","title":"The data set\u00b6","text":"<p>To illustrate the problem, we use a data set related to robotics. When robots interact with humans, the most important think is that they don't hurt people. In order to prevent such accidents, the force vector on the robot's arm is measured. However, measuring the force vector is expensive.</p> <p>Therefore, we want consider an alternative approach. We would like to predict the force vector based on other sensor data that are less costly to measure. To do so, we use machine learning.</p> <p>However, the data set contains measurements from almost 100 different sensors and we do not know which and how many sensors are relevant for predicting the force vector.</p> <p>The data set has been generously provided by Erik Berger who originally collected it for his dissertation:</p> <p>Berger, E. (2018). Behavior-Specific Proprioception Models for Robotic Force Estimation: A Machine Learning Approach. Freiberg, Germany: Technische Universitaet Bergakademie Freiberg.</p>","boost":0.9},{"location":"examples/community-notebooks/robot/#analysis","title":"Analysis\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/robot/#1-loading-data","title":"1. Loading data\u00b6","text":"<p>We begin by importing the libraries and setting the project.</p>","boost":0.9},{"location":"examples/community-notebooks/robot/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/robot/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"<p>The force vector consists of three component (f_x, f_y and f_z), meaning that we have three targets.</p>","boost":0.9},{"location":"examples/community-notebooks/robot/#13-separate-data-into-a-training-and-testing-set","title":"1.3 Separate data into a training and testing set\u00b6","text":"<p>We also want to separate the data set into a training and testing set. We do so by using the first 10,500 measurements for training and then using the remainder for testing.</p>","boost":0.9},{"location":"examples/community-notebooks/robot/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/robot/#21-building-the-pipeline","title":"2.1 Building the pipeline\u00b6","text":"<p>We then build a pipeline based on the relboost algorithm with xgboost as our predictor.</p>","boost":0.9},{"location":"examples/community-notebooks/robot/#22-fitting-the-pipeline","title":"2.2 Fitting the pipeline\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/robot/#23-evaluating-the-pipeline","title":"2.3 Evaluating the pipeline\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/robot/#24-feature-importances","title":"2.4 Feature importances\u00b6","text":"<p>It is always a good idea to study the features the relational learning algorithm has extracted.</p> <p>The feature importance is calculated by xgboost based on the improvement of the optimizing criterium at each split in the decision tree and is normalized to 100%.</p> <p>Also note that we have three different target (f_x, f_y and f_z) and that different features are relevant for different targets.</p>","boost":0.9},{"location":"examples/community-notebooks/robot/#25-column-importances","title":"2.5 Column importances\u00b6","text":"<p>Because getML is a tool for relational learning, we can also calculate the importances for the original columns, using similar methods we have used for the feature importances.</p>","boost":0.9},{"location":"examples/community-notebooks/robot/#26-visualizing-the-predictions","title":"2.6 Visualizing the predictions\u00b6","text":"<p>Sometimes a picture says more than a 1000 words. We therefore want to visualize our predictions on the testing set.</p>","boost":0.9},{"location":"examples/community-notebooks/robot/#27-features","title":"2.7 Features\u00b6","text":"<p>The most important feature looks as follows:</p>","boost":0.9},{"location":"examples/community-notebooks/robot/#3-conclusion","title":"3. Conclusion\u00b6","text":"<p>The purpose of this notebook has been to illustrate the problem of the curse of dimensionality when engineering features from datasets with many columns.</p> <p>The most important thing to remember is that this problem exists regardless of whether you engineer your features manually or using algorithms. Whether you like it or not: If you write your features in the traditional way, your search space grows quadratically with the number of columns.</p>","boost":0.9},{"location":"examples/community-notebooks/seznam/","title":"<span class=\"ntitle\">seznam.ipynb</span> <span class=\"ndesc\">Predicting transaction volume</span>","text":"<p>Let's get started with the analysis and set up your session:</p> In\u00a0[1]: Copied! <pre>%pip install -q \"getml==1.4.0\" \"numpy&lt;2.0.0\"\n\nimport getml\n\ngetml.engine.launch()\ngetml.set_project('seznam')\n</pre> %pip install -q \"getml==1.4.0\" \"numpy&lt;2.0.0\"  import getml  getml.engine.launch() getml.set_project('seznam') <pre>Note: you may need to restart the kernel to use updated packages.\nLaunching ./getML --allow-push-notifications=true --allow-remote-ips=false --home-directory=/home/alex/.local/lib/python3.10/site-packages/getml --in-memory=true --install=false --launch-browser=true --log=false in /home/alex/.local/lib/python3.10/site-packages/getml/.getML/getml-1.4.0-x64-community-edition-linux...\nLaunched the getML engine. The log output will be stored in /home/alex/.getML/logs/20240807164702.log.\n\nConnected to project 'seznam'\n</pre> In\u00a0[2]: Copied! <pre>conn = getml.database.connect_mariadb(\n    host=\"db.relational-data.org\",\n    dbname=\"Seznam\",\n    port=3306,\n    user=\"guest\",\n    password=\"relational\"\n)\n\nconn\n</pre> conn = getml.database.connect_mariadb(     host=\"db.relational-data.org\",     dbname=\"Seznam\",     port=3306,     user=\"guest\",     password=\"relational\" )  conn Out[2]: <pre>Connection(dbname='Seznam', dialect='mysql', host='db.relational-data.org', port=3306)</pre> In\u00a0[3]: Copied! <pre>def load_if_needed(name):\n    \"\"\"\n    Loads the data from the relational learning\n    repository, if the data frame has not already\n    been loaded.\n    \"\"\"\n    if not getml.data.exists(name):\n        data_frame = getml.DataFrame.from_db(\n            name=name,\n            table_name=name,\n            conn=conn\n        )\n        data_frame.save()\n    else:\n        data_frame = getml.data.load_data_frame(name)\n    return data_frame\n</pre> def load_if_needed(name):     \"\"\"     Loads the data from the relational learning     repository, if the data frame has not already     been loaded.     \"\"\"     if not getml.data.exists(name):         data_frame = getml.DataFrame.from_db(             name=name,             table_name=name,             conn=conn         )         data_frame.save()     else:         data_frame = getml.data.load_data_frame(name)     return data_frame In\u00a0[4]: Copied! <pre>dobito = load_if_needed(\"dobito\")\nprobehnuto = load_if_needed(\"probehnuto\")\nprobehnuto_mimo_penezenku = load_if_needed(\"probehnuto_mimo_penezenku\")\n</pre> dobito = load_if_needed(\"dobito\") probehnuto = load_if_needed(\"probehnuto\") probehnuto_mimo_penezenku = load_if_needed(\"probehnuto_mimo_penezenku\") In\u00a0[5]: Copied! <pre>dobito\n</pre> dobito Out[5]:   name    client_id month_year_datum_transakce sluzba        kc_dobito       role unused_float unused_string              unused_string unused_string 0 7157857 2012-10-01 c 1045.62 1 109700 2015-10-01 c 5187.28 2 51508 2015-08-01 c 408.20 3 9573550 2012-10-01 c 521.24 4 9774621 2014-11-01 c 386.22 ... ... ... ... 554341 65283 2012-09-01 g 7850.00 554342 6091446 2012-08-01 g 31400.00 554343 1264806 2013-08-01 g -8220.52 554344 101103 2012-08-01 g 3140.00 554345 8674551 2012-08-01 g 6280.00 <p>     554346 rows x 4 columns     memory usage: 29.59 MB     name: dobito     type: getml.DataFrame </p> In\u00a0[6]: Copied! <pre>probehnuto\n</pre> probehnuto Out[6]:    name    client_id month_year_datum_transakce sluzba        kc_proklikano    role unused_float unused_string              unused_string unused_string 0 109145 2013-06-01 c -31.40 1 9804394 2015-10-01 h 37.68 2 9803353 2015-10-01 h 725.34 3 9801753 2015-10-01 h 194.68 4 9800425 2015-10-01 h 1042.48 ... ... ... ... 1462073 98857 2015-08-01 NULL 153.86 1462074 95776 2015-09-01 NULL 153.86 1462075 98857 2015-09-01 NULL 153.86 1462076 90001 2015-10-01 NULL 310.86 1462077 946957 2015-10-01 NULL 153.86 <p>     1462078 rows x 4 columns     memory usage: 77.07 MB     name: probehnuto     type: getml.DataFrame </p> In\u00a0[7]: Copied! <pre>probehnuto_mimo_penezenku\n</pre> probehnuto_mimo_penezenku Out[7]:   name    client_id Month/Year    probehla_inzerce_mimo_penezenku   role unused_float unused_string unused_string                   0 3901 2012-08-01 ANO 1 3901 2012-09-01 ANO 2 3901 2012-10-01 ANO 3 3901 2012-11-01 ANO 4 3901 2012-12-01 ANO ... ... ... 599381 9804086 2015-10-01 ANO 599382 9804238 2015-10-01 ANO 599383 9804782 2015-10-01 ANO 599384 9804810 2015-10-01 ANO 599385 9805032 2015-10-01 ANO <p>     599386 rows x 3 columns     memory usage: 23.38 MB     name: probehnuto_mimo_penezenku     type: getml.DataFrame </p> <p>getML requires that we define roles for each of the columns.</p> In\u00a0[8]: Copied! <pre>dobito.set_role(\"client_id\", getml.data.roles.join_key)\ndobito.set_role(\"month_year_datum_transakce\", getml.data.roles.time_stamp)\ndobito.set_role(\"sluzba\", getml.data.roles.categorical)\ndobito.set_role(\"kc_dobito\", getml.data.roles.numerical)\n\ndobito.set_unit(\"sluzba\", \"service\")\n\ndobito\n</pre> dobito.set_role(\"client_id\", getml.data.roles.join_key) dobito.set_role(\"month_year_datum_transakce\", getml.data.roles.time_stamp) dobito.set_role(\"sluzba\", getml.data.roles.categorical) dobito.set_role(\"kc_dobito\", getml.data.roles.numerical)  dobito.set_unit(\"sluzba\", \"service\")  dobito Out[8]:   name  month_year_datum_transakce client_id sluzba      kc_dobito   role                  time_stamp  join_key categorical numerical   unit time stamp, comparison only service     0 2012-10-01 7157857 c 1045.62 1 2015-10-01 109700 c 5187.28 2 2015-08-01 51508 c 408.2 3 2012-10-01 9573550 c 521.24 4 2014-11-01 9774621 c 386.22 ... ... ... ... 554341 2012-09-01 65283 g 7850 554342 2012-08-01 6091446 g 31400 554343 2013-08-01 1264806 g -8220.52 554344 2012-08-01 101103 g 3140 554345 2012-08-01 8674551 g 6280 <p>     554346 rows x 4 columns     memory usage: 13.30 MB     name: dobito     type: getml.DataFrame </p> In\u00a0[9]: Copied! <pre>probehnuto.set_role(\"client_id\", getml.data.roles.join_key)\nprobehnuto.set_role(\"month_year_datum_transakce\", getml.data.roles.time_stamp)\nprobehnuto.set_role(\"sluzba\", getml.data.roles.categorical)\nprobehnuto.set_role(\"kc_proklikano\", getml.data.roles.target)\n\nprobehnuto.set_unit(\"sluzba\", \"service\")\n\nprobehnuto\n</pre> probehnuto.set_role(\"client_id\", getml.data.roles.join_key) probehnuto.set_role(\"month_year_datum_transakce\", getml.data.roles.time_stamp) probehnuto.set_role(\"sluzba\", getml.data.roles.categorical) probehnuto.set_role(\"kc_proklikano\", getml.data.roles.target)  probehnuto.set_unit(\"sluzba\", \"service\")  probehnuto Out[9]:    name  month_year_datum_transakce client_id kc_proklikano sluzba         role                  time_stamp  join_key        target categorical    unit time stamp, comparison only service     0 2013-06-01 109145 -31.4 c 1 2015-10-01 9804394 37.68 h 2 2015-10-01 9803353 725.34 h 3 2015-10-01 9801753 194.68 h 4 2015-10-01 9800425 1042.48 h ... ... ... ... 1462073 2015-08-01 98857 153.86 NULL 1462074 2015-09-01 95776 153.86 NULL 1462075 2015-09-01 98857 153.86 NULL 1462076 2015-10-01 90001 310.86 NULL 1462077 2015-10-01 946957 153.86 NULL <p>     1462078 rows x 4 columns     memory usage: 35.09 MB     name: probehnuto     type: getml.DataFrame </p> In\u00a0[10]: Copied! <pre>probehnuto_mimo_penezenku.set_role(\"client_id\", getml.data.roles.join_key)\nprobehnuto_mimo_penezenku.set_role(\"Month/Year\", getml.data.roles.time_stamp)\n\nprobehnuto_mimo_penezenku\n</pre> probehnuto_mimo_penezenku.set_role(\"client_id\", getml.data.roles.join_key) probehnuto_mimo_penezenku.set_role(\"Month/Year\", getml.data.roles.time_stamp)  probehnuto_mimo_penezenku Out[10]:   name                  Month/Year client_id probehla_inzerce_mimo_penezenku   role                  time_stamp  join_key unused_string                     unit time stamp, comparison only 0 2012-08-01 3901 ANO 1 2012-09-01 3901 ANO 2 2012-10-01 3901 ANO 3 2012-11-01 3901 ANO 4 2012-12-01 3901 ANO ... ... ... 599381 2015-10-01 9804086 ANO 599382 2015-10-01 9804238 ANO 599383 2015-10-01 9804782 ANO 599384 2015-10-01 9804810 ANO 599385 2015-10-01 9805032 ANO <p>     599386 rows x 3 columns     memory usage: 14.39 MB     name: probehnuto_mimo_penezenku     type: getml.DataFrame </p> In\u00a0[11]: Copied! <pre>split = getml.data.split.random(train=0.8, test=0.2)\nsplit\n</pre> split = getml.data.split.random(train=0.8, test=0.2) split Out[11]: 0 train 1 train 2 train 3 test 4 train ... <p>     infinite number of  rows          type: StringColumnView </p> In\u00a0[12]: Copied! <pre>star_schema = getml.data.StarSchema(population=probehnuto, alias=\"population\", split=split)\n\nstar_schema.join(\n    probehnuto,\n    on=\"client_id\",\n    time_stamps=\"month_year_datum_transakce\",\n    lagged_targets=True,\n    horizon=getml.data.time.days(1),\n)\n\nstar_schema.join(\n    dobito,\n    on=\"client_id\",\n    time_stamps=\"month_year_datum_transakce\",\n)\n\nstar_schema.join(\n    probehnuto_mimo_penezenku,\n    on=\"client_id\", \n    time_stamps=(\"month_year_datum_transakce\",  \"Month/Year\"),\n)\n\nstar_schema\n</pre> star_schema = getml.data.StarSchema(population=probehnuto, alias=\"population\", split=split)  star_schema.join(     probehnuto,     on=\"client_id\",     time_stamps=\"month_year_datum_transakce\",     lagged_targets=True,     horizon=getml.data.time.days(1), )  star_schema.join(     dobito,     on=\"client_id\",     time_stamps=\"month_year_datum_transakce\", )  star_schema.join(     probehnuto_mimo_penezenku,     on=\"client_id\",      time_stamps=(\"month_year_datum_transakce\",  \"Month/Year\"), )  star_schema Out[12]: data model diagram probehnutodobitoprobehnuto_mimo_penezenkupopulationclient_id = client_idmonth_year_datum_transakce &lt;= month_year_datum_transakceHorizon: 1.0 daysLagged targets allowedclient_id = client_idmonth_year_datum_transakce &lt;= month_year_datum_transakceclient_id = client_idMonth/Year &lt;= month_year_datum_transakce staging data frames               staging table                    0 population POPULATION__STAGING_TABLE_1 1 dobito DOBITO__STAGING_TABLE_2 2 probehnuto PROBEHNUTO__STAGING_TABLE_3 3 probehnuto_mimo_penezenku PROBEHNUTO_MIMO_PENEZENKU__STAGING_TABLE_4 container population subset name          rows type 0 test probehnuto 292833 View 1 train probehnuto 1169245 View peripheral name                         rows type      0 probehnuto 1462078 DataFrame 1 dobito 554346 DataFrame 2 probehnuto_mimo_penezenku 599386 DataFrame <p>Set-up the feature learner &amp; predictor</p> In\u00a0[13]: Copied! <pre>fast_prop = getml.feature_learning.FastProp(\n    aggregation=getml.feature_learning.FastProp.agg_sets.All,\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    num_threads=1,    \n    sampling_factor=0.1,\n)\n\nfeature_selector = getml.predictors.XGBoostRegressor(n_jobs=1, external_memory=True)\n\npredictor = getml.predictors.XGBoostRegressor(n_jobs=1)\n</pre> fast_prop = getml.feature_learning.FastProp(     aggregation=getml.feature_learning.FastProp.agg_sets.All,     loss_function=getml.feature_learning.loss_functions.SquareLoss,     num_threads=1,         sampling_factor=0.1, )  feature_selector = getml.predictors.XGBoostRegressor(n_jobs=1, external_memory=True)  predictor = getml.predictors.XGBoostRegressor(n_jobs=1) <p>Build the pipeline</p> In\u00a0[14]: Copied! <pre>pipe1 = getml.Pipeline(\n    tags=['fast_prop'],\n    data_model=star_schema.data_model,\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=[predictor],\n    include_categorical=True,\n)\n\npipe1\n</pre> pipe1 = getml.Pipeline(     tags=['fast_prop'],     data_model=star_schema.data_model,     feature_learners=[fast_prop],     feature_selectors=[feature_selector],     predictors=[predictor],     include_categorical=True, )  pipe1 Out[14]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=['XGBoostRegressor'],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=['dobito', 'probehnuto', 'probehnuto_mimo_penezenku'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['fast_prop'])</pre> In\u00a0[15]: Copied! <pre>pipe1.check(star_schema.train)\n</pre> pipe1.check(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\nThe pipeline check generated 2 issues labeled INFO and 0 issues labeled WARNING.\n</pre> Out[15]: type label                  message                          0 INFO FOREIGN KEYS NOT FOUND When joining POPULATION__STAGING_TABLE_1 and DOBITO__STAGING_TABLE_2 over 'client_id' and 'client_id', there are no corresponding entries for 2.228789% of entries in 'client_id' in 'POPULATION__STAGING_TABLE_1'. You might want to double-check your join keys. 1 INFO FOREIGN KEYS NOT FOUND When joining POPULATION__STAGING_TABLE_1 and PROBEHNUTO_MIMO_PENEZENKU__STAGING_TABLE_4 over 'client_id' and 'client_id', there are no corresponding entries for 26.543966% of entries in 'client_id' in 'POPULATION__STAGING_TABLE_1'. You might want to double-check your join keys. In\u00a0[16]: Copied! <pre>pipe1.fit(star_schema.train)\n</pre> pipe1.fit(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 2 issues labeled INFO and 0 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 721 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:41, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 01:33, remaining: 00:00]          \nXGBoost: Training as feature selector... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 15:04, remaining: 00:00]            \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 09:05, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:26m:46.657153\n\n</pre> Out[16]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=['XGBoostRegressor'],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=['dobito', 'probehnuto', 'probehnuto_mimo_penezenku'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['fast_prop', 'container-2blnXz'])</pre> In\u00a0[17]: Copied! <pre>pipe1.score(star_schema.test)\n</pre> pipe1.score(star_schema.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:12, remaining: 00:00]          \n\n</pre> Out[17]: date time           set used target               mae        rmse rsquared 0 2024-08-07 17:14:11 train kc_proklikano 2922.5261 14329.7277 0.9428 1 2024-08-07 17:14:32 test kc_proklikano 2985.2084 18659.0789 0.8756 In\u00a0[18]: Copied! <pre>pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name]\n</pre> pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name] Out[18]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_48\";\n\nCREATE TABLE \"FEATURE_1_48\" AS\nSELECT EWMA_1H( t2.\"kc_proklikano\" ORDER BY t1.\"month_year_datum_transakce\" - t2.\"month_year_datum_transakce, '+1.000000 days'\" ) AS \"feature_1_48\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"PROBEHNUTO__STAGING_TABLE_3\" t2\nON t1.\"client_id\" = t2.\"client_id\"\nWHERE t2.\"month_year_datum_transakce, '+1.000000 days'\" &lt;= t1.\"month_year_datum_transakce\"\nAND t1.\"sluzba\" = t2.\"sluzba\"\nGROUP BY t1.rowid;\n</pre> In\u00a0[19]: Copied! <pre>getml.engine.shutdown()\n</pre> getml.engine.shutdown()","boost":0.9},{"location":"examples/community-notebooks/seznam/#seznam-predicting-transaction-volume","title":"Seznam - Predicting transaction volume\u00b6","text":"<p>Seznam is a Czech company with a scope similar to Google. The purpose of this notebook is to analyze data from Seznam's wallet, predicting the transaction volume.</p> <p>Summary:</p> <ul> <li>Prediction type: Regression model</li> <li>Domain: E-commerce</li> <li>Prediction target: Transaction volume</li> <li>Population size: 1,462,078</li> </ul> <p>Author: Dr. Patrick Urbanke</p>","boost":0.9},{"location":"examples/community-notebooks/seznam/#background","title":"Background\u00b6","text":"<p>Seznam is a Czech company with a scope similar to Google. The purpose of this notebook is to analyze data from Seznam's wallet, predicting the transaction volume.</p> <p>Since the dataset is in Czech, we will quickly translate the meaning of the main tables:</p> <ul> <li>dobito: contains data on prepayments into a wallet</li> <li>probehnuto: contains data on charges from a wallet</li> <li>probehnuto_mimo_penezenku: contains data on charges, from sources other than a wallet</li> </ul> <p>The dataset has been downloaded from the CTU Prague relational learning repository (Motl and Schulte, 2015).</p>","boost":0.9},{"location":"examples/community-notebooks/seznam/#analysis","title":"Analysis\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/seznam/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/seznam/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>We begin by downloading the data:</p>","boost":0.9},{"location":"examples/community-notebooks/seznam/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/seznam/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"<p>We loaded the data and defined the roles and units. Next, we create a getML pipeline for relational learning.</p>","boost":0.9},{"location":"examples/community-notebooks/seznam/#21-define-relational-model","title":"2.1 Define relational model\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/seznam/#22-getml-pipeline","title":"2.2 getML pipeline\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/seznam/#23-model-training","title":"2.3 Model training\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/seznam/#24-model-evaluation","title":"2.4 Model evaluation\u00b6","text":"","boost":0.9},{"location":"examples/community-notebooks/seznam/#25-features","title":"2.5 Features\u00b6","text":"<p>The most important feature looks as follows:</p>","boost":0.9},{"location":"examples/community-notebooks/seznam/#3-conclusion","title":"3. Conclusion\u00b6","text":"<p>In this notebook, we successfully demonstrated the process of predicting transaction volume for Seznam, a Czech company similar to Google, using the getML library. The key steps we covered include:</p> <ol> <li><p>Background and Data Preparation:</p> <ul> <li>Introduced Seznam and the purpose of the analysis.</li> <li>Loaded and prepared the data from Seznam's wallet, including prepayments and charges.</li> </ul> </li> <li><p>Data Visualization and Preparation:</p> <ul> <li>Translated and understood the main tables in the dataset.</li> <li>Defined roles and units for the dataset columns to prepare them for modeling.</li> </ul> </li> <li><p>Predictive Modeling:</p> <ul> <li>Created a relational model using getML's <code>StarSchema</code> to capture the relationships between different tables.</li> <li>Built a getML pipeline using <code>FastProp</code> for feature learning and <code>XGBoostRegressor</code> for prediction.</li> </ul> </li> <li><p>Model Training and Evaluation:</p> <ul> <li>Trained the model on the prepared data.</li> <li>Evaluated the model's performance, achieving an R-squared of 87.01%, RMSE of 14,783, and MAE of 2,974.</li> </ul> </li> </ol> <p>By leveraging getML's capabilities, we efficiently handled the complexities of relational data and built a robust regression model to predict transaction volume. This approach can be extended to other e-commerce datasets and prediction tasks, providing valuable insights and accurate forecasts.</p>","boost":0.9},{"location":"examples/community-notebooks/seznam/#references","title":"References\u00b6","text":"<p>Motl, Jan, and Oliver Schulte. \"The CTU prague relational learning repository.\" arXiv preprint arXiv:1511.03086 (2015).</p>","boost":0.9},{"location":"examples/enterprise-notebooks/","title":"Enterprise Edition Notebooks","text":"<p>A diverse collection of Jupyter Notebooks that showcase relational datasets across various domains, addressing typical data science challenges like binary classification on time series and regression with complex relational data, using publicly available datasets for benchmarking. </p>","boost":0.8},{"location":"examples/enterprise-notebooks/#algorithms-and-predictors","title":"Algorithms and Predictors","text":"<p>Serving as both documentation and practical blueprints, these notebooks demonstrate the performance of getML's feature engineering algorithms (<code>FastProp</code>, <code>Multirel</code>, <code>Relboost</code>, <code>RelMT</code>) and predictors (<code>LinearRegression</code>, <code>LogisticRegression</code>, <code>XGBoostClassifier</code>, <code>XGBoostRegressor</code> ) against competing tools like featuretools, tsfresh, and Prophet. </p> <p>Enterprise edition</p> <p>While FastProp excels in speed and resource efficiency, more advanced algorithms only available in the Enterprise Edition, deliver higher accuracy with even lower resource demands. Discover the benefits of the Enterprise edition and compare their features.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/#overview","title":"Overview","text":"Task Data Size Domain AdventureWorks - Predicting customer churn Classification Relational 71 tables, 233 MB Commerce Air Pollution - Why feature learning is better than simple propositionalization Regression Multivariate time series 1 table, 41k rows Environment Atherosclerosis - Disease lethality prediction Classification Relational 3 tables, 22 MB Health Baseball - Predicting players' salary Regression Relational 25 tables, 74 MB Sports Consumer expenditure - Why relational learning matters Classification Relational 3 tables, 150 MB E-commerce CORA - Categorizing academic publications Classification Relational 3 tables, 4.6 MB Academia Dodgers - Traffic volume prediction Regression Multivariate time series 1 table, 47k rows Transportation Formula 1 - Predicting the winner of a race Classification Relational 13 tables, 56 MB Sports IMDb - Predicting actors' gender Classification Relational with text 7 tables, 477.1 MB Entertainment Interstate 94 - Multivariate time series prediction Regression Multivariate time series 1 table, 24k rows Transportation Loans - Predicting loan default risk Classification Relational 8 tables, 60 MB Financial MovieLens - Predicting a user's gender based on the movies they have watched Classification Relational 7 tables, 20 MB Entertainment Occupancy - A multivariate time series example Classification Multivariate time series 1 table, 32k rows Energy Online Retail - Predicting order cancellations Classification Relational 1 table, 398k rows E-commerce Robot - Feature engineering on sensor data Regression Multivariate time series 1 table, 15k rows Robotics Seznam - Predicting transaction volume Regression Relational 4 tables, 147 MB E-commerce SFScores - Predicting health inspection scores of restaurants Regression Relational 3 tables, 9 MB Restaurants StatsExchange - Predicting users' reputations Regression Relational 8 tables, 658 MB Internet","boost":0.8},{"location":"examples/enterprise-notebooks/#source","title":"Source","text":"<p>These notebooks are published on the getml-demo repository.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/adventure_works/","title":"<span class=\"ntitle\">adventure_works.ipynb</span> <span class=\"ndesc\">Predicting customer churn</span>","text":"<p>Let's get started with the analysis and set up your session:</p> In\u00a0[1]: Copied! <pre>import copy\nimport os\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\nfrom pathlib import Path\n\nfrom urllib import request\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline  \n\nimport featuretools\nimport woodwork as ww\nimport getml\n\ngetml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.set_project('adventure_works')\n</pre> import copy import os os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\" from pathlib import Path  from urllib import request  import numpy as np import pandas as pd from IPython.display import Image import matplotlib.pyplot as plt %matplotlib inline    import featuretools import woodwork as ww import getml  getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.set_project('adventure_works') <pre>Launching ./getML --allow-push-notifications=true --allow-remote-ips=true --home-directory=/home/getml --in-memory=true --install=false --launch-browser=true --log=false --token=token in /home/getml/.getML/getml-1.4.0-x64-linux...\nLaunched the getML engine. The log output will be stored in /home/getml/.getML/logs/20240221145013.log.\nLoading pipelines... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\nConnected to project 'adventure_works'\n</pre> In\u00a0[2]: Copied! <pre>conn = getml.database.connect_mysql(\n    host=\"db.relational-data.org\",\n    dbname=\"AdventureWorks2014\",\n    port=3306,\n    user=\"guest\",\n    password=\"relational\"\n)\n\nconn\n</pre> conn = getml.database.connect_mysql(     host=\"db.relational-data.org\",     dbname=\"AdventureWorks2014\",     port=3306,     user=\"guest\",     password=\"relational\" )  conn Out[2]: <pre>Connection(dbname='AdventureWorks2014',\n           dialect='mysql',\n           host='db.relational-data.org',\n           port=3306)</pre> In\u00a0[3]: Copied! <pre>def load_if_needed(name):\n    \"\"\"\n    Loads the data from the relational learning\n    repository, if the data frame has not already\n    been loaded.\n    \"\"\"\n    if not getml.data.exists(name):\n        data_frame = getml.DataFrame.from_db(\n            name=name,\n            table_name=name,\n            conn=conn\n        )\n        data_frame.save()\n    else:\n        data_frame = getml.data.load_data_frame(name)\n    return data_frame\n</pre> def load_if_needed(name):     \"\"\"     Loads the data from the relational learning     repository, if the data frame has not already     been loaded.     \"\"\"     if not getml.data.exists(name):         data_frame = getml.DataFrame.from_db(             name=name,             table_name=name,             conn=conn         )         data_frame.save()     else:         data_frame = getml.data.load_data_frame(name)     return data_frame In\u00a0[4]: Copied! <pre>product = load_if_needed(\"Product\")\nsales_order_detail = load_if_needed(\"SalesOrderDetail\")\nsales_order_header = load_if_needed(\"SalesOrderHeader\")\nsales_order_reason = load_if_needed(\"SalesOrderHeaderSalesReason\")\nspecial_offer = load_if_needed(\"SpecialOffer\")\nstore = load_if_needed(\"Store\")\n</pre> product = load_if_needed(\"Product\") sales_order_detail = load_if_needed(\"SalesOrderDetail\") sales_order_header = load_if_needed(\"SalesOrderHeader\") sales_order_reason = load_if_needed(\"SalesOrderHeaderSalesReason\") special_offer = load_if_needed(\"SpecialOffer\") store = load_if_needed(\"Store\") In\u00a0[5]: Copied! <pre>product\n</pre> product Out[5]: name    ProductID     MakeFlag FinishedGoodsFlag SafetyStockLevel ReorderPoint DaysToManufacture ProductSubcategoryID ProductModelID Name                  ProductNumber Color         StandardCost  ListPrice     Size          SizeUnitMeasureCode WeightUnitMeasureCode Weight        ProductLine   Class         Style         SellStartDate       SellEndDate   DiscontinuedDate rowguid                          ModifiedDate        role unused_float unused_float      unused_float     unused_float unused_float      unused_float         unused_float   unused_float unused_string         unused_string unused_string unused_string unused_string unused_string unused_string       unused_string         unused_string unused_string unused_string unused_string unused_string       unused_string unused_string    unused_string                    unused_string       0 1 0 0 1000 750 0 nan nan Adjustable Race AR-5381 NULL 0.0000 0.0000 NULL NULL NULL NULL NULL NULL NULL 2008-04-30 00:00:00 NULL NULL 694215B7-08F7-4C0D-ACB1-D734BA44... 2014-02-08 09:01:36 1 2 0 0 1000 750 0 nan nan Bearing Ball BA-8327 NULL 0.0000 0.0000 NULL NULL NULL NULL NULL NULL NULL 2008-04-30 00:00:00 NULL NULL 58AE3C20-4F3A-4749-A7D4-D568806C... 2014-02-08 09:01:36 2 3 1 0 800 600 1 nan nan BB Ball Bearing BE-2349 NULL 0.0000 0.0000 NULL NULL NULL NULL NULL NULL NULL 2008-04-30 00:00:00 NULL NULL 9C21AED2-5BFA-4F18-BCB8-F11638DC... 2014-02-08 09:01:36 3 4 0 0 800 600 0 nan nan Headset Ball Bearings BE-2908 NULL 0.0000 0.0000 NULL NULL NULL NULL NULL NULL NULL 2008-04-30 00:00:00 NULL NULL ECFED6CB-51FF-49B5-B06C-7D8AC834... 2014-02-08 09:01:36 4 316 1 0 800 600 1 nan nan Blade BL-2036 NULL 0.0000 0.0000 NULL NULL NULL NULL NULL NULL NULL 2008-04-30 00:00:00 NULL NULL E73E9750-603B-4131-89F5-3DD15ED5... 2014-02-08 09:01:36 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 499 995 1 1 500 375 1 5 96 ML Bottom Bracket BB-8107 NULL 44.9506 101.2400 NULL NULL G 168.00 NULL M NULL 2013-05-30 00:00:00 NULL NULL 71AB847F-D091-42D6-B735-7B0C2D82... 2014-02-08 09:01:36 500 996 1 1 500 375 1 5 97 HL Bottom Bracket BB-9108 NULL 53.9416 121.4900 NULL NULL G 170.00 NULL H NULL 2013-05-30 00:00:00 NULL NULL 230C47C5-08B2-4CE3-B706-69C0BDD6... 2014-02-08 09:01:36 501 997 1 1 100 75 4 2 31 Road-750 Black, 44 BK-R19B-44 Black 343.6496 539.9900 44 CM LB 19.77 R L U 2013-05-30 00:00:00 NULL NULL 44CE4802-409F-43AB-9B27-CA534218... 2014-02-08 09:01:36 502 998 1 1 100 75 4 2 31 Road-750 Black, 48 BK-R19B-48 Black 343.6496 539.9900 48 CM LB 20.13 R L U 2013-05-30 00:00:00 NULL NULL 3DE9A212-1D49-40B6-B10A-F564D981... 2014-02-08 09:01:36 503 999 1 1 100 75 4 2 31 Road-750 Black, 52 BK-R19B-52 Black 343.6496 539.9900 52 CM LB 20.42 R L U 2013-05-30 00:00:00 NULL NULL AE638923-2B67-4679-B90E-ABBAB17D... 2014-02-08 09:01:36 <p>     504 rows x 25 columns     memory usage: 0.17 MB     name: Product     type: getml.DataFrame </p> In\u00a0[6]: Copied! <pre>sales_order_detail\n</pre> sales_order_detail Out[6]:   name SalesOrderID SalesOrderDetailID     OrderQty    ProductID SpecialOfferID CarrierTrackingNumber UnitPrice     UnitPriceDiscount LineTotal     rowguid                          ModifiedDate          role unused_float       unused_float unused_float unused_float   unused_float unused_string         unused_string unused_string     unused_string unused_string                    unused_string       0 43659 1 1 776 1 4911-403C-98 2024.9940 0.0000 2024.994000 B207C96D-D9E6-402B-8470-2CC176C4... 2011-05-30 22:00:00 1 43659 2 3 777 1 4911-403C-98 2024.9940 0.0000 6074.982000 7ABB600D-1E77-41BE-9FE5-B9142CFC... 2011-05-30 22:00:00 2 43659 3 1 778 1 4911-403C-98 2024.9940 0.0000 2024.994000 475CF8C6-49F6-486E-B0AD-AFC6A50C... 2011-05-30 22:00:00 3 43659 4 1 771 1 4911-403C-98 2039.9940 0.0000 2039.994000 04C4DE91-5815-45D6-8670-F462719F... 2011-05-30 22:00:00 4 43659 5 1 772 1 4911-403C-98 2039.9940 0.0000 2039.994000 5A74C7D2-E641-438E-A7AC-37BF2328... 2011-05-30 22:00:00 ... ... ... ... ... ... ... ... ... ... ... 121312 75122 121313 1 878 1 NULL 21.9800 0.0000 21.980000 8CAD6675-18CC-4F47-8287-97B41A8E... 2014-06-29 22:00:00 121313 75122 121314 1 712 1 NULL 8.9900 0.0000 8.990000 84F1C363-1C50-4442-BE16-541C59B6... 2014-06-29 22:00:00 121314 75123 121315 1 878 1 NULL 21.9800 0.0000 21.980000 C18B6476-429F-4BB1-828E-2BE5F82A... 2014-06-29 22:00:00 121315 75123 121316 1 879 1 NULL 159.0000 0.0000 159.000000 75A89C6A-C60A-47EA-8A52-B52A9C43... 2014-06-29 22:00:00 121316 75123 121317 1 712 1 NULL 8.9900 0.0000 8.990000 73646D26-0461-450D-8019-2C6C8586... 2014-06-29 22:00:00 <p>     121317 rows x 11 columns     memory usage: 21.60 MB     name: SalesOrderDetail     type: getml.DataFrame </p> In\u00a0[7]: Copied! <pre>sales_order_header\n</pre> sales_order_header Out[7]:  name SalesOrderID RevisionNumber       Status OnlineOrderFlag   CustomerID SalesPersonID  TerritoryID BillToAddressID ShipToAddressID ShipMethodID CreditCardID CurrencyRateID OrderDate           DueDate             ShipDate            SalesOrderNumber PurchaseOrderNumber AccountNumber  CreditCardApprovalCode SubTotal      TaxAmt        Freight       TotalDue      Comment       rowguid                          ModifiedDate         role unused_float   unused_float unused_float    unused_float unused_float  unused_float unused_float    unused_float    unused_float unused_float unused_float   unused_float unused_string       unused_string       unused_string       unused_string    unused_string       unused_string  unused_string          unused_string unused_string unused_string unused_string unused_string unused_string                    unused_string       0 43659 8 5 0 29825 279 5 985 985 5 16281 nan 2011-05-30 22:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 SO43659 PO522145787 10-4020-000676 105041Vi84182 20565.6206 1971.5149 616.0984 23153.2339 NULL 79B65321-39CA-4115-9CBA-8FE0903E... 2011-06-06 22:00:00 1 43660 8 5 0 29672 279 5 921 921 5 5618 nan 2011-05-30 22:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 SO43660 PO18850127500 10-4020-000117 115213Vi29411 1294.2529 124.2483 38.8276 1457.3288 NULL 738DC42D-D03B-48A1-9822-F95A67EA... 2011-06-06 22:00:00 2 43661 8 5 0 29734 282 6 517 517 5 1346 4 2011-05-30 22:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 SO43661 PO18473189620 10-4020-000442 85274Vi6854 32726.4786 3153.7696 985.5530 36865.8012 NULL D91B9131-18A4-4A11-BC3A-90B6F53E... 2011-06-06 22:00:00 3 43662 8 5 0 29994 282 6 482 482 5 10456 4 2011-05-30 22:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 SO43662 PO18444174044 10-4020-000227 125295Vi53935 28832.5289 2775.1646 867.2389 32474.9324 NULL 4A1ECFC0-CC3A-4740-B028-1C50BB48... 2011-06-06 22:00:00 4 43663 8 5 0 29565 276 4 1073 1073 5 4322 nan 2011-05-30 22:00:00 2011-06-12 00:00:00 2011-06-07 00:00:00 SO43663 PO18009186470 10-4020-000510 45303Vi22691 419.4589 40.2681 12.5838 472.3108 NULL 9B1E7A40-6AE0-4AD3-811C-A6495185... 2011-06-06 22:00:00 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 31460 75119 8 5 1 11981 nan 1 17649 17649 1 6761 nan 2014-06-29 22:00:00 2014-07-12 00:00:00 2014-07-07 00:00:00 SO75119 NULL 10-4030-011981 429826Vi35166 42.2800 3.3824 1.0570 46.7194 NULL 9382F1C9-383A-435F-9449-0EECEA21... 2014-07-06 22:00:00 31461 75120 8 5 1 18749 nan 6 28374 28374 1 8925 nan 2014-06-29 22:00:00 2014-07-12 00:00:00 2014-07-07 00:00:00 SO75120 NULL 10-4030-018749 929849Vi46003 84.9600 6.7968 2.1240 93.8808 NULL AE6A4FCF-FF73-4CD4-AF2C-5993D00D... 2014-07-06 22:00:00 31462 75121 8 5 1 15251 nan 6 26553 26553 1 14220 nan 2014-06-29 22:00:00 2014-07-12 00:00:00 2014-07-07 00:00:00 SO75121 NULL 10-4030-015251 529864Vi73738 74.9800 5.9984 1.8745 82.8529 NULL D7395C0E-00CB-4BFA-A238-0D6A9F49... 2014-07-06 22:00:00 31463 75122 8 5 1 15868 nan 6 14616 14616 1 18719 nan 2014-06-29 22:00:00 2014-07-12 00:00:00 2014-07-07 00:00:00 SO75122 NULL 10-4030-015868 330022Vi97312 30.9700 2.4776 0.7743 34.2219 NULL 4221035A-4159-492F-AF40-4363A64F... 2014-07-06 22:00:00 31464 75123 8 5 1 18759 nan 6 14024 14024 1 10084 nan 2014-06-29 22:00:00 2014-07-12 00:00:00 2014-07-07 00:00:00 SO75123 NULL 10-4030-018759 230370Vi51970 189.9700 15.1976 4.7493 209.9169 NULL D54752FF-2B54-4BE5-95EA-3B72289C... 2014-07-06 22:00:00 <p>     31465 rows x 26 columns     memory usage: 12.56 MB     name: SalesOrderHeader     type: getml.DataFrame </p> In\u00a0[8]: Copied! <pre>sales_order_reason\n</pre> sales_order_reason Out[8]:  name SalesOrderID SalesReasonID ModifiedDate         role unused_float  unused_float unused_string       0 43697 5 2011-05-30 22:00:00 1 43697 9 2011-05-30 22:00:00 2 43702 5 2011-05-31 22:00:00 3 43702 9 2011-05-31 22:00:00 4 43703 5 2011-05-31 22:00:00 ... ... ... 27642 75119 1 2014-06-29 22:00:00 27643 75120 1 2014-06-29 22:00:00 27644 75121 1 2014-06-29 22:00:00 27645 75122 1 2014-06-29 22:00:00 27646 75123 1 2014-06-29 22:00:00 <p>     27647 rows x 3 columns     memory usage: 1.22 MB     name: SalesOrderHeaderSalesReason     type: getml.DataFrame </p> In\u00a0[9]: Copied! <pre>special_offer\n</pre> special_offer Out[9]: name SpecialOfferID       MinQty       MaxQty Description                      DiscountPct   Type                 Category      StartDate           EndDate             rowguid                          ModifiedDate        role   unused_float unused_float unused_float unused_string                    unused_string unused_string        unused_string unused_string       unused_string       unused_string                    unused_string       0 1 0 nan No Discount 0.0000 No Discount No Discount 2011-05-01 00:00:00 2014-11-30 00:00:00 0290C4F5-191F-4337-AB6B-0A2DDE03... 2011-03-31 22:00:00 1 2 11 14 Volume Discount 11 to 14 0.0200 Volume Discount Reseller 2011-05-31 00:00:00 2014-05-30 00:00:00 D7542EE7-15DB-4541-985C-5CC27AEF... 2011-04-30 22:00:00 2 3 15 24 Volume Discount 15 to 24 0.0500 Volume Discount Reseller 2011-05-31 00:00:00 2014-05-30 00:00:00 4BDBCC01-8CF7-40A9-B643-40EC5B71... 2011-04-30 22:00:00 3 4 25 40 Volume Discount 25 to 40 0.1000 Volume Discount Reseller 2011-05-31 00:00:00 2014-05-30 00:00:00 504B5E85-8F3F-4EBC-9E1D-C1BC5DEA... 2011-04-30 22:00:00 4 5 41 60 Volume Discount 41 to 60 0.1500 Volume Discount Reseller 2011-05-31 00:00:00 2014-05-30 00:00:00 677E1D9D-944F-4E81-90E8-47EB0A82... 2011-04-30 22:00:00 ... ... ... ... ... ... ... ... ... ... ... 11 12 0 nan LL Road Frame Sale 0.3500 Excess Inventory Reseller 2013-05-30 00:00:00 2013-07-14 00:00:00 C0AF1C89-9722-4235-9248-3FBA4D9E... 2013-04-29 22:00:00 12 13 0 nan Touring-3000 Promotion 0.1500 New Product Reseller 2013-05-30 00:00:00 2013-08-29 00:00:00 5061CCE4-E021-45A8-9A75-DFB36CBB... 2013-04-29 22:00:00 13 14 0 nan Touring-1000 Promotion 0.2000 New Product Reseller 2013-05-30 00:00:00 2013-08-29 00:00:00 1AF84A9E-A98C-4BD9-B48F-DC2B8B6B... 2013-04-29 22:00:00 14 15 0 nan Half-Price Pedal Sale 0.5000 Seasonal Discount Customer 2013-07-14 00:00:00 2013-08-14 00:00:00 03E3594D-6EBB-46A6-B8EE-A9289C0C... 2013-06-13 22:00:00 15 16 0 nan Mountain-500 Silver Clearance Sa... 0.4000 Discontinued Product Reseller 2014-03-31 00:00:00 2014-05-30 00:00:00 EB7CB484-BCCF-4D2D-BF73-521B2001... 2014-02-28 23:00:00 <p>     16 rows x 11 columns     memory usage: 0.00 MB     name: SpecialOffer     type: getml.DataFrame </p> In\u00a0[10]: Copied! <pre>store\n</pre> store Out[10]: name BusinessEntityID SalesPersonID Name                           Demographics                     rowguid                          ModifiedDate        role     unused_float  unused_float unused_string                  unused_string                    unused_string                    unused_string       0 292 279 Next-Door Bike Store &lt;StoreSurvey xmlns=\"http://schem... A22517E3-848D-4EBE-B9D9-7437F343... 2014-09-12 09:15:07 1 294 276 Professional Sales and Service &lt;StoreSurvey xmlns=\"http://schem... B50CA50B-C601-4A13-B07E-2C63862D... 2014-09-12 09:15:07 2 296 277 Riders Company &lt;StoreSurvey xmlns=\"http://schem... 337C3688-1339-4E1A-A08A-B54B2356... 2014-09-12 09:15:07 3 298 275 The Bike Mechanics &lt;StoreSurvey xmlns=\"http://schem... 7894F278-F0C8-4D16-BD75-213FDBF1... 2014-09-12 09:15:07 4 300 286 Nationwide Supply &lt;StoreSurvey xmlns=\"http://schem... C3FC9705-A8C4-4F3A-9550-EB2FA4B7... 2014-09-12 09:15:07 ... ... ... ... ... ... 696 1988 282 Retreat Inn &lt;StoreSurvey xmlns=\"http://schem... EA21EC81-1BFA-4A07-9B4D-73D9852A... 2014-09-12 09:15:07 697 1990 281 Technical Parts Manufacturing &lt;StoreSurvey xmlns=\"http://schem... C8E3C4ED-8F58-4DB2-B600-E0CD11D9... 2014-09-12 09:15:07 698 1992 277 Totes &amp; Baskets Company &lt;StoreSurvey xmlns=\"http://schem... CE860B58-643C-4567-BFD8-06E97969... 2014-09-12 09:15:07 699 1994 277 World of Bikes &lt;StoreSurvey xmlns=\"http://schem... 0C10F2B6-A13A-440C-9C25-5B28D482... 2014-09-12 09:15:07 700 2051 275 A Bicycle Association &lt;StoreSurvey xmlns=\"http://schem... 82237172-D3FE-4A95-82EF-636F6552... 2014-09-12 09:15:07 <p>     701 rows x 6 columns     memory usage: 0.38 MB     name: Store     type: getml.DataFrame </p> <p>getML requires that we define roles for each of the columns.</p> In\u00a0[11]: Copied! <pre>product.set_role(\"ProductID\", getml.data.roles.join_key)\nproduct.set_role([\"MakeFlag\", \"ProductSubcategoryID\", \"ProductModelID\"], getml.data.roles.categorical)\nproduct.set_role([\"SafetyStockLevel\", \"ReorderPoint\", \"StandardCost\", \"ListPrice\"], getml.data.roles.numerical)\n\nproduct\n</pre> product.set_role(\"ProductID\", getml.data.roles.join_key) product.set_role([\"MakeFlag\", \"ProductSubcategoryID\", \"ProductModelID\"], getml.data.roles.categorical) product.set_role([\"SafetyStockLevel\", \"ReorderPoint\", \"StandardCost\", \"ListPrice\"], getml.data.roles.numerical)  product Out[11]: name ProductID MakeFlag    ProductSubcategoryID ProductModelID SafetyStockLevel ReorderPoint StandardCost ListPrice FinishedGoodsFlag DaysToManufacture Name                  ProductNumber Color         Size          SizeUnitMeasureCode WeightUnitMeasureCode Weight        ProductLine   Class         Style         SellStartDate       SellEndDate   DiscontinuedDate rowguid                          ModifiedDate        role  join_key categorical categorical          categorical           numerical    numerical    numerical numerical      unused_float      unused_float unused_string         unused_string unused_string unused_string unused_string       unused_string         unused_string unused_string unused_string unused_string unused_string       unused_string unused_string    unused_string                    unused_string       0 1 0 nan nan 1000 750 0 0 0 0 Adjustable Race AR-5381 NULL NULL NULL NULL NULL NULL NULL NULL 2008-04-30 00:00:00 NULL NULL 694215B7-08F7-4C0D-ACB1-D734BA44... 2014-02-08 09:01:36 1 2 0 nan nan 1000 750 0 0 0 0 Bearing Ball BA-8327 NULL NULL NULL NULL NULL NULL NULL NULL 2008-04-30 00:00:00 NULL NULL 58AE3C20-4F3A-4749-A7D4-D568806C... 2014-02-08 09:01:36 2 3 1 nan nan 800 600 0 0 0 1 BB Ball Bearing BE-2349 NULL NULL NULL NULL NULL NULL NULL NULL 2008-04-30 00:00:00 NULL NULL 9C21AED2-5BFA-4F18-BCB8-F11638DC... 2014-02-08 09:01:36 3 4 0 nan nan 800 600 0 0 0 0 Headset Ball Bearings BE-2908 NULL NULL NULL NULL NULL NULL NULL NULL 2008-04-30 00:00:00 NULL NULL ECFED6CB-51FF-49B5-B06C-7D8AC834... 2014-02-08 09:01:36 4 316 1 nan nan 800 600 0 0 0 1 Blade BL-2036 NULL NULL NULL NULL NULL NULL NULL NULL 2008-04-30 00:00:00 NULL NULL E73E9750-603B-4131-89F5-3DD15ED5... 2014-02-08 09:01:36 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 499 995 1 5 96 500 375 44.9506 101.24 1 1 ML Bottom Bracket BB-8107 NULL NULL NULL G 168.00 NULL M NULL 2013-05-30 00:00:00 NULL NULL 71AB847F-D091-42D6-B735-7B0C2D82... 2014-02-08 09:01:36 500 996 1 5 97 500 375 53.9416 121.49 1 1 HL Bottom Bracket BB-9108 NULL NULL NULL G 170.00 NULL H NULL 2013-05-30 00:00:00 NULL NULL 230C47C5-08B2-4CE3-B706-69C0BDD6... 2014-02-08 09:01:36 501 997 1 2 31 100 75 343.6496 539.99 1 4 Road-750 Black, 44 BK-R19B-44 Black 44 CM LB 19.77 R L U 2013-05-30 00:00:00 NULL NULL 44CE4802-409F-43AB-9B27-CA534218... 2014-02-08 09:01:36 502 998 1 2 31 100 75 343.6496 539.99 1 4 Road-750 Black, 48 BK-R19B-48 Black 48 CM LB 20.13 R L U 2013-05-30 00:00:00 NULL NULL 3DE9A212-1D49-40B6-B10A-F564D981... 2014-02-08 09:01:36 503 999 1 2 31 100 75 343.6496 539.99 1 4 Road-750 Black, 52 BK-R19B-52 Black 52 CM LB 20.42 R L U 2013-05-30 00:00:00 NULL NULL AE638923-2B67-4679-B90E-ABBAB17D... 2014-02-08 09:01:36 <p>     504 rows x 25 columns     memory usage: 0.16 MB     name: Product     type: getml.DataFrame </p> In\u00a0[12]: Copied! <pre>sales_order_detail.set_role([\"SalesOrderID\", \"SalesOrderDetailID\", \"ProductID\", \"SpecialOfferID\"], getml.data.roles.join_key)\nsales_order_detail.set_role([\"OrderQty\", \"UnitPrice\", \"UnitPriceDiscount\", \"LineTotal\"], getml.data.roles.numerical)\nsales_order_detail.set_role(\"ModifiedDate\", getml.data.roles.time_stamp)\n\nsales_order_detail\n</pre> sales_order_detail.set_role([\"SalesOrderID\", \"SalesOrderDetailID\", \"ProductID\", \"SpecialOfferID\"], getml.data.roles.join_key) sales_order_detail.set_role([\"OrderQty\", \"UnitPrice\", \"UnitPriceDiscount\", \"LineTotal\"], getml.data.roles.numerical) sales_order_detail.set_role(\"ModifiedDate\", getml.data.roles.time_stamp)  sales_order_detail Out[12]:   name                ModifiedDate SalesOrderID SalesOrderDetailID ProductID SpecialOfferID  OrderQty UnitPrice UnitPriceDiscount LineTotal CarrierTrackingNumber rowguid                            role                  time_stamp     join_key           join_key  join_key       join_key numerical numerical         numerical numerical unused_string         unused_string                      unit time stamp, comparison only 0 2011-05-30 22:00:00 43659 1 776 1 1 2024.994 0 2024.994 4911-403C-98 B207C96D-D9E6-402B-8470-2CC176C4... 1 2011-05-30 22:00:00 43659 2 777 1 3 2024.994 0 6074.982 4911-403C-98 7ABB600D-1E77-41BE-9FE5-B9142CFC... 2 2011-05-30 22:00:00 43659 3 778 1 1 2024.994 0 2024.994 4911-403C-98 475CF8C6-49F6-486E-B0AD-AFC6A50C... 3 2011-05-30 22:00:00 43659 4 771 1 1 2039.994 0 2039.994 4911-403C-98 04C4DE91-5815-45D6-8670-F462719F... 4 2011-05-30 22:00:00 43659 5 772 1 1 2039.994 0 2039.994 4911-403C-98 5A74C7D2-E641-438E-A7AC-37BF2328... ... ... ... ... ... ... ... ... ... ... ... 121312 2014-06-29 22:00:00 75122 121313 878 1 1 21.98 0 21.98 NULL 8CAD6675-18CC-4F47-8287-97B41A8E... 121313 2014-06-29 22:00:00 75122 121314 712 1 1 8.99 0 8.99 NULL 84F1C363-1C50-4442-BE16-541C59B6... 121314 2014-06-29 22:00:00 75123 121315 878 1 1 21.98 0 21.98 NULL C18B6476-429F-4BB1-828E-2BE5F82A... 121315 2014-06-29 22:00:00 75123 121316 879 1 1 159 0 159 NULL 75A89C6A-C60A-47EA-8A52-B52A9C43... 121316 2014-06-29 22:00:00 75123 121317 712 1 1 8.99 0 8.99 NULL 73646D26-0461-450D-8019-2C6C8586... <p>     121317 rows x 11 columns     memory usage: 14.08 MB     name: SalesOrderDetail     type: getml.DataFrame </p> In\u00a0[13]: Copied! <pre>sales_order_reason.set_role(\"SalesOrderID\", getml.data.roles.join_key)\nsales_order_reason.set_role(\"SalesReasonID\", getml.data.roles.categorical)\n\nsales_order_reason\n</pre> sales_order_reason.set_role(\"SalesOrderID\", getml.data.roles.join_key) sales_order_reason.set_role(\"SalesReasonID\", getml.data.roles.categorical)  sales_order_reason Out[13]:  name SalesOrderID SalesReasonID ModifiedDate         role     join_key categorical   unused_string       0 43697 5 2011-05-30 22:00:00 1 43697 9 2011-05-30 22:00:00 2 43702 5 2011-05-31 22:00:00 3 43702 9 2011-05-31 22:00:00 4 43703 5 2011-05-31 22:00:00 ... ... ... 27642 75119 1 2014-06-29 22:00:00 27643 75120 1 2014-06-29 22:00:00 27644 75121 1 2014-06-29 22:00:00 27645 75122 1 2014-06-29 22:00:00 27646 75123 1 2014-06-29 22:00:00 <p>     27647 rows x 3 columns     memory usage: 1.00 MB     name: SalesOrderHeaderSalesReason     type: getml.DataFrame </p> In\u00a0[14]: Copied! <pre>special_offer.set_role([\"SpecialOfferID\"], getml.data.roles.join_key)\nspecial_offer.set_role([\"MinQty\", \"DiscountPct\"], getml.data.roles.numerical)\nspecial_offer.set_role([\"Category\", \"Description\", \"Type\"], getml.data.roles.categorical)\nspecial_offer.set_role([\"StartDate\", \"EndDate\"], getml.data.roles.time_stamp)\n\nspecial_offer\n</pre> special_offer.set_role([\"SpecialOfferID\"], getml.data.roles.join_key) special_offer.set_role([\"MinQty\", \"DiscountPct\"], getml.data.roles.numerical) special_offer.set_role([\"Category\", \"Description\", \"Type\"], getml.data.roles.categorical) special_offer.set_role([\"StartDate\", \"EndDate\"], getml.data.roles.time_stamp)  special_offer Out[14]: name                   StartDate                     EndDate SpecialOfferID Category    Description                      Type                    MinQty DiscountPct       MaxQty rowguid                          ModifiedDate        role                  time_stamp                  time_stamp       join_key categorical categorical                      categorical          numerical   numerical unused_float unused_string                    unused_string       unit time stamp, comparison only time stamp, comparison only 0 2011-05-01 2014-11-30 1 No Discount No Discount No Discount 0 0 nan 0290C4F5-191F-4337-AB6B-0A2DDE03... 2011-03-31 22:00:00 1 2011-05-31 2014-05-30 2 Reseller Volume Discount 11 to 14 Volume Discount 11 0.02 14 D7542EE7-15DB-4541-985C-5CC27AEF... 2011-04-30 22:00:00 2 2011-05-31 2014-05-30 3 Reseller Volume Discount 15 to 24 Volume Discount 15 0.05 24 4BDBCC01-8CF7-40A9-B643-40EC5B71... 2011-04-30 22:00:00 3 2011-05-31 2014-05-30 4 Reseller Volume Discount 25 to 40 Volume Discount 25 0.1 40 504B5E85-8F3F-4EBC-9E1D-C1BC5DEA... 2011-04-30 22:00:00 4 2011-05-31 2014-05-30 5 Reseller Volume Discount 41 to 60 Volume Discount 41 0.15 60 677E1D9D-944F-4E81-90E8-47EB0A82... 2011-04-30 22:00:00 ... ... ... ... ... ... ... ... ... ... ... 11 2013-05-30 2013-07-14 12 Reseller LL Road Frame Sale Excess Inventory 0 0.35 nan C0AF1C89-9722-4235-9248-3FBA4D9E... 2013-04-29 22:00:00 12 2013-05-30 2013-08-29 13 Reseller Touring-3000 Promotion New Product 0 0.15 nan 5061CCE4-E021-45A8-9A75-DFB36CBB... 2013-04-29 22:00:00 13 2013-05-30 2013-08-29 14 Reseller Touring-1000 Promotion New Product 0 0.2 nan 1AF84A9E-A98C-4BD9-B48F-DC2B8B6B... 2013-04-29 22:00:00 14 2013-07-14 2013-08-14 15 Customer Half-Price Pedal Sale Seasonal Discount 0 0.5 nan 03E3594D-6EBB-46A6-B8EE-A9289C0C... 2013-06-13 22:00:00 15 2014-03-31 2014-05-30 16 Reseller Mountain-500 Silver Clearance Sa... Discontinued Product 0 0.4 nan EB7CB484-BCCF-4D2D-BF73-521B2001... 2014-02-28 23:00:00 <p>     16 rows x 11 columns     memory usage: 0.00 MB     name: SpecialOffer     type: getml.DataFrame </p> In\u00a0[15]: Copied! <pre>store.set_role([\"SalesPersonID\"], getml.data.roles.join_key)\n\nstore[\"test\"] = store[\"ModifiedDate\"].update(getml.data.random() &gt; 0.5, \"NULL\")\n\nstore.set_role([\"SalesPersonID\"], getml.data.roles.join_key)\nstore.set_role([\"test\"], getml.data.roles.time_stamp)\n\nstore\n</pre> store.set_role([\"SalesPersonID\"], getml.data.roles.join_key)  store[\"test\"] = store[\"ModifiedDate\"].update(getml.data.random() &gt; 0.5, \"NULL\")  store.set_role([\"SalesPersonID\"], getml.data.roles.join_key) store.set_role([\"test\"], getml.data.roles.time_stamp)  store Out[15]: name                        test SalesPersonID BusinessEntityID Name                           Demographics                     rowguid                          ModifiedDate        role                  time_stamp      join_key     unused_float unused_string                  unused_string                    unused_string                    unused_string       unit time stamp, comparison only 0 2014-09-12 09:15:07 279 292 Next-Door Bike Store &lt;StoreSurvey xmlns=\"http://schem... A22517E3-848D-4EBE-B9D9-7437F343... 2014-09-12 09:15:07 1 NULL 276 294 Professional Sales and Service &lt;StoreSurvey xmlns=\"http://schem... B50CA50B-C601-4A13-B07E-2C63862D... 2014-09-12 09:15:07 2 2014-09-12 09:15:07 277 296 Riders Company &lt;StoreSurvey xmlns=\"http://schem... 337C3688-1339-4E1A-A08A-B54B2356... 2014-09-12 09:15:07 3 NULL 275 298 The Bike Mechanics &lt;StoreSurvey xmlns=\"http://schem... 7894F278-F0C8-4D16-BD75-213FDBF1... 2014-09-12 09:15:07 4 NULL 286 300 Nationwide Supply &lt;StoreSurvey xmlns=\"http://schem... C3FC9705-A8C4-4F3A-9550-EB2FA4B7... 2014-09-12 09:15:07 ... ... ... ... ... ... ... 696 NULL 282 1988 Retreat Inn &lt;StoreSurvey xmlns=\"http://schem... EA21EC81-1BFA-4A07-9B4D-73D9852A... 2014-09-12 09:15:07 697 2014-09-12 09:15:07 281 1990 Technical Parts Manufacturing &lt;StoreSurvey xmlns=\"http://schem... C8E3C4ED-8F58-4DB2-B600-E0CD11D9... 2014-09-12 09:15:07 698 NULL 277 1992 Totes &amp; Baskets Company &lt;StoreSurvey xmlns=\"http://schem... CE860B58-643C-4567-BFD8-06E97969... 2014-09-12 09:15:07 699 NULL 277 1994 World of Bikes &lt;StoreSurvey xmlns=\"http://schem... 0C10F2B6-A13A-440C-9C25-5B28D482... 2014-09-12 09:15:07 700 2014-09-12 09:15:07 275 2051 A Bicycle Association &lt;StoreSurvey xmlns=\"http://schem... 82237172-D3FE-4A95-82EF-636F6552... 2014-09-12 09:15:07 <p>     701 rows x 7 columns     memory usage: 0.38 MB     name: Store     type: getml.DataFrame </p> In\u00a0[16]: Copied! <pre>sales_order_header[\"SalesPersonIDCat\"] = sales_order_header[\"SalesPersonID\"]\nsales_order_header[\"TerritoryIDCat\"] = sales_order_header[\"TerritoryID\"]\n\nsales_order_header.set_role([\"CustomerID\", \"SalesOrderID\", \"SalesPersonID\", \"TerritoryID\"], getml.data.roles.join_key)\nsales_order_header.set_role(\n    [\"RevisionNumber\", \"OnlineOrderFlag\", \"SalesPersonIDCat\", \"TerritoryIDCat\", \"ShipMethodID\"], \n    getml.data.roles.categorical)\nsales_order_header.set_role([\"SubTotal\", \"TaxAmt\", \"Freight\", \"TotalDue\"], getml.data.roles.numerical)\nsales_order_header.set_role([\"OrderDate\", \"DueDate\", \"ShipDate\", \"ModifiedDate\"], getml.data.roles.time_stamp)\n\nsales_order_header\n</pre> sales_order_header[\"SalesPersonIDCat\"] = sales_order_header[\"SalesPersonID\"] sales_order_header[\"TerritoryIDCat\"] = sales_order_header[\"TerritoryID\"]  sales_order_header.set_role([\"CustomerID\", \"SalesOrderID\", \"SalesPersonID\", \"TerritoryID\"], getml.data.roles.join_key) sales_order_header.set_role(     [\"RevisionNumber\", \"OnlineOrderFlag\", \"SalesPersonIDCat\", \"TerritoryIDCat\", \"ShipMethodID\"],      getml.data.roles.categorical) sales_order_header.set_role([\"SubTotal\", \"TaxAmt\", \"Freight\", \"TotalDue\"], getml.data.roles.numerical) sales_order_header.set_role([\"OrderDate\", \"DueDate\", \"ShipDate\", \"ModifiedDate\"], getml.data.roles.time_stamp)  sales_order_header Out[16]:  name                   OrderDate                     DueDate                    ShipDate                ModifiedDate CustomerID SalesOrderID SalesPersonID TerritoryID RevisionNumber OnlineOrderFlag SalesPersonIDCat TerritoryIDCat ShipMethodID   SubTotal    TaxAmt   Freight   TotalDue       Status BillToAddressID ShipToAddressID CreditCardID CurrencyRateID SalesOrderNumber PurchaseOrderNumber AccountNumber  CreditCardApprovalCode Comment       rowguid                           role                  time_stamp                  time_stamp                  time_stamp                  time_stamp   join_key     join_key      join_key    join_key categorical    categorical     categorical      categorical    categorical   numerical numerical numerical  numerical unused_float    unused_float    unused_float unused_float   unused_float unused_string    unused_string       unused_string  unused_string          unused_string unused_string                     unit time stamp, comparison only time stamp, comparison only time stamp, comparison only time stamp, comparison only 0 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 29825 43659 279 5 8 0 279 5 5 20565.6206 1971.5149 616.0984 23153.2339 5 985 985 16281 nan SO43659 PO522145787 10-4020-000676 105041Vi84182 NULL 79B65321-39CA-4115-9CBA-8FE0903E... 1 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 29672 43660 279 5 8 0 279 5 5 1294.2529 124.2483 38.8276 1457.3288 5 921 921 5618 nan SO43660 PO18850127500 10-4020-000117 115213Vi29411 NULL 738DC42D-D03B-48A1-9822-F95A67EA... 2 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 29734 43661 282 6 8 0 282 6 5 32726.4786 3153.7696 985.553 36865.8012 5 517 517 1346 4 SO43661 PO18473189620 10-4020-000442 85274Vi6854 NULL D91B9131-18A4-4A11-BC3A-90B6F53E... 3 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 29994 43662 282 6 8 0 282 6 5 28832.5289 2775.1646 867.2389 32474.9324 5 482 482 10456 4 SO43662 PO18444174044 10-4020-000227 125295Vi53935 NULL 4A1ECFC0-CC3A-4740-B028-1C50BB48... 4 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 29565 43663 276 4 8 0 276 4 5 419.4589 40.2681 12.5838 472.3108 5 1073 1073 4322 nan SO43663 PO18009186470 10-4020-000510 45303Vi22691 NULL 9B1E7A40-6AE0-4AD3-811C-A6495185... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 31460 2014-06-29 22:00:00 2014-07-12 2014-07-07 2014-07-06 22:00:00 11981 75119 nan 1 8 1 nan 1 1 42.28 3.3824 1.057 46.7194 5 17649 17649 6761 nan SO75119 NULL 10-4030-011981 429826Vi35166 NULL 9382F1C9-383A-435F-9449-0EECEA21... 31461 2014-06-29 22:00:00 2014-07-12 2014-07-07 2014-07-06 22:00:00 18749 75120 nan 6 8 1 nan 6 1 84.96 6.7968 2.124 93.8808 5 28374 28374 8925 nan SO75120 NULL 10-4030-018749 929849Vi46003 NULL AE6A4FCF-FF73-4CD4-AF2C-5993D00D... 31462 2014-06-29 22:00:00 2014-07-12 2014-07-07 2014-07-06 22:00:00 15251 75121 nan 6 8 1 nan 6 1 74.98 5.9984 1.8745 82.8529 5 26553 26553 14220 nan SO75121 NULL 10-4030-015251 529864Vi73738 NULL D7395C0E-00CB-4BFA-A238-0D6A9F49... 31463 2014-06-29 22:00:00 2014-07-12 2014-07-07 2014-07-06 22:00:00 15868 75122 nan 6 8 1 nan 6 1 30.97 2.4776 0.7743 34.2219 5 14616 14616 18719 nan SO75122 NULL 10-4030-015868 330022Vi97312 NULL 4221035A-4159-492F-AF40-4363A64F... 31464 2014-06-29 22:00:00 2014-07-12 2014-07-07 2014-07-06 22:00:00 18759 75123 nan 6 8 1 nan 6 1 189.97 15.1976 4.7493 209.9169 5 14024 14024 10084 nan SO75123 NULL 10-4030-018759 230370Vi51970 NULL D54752FF-2B54-4BE5-95EA-3B72289C... <p>     31465 rows x 28 columns     memory usage: 8.34 MB     name: SalesOrderHeader     type: getml.DataFrame </p> <p>We must also define customer churn. In this case, we define customer churn as a customer not making another purchase within 180 days of his or her last purchase.</p> <p>Thus, the churn variable is defined as follows:</p> <ul> <li>0, if another purchase by the same customer has been made within 180 days after <code>OrderDate</code></li> <li>1, if no purchase by the same customer has been made within 180 days after <code>OrderDate</code></li> <li>NULL, if <code>max(OrderDate) - OrderDate &lt;= 180 days</code></li> </ul> <p>NULL targets can not be used in our analysis.</p> In\u00a0[17]: Copied! <pre>sales_order_header_pd = sales_order_header[[\"OrderDate\", \"CustomerID\", \"SalesOrderID\"]].to_pandas()\n\nrepeat_purchases = sales_order_header_pd.merge(\n    sales_order_header_pd[[\"OrderDate\", \"CustomerID\"]],\n    on=\"CustomerID\",\n    how=\"left\",\n)\n\nrepeat_purchases = repeat_purchases[\n    repeat_purchases[\"OrderDate_y\"] &gt; repeat_purchases[\"OrderDate_x\"]\n]\n\nrepeat_purchases = repeat_purchases[\n    repeat_purchases[\"OrderDate_y\"] - repeat_purchases[\"OrderDate_x\"] &gt; pd.Timedelta('180 days')\n]\n\nrepeat_purchases.groupby(\"SalesOrderID\", as_index=False).aggregate({\"CustomerID\": \"max\"})\n\nrepeat_purchase_ids = {sid: True for sid in repeat_purchases[\"SalesOrderID\"]}\n\ncut_off_date = max(sales_order_header_pd[\"OrderDate\"]) - pd.Timedelta('180 days')\n\nchurn = np.asarray([\n    np.nan if order_date &gt;= cut_off_date else 0 if order_id in repeat_purchase_ids else 1 \n    for (order_date, order_id) in zip(sales_order_header_pd[\"OrderDate\"], sales_order_header_pd[\"SalesOrderID\"])\n])\n\nsales_order_header[\"churn\"] = churn\n\nsales_order_header = sales_order_header[~sales_order_header.churn.is_nan()].to_df(\"SalesOrderHeaderRefined\")\n\nsales_order_header.set_role(\"churn\", getml.data.roles.target)\n\nsales_order_header\n</pre> sales_order_header_pd = sales_order_header[[\"OrderDate\", \"CustomerID\", \"SalesOrderID\"]].to_pandas()  repeat_purchases = sales_order_header_pd.merge(     sales_order_header_pd[[\"OrderDate\", \"CustomerID\"]],     on=\"CustomerID\",     how=\"left\", )  repeat_purchases = repeat_purchases[     repeat_purchases[\"OrderDate_y\"] &gt; repeat_purchases[\"OrderDate_x\"] ]  repeat_purchases = repeat_purchases[     repeat_purchases[\"OrderDate_y\"] - repeat_purchases[\"OrderDate_x\"] &gt; pd.Timedelta('180 days') ]  repeat_purchases.groupby(\"SalesOrderID\", as_index=False).aggregate({\"CustomerID\": \"max\"})  repeat_purchase_ids = {sid: True for sid in repeat_purchases[\"SalesOrderID\"]}  cut_off_date = max(sales_order_header_pd[\"OrderDate\"]) - pd.Timedelta('180 days')  churn = np.asarray([     np.nan if order_date &gt;= cut_off_date else 0 if order_id in repeat_purchase_ids else 1      for (order_date, order_id) in zip(sales_order_header_pd[\"OrderDate\"], sales_order_header_pd[\"SalesOrderID\"]) ])  sales_order_header[\"churn\"] = churn  sales_order_header = sales_order_header[~sales_order_header.churn.is_nan()].to_df(\"SalesOrderHeaderRefined\")  sales_order_header.set_role(\"churn\", getml.data.roles.target)  sales_order_header Out[17]:  name                   OrderDate                     DueDate                    ShipDate                ModifiedDate CustomerID SalesOrderID SalesPersonID TerritoryID  churn RevisionNumber OnlineOrderFlag SalesPersonIDCat TerritoryIDCat ShipMethodID   SubTotal    TaxAmt   Freight   TotalDue       Status BillToAddressID ShipToAddressID CreditCardID CurrencyRateID SalesOrderNumber PurchaseOrderNumber AccountNumber  CreditCardApprovalCode Comment       rowguid                           role                  time_stamp                  time_stamp                  time_stamp                  time_stamp   join_key     join_key      join_key    join_key target categorical    categorical     categorical      categorical    categorical   numerical numerical numerical  numerical unused_float    unused_float    unused_float unused_float   unused_float unused_string    unused_string       unused_string  unused_string          unused_string unused_string                     unit time stamp, comparison only time stamp, comparison only time stamp, comparison only time stamp, comparison only 0 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 29825 43659 279 5 0 8 0 279 5 5 20565.6206 1971.5149 616.0984 23153.2339 5 985 985 16281 nan SO43659 PO522145787 10-4020-000676 105041Vi84182 NULL 79B65321-39CA-4115-9CBA-8FE0903E... 1 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 29672 43660 279 5 0 8 0 279 5 5 1294.2529 124.2483 38.8276 1457.3288 5 921 921 5618 nan SO43660 PO18850127500 10-4020-000117 115213Vi29411 NULL 738DC42D-D03B-48A1-9822-F95A67EA... 2 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 29734 43661 282 6 0 8 0 282 6 5 32726.4786 3153.7696 985.553 36865.8012 5 517 517 1346 4 SO43661 PO18473189620 10-4020-000442 85274Vi6854 NULL D91B9131-18A4-4A11-BC3A-90B6F53E... 3 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 29994 43662 282 6 0 8 0 282 6 5 28832.5289 2775.1646 867.2389 32474.9324 5 482 482 10456 4 SO43662 PO18444174044 10-4020-000227 125295Vi53935 NULL 4A1ECFC0-CC3A-4740-B028-1C50BB48... 4 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 29565 43663 276 4 0 8 0 276 4 5 419.4589 40.2681 12.5838 472.3108 5 1073 1073 4322 nan SO43663 PO18009186470 10-4020-000510 45303Vi22691 NULL 9B1E7A40-6AE0-4AD3-811C-A6495185... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 19699 2013-12-30 23:00:00 2014-01-12 2014-01-07 2014-01-06 23:00:00 20826 63358 nan 7 1 8 1 nan 7 1 1173.96 93.9168 29.349 1297.2258 5 24387 24387 3239 nan SO63358 NULL 10-4030-020826 1142084Vi17039 NULL 41278FBB-3DD8-488B-AEA5-8BF4A6F1... 19700 2013-12-30 23:00:00 2014-01-12 2014-01-07 2014-01-06 23:00:00 24114 63359 nan 10 1 8 1 nan 10 1 1179.47 94.3576 29.4868 1303.3144 5 29682 29682 nan 10770 SO63359 NULL 10-4030-024114 NULL NULL 749532D4-BFF7-4FEC-9F77-396C4A96... 19701 2013-12-30 23:00:00 2014-01-12 2014-01-07 2014-01-06 23:00:00 11417 63360 nan 7 1 8 1 nan 7 1 548.98 43.9184 13.7245 606.6229 5 21465 21465 6582 nan SO63360 NULL 10-4030-011417 242387Vi34223 NULL F92FA2A3-73E1-4DD0-987C-99D1C87E... 19702 2013-12-30 23:00:00 2014-01-12 2014-01-07 2014-01-06 23:00:00 18125 63361 nan 8 1 8 1 nan 8 1 2384.07 190.7256 59.6018 2634.3974 5 26562 26562 11994 nan SO63361 NULL 10-4030-018125 1242859Vi61993 NULL 3A3758BF-CDFA-4740-9104-87BD5A08... 19703 2013-12-30 23:00:00 2014-01-12 2014-01-07 2014-01-06 23:00:00 15692 63362 nan 10 1 8 1 nan 10 1 2419.06 193.5248 60.4765 2673.0613 5 27090 27090 16083 10770 SO63362 NULL 10-4030-015692 242864Vi83167 NULL 3151D31F-4020-41B2-87F3-B6732869... <p>     19704 rows x 29 columns     memory usage: 5.39 MB     name: SalesOrderHeaderRefined     type: getml.DataFrame </p> In\u00a0[18]: Copied! <pre>split = getml.data.split.random(train=0.8, test=0.2)\nsplit\n</pre> split = getml.data.split.random(train=0.8, test=0.2) split Out[18]: 0 train 1 train 2 train 3 test 4 train ... <p>     infinite number of  rows          type: StringColumnView </p> In\u00a0[19]: Copied! <pre>container = getml.data.Container(population=sales_order_header, split=split)\n\ncontainer.add(\n    product=product,\n    sales_order_detail=sales_order_detail, \n    sales_order_header=sales_order_header, \n    sales_order_reason=sales_order_reason,\n    special_offer=special_offer,\n    store=store,\n)\n\ncontainer\n</pre> container = getml.data.Container(population=sales_order_header, split=split)  container.add(     product=product,     sales_order_detail=sales_order_detail,      sales_order_header=sales_order_header,      sales_order_reason=sales_order_reason,     special_offer=special_offer,     store=store, )  container Out[19]: population subset name                     rows type 0 test SalesOrderHeaderRefined 3879 View 1 train SalesOrderHeaderRefined 15825 View peripheral alias              name                          rows type      0 product Product 504 DataFrame 1 sales_order_detail SalesOrderDetail 121317 DataFrame 2 sales_order_header SalesOrderHeaderRefined 19704 DataFrame 3 sales_order_reason SalesOrderHeaderSalesReason 27647 DataFrame 4 special_offer SpecialOffer 16 DataFrame 5 store Store 701 DataFrame In\u00a0[20]: Copied! <pre>dm = getml.data.DataModel(sales_order_header.to_placeholder(\"population\"))\n\ndm.add(getml.data.to_placeholder(\n    product=product,\n    sales_order_detail=sales_order_detail, \n    sales_order_header=sales_order_header, \n    sales_order_reason=sales_order_reason,\n    special_offer=special_offer,\n    store=store,\n))\n\ndm.population.join(\n    dm.sales_order_header,\n    on=\"CustomerID\",\n    time_stamps=\"OrderDate\",\n    lagged_targets=True,\n    horizon=getml.data.time.days(1),\n)\n\ndm.population.join(\n    dm.sales_order_detail,\n    on=\"SalesOrderID\",\n)\n\ndm.population.join(\n    dm.sales_order_reason,\n    on=\"SalesOrderID\", \n)\n\ndm.population.join(\n    dm.store,\n    on=\"SalesPersonID\", \n)\n\ndm.sales_order_detail.join(\n    dm.product,\n    on=\"ProductID\",\n    relationship=getml.data.relationship.many_to_one,\n)\n\ndm.sales_order_detail.join(\n    dm.special_offer,\n    on=\"SpecialOfferID\",\n    relationship=getml.data.relationship.many_to_one,\n)\n\ndm\n</pre> dm = getml.data.DataModel(sales_order_header.to_placeholder(\"population\"))  dm.add(getml.data.to_placeholder(     product=product,     sales_order_detail=sales_order_detail,      sales_order_header=sales_order_header,      sales_order_reason=sales_order_reason,     special_offer=special_offer,     store=store, ))  dm.population.join(     dm.sales_order_header,     on=\"CustomerID\",     time_stamps=\"OrderDate\",     lagged_targets=True,     horizon=getml.data.time.days(1), )  dm.population.join(     dm.sales_order_detail,     on=\"SalesOrderID\", )  dm.population.join(     dm.sales_order_reason,     on=\"SalesOrderID\",  )  dm.population.join(     dm.store,     on=\"SalesPersonID\",  )  dm.sales_order_detail.join(     dm.product,     on=\"ProductID\",     relationship=getml.data.relationship.many_to_one, )  dm.sales_order_detail.join(     dm.special_offer,     on=\"SpecialOfferID\",     relationship=getml.data.relationship.many_to_one, )  dm Out[20]: diagram sales_order_headerproductspecial_offersales_order_detailsales_order_reasonstorepopulationProductID = ProductIDRelationship: many-to-oneSpecialOfferID = SpecialOfferIDRelationship: many-to-oneCustomerID = CustomerIDOrderDate &lt;= OrderDateHorizon: 1.0 daysLagged targets allowedSalesOrderID = SalesOrderIDSalesOrderID = SalesOrderIDSalesPersonID = SalesPersonID staging data frames                      staging table                    0 population POPULATION__STAGING_TABLE_1 1 sales_order_detail, product, special_offer SALES_ORDER_DETAIL__STAGING_TABLE_2 2 sales_order_header SALES_ORDER_HEADER__STAGING_TABLE_3 3 sales_order_reason SALES_ORDER_REASON__STAGING_TABLE_4 4 store STORE__STAGING_TABLE_5 <p>Set-up the feature learner &amp; predictor</p> In\u00a0[21]: Copied! <pre>seasonal = getml.preprocessors.Seasonal()\n\nmapping = getml.preprocessors.Mapping()\n\nfast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    num_threads=1,    \n    num_features=400,\n)\n\nrelboost = getml.feature_learning.Relboost(\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    num_threads=1,    \n)\n\npredictor = getml.predictors.XGBoostClassifier(n_jobs=1)\n</pre> seasonal = getml.preprocessors.Seasonal()  mapping = getml.preprocessors.Mapping()  fast_prop = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     num_threads=1,         num_features=400, )  relboost = getml.feature_learning.Relboost(     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     num_threads=1,     )  predictor = getml.predictors.XGBoostClassifier(n_jobs=1) <p>Build the pipeline</p> In\u00a0[22]: Copied! <pre>pipe1 = getml.Pipeline(\n    tags=['fast_prop'],\n    data_model=dm,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    predictors=[predictor],\n    include_categorical=True,\n)\n\npipe1\n</pre> pipe1 = getml.Pipeline(     tags=['fast_prop'],     data_model=dm,     preprocessors=[mapping],     feature_learners=[fast_prop],     predictors=[predictor],     include_categorical=True, )  pipe1 Out[22]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='CrossEntropyLoss',\n         peripheral=['product', 'sales_order_detail', 'sales_order_header',\n                     'sales_order_reason', 'special_offer', 'store'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['fast_prop'])</pre> In\u00a0[23]: Copied! <pre>pipe2 = getml.Pipeline(\n    tags=['relboost'],\n    data_model=dm,\n    preprocessors=[seasonal, mapping],\n    feature_learners=[relboost],\n    predictors=[predictor],\n    include_categorical=True,\n)\n\npipe2\n</pre> pipe2 = getml.Pipeline(     tags=['relboost'],     data_model=dm,     preprocessors=[seasonal, mapping],     feature_learners=[relboost],     predictors=[predictor],     include_categorical=True, )  pipe2 Out[23]: <pre>Pipeline(data_model='population',\n         feature_learners=['Relboost'],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='CrossEntropyLoss',\n         peripheral=['product', 'sales_order_detail', 'sales_order_header',\n                     'sales_order_reason', 'special_offer', 'store'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Seasonal', 'Mapping'],\n         share_selected_features=0.5,\n         tags=['relboost'])</pre> In\u00a0[24]: Copied! <pre>pipe1.check(container.train)\n</pre> pipe1.check(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 2 issues labeled INFO and 0 issues labeled WARNING.\n</pre> Out[24]: type label                  message                          0 INFO FOREIGN KEYS NOT FOUND When joining POPULATION__STAGING_TABLE_1 and SALES_ORDER_REASON__STAGING_TABLE_4 over 'SalesOrderID' and 'SalesOrderID', there are no corresponding entries for 33.769352% of entries in 'SalesOrderID' in 'POPULATION__STAGING_TABLE_1'. You might want to double-check your join keys. 1 INFO FOREIGN KEYS NOT FOUND When joining POPULATION__STAGING_TABLE_1 and STORE__STAGING_TABLE_5 over 'SalesPersonID' and 'SalesPersonID', there are no corresponding entries for 84.941548% of entries in 'SalesPersonID' in 'POPULATION__STAGING_TABLE_1'. You might want to double-check your join keys. In\u00a0[25]: Copied! <pre>pipe1.fit(container.train)\n</pre> pipe1.fit(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 2 issues labeled INFO and 0 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 710 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:10, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:13.940875\n\n</pre> Out[25]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='CrossEntropyLoss',\n         peripheral=['product', 'sales_order_detail', 'sales_order_header',\n                     'sales_order_reason', 'special_offer', 'store'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['fast_prop', 'container-lYi3CR'])</pre> In\u00a0[26]: Copied! <pre>pipe2.check(container.train)\n</pre> pipe2.check(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 2 issues labeled INFO and 0 issues labeled WARNING.\n</pre> Out[26]: type label                  message                          0 INFO FOREIGN KEYS NOT FOUND When joining POPULATION__STAGING_TABLE_1 and SALES_ORDER_REASON__STAGING_TABLE_4 over 'SalesOrderID' and 'SalesOrderID', there are no corresponding entries for 33.769352% of entries in 'SalesOrderID' in 'POPULATION__STAGING_TABLE_1'. You might want to double-check your join keys. 1 INFO FOREIGN KEYS NOT FOUND When joining POPULATION__STAGING_TABLE_1 and STORE__STAGING_TABLE_5 over 'SalesPersonID' and 'SalesPersonID', there are no corresponding entries for 84.941548% of entries in 'SalesPersonID' in 'POPULATION__STAGING_TABLE_1'. You might want to double-check your join keys. In\u00a0[27]: Copied! <pre>pipe2.fit(container.train)\n</pre> pipe2.fit(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 2 issues labeled INFO and 0 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:24, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:03, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:03, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:30.068949\n\n</pre> Out[27]: <pre>Pipeline(data_model='population',\n         feature_learners=['Relboost'],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='CrossEntropyLoss',\n         peripheral=['product', 'sales_order_detail', 'sales_order_header',\n                     'sales_order_reason', 'special_offer', 'store'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Seasonal', 'Mapping'],\n         share_selected_features=0.5,\n         tags=['relboost', 'container-lYi3CR'])</pre> In\u00a0[28]: Copied! <pre>fastprop_score = pipe1.score(container.test)\nfastprop_score\n</pre> fastprop_score = pipe1.score(container.test) fastprop_score <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[28]: date time           set used target accuracy     auc cross entropy 0 2024-02-21 14:50:35 train churn 0.9187 0.975 0.2106 1 2024-02-21 14:51:07 test churn 0.9142 0.9723 0.2199 In\u00a0[29]: Copied! <pre>relboost_score = pipe2.score(container.test)\nrelboost_score\n</pre> relboost_score = pipe2.score(container.test) relboost_score <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\n</pre> Out[29]: date time           set used target accuracy     auc cross entropy 0 2024-02-21 14:51:07 train churn 0.9329 0.9835 0.1664 1 2024-02-21 14:51:09 test churn 0.9255 0.9788 0.188 In\u00a0[30]: Copied! <pre>population_train_pd = container.train.population.drop(container.train.population.roles.unused).to_pandas()\npopulation_test_pd = container.test.population.drop(container.train.population.roles.unused).to_pandas()\n</pre> population_train_pd = container.train.population.drop(container.train.population.roles.unused).to_pandas() population_test_pd = container.test.population.drop(container.train.population.roles.unused).to_pandas() In\u00a0[31]: Copied! <pre>product_pd = product.drop(product.roles.unused).to_pandas()\nsales_order_detail_pd = sales_order_detail.drop(sales_order_detail.roles.unused).to_pandas()\nsales_order_header_pd = sales_order_header.drop(sales_order_header.roles.unused).to_pandas()\nsales_order_reason_pd = sales_order_reason.drop(sales_order_reason.roles.unused).to_pandas()\nspecial_offer_pd = special_offer.drop(special_offer.roles.unused).to_pandas()\nstore_pd = store.drop(store.roles.unused).to_pandas()\n</pre> product_pd = product.drop(product.roles.unused).to_pandas() sales_order_detail_pd = sales_order_detail.drop(sales_order_detail.roles.unused).to_pandas() sales_order_header_pd = sales_order_header.drop(sales_order_header.roles.unused).to_pandas() sales_order_reason_pd = sales_order_reason.drop(sales_order_reason.roles.unused).to_pandas() special_offer_pd = special_offer.drop(special_offer.roles.unused).to_pandas() store_pd = store.drop(store.roles.unused).to_pandas() <p>featuretools does not support many-to-one joins. Therefore, we must manually merge <code>sales_order_detail_pd</code>, <code>product_pd</code> and <code>special_offer_pd</code>.</p> In\u00a0[32]: Copied! <pre>sales_order_detail_pd = sales_order_detail_pd.merge(\n    product_pd,\n    on=\"ProductID\",\n    how=\"left\",\n)\n\nsales_order_detail_pd = sales_order_detail_pd.merge(\n    special_offer_pd,\n    on=\"SpecialOfferID\",\n    how=\"left\",\n)\n\ndel sales_order_detail_pd[\"SalesOrderDetailID\"]\ndel sales_order_detail_pd[\"ProductID\"]\ndel sales_order_detail_pd[\"SpecialOfferID\"]\n\nsales_order_detail_pd\n</pre> sales_order_detail_pd = sales_order_detail_pd.merge(     product_pd,     on=\"ProductID\",     how=\"left\", )  sales_order_detail_pd = sales_order_detail_pd.merge(     special_offer_pd,     on=\"SpecialOfferID\",     how=\"left\", )  del sales_order_detail_pd[\"SalesOrderDetailID\"] del sales_order_detail_pd[\"ProductID\"] del sales_order_detail_pd[\"SpecialOfferID\"]  sales_order_detail_pd Out[32]: SalesOrderID OrderQty UnitPrice UnitPriceDiscount LineTotal ModifiedDate MakeFlag ProductSubcategoryID ProductModelID SafetyStockLevel ReorderPoint StandardCost ListPrice Category Description Type MinQty DiscountPct StartDate EndDate 0 43659 1.0 2024.994 0.0 2024.994 2011-05-30 22:00:00 1 1 19 100.0 75.0 1898.0944 3374.99 No Discount No Discount No Discount 0.0 0.0 2011-05-01 2014-11-30 1 43659 3.0 2024.994 0.0 6074.982 2011-05-30 22:00:00 1 1 19 100.0 75.0 1898.0944 3374.99 No Discount No Discount No Discount 0.0 0.0 2011-05-01 2014-11-30 2 43659 1.0 2024.994 0.0 2024.994 2011-05-30 22:00:00 1 1 19 100.0 75.0 1898.0944 3374.99 No Discount No Discount No Discount 0.0 0.0 2011-05-01 2014-11-30 3 43659 1.0 2039.994 0.0 2039.994 2011-05-30 22:00:00 1 1 19 100.0 75.0 1912.1544 3399.99 No Discount No Discount No Discount 0.0 0.0 2011-05-01 2014-11-30 4 43659 1.0 2039.994 0.0 2039.994 2011-05-30 22:00:00 1 1 19 100.0 75.0 1912.1544 3399.99 No Discount No Discount No Discount 0.0 0.0 2011-05-01 2014-11-30 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 121312 75122 1.0 21.980 0.0 21.980 2014-06-29 22:00:00 0 30 121 4.0 3.0 8.2205 21.98 No Discount No Discount No Discount 0.0 0.0 2011-05-01 2014-11-30 121313 75122 1.0 8.990 0.0 8.990 2014-06-29 22:00:00 0 19 2 4.0 3.0 6.9223 8.99 No Discount No Discount No Discount 0.0 0.0 2011-05-01 2014-11-30 121314 75123 1.0 21.980 0.0 21.980 2014-06-29 22:00:00 0 30 121 4.0 3.0 8.2205 21.98 No Discount No Discount No Discount 0.0 0.0 2011-05-01 2014-11-30 121315 75123 1.0 159.000 0.0 159.000 2014-06-29 22:00:00 0 27 122 4.0 3.0 59.4660 159.00 No Discount No Discount No Discount 0.0 0.0 2011-05-01 2014-11-30 121316 75123 1.0 8.990 0.0 8.990 2014-06-29 22:00:00 0 19 2 4.0 3.0 6.9223 8.99 No Discount No Discount No Discount 0.0 0.0 2011-05-01 2014-11-30 <p>121317 rows \u00d7 20 columns</p> In\u00a0[33]: Copied! <pre>def prepare_sales_order_header(peripheral_pd, train_or_test):\n    \"\"\"\n    Helper function that imitates the behavior of \n    the data model defined above.\n    \"\"\"\n    peripheral_new = peripheral_pd.merge(\n        train_or_test[[\"CustomerID\", \"OrderDate\", \"SalesOrderID\"]],\n        on=\"CustomerID\"\n    )\n\n    peripheral_new = peripheral_new[\n        peripheral_new[\"OrderDate_x\"] &lt; peripheral_new[\"OrderDate_y\"]\n    ]\n    \n    del peripheral_new[\"SalesOrderID_x\"]\n    del peripheral_new[\"OrderDate_y\"]\n    del peripheral_new[\"CustomerID\"]\n    del peripheral_new[\"SalesPersonIDCat\"]\n    del peripheral_new[\"TerritoryIDCat\"]\n    \n    return peripheral_new.rename(columns={\"OrderDate_x\": \"OrderDate\", \"SalesOrderID_y\": \"SalesOrderID\"})\n</pre> def prepare_sales_order_header(peripheral_pd, train_or_test):     \"\"\"     Helper function that imitates the behavior of      the data model defined above.     \"\"\"     peripheral_new = peripheral_pd.merge(         train_or_test[[\"CustomerID\", \"OrderDate\", \"SalesOrderID\"]],         on=\"CustomerID\"     )      peripheral_new = peripheral_new[         peripheral_new[\"OrderDate_x\"] &lt; peripheral_new[\"OrderDate_y\"]     ]          del peripheral_new[\"SalesOrderID_x\"]     del peripheral_new[\"OrderDate_y\"]     del peripheral_new[\"CustomerID\"]     del peripheral_new[\"SalesPersonIDCat\"]     del peripheral_new[\"TerritoryIDCat\"]          return peripheral_new.rename(columns={\"OrderDate_x\": \"OrderDate\", \"SalesOrderID_y\": \"SalesOrderID\"}) In\u00a0[34]: Copied! <pre>def prepare_store(peripheral_pd, train_or_test):\n    \"\"\"\n    Helper function that imitates the behavior of \n    the data model defined above.\n    \"\"\"\n    peripheral_new = peripheral_pd.merge(\n        train_or_test[[\"SalesPersonID\", \"SalesOrderID\"]],\n        on=\"SalesPersonID\"\n    )\n\n    return peripheral_new\n</pre> def prepare_store(peripheral_pd, train_or_test):     \"\"\"     Helper function that imitates the behavior of      the data model defined above.     \"\"\"     peripheral_new = peripheral_pd.merge(         train_or_test[[\"SalesPersonID\", \"SalesOrderID\"]],         on=\"SalesPersonID\"     )      return peripheral_new In\u00a0[35]: Copied! <pre>store_train_pd = prepare_store(store_pd, population_train_pd)\nstore_test_pd = prepare_store(store_pd, population_test_pd)\nstore_train_pd\n</pre> store_train_pd = prepare_store(store_pd, population_train_pd) store_test_pd = prepare_store(store_pd, population_test_pd) store_train_pd Out[35]: SalesPersonID test SalesOrderID 0 279 2014-09-12 09:15:07 43659 1 279 2014-09-12 09:15:07 43660 2 279 2014-09-12 09:15:07 43681 3 279 2014-09-12 09:15:07 43685 4 279 2014-09-12 09:15:07 43695 ... ... ... ... 142427 290 NaT 63216 142428 290 NaT 63217 142429 290 NaT 63223 142430 290 NaT 63282 142431 290 NaT 63284 <p>142432 rows \u00d7 3 columns</p> In\u00a0[36]: Copied! <pre>sales_order_header_train_pd = prepare_sales_order_header(sales_order_header_pd, population_train_pd)\nsales_order_header_test_pd = prepare_sales_order_header(sales_order_header_pd, population_test_pd)\nsales_order_header_train_pd\n</pre> sales_order_header_train_pd = prepare_sales_order_header(sales_order_header_pd, population_train_pd) sales_order_header_test_pd = prepare_sales_order_header(sales_order_header_pd, population_test_pd) sales_order_header_train_pd Out[36]: RevisionNumber OnlineOrderFlag ShipMethodID SalesPersonID TerritoryID SubTotal TaxAmt Freight TotalDue churn OrderDate DueDate ShipDate ModifiedDate SalesOrderID 1 8 0 5 279 5 20565.6206 1971.5149 616.0984 23153.2339 0.0 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 44305 2 8 0 5 279 5 20565.6206 1971.5149 616.0984 23153.2339 0.0 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 45061 3 8 0 5 279 5 20565.6206 1971.5149 616.0984 23153.2339 0.0 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 45779 4 8 0 5 279 5 20565.6206 1971.5149 616.0984 23153.2339 0.0 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 46604 5 8 0 5 279 5 20565.6206 1971.5149 616.0984 23153.2339 0.0 2011-05-30 22:00:00 2011-06-12 2011-06-07 2011-06-06 22:00:00 47693 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 39894 8 1 1 None 6 32.6000 2.6080 0.8150 36.0230 1.0 2013-12-14 23:00:00 2013-12-27 2013-12-22 2013-12-21 23:00:00 62799 39908 8 1 1 None 4 42.2800 3.3824 1.0570 46.7194 1.0 2013-12-12 23:00:00 2013-12-25 2013-12-20 2013-12-19 23:00:00 62770 39952 8 1 1 None 1 53.9900 4.3192 1.3498 59.6590 1.0 2013-12-14 23:00:00 2013-12-27 2013-12-22 2013-12-21 23:00:00 62796 40080 8 1 1 None 4 35.0000 2.8000 0.8750 38.6750 1.0 2013-12-18 23:00:00 2013-12-31 2013-12-26 2013-12-25 23:00:00 62643 40101 8 1 1 None 10 12.9400 1.0352 0.3235 14.2987 1.0 2013-12-19 23:00:00 2014-01-01 2013-12-27 2013-12-26 23:00:00 62914 <p>12271 rows \u00d7 15 columns</p> In\u00a0[37]: Copied! <pre>del population_train_pd[\"CustomerID\"]\ndel population_train_pd[\"SalesPersonIDCat\"]\n\ndel population_test_pd[\"CustomerID\"]\ndel population_test_pd[\"SalesPersonIDCat\"]\n</pre> del population_train_pd[\"CustomerID\"] del population_train_pd[\"SalesPersonIDCat\"]  del population_test_pd[\"CustomerID\"] del population_test_pd[\"SalesPersonIDCat\"] <p>featuretools's dataframes are similar for getML's container class. featuretools uses woodwork for typing information, which will be guessed automatically, but can also be provided manually.</p> In\u00a0[38]: Copied! <pre>def add_index(df):\n    df.insert(0, \"index\", range(len(df)))\n\npopulation_pd_logical_types = {\n    'RevisionNumber': ww.logical_types.Categorical,\n    'OnlineOrderFlag': ww.logical_types.Categorical,\n    'ShipMethodID': ww.logical_types.Integer,\n    'TerritoryIDCat': ww.logical_types.Integer,\n    'SalesOrderID': ww.logical_types.Integer,\n    'SalesPersonID': ww.logical_types.IntegerNullable,\n    'TerritoryID': ww.logical_types.IntegerNullable,\n    'SubTotal': ww.logical_types.Double,\n    'TaxAmt': ww.logical_types.Double,\n    'Freight': ww.logical_types.Double,\n    'TotalDue': ww.logical_types.Double,\n    'churn': ww.logical_types.Categorical,\n    'OrderDate': ww.logical_types.Datetime,\n    'DueDate': ww.logical_types.Datetime,\n    'ShipDate': ww.logical_types.Datetime,\n    'ModifiedDate': ww.logical_types.Datetime\n}\npopulation_train_pd.ww.init(logical_types=population_pd_logical_types, index=\"SalesOrderID\", name=\"population\")\npopulation_test_pd.ww.init(logical_types=population_pd_logical_types, index=\"SalesOrderID\", name=\"population\")\n\nsales_order_detail_pd_logical_types = {\n    'index': ww.logical_types.Integer,\n    'SalesOrderID': ww.logical_types.Integer,\n    'OrderQty': ww.logical_types.Integer,\n    'UnitPrice': ww.logical_types.Double,\n    'UnitPriceDiscount': ww.logical_types.Double,\n    'LineTotal': ww.logical_types.Double,\n    'ModifiedDate': ww.logical_types.Datetime,\n    # merged with product_pd\n    'MakeFlag': ww.logical_types.Categorical,\n    'ProductSubcategoryID': ww.logical_types.Categorical,\n    'ProductModelID': ww.logical_types.Integer,\n    'ReorderPoint': ww.logical_types.Integer,\n    'StandardCost': ww.logical_types.Double,\n    'ListPrice': ww.logical_types.Double,\n    # merged with special_offer_pd\n    'Category': ww.logical_types.Categorical,\n    'Description': ww.logical_types.Categorical,\n    'Type': ww.logical_types.Categorical,\n    'MinQty': ww.logical_types.Integer,\n    'DiscountPct': ww.logical_types.Double,\n    'StartDate': ww.logical_types.Datetime,\n    'EndDate': ww.logical_types.Datetime\n}\nadd_index(sales_order_detail_pd)\nsales_order_detail_pd.ww.init(logical_types=sales_order_detail_pd_logical_types, index=\"index\", name=\"sales_order_detail\")\n\nsales_order_header_pd_logical_types = {\n    'index': ww.logical_types.Integer,\n    'RevisionNumber': ww.logical_types.Categorical,\n    'OnlineOrderFlag': ww.logical_types.Categorical,\n    'ShipMethodID': ww.logical_types.Integer,\n    'SalesOrderID': ww.logical_types.Integer,\n    'SalesPersonID': ww.logical_types.IntegerNullable,\n    'TerritoryID': ww.logical_types.IntegerNullable,\n    'SubTotal': ww.logical_types.Double,\n    'TaxAmt': ww.logical_types.Double,\n    'Freight': ww.logical_types.Double,\n    'TotalDue': ww.logical_types.Double,\n    'churn': ww.logical_types.Categorical,\n    'OrderDate': ww.logical_types.Datetime,\n    'DueDate': ww.logical_types.Datetime,\n    'ShipDate': ww.logical_types.Datetime,\n    'ModifiedDate': ww.logical_types.Datetime\n}\nadd_index(sales_order_header_train_pd)\nsales_order_header_train_pd.ww.init(logical_types=sales_order_header_pd_logical_types, index=\"index\", name=\"sales_order_header\")\nadd_index(sales_order_header_test_pd)\nsales_order_header_test_pd.ww.init(logical_types=sales_order_header_pd_logical_types, index=\"index\", name=\"sales_order_header\")\n\nsales_order_reason_pd_logical_types = {\n    'index': ww.logical_types.Integer,\n    'SalesReasonID': ww.logical_types.Categorical,\n    'SalesOrderID': ww.logical_types.Integer\n}\nadd_index(sales_order_reason_pd)\nsales_order_reason_pd.ww.init(logical_types=sales_order_reason_pd_logical_types, index=\"index\", name=\"sales_order_reason\")\n\nstore_pd_logical_types = {\n    'index': ww.logical_types.Integer,\n    'SalesPersonID': ww.logical_types.IntegerNullable,\n    'test': ww.logical_types.Datetime,\n    'SalesOrderID': ww.logical_types.Integer\n}\nadd_index(store_train_pd)\nstore_train_pd.ww.init(logical_types=store_pd_logical_types, index=\"index\", name=\"store\")\nadd_index(store_test_pd)\nstore_test_pd.ww.init(logical_types=store_pd_logical_types, index=\"index\", name=\"store\")\n</pre> def add_index(df):     df.insert(0, \"index\", range(len(df)))  population_pd_logical_types = {     'RevisionNumber': ww.logical_types.Categorical,     'OnlineOrderFlag': ww.logical_types.Categorical,     'ShipMethodID': ww.logical_types.Integer,     'TerritoryIDCat': ww.logical_types.Integer,     'SalesOrderID': ww.logical_types.Integer,     'SalesPersonID': ww.logical_types.IntegerNullable,     'TerritoryID': ww.logical_types.IntegerNullable,     'SubTotal': ww.logical_types.Double,     'TaxAmt': ww.logical_types.Double,     'Freight': ww.logical_types.Double,     'TotalDue': ww.logical_types.Double,     'churn': ww.logical_types.Categorical,     'OrderDate': ww.logical_types.Datetime,     'DueDate': ww.logical_types.Datetime,     'ShipDate': ww.logical_types.Datetime,     'ModifiedDate': ww.logical_types.Datetime } population_train_pd.ww.init(logical_types=population_pd_logical_types, index=\"SalesOrderID\", name=\"population\") population_test_pd.ww.init(logical_types=population_pd_logical_types, index=\"SalesOrderID\", name=\"population\")  sales_order_detail_pd_logical_types = {     'index': ww.logical_types.Integer,     'SalesOrderID': ww.logical_types.Integer,     'OrderQty': ww.logical_types.Integer,     'UnitPrice': ww.logical_types.Double,     'UnitPriceDiscount': ww.logical_types.Double,     'LineTotal': ww.logical_types.Double,     'ModifiedDate': ww.logical_types.Datetime,     # merged with product_pd     'MakeFlag': ww.logical_types.Categorical,     'ProductSubcategoryID': ww.logical_types.Categorical,     'ProductModelID': ww.logical_types.Integer,     'ReorderPoint': ww.logical_types.Integer,     'StandardCost': ww.logical_types.Double,     'ListPrice': ww.logical_types.Double,     # merged with special_offer_pd     'Category': ww.logical_types.Categorical,     'Description': ww.logical_types.Categorical,     'Type': ww.logical_types.Categorical,     'MinQty': ww.logical_types.Integer,     'DiscountPct': ww.logical_types.Double,     'StartDate': ww.logical_types.Datetime,     'EndDate': ww.logical_types.Datetime } add_index(sales_order_detail_pd) sales_order_detail_pd.ww.init(logical_types=sales_order_detail_pd_logical_types, index=\"index\", name=\"sales_order_detail\")  sales_order_header_pd_logical_types = {     'index': ww.logical_types.Integer,     'RevisionNumber': ww.logical_types.Categorical,     'OnlineOrderFlag': ww.logical_types.Categorical,     'ShipMethodID': ww.logical_types.Integer,     'SalesOrderID': ww.logical_types.Integer,     'SalesPersonID': ww.logical_types.IntegerNullable,     'TerritoryID': ww.logical_types.IntegerNullable,     'SubTotal': ww.logical_types.Double,     'TaxAmt': ww.logical_types.Double,     'Freight': ww.logical_types.Double,     'TotalDue': ww.logical_types.Double,     'churn': ww.logical_types.Categorical,     'OrderDate': ww.logical_types.Datetime,     'DueDate': ww.logical_types.Datetime,     'ShipDate': ww.logical_types.Datetime,     'ModifiedDate': ww.logical_types.Datetime } add_index(sales_order_header_train_pd) sales_order_header_train_pd.ww.init(logical_types=sales_order_header_pd_logical_types, index=\"index\", name=\"sales_order_header\") add_index(sales_order_header_test_pd) sales_order_header_test_pd.ww.init(logical_types=sales_order_header_pd_logical_types, index=\"index\", name=\"sales_order_header\")  sales_order_reason_pd_logical_types = {     'index': ww.logical_types.Integer,     'SalesReasonID': ww.logical_types.Categorical,     'SalesOrderID': ww.logical_types.Integer } add_index(sales_order_reason_pd) sales_order_reason_pd.ww.init(logical_types=sales_order_reason_pd_logical_types, index=\"index\", name=\"sales_order_reason\")  store_pd_logical_types = {     'index': ww.logical_types.Integer,     'SalesPersonID': ww.logical_types.IntegerNullable,     'test': ww.logical_types.Datetime,     'SalesOrderID': ww.logical_types.Integer } add_index(store_train_pd) store_train_pd.ww.init(logical_types=store_pd_logical_types, index=\"index\", name=\"store\") add_index(store_test_pd) store_test_pd.ww.init(logical_types=store_pd_logical_types, index=\"index\", name=\"store\") In\u00a0[39]: Copied! <pre>dataframes_train = {\n    \"population\" : (population_train_pd, ),\n    \"sales_order_header\": (sales_order_header_train_pd, ),\n    \"sales_order_detail\": (sales_order_detail_pd, ),\n    \"sales_order_reason\": (sales_order_reason_pd, ),\n    \"store\": (store_train_pd, ),\n}\n</pre> dataframes_train = {     \"population\" : (population_train_pd, ),     \"sales_order_header\": (sales_order_header_train_pd, ),     \"sales_order_detail\": (sales_order_detail_pd, ),     \"sales_order_reason\": (sales_order_reason_pd, ),     \"store\": (store_train_pd, ), } In\u00a0[40]: Copied! <pre>dataframes_test = {\n    \"population\" : (population_test_pd, ),\n    \"sales_order_header\": (sales_order_header_test_pd, ),\n    \"sales_order_detail\": (sales_order_detail_pd, ),\n    \"sales_order_reason\": (sales_order_reason_pd, ),\n    \"store\": (store_test_pd, ),\n}\n</pre> dataframes_test = {     \"population\" : (population_test_pd, ),     \"sales_order_header\": (sales_order_header_test_pd, ),     \"sales_order_detail\": (sales_order_detail_pd, ),     \"sales_order_reason\": (sales_order_reason_pd, ),     \"store\": (store_test_pd, ), } <p>featuretools's relationships are similar for getML's data model.</p> In\u00a0[41]: Copied! <pre>relationships = [\n    (\"population\", \"SalesOrderID\", \"sales_order_header\", \"SalesOrderID\"),\n    (\"population\", \"SalesOrderID\", \"sales_order_detail\", \"SalesOrderID\"),\n    (\"population\", \"SalesOrderID\", \"sales_order_reason\", \"SalesOrderID\"),\n    (\"population\", \"SalesOrderID\", \"store\", \"SalesOrderID\"),\n]\n</pre> relationships = [     (\"population\", \"SalesOrderID\", \"sales_order_header\", \"SalesOrderID\"),     (\"population\", \"SalesOrderID\", \"sales_order_detail\", \"SalesOrderID\"),     (\"population\", \"SalesOrderID\", \"sales_order_reason\", \"SalesOrderID\"),     (\"population\", \"SalesOrderID\", \"store\", \"SalesOrderID\"), ] In\u00a0[42]: Copied! <pre>featuretools_train_pd = featuretools.dfs(\n    dataframes=dataframes_train,\n    relationships=relationships,\n    target_dataframe_name=\"population\")[0]\n</pre> featuretools_train_pd = featuretools.dfs(     dataframes=dataframes_train,     relationships=relationships,     target_dataframe_name=\"population\")[0] In\u00a0[43]: Copied! <pre>featuretools_test_pd = featuretools.dfs(\n    dataframes=dataframes_test,\n    relationships=relationships,\n    target_dataframe_name=\"population\")[0]\n</pre> featuretools_test_pd = featuretools.dfs(     dataframes=dataframes_test,     relationships=relationships,     target_dataframe_name=\"population\")[0] In\u00a0[44]: Copied! <pre>featuretools_train_pd\n</pre> featuretools_train_pd Out[44]: RevisionNumber OnlineOrderFlag TerritoryIDCat ShipMethodID SalesPersonID TerritoryID SubTotal TaxAmt Freight TotalDue ... NUM_UNIQUE(sales_order_detail.YEAR(ModifiedDate)) NUM_UNIQUE(sales_order_detail.YEAR(StartDate)) MODE(store.DAY(test)) MODE(store.MONTH(test)) MODE(store.WEEKDAY(test)) MODE(store.YEAR(test)) NUM_UNIQUE(store.DAY(test)) NUM_UNIQUE(store.MONTH(test)) NUM_UNIQUE(store.WEEKDAY(test)) NUM_UNIQUE(store.YEAR(test)) SalesOrderID 43659 8 0 5 5 279 5 20565.6206 1971.5149 616.0984 23153.2339 ... 1 1 12 9 4 2014 1 1 1 1 43660 8 0 5 5 279 5 1294.2529 124.2483 38.8276 1457.3288 ... 1 1 12 9 4 2014 1 1 1 1 43661 8 0 6 5 282 6 32726.4786 3153.7696 985.5530 36865.8012 ... 1 1 12 9 4 2014 1 1 1 1 43663 8 0 4 5 276 4 419.4589 40.2681 12.5838 472.3108 ... 1 1 12 9 4 2014 1 1 1 1 43664 8 0 1 5 280 1 24432.6088 2344.9921 732.8100 27510.4109 ... 1 1 12 9 4 2014 1 1 1 1 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 63358 8 1 7 1 &lt;NA&gt; 7 1173.9600 93.9168 29.3490 1297.2258 ... 1 1 NaN NaN NaN NaN &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 63359 8 1 10 1 &lt;NA&gt; 10 1179.4700 94.3576 29.4868 1303.3144 ... 1 1 NaN NaN NaN NaN &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 63360 8 1 7 1 &lt;NA&gt; 7 548.9800 43.9184 13.7245 606.6229 ... 1 1 NaN NaN NaN NaN &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 63361 8 1 8 1 &lt;NA&gt; 8 2384.0700 190.7256 59.6018 2634.3974 ... 1 1 NaN NaN NaN NaN &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 63362 8 1 10 1 &lt;NA&gt; 10 2419.0600 193.5248 60.4765 2673.0613 ... 1 1 NaN NaN NaN NaN &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; <p>15825 rows \u00d7 227 columns</p> In\u00a0[45]: Copied! <pre>featuretools_train = getml.data.DataFrame.from_pandas(featuretools_train_pd, \"featuretools_train\")\nfeaturetools_test = getml.data.DataFrame.from_pandas(featuretools_test_pd, \"featuretools_test\")\n</pre> featuretools_train = getml.data.DataFrame.from_pandas(featuretools_train_pd, \"featuretools_train\") featuretools_test = getml.data.DataFrame.from_pandas(featuretools_test_pd, \"featuretools_test\") In\u00a0[46]: Copied! <pre>featuretools_train.set_role(\"churn\", getml.data.roles.target)\nfeaturetools_train.set_role(featuretools_train.roles.unused_float, getml.data.roles.numerical)\nfeaturetools_train.set_role(featuretools_train.roles.unused_string, getml.data.roles.categorical)\n\nfeaturetools_train\n</pre> featuretools_train.set_role(\"churn\", getml.data.roles.target) featuretools_train.set_role(featuretools_train.roles.unused_float, getml.data.roles.numerical) featuretools_train.set_role(featuretools_train.roles.unused_string, getml.data.roles.categorical)  featuretools_train Out[46]:  name  churn RevisionNumber OnlineOrderFlag SalesPersonID TerritoryID COUNT(sales_order_header) MODE(sales_order_header.OnlineOrderFlag) MODE(sales_order_header.RevisionNumber) MODE(sales_order_header.churn) NUM_UNIQUE(sales_order_header.OnlineOrderFlag) NUM_UNIQUE(sales_order_header.RevisionNumber) NUM_UNIQUE(sales_order_header.churn) COUNT(sales_order_detail) MODE(sales_order_detail.Category) MODE(sales_order_detail.Description) MODE(sales_order_detail.MakeFlag) MODE(sales_order_detail.ProductSubcategoryID) MODE(sales_order_detail.Type) NUM_UNIQUE(sales_order_detail.Category) NUM_UNIQUE(sales_order_detail.Description) NUM_UNIQUE(sales_order_detail.MakeFlag) NUM_UNIQUE(sales_order_detail.ProductSubcategoryID) NUM_UNIQUE(sales_order_detail.Type) COUNT(sales_order_reason) MODE(sales_order_reason.SalesReasonID) NUM_UNIQUE(sales_order_reason.SalesReasonID) COUNT(store) DAY(DueDate) DAY(ModifiedDate) DAY(OrderDate) DAY(ShipDate) MONTH(DueDate) MONTH(ModifiedDate) MONTH(OrderDate) MONTH(ShipDate) WEEKDAY(DueDate) WEEKDAY(ModifiedDate) WEEKDAY(OrderDate) WEEKDAY(ShipDate) YEAR(DueDate) YEAR(ModifiedDate) YEAR(OrderDate) YEAR(ShipDate) MODE(sales_order_header.DAY(DueDate)) MODE(sales_order_header.DAY(ModifiedDate)) MODE(sales_order_header.DAY(OrderDate)) MODE(sales_order_header.DAY(ShipDate)) MODE(sales_order_header.MONTH(DueDate)) MODE(sales_order_header.MONTH(ModifiedDate)) MODE(sales_order_header.MONTH(OrderDate)) MODE(sales_order_header.MONTH(ShipDate)) MODE(sales_order_header.WEEKDAY(DueDate)) MODE(sales_order_header.WEEKDAY(ModifiedDate)) MODE(sales_order_header.WEEKDAY(OrderDate)) MODE(sales_order_header.WEEKDAY(ShipDate)) MODE(sales_order_header.YEAR(DueDate)) MODE(sales_order_header.YEAR(ModifiedDate)) MODE(sales_order_header.YEAR(OrderDate)) MODE(sales_order_header.YEAR(ShipDate)) NUM_UNIQUE(sales_order_header.DAY(DueDate)) NUM_UNIQUE(sales_order_header.DAY(ModifiedDate)) NUM_UNIQUE(sales_order_header.DAY(OrderDate)) NUM_UNIQUE(sales_order_header.DAY(ShipDate)) NUM_UNIQUE(sales_order_header.MONTH(DueDate)) NUM_UNIQUE(sales_order_header.MONTH(ModifiedDate)) NUM_UNIQUE(sales_order_header.MONTH(OrderDate)) NUM_UNIQUE(sales_order_header.MONTH(ShipDate)) NUM_UNIQUE(sales_order_header.WEEKDAY(DueDate)) NUM_UNIQUE(sales_order_header.WEEKDAY(ModifiedDate)) NUM_UNIQUE(sales_order_header.WEEKDAY(OrderDate)) NUM_UNIQUE(sales_order_header.WEEKDAY(ShipDate)) NUM_UNIQUE(sales_order_header.YEAR(DueDate)) NUM_UNIQUE(sales_order_header.YEAR(ModifiedDate)) NUM_UNIQUE(sales_order_header.YEAR(OrderDate)) NUM_UNIQUE(sales_order_header.YEAR(ShipDate)) MODE(sales_order_detail.DAY(EndDate)) MODE(sales_order_detail.DAY(ModifiedDate)) MODE(sales_order_detail.DAY(StartDate)) MODE(sales_order_detail.MONTH(EndDate)) MODE(sales_order_detail.MONTH(ModifiedDate)) MODE(sales_order_detail.MONTH(StartDate)) MODE(sales_order_detail.WEEKDAY(EndDate)) MODE(sales_order_detail.WEEKDAY(ModifiedDate)) MODE(sales_order_detail.WEEKDAY(StartDate)) MODE(sales_order_detail.YEAR(EndDate)) MODE(sales_order_detail.YEAR(ModifiedDate)) MODE(sales_order_detail.YEAR(StartDate)) NUM_UNIQUE(sales_order_detail.DAY(EndDate)) NUM_UNIQUE(sales_order_detail.DAY(ModifiedDate)) NUM_UNIQUE(sales_order_detail.DAY(StartDate)) NUM_UNIQUE(sales_order_detail.MONTH(EndDate)) NUM_UNIQUE(sales_order_detail.MONTH(ModifiedDate)) NUM_UNIQUE(sales_order_detail.MONTH(StartDate)) NUM_UNIQUE(sales_order_detail.WEEKDAY(EndDate)) NUM_UNIQUE(sales_order_detail.WEEKDAY(ModifiedDate)) NUM_UNIQUE(sales_order_detail.WEEKDAY(StartDate)) NUM_UNIQUE(sales_order_detail.YEAR(EndDate)) NUM_UNIQUE(sales_order_detail.YEAR(ModifiedDate)) NUM_UNIQUE(sales_order_detail.YEAR(StartDate)) MODE(store.DAY(test)) MODE(store.MONTH(test)) MODE(store.WEEKDAY(test)) MODE(store.YEAR(test)) NUM_UNIQUE(store.DAY(test)) NUM_UNIQUE(store.MONTH(test)) NUM_UNIQUE(store.WEEKDAY(test)) NUM_UNIQUE(store.YEAR(test)) TerritoryIDCat ShipMethodID   SubTotal    TaxAmt   Freight   TotalDue MAX(sales_order_header.Freight) MAX(sales_order_header.SalesPersonID) MAX(sales_order_header.ShipMethodID) MAX(sales_order_header.SubTotal) MAX(sales_order_header.TaxAmt) MAX(sales_order_header.TerritoryID) MAX(sales_order_header.TotalDue) MEAN(sales_order_header.Freight) MEAN(sales_order_header.SalesPersonID) MEAN(sales_order_header.ShipMethodID) MEAN(sales_order_header.SubTotal) MEAN(sales_order_header.TaxAmt) MEAN(sales_order_header.TerritoryID) MEAN(sales_order_header.TotalDue) MIN(sales_order_header.Freight) MIN(sales_order_header.SalesPersonID) MIN(sales_order_header.ShipMethodID) MIN(sales_order_header.SubTotal) MIN(sales_order_header.TaxAmt) MIN(sales_order_header.TerritoryID) MIN(sales_order_header.TotalDue) SKEW(sales_order_header.Freight) SKEW(sales_order_header.SalesPersonID) SKEW(sales_order_header.ShipMethodID) SKEW(sales_order_header.SubTotal) SKEW(sales_order_header.TaxAmt) SKEW(sales_order_header.TerritoryID) SKEW(sales_order_header.TotalDue) STD(sales_order_header.Freight) STD(sales_order_header.SalesPersonID) STD(sales_order_header.ShipMethodID) STD(sales_order_header.SubTotal) STD(sales_order_header.TaxAmt) STD(sales_order_header.TerritoryID) STD(sales_order_header.TotalDue) SUM(sales_order_header.Freight) SUM(sales_order_header.SalesPersonID) SUM(sales_order_header.ShipMethodID) SUM(sales_order_header.SubTotal) SUM(sales_order_header.TaxAmt) SUM(sales_order_header.TerritoryID) SUM(sales_order_header.TotalDue) MAX(sales_order_detail.DiscountPct) MAX(sales_order_detail.LineTotal) MAX(sales_order_detail.ListPrice) MAX(sales_order_detail.MinQty) MAX(sales_order_detail.OrderQty) MAX(sales_order_detail.ProductModelID) MAX(sales_order_detail.ReorderPoint) MAX(sales_order_detail.SafetyStockLevel) MAX(sales_order_detail.StandardCost) MAX(sales_order_detail.UnitPrice) MAX(sales_order_detail.UnitPriceDiscount) MEAN(sales_order_detail.DiscountPct) MEAN(sales_order_detail.LineTotal) MEAN(sales_order_detail.ListPrice) MEAN(sales_order_detail.MinQty) MEAN(sales_order_detail.OrderQty) MEAN(sales_order_detail.ProductModelID) MEAN(sales_order_detail.ReorderPoint) MEAN(sales_order_detail.SafetyStockLevel) MEAN(sales_order_detail.StandardCost) MEAN(sales_order_detail.UnitPrice) MEAN(sales_order_detail.UnitPriceDiscount) MIN(sales_order_detail.DiscountPct) MIN(sales_order_detail.LineTotal) MIN(sales_order_detail.ListPrice) MIN(sales_order_detail.MinQty) MIN(sales_order_detail.OrderQty) MIN(sales_order_detail.ProductModelID) MIN(sales_order_detail.ReorderPoint) MIN(sales_order_detail.SafetyStockLevel) MIN(sales_order_detail.StandardCost) MIN(sales_order_detail.UnitPrice) MIN(sales_order_detail.UnitPriceDiscount) SKEW(sales_order_detail.DiscountPct) SKEW(sales_order_detail.LineTotal) SKEW(sales_order_detail.ListPrice) SKEW(sales_order_detail.MinQty) SKEW(sales_order_detail.OrderQty) SKEW(sales_order_detail.ProductModelID) SKEW(sales_order_detail.ReorderPoint) SKEW(sales_order_detail.SafetyStockLevel) SKEW(sales_order_detail.StandardCost) SKEW(sales_order_detail.UnitPrice) SKEW(sales_order_detail.UnitPriceDiscount) STD(sales_order_detail.DiscountPct) STD(sales_order_detail.LineTotal) STD(sales_order_detail.ListPrice) STD(sales_order_detail.MinQty) STD(sales_order_detail.OrderQty) STD(sales_order_detail.ProductModelID) STD(sales_order_detail.ReorderPoint) STD(sales_order_detail.SafetyStockLevel) STD(sales_order_detail.StandardCost) STD(sales_order_detail.UnitPrice) STD(sales_order_detail.UnitPriceDiscount) SUM(sales_order_detail.DiscountPct) SUM(sales_order_detail.LineTotal) SUM(sales_order_detail.ListPrice) SUM(sales_order_detail.MinQty) SUM(sales_order_detail.OrderQty) SUM(sales_order_detail.ProductModelID) SUM(sales_order_detail.ReorderPoint) SUM(sales_order_detail.SafetyStockLevel) SUM(sales_order_detail.StandardCost) SUM(sales_order_detail.UnitPrice) SUM(sales_order_detail.UnitPriceDiscount) MAX(store.SalesPersonID) MEAN(store.SalesPersonID) MIN(store.SalesPersonID) SKEW(store.SalesPersonID) STD(store.SalesPersonID) SUM(store.SalesPersonID)  role target categorical    categorical     categorical   categorical categorical               categorical                      categorical                      categorical                    categorical                      categorical                      categorical                      categorical               categorical                      categorical                      categorical                      categorical                      categorical                   categorical                      categorical                      categorical                      categorical                      categorical                      categorical               categorical                      categorical                      categorical  categorical  categorical       categorical    categorical   categorical    categorical         categorical      categorical     categorical      categorical           categorical        categorical       categorical   categorical        categorical     categorical    categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical           categorical             categorical               categorical            categorical                 categorical                   categorical                     categorical                       numerical    numerical  numerical numerical numerical  numerical                       numerical                        numerical                        numerical                        numerical                      numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                       numerical                        numerical                        numerical                       numerical                        numerical                        numerical                        numerical                      numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                       numerical                        numerical                        numerical                       numerical                        numerical                        numerical                        numerical                      numerical                        numerical                        numerical                       numerical                        numerical                        numerical                        numerical                      numerical                        numerical                        numerical                        numerical                        numerical                        numerical                      numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                       numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                      numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                       numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                      numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                      numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                numerical                 numerical                numerical                 numerical                numerical                numerical 0 0 8 0 279 5 0 NULL NULL NULL NULL NULL NULL 12 No Discount No Discount 1 1 No Discount 1 1 2 5 1 0 NULL NULL 80 12 6 30 7 6 6 5 6 6 0 0 1 2011 2011 2011 2011 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 30 30 1 11 5 5 6 0 6 2014 2011 2011 1 1 1 1 1 1 1 1 1 1 1 1 12 9 4 2014 1 1 1 1 5 5 20565.6206 1971.5149 616.0984 23153.2339 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 6074.982 3399.99 0 6 33 75 100 1912.1544 2039.994 0 0 1713.8017 1989.8658 0 2.1667 17.3333 45 60 1120.2742 1193.6427 0 0 10.373 8.99 0 1 2 3 4 3.3963 5.1865 0 0 1.1963 -0.3885 0 1.4799 -0.06687 -0.3884 -0.3884 -0.3887 -0.3885 0 0 1883.1394 1729.5034 0 1.5859 7.2655 37.0749 49.4332 971.2526 1038.0419 0 0 20565.6206 23878.39 0 26 208 540 720 13443.2903 14323.7118 0 279 279 279 0 0 22320 1 0 8 0 279 5 0 NULL NULL NULL NULL NULL NULL 2 No Discount No Discount 1 2 No Discount 1 1 1 1 1 0 NULL NULL 80 12 6 30 7 6 6 5 6 6 0 0 1 2011 2011 2011 2011 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 30 30 1 11 5 5 6 0 6 2014 2011 2011 1 1 1 1 1 1 1 1 1 1 1 1 12 9 4 2014 1 1 1 1 5 5 1294.2529 124.2483 38.8276 1457.3288 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 874.794 1457.99 0 1 30 75 100 884.7083 874.794 0 0 647.1264 1120.49 0 1 29 75 100 685.7074 647.1264 0 0 419.4589 782.99 0 1 28 75 100 486.7066 419.4589 0 nan nan nan nan nan nan nan nan nan nan nan 0 321.9705 477.2971 0 0 1.4142 0 0 281.4297 321.9705 0 0 1294.2529 2240.98 0 2 58 150 200 1371.4149 1294.2529 0 279 279 279 0 0 22320 2 0 8 0 282 6 0 NULL NULL NULL NULL NULL NULL 15 No Discount No Discount 1 1 No Discount 1 1 2 5 1 0 NULL NULL 74 12 6 30 7 6 6 5 6 6 0 0 1 2011 2011 2011 2011 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 30 30 1 11 5 5 6 0 6 2014 2011 2011 1 1 1 1 1 1 1 1 1 1 1 1 12 9 4 2014 1 1 1 1 6 5 32726.4786 3153.7696 985.553 36865.8012 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 8099.976 3399.99 0 5 33 375 500 1912.1544 2039.994 0 0 2181.7652 1590.4467 0 2.5333 14 151 201.3333 883.0532 934.9116 0 0 20.746 8.99 0 1 2 3 4 6.9223 5.1865 0 0 1.2022 0.2799 0 0.7955 0.7239 0.6718 0.6718 0.3285 0.348 0 0 2515.2803 1423.6488 0 1.1872 10.0499 166.7505 222.3339 802.3616 858.7914 0 0 32726.4786 23856.7 0 38 210 2265 3020 13245.7975 14023.6738 0 282 282 282 0 0 20868 3 0 8 0 276 4 0 NULL NULL NULL NULL NULL NULL 1 No Discount No Discount 1 2 No Discount 1 1 1 1 1 0 NULL NULL 39 12 6 30 7 6 6 5 6 6 0 0 1 2011 2011 2011 2011 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 30 30 1 11 5 5 6 0 6 2014 2011 2011 1 1 1 1 1 1 1 1 1 1 1 1 12 9 4 2014 1 1 1 1 4 5 419.4589 40.2681 12.5838 472.3108 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 419.4589 782.99 0 1 30 75 100 486.7066 419.4589 0 0 419.4589 782.99 0 1 30 75 100 486.7066 419.4589 0 0 419.4589 782.99 0 1 30 75 100 486.7066 419.4589 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 419.4589 782.99 0 1 30 75 100 486.7066 419.4589 0 276 276 276 0 0 10764 4 0 8 0 280 1 0 NULL NULL NULL NULL NULL NULL 8 No Discount No Discount 1 1 No Discount 1 1 2 2 1 0 NULL NULL 38 12 6 30 7 6 6 5 6 6 0 0 1 2011 2011 2011 2011 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 30 30 1 11 5 5 6 0 6 2014 2011 2011 1 1 1 1 1 1 1 1 1 1 1 1 12 9 4 2014 1 1 1 1 1 5 24432.6088 2344.9921 732.81 27510.4109 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 8099.976 3399.99 0 4 19 75 100 1912.1544 2039.994 0 0 3054.0761 2553.115 0 1.75 17 57 76 1438.4664 1531.5806 0 0 28.8404 49.99 0 1 11 3 4 38.4923 28.8404 0 0 0.8117 -1.4399 0 1.3554 -1.4402 -1.4402 -1.4402 -1.4399 -1.4399 0 0 2860.3836 1545.0056 0 1.165 3.7033 33.3295 44.4394 864.1073 927.5374 0 0 24432.6088 20424.92 0 14 136 456 608 11507.731 12252.6448 0 280 280 280 0 0 10640 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 15820 1 8 1 NULL 7 0 NULL NULL NULL NULL NULL NULL 4 No Discount No Discount 0 37 No Discount 1 1 2 3 1 1 1 1 0 12 6 30 7 1 1 12 1 6 0 0 1 2014 2014 2013 2014 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 30 30 1 11 12 5 6 0 6 2014 2013 2011 1 1 1 1 1 1 1 1 1 1 1 1 NULL NULL NULL NULL NULL NULL NULL NULL 7 1 1173.96 93.9168 29.349 1297.2258 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 1120.49 1120.49 0 1 93 375 500 713.0798 1120.49 0 0 293.49 293.49 0 1 53.75 207 276 183.2694 293.49 0 0 3.99 3.99 0 1 4 3 4 1.4923 3.99 0 0 1.9981 1.9981 0 0 -0.2621 -0.1153 -0.1153 1.9994 1.9981 0 0 551.4201 551.4201 0 0 44.2371 196.204 261.6053 353.2259 551.4201 0 0 1173.96 1173.96 0 4 215 828 1104 733.0777 1173.96 0 nan nan nan nan nan 0 15821 1 8 1 NULL 10 0 NULL NULL NULL NULL NULL NULL 3 No Discount No Discount 0 19 No Discount 1 1 2 3 1 1 1 1 0 12 6 30 7 1 1 12 1 6 0 0 1 2014 2014 2013 2014 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 30 30 1 11 12 5 6 0 6 2014 2013 2011 1 1 1 1 1 1 1 1 1 1 1 1 NULL NULL NULL NULL NULL NULL NULL NULL 10 1 1179.47 94.3576 29.4868 1303.3144 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 1120.49 1120.49 0 1 29 75 100 713.0798 1120.49 0 0 393.1567 393.1567 0 1 14 27 36 252.8315 393.1567 0 0 8.99 8.99 0 1 2 3 4 6.9223 8.99 0 0 1.7238 1.7238 0 0 0.9352 1.7321 1.7321 1.7199 1.7238 0 0 630.2226 630.2226 0 0 13.7477 41.5692 55.4256 398.8992 630.2226 0 0 1179.47 1179.47 0 3 42 81 108 758.4944 1179.47 0 nan nan nan nan nan 0 15822 1 8 1 NULL 7 3 1 8 0 1 1 1 2 No Discount No Discount 0 2 No Discount 1 1 2 2 1 1 1 1 0 12 6 30 7 1 1 12 1 6 0 0 1 2014 2014 2013 2014 14 8 1 9 4 4 4 4 3 4 4 5 2013 2013 2013 2013 3 3 3 3 3 3 3 3 2 2 2 2 1 1 1 1 30 30 1 11 12 5 6 0 6 2014 2013 2011 1 1 1 1 1 1 1 1 1 1 1 1 NULL NULL NULL NULL NULL NULL NULL NULL 7 1 548.98 43.9184 13.7245 606.6229 61.5493 nan 1 2461.97 196.9576 7 2720.4769 42.5419 nan 1 1701.6761 136.1341 7 1880.3521 14.849 nan 1 593.96 47.5168 7 656.3258 -1.3935 nan 0 -1.3935 -1.3935 0 -1.3935 24.5318 nan 0 981.2706 78.5017 0 1084.304 127.6258 0 3 5105.0282 408.4023 21 5641.0563 0 539.99 539.99 0 1 113 75 100 343.6496 539.99 0 0 274.49 274.49 0 1 72 39 52 173.506 274.49 0 0 8.99 8.99 0 1 31 3 4 3.3623 8.99 0 nan nan nan nan nan nan nan nan nan nan nan 0 375.4737 375.4737 0 0 57.9828 50.9117 67.8823 240.6195 375.4737 0 0 548.98 548.98 0 2 144 78 104 347.0119 548.98 0 nan nan nan nan nan 0 15823 1 8 1 NULL 8 1 1 8 0 1 1 1 1 No Discount No Discount 1 3 No Discount 1 1 1 1 1 1 5 1 0 12 6 30 7 1 1 12 1 6 0 0 1 2014 2014 2013 2014 12 6 30 7 6 6 5 6 1 2 2 3 2012 2012 2012 2012 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 30 30 1 11 12 5 6 0 6 2014 2013 2011 1 1 1 1 1 1 1 1 1 1 1 1 NULL NULL NULL NULL NULL NULL NULL NULL 8 1 2384.07 190.7256 59.6018 2634.3974 25.0109 nan 1 1000.4375 80.035 8 1105.4834 25.0109 nan 1 1000.4375 80.035 8 1105.4834 25.0109 nan 1 1000.4375 80.035 8 1105.4834 nan nan nan nan nan nan nan nan nan nan nan nan nan nan 25.0109 0 1 1000.4375 80.035 8 1105.4834 0 2384.07 2384.07 0 1 34 75 100 1481.9379 2384.07 0 0 2384.07 2384.07 0 1 34 75 100 1481.9379 2384.07 0 0 2384.07 2384.07 0 1 34 75 100 1481.9379 2384.07 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 2384.07 2384.07 0 1 34 75 100 1481.9379 2384.07 0 nan nan nan nan nan 0 15824 1 8 1 NULL 10 2 1 8 0 1 1 2 2 No Discount No Discount 0 3 No Discount 1 1 2 2 1 0 NULL NULL 0 12 6 30 7 1 1 12 1 6 0 0 1 2014 2014 2013 2014 13 7 18 8 5 5 4 5 1 0 0 1 2012 2012 2012 2012 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 30 30 1 11 12 5 6 0 6 2014 2013 2011 1 1 1 1 1 1 1 1 1 1 1 1 NULL NULL NULL NULL NULL NULL NULL NULL 10 1 2419.06 193.5248 60.4765 2673.0613 89.4568 nan 1 3578.27 286.2616 10 3953.9884 74.6655 nan 1 2986.62 238.9296 10 3300.2151 59.8743 nan 1 2394.97 191.5976 10 2646.4419 nan nan nan nan nan nan nan 20.918 nan 0 836.7195 66.9376 0 924.575 149.3311 0 2 5973.24 477.8592 20 6600.4303 0 2384.07 2384.07 0 1 34 75 100 1481.9379 2384.07 0 0 1209.53 1209.53 0 1 33.5 39 52 747.5121 1209.53 0 0 34.99 34.99 0 1 33 3 4 13.0863 34.99 0 nan nan nan nan nan nan nan nan nan nan nan 0 1661.0504 1661.0504 0 0 0.7071 50.9117 67.8823 1038.6349 1661.0504 0 0 2419.06 2419.06 0 2 67 78 104 1495.0242 2419.06 0 nan nan nan nan nan 0 <p>     15825 rows x 227 columns     memory usage: 22.03 MB     name: featuretools_train     type: getml.DataFrame </p> In\u00a0[47]: Copied! <pre>featuretools_test.set_role(\"churn\", getml.data.roles.target)\nfeaturetools_test.set_role(featuretools_test.roles.unused_float, getml.data.roles.numerical)\nfeaturetools_test.set_role(featuretools_test.roles.unused_string, getml.data.roles.categorical)\n\nfeaturetools_test\n</pre> featuretools_test.set_role(\"churn\", getml.data.roles.target) featuretools_test.set_role(featuretools_test.roles.unused_float, getml.data.roles.numerical) featuretools_test.set_role(featuretools_test.roles.unused_string, getml.data.roles.categorical)  featuretools_test Out[47]: name  churn RevisionNumber OnlineOrderFlag SalesPersonID TerritoryID COUNT(sales_order_header) MODE(sales_order_header.OnlineOrderFlag) MODE(sales_order_header.RevisionNumber) MODE(sales_order_header.churn) NUM_UNIQUE(sales_order_header.OnlineOrderFlag) NUM_UNIQUE(sales_order_header.RevisionNumber) NUM_UNIQUE(sales_order_header.churn) COUNT(sales_order_detail) MODE(sales_order_detail.Category) MODE(sales_order_detail.Description) MODE(sales_order_detail.MakeFlag) MODE(sales_order_detail.ProductSubcategoryID) MODE(sales_order_detail.Type) NUM_UNIQUE(sales_order_detail.Category) NUM_UNIQUE(sales_order_detail.Description) NUM_UNIQUE(sales_order_detail.MakeFlag) NUM_UNIQUE(sales_order_detail.ProductSubcategoryID) NUM_UNIQUE(sales_order_detail.Type) COUNT(sales_order_reason) MODE(sales_order_reason.SalesReasonID) NUM_UNIQUE(sales_order_reason.SalesReasonID) COUNT(store) DAY(DueDate) DAY(ModifiedDate) DAY(OrderDate) DAY(ShipDate) MONTH(DueDate) MONTH(ModifiedDate) MONTH(OrderDate) MONTH(ShipDate) WEEKDAY(DueDate) WEEKDAY(ModifiedDate) WEEKDAY(OrderDate) WEEKDAY(ShipDate) YEAR(DueDate) YEAR(ModifiedDate) YEAR(OrderDate) YEAR(ShipDate) MODE(sales_order_header.DAY(DueDate)) MODE(sales_order_header.DAY(ModifiedDate)) MODE(sales_order_header.DAY(OrderDate)) MODE(sales_order_header.DAY(ShipDate)) MODE(sales_order_header.MONTH(DueDate)) MODE(sales_order_header.MONTH(ModifiedDate)) MODE(sales_order_header.MONTH(OrderDate)) MODE(sales_order_header.MONTH(ShipDate)) MODE(sales_order_header.WEEKDAY(DueDate)) MODE(sales_order_header.WEEKDAY(ModifiedDate)) MODE(sales_order_header.WEEKDAY(OrderDate)) MODE(sales_order_header.WEEKDAY(ShipDate)) MODE(sales_order_header.YEAR(DueDate)) MODE(sales_order_header.YEAR(ModifiedDate)) MODE(sales_order_header.YEAR(OrderDate)) MODE(sales_order_header.YEAR(ShipDate)) NUM_UNIQUE(sales_order_header.DAY(DueDate)) NUM_UNIQUE(sales_order_header.DAY(ModifiedDate)) NUM_UNIQUE(sales_order_header.DAY(OrderDate)) NUM_UNIQUE(sales_order_header.DAY(ShipDate)) NUM_UNIQUE(sales_order_header.MONTH(DueDate)) NUM_UNIQUE(sales_order_header.MONTH(ModifiedDate)) NUM_UNIQUE(sales_order_header.MONTH(OrderDate)) NUM_UNIQUE(sales_order_header.MONTH(ShipDate)) NUM_UNIQUE(sales_order_header.WEEKDAY(DueDate)) NUM_UNIQUE(sales_order_header.WEEKDAY(ModifiedDate)) NUM_UNIQUE(sales_order_header.WEEKDAY(OrderDate)) NUM_UNIQUE(sales_order_header.WEEKDAY(ShipDate)) NUM_UNIQUE(sales_order_header.YEAR(DueDate)) NUM_UNIQUE(sales_order_header.YEAR(ModifiedDate)) NUM_UNIQUE(sales_order_header.YEAR(OrderDate)) NUM_UNIQUE(sales_order_header.YEAR(ShipDate)) MODE(sales_order_detail.DAY(EndDate)) MODE(sales_order_detail.DAY(ModifiedDate)) MODE(sales_order_detail.DAY(StartDate)) MODE(sales_order_detail.MONTH(EndDate)) MODE(sales_order_detail.MONTH(ModifiedDate)) MODE(sales_order_detail.MONTH(StartDate)) MODE(sales_order_detail.WEEKDAY(EndDate)) MODE(sales_order_detail.WEEKDAY(ModifiedDate)) MODE(sales_order_detail.WEEKDAY(StartDate)) MODE(sales_order_detail.YEAR(EndDate)) MODE(sales_order_detail.YEAR(ModifiedDate)) MODE(sales_order_detail.YEAR(StartDate)) NUM_UNIQUE(sales_order_detail.DAY(EndDate)) NUM_UNIQUE(sales_order_detail.DAY(ModifiedDate)) NUM_UNIQUE(sales_order_detail.DAY(StartDate)) NUM_UNIQUE(sales_order_detail.MONTH(EndDate)) NUM_UNIQUE(sales_order_detail.MONTH(ModifiedDate)) NUM_UNIQUE(sales_order_detail.MONTH(StartDate)) NUM_UNIQUE(sales_order_detail.WEEKDAY(EndDate)) NUM_UNIQUE(sales_order_detail.WEEKDAY(ModifiedDate)) NUM_UNIQUE(sales_order_detail.WEEKDAY(StartDate)) NUM_UNIQUE(sales_order_detail.YEAR(EndDate)) NUM_UNIQUE(sales_order_detail.YEAR(ModifiedDate)) NUM_UNIQUE(sales_order_detail.YEAR(StartDate)) MODE(store.DAY(test)) MODE(store.MONTH(test)) MODE(store.WEEKDAY(test)) MODE(store.YEAR(test)) NUM_UNIQUE(store.DAY(test)) NUM_UNIQUE(store.MONTH(test)) NUM_UNIQUE(store.WEEKDAY(test)) NUM_UNIQUE(store.YEAR(test)) TerritoryIDCat ShipMethodID   SubTotal    TaxAmt   Freight   TotalDue MAX(sales_order_header.Freight) MAX(sales_order_header.SalesPersonID) MAX(sales_order_header.ShipMethodID) MAX(sales_order_header.SubTotal) MAX(sales_order_header.TaxAmt) MAX(sales_order_header.TerritoryID) MAX(sales_order_header.TotalDue) MEAN(sales_order_header.Freight) MEAN(sales_order_header.SalesPersonID) MEAN(sales_order_header.ShipMethodID) MEAN(sales_order_header.SubTotal) MEAN(sales_order_header.TaxAmt) MEAN(sales_order_header.TerritoryID) MEAN(sales_order_header.TotalDue) MIN(sales_order_header.Freight) MIN(sales_order_header.SalesPersonID) MIN(sales_order_header.ShipMethodID) MIN(sales_order_header.SubTotal) MIN(sales_order_header.TaxAmt) MIN(sales_order_header.TerritoryID) MIN(sales_order_header.TotalDue) SKEW(sales_order_header.Freight) SKEW(sales_order_header.SalesPersonID) SKEW(sales_order_header.ShipMethodID) SKEW(sales_order_header.SubTotal) SKEW(sales_order_header.TaxAmt) SKEW(sales_order_header.TerritoryID) SKEW(sales_order_header.TotalDue) STD(sales_order_header.Freight) STD(sales_order_header.SalesPersonID) STD(sales_order_header.ShipMethodID) STD(sales_order_header.SubTotal) STD(sales_order_header.TaxAmt) STD(sales_order_header.TerritoryID) STD(sales_order_header.TotalDue) SUM(sales_order_header.Freight) SUM(sales_order_header.SalesPersonID) SUM(sales_order_header.ShipMethodID) SUM(sales_order_header.SubTotal) SUM(sales_order_header.TaxAmt) SUM(sales_order_header.TerritoryID) SUM(sales_order_header.TotalDue) MAX(sales_order_detail.DiscountPct) MAX(sales_order_detail.LineTotal) MAX(sales_order_detail.ListPrice) MAX(sales_order_detail.MinQty) MAX(sales_order_detail.OrderQty) MAX(sales_order_detail.ProductModelID) MAX(sales_order_detail.ReorderPoint) MAX(sales_order_detail.SafetyStockLevel) MAX(sales_order_detail.StandardCost) MAX(sales_order_detail.UnitPrice) MAX(sales_order_detail.UnitPriceDiscount) MEAN(sales_order_detail.DiscountPct) MEAN(sales_order_detail.LineTotal) MEAN(sales_order_detail.ListPrice) MEAN(sales_order_detail.MinQty) MEAN(sales_order_detail.OrderQty) MEAN(sales_order_detail.ProductModelID) MEAN(sales_order_detail.ReorderPoint) MEAN(sales_order_detail.SafetyStockLevel) MEAN(sales_order_detail.StandardCost) MEAN(sales_order_detail.UnitPrice) MEAN(sales_order_detail.UnitPriceDiscount) MIN(sales_order_detail.DiscountPct) MIN(sales_order_detail.LineTotal) MIN(sales_order_detail.ListPrice) MIN(sales_order_detail.MinQty) MIN(sales_order_detail.OrderQty) MIN(sales_order_detail.ProductModelID) MIN(sales_order_detail.ReorderPoint) MIN(sales_order_detail.SafetyStockLevel) MIN(sales_order_detail.StandardCost) MIN(sales_order_detail.UnitPrice) MIN(sales_order_detail.UnitPriceDiscount) SKEW(sales_order_detail.DiscountPct) SKEW(sales_order_detail.LineTotal) SKEW(sales_order_detail.ListPrice) SKEW(sales_order_detail.MinQty) SKEW(sales_order_detail.OrderQty) SKEW(sales_order_detail.ProductModelID) SKEW(sales_order_detail.ReorderPoint) SKEW(sales_order_detail.SafetyStockLevel) SKEW(sales_order_detail.StandardCost) SKEW(sales_order_detail.UnitPrice) SKEW(sales_order_detail.UnitPriceDiscount) STD(sales_order_detail.DiscountPct) STD(sales_order_detail.LineTotal) STD(sales_order_detail.ListPrice) STD(sales_order_detail.MinQty) STD(sales_order_detail.OrderQty) STD(sales_order_detail.ProductModelID) STD(sales_order_detail.ReorderPoint) STD(sales_order_detail.SafetyStockLevel) STD(sales_order_detail.StandardCost) STD(sales_order_detail.UnitPrice) STD(sales_order_detail.UnitPriceDiscount) SUM(sales_order_detail.DiscountPct) SUM(sales_order_detail.LineTotal) SUM(sales_order_detail.ListPrice) SUM(sales_order_detail.MinQty) SUM(sales_order_detail.OrderQty) SUM(sales_order_detail.ProductModelID) SUM(sales_order_detail.ReorderPoint) SUM(sales_order_detail.SafetyStockLevel) SUM(sales_order_detail.StandardCost) SUM(sales_order_detail.UnitPrice) SUM(sales_order_detail.UnitPriceDiscount) MAX(store.SalesPersonID) MEAN(store.SalesPersonID) MIN(store.SalesPersonID) SKEW(store.SalesPersonID) STD(store.SalesPersonID) SUM(store.SalesPersonID) role target categorical    categorical     categorical   categorical categorical               categorical                      categorical                      categorical                    categorical                      categorical                      categorical                      categorical               categorical                      categorical                      categorical                      categorical                      categorical                   categorical                      categorical                      categorical                      categorical                      categorical                      categorical               categorical                      categorical                      categorical  categorical  categorical       categorical    categorical   categorical    categorical         categorical      categorical     categorical      categorical           categorical        categorical       categorical   categorical        categorical     categorical    categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical           categorical             categorical               categorical            categorical                 categorical                   categorical                     categorical                       numerical    numerical  numerical numerical numerical  numerical                       numerical                        numerical                        numerical                        numerical                      numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                       numerical                        numerical                        numerical                       numerical                        numerical                        numerical                        numerical                      numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                       numerical                        numerical                        numerical                       numerical                        numerical                        numerical                        numerical                      numerical                        numerical                        numerical                       numerical                        numerical                        numerical                        numerical                      numerical                        numerical                        numerical                        numerical                        numerical                        numerical                      numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                       numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                      numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                       numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                      numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                      numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                numerical                 numerical                numerical                 numerical                numerical                numerical 0 0 8 0 282 6 0 NULL NULL NULL NULL NULL NULL 22 No Discount No Discount 1 2 No Discount 1 1 1 2 1 0 NULL NULL 74 12 6 30 7 6 6 5 6 6 0 0 1 2011 2011 2011 2011 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 30 30 1 11 5 5 6 0 6 2014 2011 2011 1 1 1 1 1 1 1 1 1 1 1 1 12 9 4 2014 1 1 1 1 6 5 28832.5289 2775.1646 867.2389 32474.9324 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 5248.764 3578.27 0 6 30 375 500 2171.2942 2146.962 0 0 1310.5695 1021.1545 0 2.4545 22.1818 184.0909 245.4545 619.8742 588.8855 0 0 178.5808 337.22 0 1 9 75 100 187.1571 178.5808 0 0 1.8938 2.1819 0 0.7899 -0.6531 0.6093 0.6093 2.1479 2.1812 0 0 1219.991 911.954 0 1.5346 9.1529 147.7098 196.9464 555.2103 556.8657 0 0 28832.5289 22465.4 0 54 488 4050 5400 13637.2318 12955.4816 0 282 282 282 0 0 20868 1 0 8 0 283 1 0 NULL NULL NULL NULL NULL NULL 10 No Discount No Discount 0 1 No Discount 1 1 2 5 1 0 NULL NULL 38 12 6 30 7 6 6 5 6 6 0 0 1 2011 2011 2011 2011 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 30 30 1 11 5 5 6 0 6 2014 2011 2011 1 1 1 1 1 1 1 1 1 1 1 1 12 9 4 2014 1 1 1 1 1 5 14352.7713 1375.9427 429.9821 16158.6961 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 4049.988 3399.99 0 6 33 75 100 1912.1544 2039.994 0 0 1435.2771 1703.841 0 2 19.2 39 52 957.9515 1022.007 0 0 10.373 8.99 0 1 2 3 4 3.3963 5.1865 0 0 0.695 -9.915e-05 0 2.5156 -0.071 0 0 -0.0002387 -8.522e-05 0 0 1652.4063 1766.8723 0 1.4907 9.1019 37.9473 50.5964 994.013 1060.435 0 0 14352.7713 17038.41 0 20 192 390 520 9579.5155 10220.0699 0 283 283 283 0 0 10754 2 0 8 0 283 1 0 NULL NULL NULL NULL NULL NULL 1 No Discount No Discount 1 12 No Discount 1 1 1 1 1 0 NULL NULL 38 12 6 30 7 6 6 5 6 6 0 0 1 2011 2011 2011 2011 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 30 30 1 11 5 5 6 0 6 2014 2011 2011 1 1 1 1 1 1 1 1 1 1 1 1 12 9 4 2014 1 1 1 1 1 5 714.7043 70.5175 22.0367 807.2585 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 714.7043 1349.6 0 1 5 375 500 739.041 714.7043 0 0 714.7043 1349.6 0 1 5 375 500 739.041 714.7043 0 0 714.7043 1349.6 0 1 5 375 500 739.041 714.7043 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 714.7043 1349.6 0 1 5 375 500 739.041 714.7043 0 283 283 283 0 0 10754 3 0 8 0 279 5 0 NULL NULL NULL NULL NULL NULL 6 No Discount No Discount 1 2 No Discount 1 1 1 1 1 0 NULL NULL 80 12 6 30 7 6 6 5 6 6 0 0 1 2011 2011 2011 2011 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 30 30 1 11 5 5 6 0 6 2014 2011 2011 1 1 1 1 1 1 1 1 1 1 1 1 12 9 4 2014 1 1 1 1 5 5 5596.4705 537.2612 167.8941 6301.6258 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 1749.588 1457.99 0 2 30 75 100 884.7083 874.794 0 0 932.7451 1007.99 0 1.5 29.3333 75 100 619.3738 571.2373 0 0 419.4589 782.99 0 1 28 75 100 486.7066 419.4589 0 0 0.7356 0.9682 0 0 -0.9682 0 0 0.9682 0.9682 0 0 653.2467 348.5685 0 0.5477 1.0328 0 0 205.5272 235.134 0 0 5596.4705 6047.94 0 9 176 450 600 3716.243 3427.4236 0 279 279 279 0 0 22320 4 0 8 0 277 4 0 NULL NULL NULL NULL NULL NULL 1 No Discount No Discount 1 2 No Discount 1 1 1 1 1 0 NULL NULL 76 12 6 30 7 6 6 5 6 6 0 0 1 2011 2011 2011 2011 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 30 30 1 11 5 5 6 0 6 2014 2011 2011 1 1 1 1 1 1 1 1 1 1 1 1 12 9 4 2014 1 1 1 1 4 5 874.794 83.9802 26.2438 985.018 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 874.794 1457.99 0 1 28 75 100 884.7083 874.794 0 0 874.794 1457.99 0 1 28 75 100 884.7083 874.794 0 0 874.794 1457.99 0 1 28 75 100 884.7083 874.794 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 874.794 1457.99 0 1 28 75 100 884.7083 874.794 0 277 277 277 0 0 21052 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 3874 1 8 1 NULL 9 1 1 8 0 1 1 1 2 No Discount No Discount 0 2 No Discount 1 1 2 2 1 1 1 1 0 12 6 30 7 1 1 12 1 6 0 0 1 2014 2014 2013 2014 8 2 26 3 4 4 3 4 0 1 1 2 2013 2013 2013 2013 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 30 30 1 11 12 5 6 0 6 2014 2013 2011 1 1 1 1 1 1 1 1 1 1 1 1 NULL NULL NULL NULL NULL NULL NULL NULL 9 1 1725.98 138.0784 43.1495 1907.2079 51.7855 nan 1 2071.4196 165.7136 9 2288.9187 51.7855 nan 1 2071.4196 165.7136 9 2288.9187 51.7855 nan 1 2071.4196 165.7136 9 2288.9187 nan nan nan nan nan nan nan nan nan nan nan nan nan nan 51.7855 0 1 2071.4196 165.7136 9 2288.9187 0 1700.99 1700.99 0 1 89 375 500 1082.51 1700.99 0 0 862.99 862.99 0 1 58 225 300 545.9281 862.99 0 0 24.99 24.99 0 1 27 75 100 9.3463 24.99 0 nan nan nan nan nan nan nan nan nan nan nan 0 1185.111 1185.111 0 0 43.8406 212.132 282.8427 758.8413 1185.111 0 0 1725.98 1725.98 0 2 116 450 600 1091.8563 1725.98 0 nan nan nan nan nan 0 3875 1 8 1 NULL 9 1 1 8 0 1 1 1 4 No Discount No Discount 0 37 No Discount 1 1 2 3 1 1 1 1 0 12 6 30 7 1 1 12 1 6 0 0 1 2014 2014 2013 2014 24 18 11 19 10 10 10 10 0 1 1 2 2011 2011 2011 2011 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 30 30 1 11 12 5 6 0 6 2014 2013 2011 1 1 1 1 1 1 1 1 1 1 1 1 NULL NULL NULL NULL NULL NULL NULL NULL 9 1 2488.93 199.1144 62.2233 2750.2677 89.4568 nan 1 3578.27 286.2616 9 3953.9884 89.4568 nan 1 3578.27 286.2616 9 3953.9884 89.4568 nan 1 3578.27 286.2616 9 3953.9884 nan nan nan nan nan nan nan nan nan nan nan nan nan nan 89.4568 0 1 3578.27 286.2616 9 3953.9884 0 2443.35 2443.35 0 1 93 375 500 1554.9479 2443.35 0 0 622.2325 622.2325 0 1 58.25 207 276 392.9987 622.2325 0 0 3.99 3.99 0 1 24 3 4 1.4923 3.99 0 0 1.9994 1.9994 0 0 0.002931 -0.1153 -0.1153 1.9998 1.9994 0 0 1214.1424 1214.1424 0 0 38.422 196.204 261.6053 774.6468 1214.1424 0 0 2488.93 2488.93 0 4 233 828 1104 1571.9949 2488.93 0 nan nan nan nan nan 0 3876 1 8 1 NULL 1 1 1 8 0 1 1 1 3 No Discount No Discount 0 37 No Discount 2 2 2 2 2 2 1 2 0 12 6 30 7 1 1 12 1 6 0 0 1 2014 2014 2013 2014 21 15 8 16 8 8 8 8 6 0 0 1 2011 2011 2011 2011 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 30 30 1 11 12 5 6 0 6 2014 2013 2011 1 1 2 2 1 1 2 1 2 1 1 1 NULL NULL NULL NULL NULL NULL NULL NULL 1 1 1248.83 99.9064 31.2208 1379.9572 84.3748 nan 1 3374.99 269.9992 1 3729.364 84.3748 nan 1 3374.99 269.9992 1 3729.364 84.3748 nan 1 3374.99 269.9992 1 3729.364 nan nan nan nan nan nan nan nan nan nan nan nan nan nan 84.3748 0 1 3374.99 269.9992 1 3729.364 0.02 1214.85 1214.85 11 1 94 375 500 755.1508 1214.85 0 0.006667 416.2767 416.2767 3.6667 1 73.3333 275 366.6667 255.9531 416.2767 0 0 4.99 4.99 0 1 35 75 100 1.8663 4.99 0 1.7321 1.7297 1.7297 1.7321 0 -1.7162 -1.7321 -1.7321 1.7312 1.7297 0 0.01155 691.6889 691.6889 6.3509 0 33.2315 173.2051 230.9401 432.3412 691.6889 0 0.02 1248.83 1248.83 11 3 220 825 1100 767.8594 1248.83 0 nan nan nan nan nan 0 3877 1 8 1 NULL 4 1 1 8 0 1 1 1 4 No Discount No Discount 0 28 No Discount 1 1 2 3 1 1 1 1 0 12 6 30 7 1 1 12 1 6 0 0 1 2014 2014 2013 2014 24 18 11 19 4 4 4 4 1 2 2 3 2012 2012 2012 2012 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 30 30 1 11 12 5 6 0 6 2014 2013 2011 1 1 1 1 1 1 1 1 1 1 1 1 NULL NULL NULL NULL NULL NULL NULL NULL 4 1 2453.04 196.2432 61.326 2710.6092 84.3748 nan 1 3374.99 269.9992 4 3729.364 84.3748 nan 1 3374.99 269.9992 4 3729.364 84.3748 nan 1 3374.99 269.9992 4 3729.364 nan nan nan nan nan nan nan nan nan nan nan nan nan nan 84.3748 0 1 3374.99 269.9992 4 3729.364 0 2384.07 2384.07 0 1 113 75 100 1481.9379 2384.07 0 0 613.26 613.26 0 1 91.25 21 28 376.9332 613.26 0 0 4.99 4.99 0 1 34 3 4 1.8663 4.99 0 0 1.9978 1.9978 0 0 -1.9747 2. 2. 1.9992 1.9978 0 0 1180.758 1180.758 0 0 38.2481 36 48 736.7187 1180.758 0 0 2453.04 2453.04 0 4 365 84 112 1507.7328 2453.04 0 nan nan nan nan nan 0 3878 1 8 1 NULL 1 0 NULL NULL NULL NULL NULL NULL 3 No Discount No Discount 0 2 No Discount 2 2 2 3 2 2 1 2 0 12 6 30 7 1 1 12 1 6 0 0 1 2014 2014 2013 2014 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 30 30 1 11 12 5 6 0 6 2014 2013 2011 1 1 2 2 1 1 2 1 2 1 1 1 NULL NULL NULL NULL NULL NULL NULL NULL 1 1 1179.97 94.3976 29.4993 1303.8669 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0.02 1120.49 1120.49 11 1 33 75 100 713.0798 1120.49 0 0.006667 393.3233 393.3233 3.6667 1 22 27 36 245.1085 393.3233 0 0 24.49 24.49 0 1 4 3 4 9.1593 24.49 0 1.7321 1.7315 1.7315 1.7321 0 -1.6067 1.7321 1.7321 1.7319 1.7315 0 0.01155 629.7667 629.7667 6.3509 0 15.7162 41.5692 55.4256 405.2798 629.7667 0 0.02 1179.97 1179.97 11 3 66 81 108 735.3254 1179.97 0 nan nan nan nan nan 0 <p>     3879 rows x 227 columns     memory usage: 5.40 MB     name: featuretools_test     type: getml.DataFrame </p> <p>We train an untuned XGBoostRegressor on top of featuretools' features, just like we have done for getML's features.</p> <p>Since some of featuretools features are categorical, we allow the pipeline to include these features as well. Other features contain NaN values, which is why we also apply getML's Imputation preprocessor.</p> In\u00a0[48]: Copied! <pre>imputation = getml.preprocessors.Imputation()\n\npredictor = getml.predictors.XGBoostClassifier(n_jobs=1)\n\npipe3 = getml.Pipeline(\n    tags=['featuretools'],\n    preprocessors=[imputation],\n    predictors=[predictor],\n    include_categorical=True,\n)\n\npipe3\n</pre> imputation = getml.preprocessors.Imputation()  predictor = getml.predictors.XGBoostClassifier(n_jobs=1)  pipe3 = getml.Pipeline(     tags=['featuretools'],     preprocessors=[imputation],     predictors=[predictor],     include_categorical=True, )  pipe3 Out[48]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Imputation'],\n         share_selected_features=0.5,\n         tags=['featuretools'])</pre> In\u00a0[49]: Copied! <pre>pipe3.fit(featuretools_train)\n</pre> pipe3.fit(featuretools_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 0 issues labeled INFO and 19 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:05, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:5.143083\n\n</pre> Out[49]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Imputation'],\n         share_selected_features=0.5,\n         tags=['featuretools'])</pre> In\u00a0[50]: Copied! <pre>featuretools_score = pipe3.score(featuretools_test)\nfeaturetools_score\n</pre> featuretools_score = pipe3.score(featuretools_test) featuretools_score <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[50]: date time           set used           target accuracy     auc cross entropy 0 2024-02-21 14:52:12 featuretools_train churn 0.9144 0.9697 0.2329 1 2024-02-21 14:52:12 featuretools_test churn 0.9118 0.966 0.2428 In\u00a0[51]: Copied! <pre>pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name]\n</pre> pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name] Out[51]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_304\";\n\nCREATE TABLE \"FEATURE_1_304\" AS\nSELECT MIN( t1.\"orderdate\" - t2.\"t4__startdate\" ) AS \"feature_1_304\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"SALES_ORDER_DETAIL__STAGING_TABLE_2\" t2\nON t1.\"salesorderid\" = t2.\"salesorderid\"\nGROUP BY t1.rowid;\n</pre> In\u00a0[52]: Copied! <pre>pipe2.features.to_sql()[pipe2.features.sort(by=\"importances\")[0].name]\n</pre> pipe2.features.to_sql()[pipe2.features.sort(by=\"importances\")[0].name] Out[52]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_9\";\n\nCREATE TABLE \"FEATURE_1_9\" AS\nSELECT AVG( \n    CASE\n        WHEN ( t1.\"orderdate\" - t2.\"t4__startdate\" &gt; 66066211.764706 ) AND ( t1.\"salespersonidcat\" IN ( '279', '282', '276', '280', '283', '277', '275', '278', '281', '289', '290', '287', '284', '286', '288', '285' ) ) AND ( t2.\"weekday__strftime__w__modifieddate__mapping_2_target_1_avg\" &gt; 0.438061 ) THEN 1.475028927634943\n        WHEN ( t1.\"orderdate\" - t2.\"t4__startdate\" &gt; 66066211.764706 ) AND ( t1.\"salespersonidcat\" IN ( '279', '282', '276', '280', '283', '277', '275', '278', '281', '289', '290', '287', '284', '286', '288', '285' ) ) AND ( t2.\"weekday__strftime__w__modifieddate__mapping_2_target_1_avg\" &lt;= 0.438061 OR t2.\"weekday__strftime__w__modifieddate__mapping_2_target_1_avg\" IS NULL ) THEN -1.215333304628433\n        WHEN ( t1.\"orderdate\" - t2.\"t4__startdate\" &gt; 66066211.764706 ) AND ( t1.\"salespersonidcat\" NOT IN ( '279', '282', '276', '280', '283', '277', '275', '278', '281', '289', '290', '287', '284', '286', '288', '285' ) OR t1.\"salespersonidcat\" IS NULL ) AND ( t1.\"territoryidcat\" IN ( '6' ) ) THEN 0.1334003446866183\n        WHEN ( t1.\"orderdate\" - t2.\"t4__startdate\" &gt; 66066211.764706 ) AND ( t1.\"salespersonidcat\" NOT IN ( '279', '282', '276', '280', '283', '277', '275', '278', '281', '289', '290', '287', '284', '286', '288', '285' ) OR t1.\"salespersonidcat\" IS NULL ) AND ( t1.\"territoryidcat\" NOT IN ( '6' ) OR t1.\"territoryidcat\" IS NULL ) THEN 1.156934290335679\n        WHEN ( t1.\"orderdate\" - t2.\"t4__startdate\" &lt;= 66066211.764706 OR t1.\"orderdate\" IS NULL OR t2.\"t4__startdate\" IS NULL ) AND ( t1.\"territoryidcat\" IN ( '2', '10', '9', '8' ) ) AND ( t2.\"t3__productsubcategoryid__mapping_2_target_1_avg\" &gt; 0.552951 ) THEN 2.165605935978479\n        WHEN ( t1.\"orderdate\" - t2.\"t4__startdate\" &lt;= 66066211.764706 OR t1.\"orderdate\" IS NULL OR t2.\"t4__startdate\" IS NULL ) AND ( t1.\"territoryidcat\" IN ( '2', '10', '9', '8' ) ) AND ( t2.\"t3__productsubcategoryid__mapping_2_target_1_avg\" &lt;= 0.552951 OR t2.\"t3__productsubcategoryid__mapping_2_target_1_avg\" IS NULL ) THEN -1.443543931433293\n        WHEN ( t1.\"orderdate\" - t2.\"t4__startdate\" &lt;= 66066211.764706 OR t1.\"orderdate\" IS NULL OR t2.\"t4__startdate\" IS NULL ) AND ( t1.\"territoryidcat\" NOT IN ( '2', '10', '9', '8' ) OR t1.\"territoryidcat\" IS NULL ) AND ( t2.\"t3__productmodelid\" IN ( '1', '14', '19', '21', '2', '10', '11', '4', '17', '35', '34', '36', '33', '24', '28', '37', '26', '29', '30', '32', '9', '6', '13', '8', '7', '104', '106', '59', '61', '52', '56', '42', '45', '46', '50', '51', '123', '124', '125', '78', '116', '38', '111', '118', '107', '127', '128', '79', '80', '81', '84', '66', '67', '62', '63', '64', '68', '69', '70', '53', '103', '47', '48', '102', '99', '101', '98', '95', '97' ) ) THEN -1.113358383809923\n        WHEN ( t1.\"orderdate\" - t2.\"t4__startdate\" &lt;= 66066211.764706 OR t1.\"orderdate\" IS NULL OR t2.\"t4__startdate\" IS NULL ) AND ( t1.\"territoryidcat\" NOT IN ( '2', '10', '9', '8' ) OR t1.\"territoryidcat\" IS NULL ) AND ( t2.\"t3__productmodelid\" NOT IN ( '1', '14', '19', '21', '2', '10', '11', '4', '17', '35', '34', '36', '33', '24', '28', '37', '26', '29', '30', '32', '9', '6', '13', '8', '7', '104', '106', '59', '61', '52', '56', '42', '45', '46', '50', '51', '123', '124', '125', '78', '116', '38', '111', '118', '107', '127', '128', '79', '80', '81', '84', '66', '67', '62', '63', '64', '68', '69', '70', '53', '103', '47', '48', '102', '99', '101', '98', '95', '97' ) OR t2.\"t3__productmodelid\" IS NULL ) THEN -0.4498453552707536\n        ELSE NULL\n    END\n) AS \"feature_1_9\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"SALES_ORDER_DETAIL__STAGING_TABLE_2\" t2\nON t1.\"salesorderid\" = t2.\"salesorderid\"\nGROUP BY t1.rowid;\n</pre> In\u00a0[53]: Copied! <pre># Creates a folder containing the SQL code.\npipe2.features.to_sql().save(\"adventure_works\", remove=True)\n</pre> # Creates a folder containing the SQL code. pipe2.features.to_sql().save(\"adventure_works\", remove=True) In\u00a0[54]: Copied! <pre># Creates a folder containing the SQL code.\npipe2.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"adventure_works_spark\", remove=True)\n</pre> # Creates a folder containing the SQL code. pipe2.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"adventure_works_spark\", remove=True) In\u00a0[55]: Copied! <pre>scores = [fastprop_score, relboost_score, featuretools_score]\npd.DataFrame(\n    data={\n        'Name': ['getML: FastProp', 'getML: Relboost', 'featuretools'],\n        'Accuracy': [f\"{score.accuracy:.2%}\" for score in scores],\n        'AUC': [f\"{score.auc:.4f}\" for score in scores],\n        'Cross entropy': [f\"{score.cross_entropy:.4f}\" for score in scores]\n    }\n)\n</pre> scores = [fastprop_score, relboost_score, featuretools_score] pd.DataFrame(     data={         'Name': ['getML: FastProp', 'getML: Relboost', 'featuretools'],         'Accuracy': [f\"{score.accuracy:.2%}\" for score in scores],         'AUC': [f\"{score.auc:.4f}\" for score in scores],         'Cross entropy': [f\"{score.cross_entropy:.4f}\" for score in scores]     } ) Out[55]: Name Accuracy AUC Cross entropy 0 getML: FastProp 91.42% 0.9723 0.2199 1 getML: Relboost 92.55% 0.9788 0.1880 2 featuretools 91.18% 0.9660 0.2428 <p>The picture we get is very consistent: Relboost outperforms FastProp and FastProp outperforms featuretools for all three measures.</p> <p>These results also clearly demonstrate that this is a synthetic dataset - in the real-world, customer churn can not be predicted at this level of accuracy.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/adventure_works/#adventureworks-predicting-customer-churn","title":"AdventureWorks - Predicting customer churn\u00b6","text":"<p>In this notebook, we will demonstrate how getML can be used for a customer churn project using a synthetic dataset of a fictional company. We will also benchmark getML against featuretools.</p> <p>Summary:</p> <ul> <li>Prediction type: Classification model</li> <li>Domain: Customer loyalty</li> <li>Prediction target: churn</li> <li>Population size: 19704</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/adventure_works/#background","title":"Background\u00b6","text":"<p>AdventureWorks is a fictional company that sells bicycles. It is used by Microsoft to showcase how its MS SQL Server can be used to manage business data. Since this dataset resembles a real-world customer database and it is open-source, we will use it to showcase how getML can be used for a classic customer churn project (real customer databases are not easily available for the purposes of showcasing and benchmarking, for reasons of data privacy).</p> <p>The dataset has been downloaded from the CTU Prague relational learning repository (Motl and Schulte, 2015) (Now residing at relational-data.org.).</p> <p>We will benchmark getML 's feature learning algorithms against featuretools, an open-source implementation of the propositionalization algorithm, similar to getML's FastProp.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/adventure_works/#analysis","title":"Analysis\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/adventure_works/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/adventure_works/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>We begin by downloading the data:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/adventure_works/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/adventure_works/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"<p>We loaded the data and defined the roles and units. Next, we create a getML pipeline for relational learning.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/adventure_works/#21-define-relational-model","title":"2.1 Define relational model\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/adventure_works/#22-getml-pipeline","title":"2.2 getML pipeline\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/adventure_works/#23-model-training","title":"2.3 Model training\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/adventure_works/#24-model-evaluation","title":"2.4 Model evaluation\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/adventure_works/#25-featuretools","title":"2.5 featuretools\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/adventure_works/#26-features","title":"2.6 Features\u00b6","text":"<p>The most important features look as follows:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/adventure_works/#27-productionization","title":"2.7 Productionization\u00b6","text":"<p>It is possible to productionize the pipeline by transpiling the features into production-ready SQL code. Please also refer to getML's <code>sqlite3</code> and <code>spark</code> modules.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/adventure_works/#28-discussion","title":"2.8 Discussion\u00b6","text":"<p>For a more convenient overview, we summarize our results into a table.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/adventure_works/#3-conclusion","title":"3. Conclusion\u00b6","text":"<p>We demonstrated how getML can be used for a classic customer churn project. We have also benchmarked against featuretools. We found that getML outperforms featuretools.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/adventure_works/#references","title":"References\u00b6","text":"<p>Motl, Jan, and Oliver Schulte. \"The CTU prague relational learning repository.\" arXiv preprint arXiv:1511.03086 (2015).</p>","boost":0.8},{"location":"examples/enterprise-notebooks/air_pollution/","title":"<span class=\"ntitle\">air_pollution.ipynb</span> <span class=\"ndesc\">Why feature learning is better than simple propositionalization</span>","text":"<ol> <li>Loading data</li> <li>Predictive modeling</li> <li>Discussion</li> <li>Conclusion</li> </ol> <p>We start the analysis with the setup of our session.</p> In\u00a0[1]: Copied! <pre>import os\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\nfrom pathlib import Path\n\nfrom urllib import request\n\nimport getml\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import pearsonr\n\nfrom utils.load import load_or_retrieve\n\n%matplotlib inline\n</pre> import os os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\" from pathlib import Path  from urllib import request  import getml import matplotlib.pyplot as plt import numpy as np import pandas as pd from scipy.stats import pearsonr  from utils.load import load_or_retrieve  %matplotlib inline In\u00a0[2]: Copied! <pre># NOTE: Due to featuretools's and tsfresh's substantial resource requirements, prepared data can be used via RUN_FEATURETOOLS or RUN_TSFRESH.\n\nRUN_FEATURETOOLS = False\nRUN_TSFRESH = False\n\nif RUN_FEATURETOOLS:\n    from utils import FTTimeSeriesBuilder\n\nif RUN_TSFRESH:\n    from utils import TSFreshBuilder\n</pre> # NOTE: Due to featuretools's and tsfresh's substantial resource requirements, prepared data can be used via RUN_FEATURETOOLS or RUN_TSFRESH.  RUN_FEATURETOOLS = False RUN_TSFRESH = False  if RUN_FEATURETOOLS:     from utils import FTTimeSeriesBuilder  if RUN_TSFRESH:     from utils import TSFreshBuilder In\u00a0[3]: Copied! <pre>getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.set_project(\"air_pollution\")\n</pre> getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.set_project(\"air_pollution\") <pre>getML engine is already running.\nLoading pipelines... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nConnected to project 'air_pollution'\n</pre> In\u00a0[4]: Copied! <pre>data = getml.datasets.load_air_pollution()\n</pre> data = getml.datasets.load_air_pollution() <pre>\nLoading population...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n</pre> <p>First, we spilt our data. We introduce a simple, time-based split and use all data until 2013-12-31 for training and everything starting from 2014-01-01 for testing.</p> In\u00a0[5]: Copied! <pre>split = getml.data.split.time(\n    population=data, time_stamp=\"date\", test=getml.data.time.datetime(2014, 1, 1)\n)\n\nsplit\n</pre> split = getml.data.split.time(     population=data, time_stamp=\"date\", test=getml.data.time.datetime(2014, 1, 1) )  split Out[5]: 0 train 1 train 2 train 3 train 4 train ... <p>     41757 rows          type: StringColumnView </p> <p>For our first experiment, we will learn complex features and allow a memory of up to seven days. That means at every given point in time, the algorithm is allowed to back seven days into the past.</p> In\u00a0[6]: Copied! <pre>time_series1 = getml.data.TimeSeries(\n    population=data,\n    alias=\"population\",\n    split=split,\n    time_stamps=\"date\",\n    memory=getml.data.time.days(7),\n)\n\ntime_series1\n</pre> time_series1 = getml.data.TimeSeries(     population=data,     alias=\"population\",     split=split,     time_stamps=\"date\",     memory=getml.data.time.days(7), )  time_series1 Out[6]: data model diagram populationpopulationdate &lt;= dateMemory: 7.0 days staging data frames staging table               0 population POPULATION__STAGING_TABLE_1 1 population POPULATION__STAGING_TABLE_2 container population subset name        rows type 0 test population 8661 View 1 train population 33096 View peripheral name        rows type      0 population 41757 DataFrame In\u00a0[7]: Copied! <pre>relmt = getml.feature_learning.RelMT(\n    num_features=10,\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    seed=4367,\n    num_threads=1,\n)\n\npredictor = getml.predictors.XGBoostRegressor(n_jobs=1)\n\npipe1 = getml.pipeline.Pipeline(\n    tags=[\"getML: RelMT\", \"memory: 7d\", \"complex features\"],\n    data_model=time_series1.data_model,\n    feature_learners=[relmt],\n    predictors=[predictor],\n)\n\npipe1\n</pre> relmt = getml.feature_learning.RelMT(     num_features=10,     loss_function=getml.feature_learning.loss_functions.SquareLoss,     seed=4367,     num_threads=1, )  predictor = getml.predictors.XGBoostRegressor(n_jobs=1)  pipe1 = getml.pipeline.Pipeline(     tags=[\"getML: RelMT\", \"memory: 7d\", \"complex features\"],     data_model=time_series1.data_model,     feature_learners=[relmt],     predictors=[predictor], )  pipe1 Out[7]: <pre>Pipeline(data_model='population',\n         feature_learners=['RelMT'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=['population'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['getML: RelMT', 'memory: 7d', 'complex features'])</pre> <p>It is good practice to always check your data model first, even though <code>check(...)</code> is also called by <code>fit(...)</code>. That enables us to make last-minute changes.</p> In\u00a0[8]: Copied! <pre>pipe1.check(time_series1.train)\n</pre> pipe1.check(time_series1.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \n\nOK.\n</pre> <p>We now fit our data on the training set and evaluate our findings, both, in-sample and out-of-sample.</p> In\u00a0[9]: Copied! <pre>pipe1.fit(time_series1.train)\n</pre> pipe1.fit(time_series1.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelMT: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 01:32, remaining: 00:00]          \nRelMT: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:15, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:1m:48.467388\n\n</pre> Out[9]: <pre>Pipeline(data_model='population',\n         feature_learners=['RelMT'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=['population'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['getML: RelMT', 'memory: 7d', 'complex features', 'container-lahmUK'])</pre> In\u00a0[10]: Copied! <pre>pipe1.score(time_series1.test)\n</pre> pipe1.score(time_series1.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelMT: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:04, remaining: 00:00]          \n\n</pre> Out[10]: date time           set used target      mae     rmse rsquared 0 2024-02-21 14:55:35 train pm2.5 35.1664 50.9038 0.6925 1 2024-02-21 14:55:39 test pm2.5 39.6596 57.5014 0.6306 In\u00a0[11]: Copied! <pre>time_series2 = getml.data.TimeSeries(\n    population=data,\n    alias=\"population\",\n    split=split,\n    time_stamps=\"date\",\n    memory=getml.data.time.days(1),\n)\n\ntime_series2\n</pre> time_series2 = getml.data.TimeSeries(     population=data,     alias=\"population\",     split=split,     time_stamps=\"date\",     memory=getml.data.time.days(1), )  time_series2 Out[11]: data model diagram populationpopulationdate &lt;= dateMemory: 1.0 days staging data frames staging table               0 population POPULATION__STAGING_TABLE_1 1 population POPULATION__STAGING_TABLE_2 container population subset name        rows type 0 test population 8661 View 1 train population 33096 View peripheral name        rows type      0 population 41757 DataFrame In\u00a0[12]: Copied! <pre>relmt = getml.feature_learning.RelMT(\n    num_features=10,\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    seed=4367,\n    num_threads=1,\n)\n\npredictor = getml.predictors.XGBoostRegressor(n_jobs=1)\n\npipe2 = getml.pipeline.Pipeline(\n    tags=[\"getML: RelMT\", \"memory: 1d\", \"complex features\"],\n    data_model=time_series2.data_model,\n    feature_learners=[relmt],\n    predictors=[predictor],\n)\n\npipe2\n</pre> relmt = getml.feature_learning.RelMT(     num_features=10,     loss_function=getml.feature_learning.loss_functions.SquareLoss,     seed=4367,     num_threads=1, )  predictor = getml.predictors.XGBoostRegressor(n_jobs=1)  pipe2 = getml.pipeline.Pipeline(     tags=[\"getML: RelMT\", \"memory: 1d\", \"complex features\"],     data_model=time_series2.data_model,     feature_learners=[relmt],     predictors=[predictor], )  pipe2 Out[12]: <pre>Pipeline(data_model='population',\n         feature_learners=['RelMT'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=['population'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['getML: RelMT', 'memory: 1d', 'complex features'])</pre> In\u00a0[13]: Copied! <pre>pipe2.check(time_series2.train)\n</pre> pipe2.check(time_series2.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \n\nOK.\n</pre> In\u00a0[14]: Copied! <pre>pipe2.fit(time_series2.train)\n</pre> pipe2.fit(time_series2.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelMT: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:28, remaining: 00:00]          \nRelMT: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:32.22083\n\n</pre> Out[14]: <pre>Pipeline(data_model='population',\n         feature_learners=['RelMT'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=['population'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['getML: RelMT', 'memory: 1d', 'complex features', 'container-JKQRr0'])</pre> In\u00a0[15]: Copied! <pre>pipe2.score(time_series2.test)\n</pre> pipe2.score(time_series2.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelMT: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\n</pre> Out[15]: date time           set used target      mae     rmse rsquared 0 2024-02-21 14:56:13 train pm2.5 38.1593 55.3541 0.6366 1 2024-02-21 14:56:14 test pm2.5 47.5451 66.9418 0.4901 In\u00a0[16]: Copied! <pre>time_series3 = getml.data.TimeSeries(\n    population=data,\n    alias=\"population\",\n    split=split,\n    time_stamps=\"date\",\n    memory=getml.data.time.days(7),\n)\n\ntime_series3\n</pre> time_series3 = getml.data.TimeSeries(     population=data,     alias=\"population\",     split=split,     time_stamps=\"date\",     memory=getml.data.time.days(7), )  time_series3 Out[16]: data model diagram populationpopulationdate &lt;= dateMemory: 7.0 days staging data frames staging table               0 population POPULATION__STAGING_TABLE_1 1 population POPULATION__STAGING_TABLE_2 container population subset name        rows type 0 test population 8661 View 1 train population 33096 View peripheral name        rows type      0 population 41757 DataFrame In\u00a0[17]: Copied! <pre>fast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    num_threads=1,\n    aggregation=getml.feature_learning.FastProp.agg_sets.All,\n)\n\npredictor = getml.predictors.XGBoostRegressor(n_jobs=1)\n\npipe3 = getml.pipeline.Pipeline(\n    tags=[\"getML: FastProp\", \"memory: 7d\", \"simple features\"],\n    data_model=time_series3.data_model,\n    feature_learners=[fast_prop],\n    predictors=[predictor],\n)\n\npipe3\n</pre> fast_prop = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.SquareLoss,     num_threads=1,     aggregation=getml.feature_learning.FastProp.agg_sets.All, )  predictor = getml.predictors.XGBoostRegressor(n_jobs=1)  pipe3 = getml.pipeline.Pipeline(     tags=[\"getML: FastProp\", \"memory: 7d\", \"simple features\"],     data_model=time_series3.data_model,     feature_learners=[fast_prop],     predictors=[predictor], )  pipe3 Out[17]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=['population'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['getML: FastProp', 'memory: 7d', 'simple features'])</pre> In\u00a0[18]: Copied! <pre>pipe3.check(time_series3.train)\n</pre> pipe3.check(time_series3.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \n\nOK.\n</pre> In\u00a0[19]: Copied! <pre>pipe3.fit(time_series3.train)\n</pre> pipe3.fit(time_series3.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 378 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:37, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:21, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:18, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:1m:16.310261\n\n</pre> Out[19]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=['population'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['getML: FastProp', 'memory: 7d', 'simple features', 'container-O5cthD'])</pre> In\u00a0[20]: Copied! <pre>pipe3.score(time_series3.test)\n</pre> pipe3.score(time_series3.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:06, remaining: 00:00]          \n\n</pre> Out[20]: date time           set used target      mae     rmse rsquared 0 2024-02-21 14:57:32 train pm2.5 35.9677 50.7711 0.7036 1 2024-02-21 14:57:38 test pm2.5 45.4586 62.6197 0.5617 In\u00a0[21]: Copied! <pre>time_series4 = getml.data.TimeSeries(\n    population=data,\n    alias=\"population\",\n    split=split,\n    time_stamps=\"date\",\n    memory=getml.data.time.days(1),\n)\n\ntime_series4\n</pre> time_series4 = getml.data.TimeSeries(     population=data,     alias=\"population\",     split=split,     time_stamps=\"date\",     memory=getml.data.time.days(1), )  time_series4 Out[21]: data model diagram populationpopulationdate &lt;= dateMemory: 1.0 days staging data frames staging table               0 population POPULATION__STAGING_TABLE_1 1 population POPULATION__STAGING_TABLE_2 container population subset name        rows type 0 test population 8661 View 1 train population 33096 View peripheral name        rows type      0 population 41757 DataFrame In\u00a0[22]: Copied! <pre>fast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    num_threads=1,\n    aggregation=getml.feature_learning.FastProp.agg_sets.All,\n)\n\npredictor = getml.predictors.XGBoostRegressor(n_jobs=1)\n\npipe4 = getml.pipeline.Pipeline(\n    tags=[\"getML: FastProp\", \"memory: 1d\", \"simple features\"],\n    data_model=time_series4.data_model,\n    feature_learners=[fast_prop],\n    predictors=[predictor],\n)\n\npipe4\n</pre> fast_prop = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.SquareLoss,     num_threads=1,     aggregation=getml.feature_learning.FastProp.agg_sets.All, )  predictor = getml.predictors.XGBoostRegressor(n_jobs=1)  pipe4 = getml.pipeline.Pipeline(     tags=[\"getML: FastProp\", \"memory: 1d\", \"simple features\"],     data_model=time_series4.data_model,     feature_learners=[fast_prop],     predictors=[predictor], )  pipe4 Out[22]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=['population'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['getML: FastProp', 'memory: 1d', 'simple features'])</pre> In\u00a0[23]: Copied! <pre>pipe4.check(time_series4.train)\n</pre> pipe4.check(time_series4.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \n\nOK.\n</pre> In\u00a0[24]: Copied! <pre>pipe4.fit(time_series4.train)\n</pre> pipe4.fit(time_series4.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 378 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:07, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:03, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:18, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:27.714016\n\n</pre> Out[24]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=['population'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['getML: FastProp', 'memory: 1d', 'simple features', 'container-EGLb2M'])</pre> In\u00a0[25]: Copied! <pre>pipe4.score(time_series4.test)\n</pre> pipe4.score(time_series4.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\n</pre> Out[25]: date time           set used target      mae     rmse rsquared 0 2024-02-21 14:58:08 train pm2.5 38.3028 55.2472 0.6438 1 2024-02-21 14:58:09 test pm2.5 44.2526 63.4191 0.5462 In\u00a0[26]: Copied! <pre>data_train_pandas = time_series1.train.population.to_pandas()\ndata_test_pandas = time_series1.test.population.to_pandas()\n</pre> data_train_pandas = time_series1.train.population.to_pandas() data_test_pandas = time_series1.test.population.to_pandas() <p>tsfresh and featuretools require the time series to have ids. Since there is only a single time series, that series has the same id.</p> In\u00a0[27]: Copied! <pre>data_train_pandas[\"id\"] = 1\ndata_test_pandas[\"id\"] = 1\n</pre> data_train_pandas[\"id\"] = 1 data_test_pandas[\"id\"] = 1 In\u00a0[28]: Copied! <pre>if RUN_FEATURETOOLS:\n    ft_builder = FTTimeSeriesBuilder(\n        num_features=200,\n        horizon=pd.Timedelta(days=0),\n        memory=pd.Timedelta(days=1),\n        column_id=\"id\",\n        time_stamp=\"date\",\n        target=\"pm2.5\",\n    )\n    #\n    featuretools_training = ft_builder.fit(data_train_pandas)\n    featuretools_test = ft_builder.transform(data_test_pandas)\n\n    data_featuretools_training = getml.data.DataFrame.from_pandas(\n        featuretools_training, name=\"featuretools_training\"\n    )\n    data_featuretools_test = getml.data.DataFrame.from_pandas(\n        featuretools_test, name=\"featuretools_test\"\n    )\n</pre> if RUN_FEATURETOOLS:     ft_builder = FTTimeSeriesBuilder(         num_features=200,         horizon=pd.Timedelta(days=0),         memory=pd.Timedelta(days=1),         column_id=\"id\",         time_stamp=\"date\",         target=\"pm2.5\",     )     #     featuretools_training = ft_builder.fit(data_train_pandas)     featuretools_test = ft_builder.transform(data_test_pandas)      data_featuretools_training = getml.data.DataFrame.from_pandas(         featuretools_training, name=\"featuretools_training\"     )     data_featuretools_test = getml.data.DataFrame.from_pandas(         featuretools_test, name=\"featuretools_test\"     ) In\u00a0[29]: Copied! <pre>if not RUN_FEATURETOOLS:\n    data_featuretools_training = load_or_retrieve(\n        \"https://static.getml.com/datasets/air_pollution/featuretools/featuretools_training.csv\"\n    )\n    data_featuretools_test = load_or_retrieve(\n        \"https://static.getml.com/datasets/air_pollution/featuretools/featuretools_test.csv\"\n    )\n</pre> if not RUN_FEATURETOOLS:     data_featuretools_training = load_or_retrieve(         \"https://static.getml.com/datasets/air_pollution/featuretools/featuretools_training.csv\"     )     data_featuretools_test = load_or_retrieve(         \"https://static.getml.com/datasets/air_pollution/featuretools/featuretools_test.csv\"     ) <pre>Loading 'featuretools_training' from disk (project folder).\n\nLoading 'featuretools_test' from disk (project folder).\n\n</pre> In\u00a0[30]: Copied! <pre>def set_roles_featuretools(df):\n    df.set_role([\"date\"], getml.data.roles.time_stamp)\n    df.set_role([\"pm2.5\"], getml.data.roles.target)\n    df.set_role([\"date\"], getml.data.roles.time_stamp)\n    df.set_role(df.roles.unused, getml.data.roles.numerical)\n    df.set_role([\"id\"], getml.data.roles.unused_float)\n    return df\n\ndf_featuretools_training = set_roles_featuretools(data_featuretools_training)\ndf_featuretools_test = set_roles_featuretools(data_featuretools_test)\n</pre> def set_roles_featuretools(df):     df.set_role([\"date\"], getml.data.roles.time_stamp)     df.set_role([\"pm2.5\"], getml.data.roles.target)     df.set_role([\"date\"], getml.data.roles.time_stamp)     df.set_role(df.roles.unused, getml.data.roles.numerical)     df.set_role([\"id\"], getml.data.roles.unused_float)     return df  df_featuretools_training = set_roles_featuretools(data_featuretools_training) df_featuretools_test = set_roles_featuretools(data_featuretools_test) In\u00a0[31]: Copied! <pre>predictor = getml.predictors.XGBoostRegressor()\n\npipe5 = getml.pipeline.Pipeline(\n    tags=[\"featuretools\", \"memory: 1d\", \"simple features\"], predictors=[predictor]\n)\n\npipe5\n</pre> predictor = getml.predictors.XGBoostRegressor()  pipe5 = getml.pipeline.Pipeline(     tags=[\"featuretools\", \"memory: 1d\", \"simple features\"], predictors=[predictor] )  pipe5 Out[31]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['featuretools', 'memory: 1d', 'simple features'])</pre> In\u00a0[32]: Copied! <pre>pipe5.check(df_featuretools_training)\n</pre> pipe5.check(df_featuretools_training) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\n</pre> In\u00a0[33]: Copied! <pre>pipe5.fit(df_featuretools_training)\n</pre> pipe5.fit(df_featuretools_training) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:08, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:8.163012\n\n</pre> Out[33]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['featuretools', 'memory: 1d', 'simple features'])</pre> In\u00a0[34]: Copied! <pre>pipe5.score(df_featuretools_test)\n</pre> pipe5.score(df_featuretools_test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[34]: date time           set used              target      mae     rmse rsquared 0 2024-02-21 14:58:19 featuretools_training pm2.5 38.0455 54.4693 0.6567 1 2024-02-21 14:58:19 featuretools_test pm2.5 45.3084 64.2717 0.5373 In\u00a0[35]: Copied! <pre>data_train_pandas\n</pre> data_train_pandas Out[35]: DEWP TEMP PRES Iws Is Ir pm2.5 date id 0 -16.0 -4.0 1020.0 1.79 0.0 0.0 129.0 2010-01-02 00:00:00 1 1 -15.0 -4.0 1020.0 2.68 0.0 0.0 148.0 2010-01-02 01:00:00 1 2 -11.0 -5.0 1021.0 3.57 0.0 0.0 159.0 2010-01-02 02:00:00 1 3 -7.0 -5.0 1022.0 5.36 1.0 0.0 181.0 2010-01-02 03:00:00 1 4 -7.0 -5.0 1022.0 6.25 2.0 0.0 138.0 2010-01-02 04:00:00 1 ... ... ... ... ... ... ... ... ... ... 33091 -19.0 7.0 1013.0 114.87 0.0 0.0 22.0 2013-12-31 19:00:00 1 33092 -21.0 7.0 1014.0 119.79 0.0 0.0 18.0 2013-12-31 20:00:00 1 33093 -21.0 7.0 1014.0 125.60 0.0 0.0 23.0 2013-12-31 21:00:00 1 33094 -21.0 6.0 1014.0 130.52 0.0 0.0 20.0 2013-12-31 22:00:00 1 33095 -20.0 7.0 1014.0 137.67 0.0 0.0 23.0 2013-12-31 23:00:00 1 <p>33096 rows \u00d7 9 columns</p> In\u00a0[36]: Copied! <pre>if RUN_TSFRESH:\n    tsfresh_builder = TSFreshBuilder(\n        num_features=200, memory=24, column_id=\"id\", time_stamp=\"date\", target=\"pm2.5\"\n    )\n    #\n    tsfresh_training = tsfresh_builder.fit(data_train_pandas)\n    tsfresh_test = tsfresh_builder.transform(data_test_pandas)\n    #\n    data_tsfresh_training = getml.data.DataFrame.from_pandas(\n        tsfresh_training, name=\"tsfresh_training\"\n    )\n    data_tsfresh_test = getml.data.DataFrame.from_pandas(\n        tsfresh_test, name=\"tsfresh_test\"\n    )\n</pre> if RUN_TSFRESH:     tsfresh_builder = TSFreshBuilder(         num_features=200, memory=24, column_id=\"id\", time_stamp=\"date\", target=\"pm2.5\"     )     #     tsfresh_training = tsfresh_builder.fit(data_train_pandas)     tsfresh_test = tsfresh_builder.transform(data_test_pandas)     #     data_tsfresh_training = getml.data.DataFrame.from_pandas(         tsfresh_training, name=\"tsfresh_training\"     )     data_tsfresh_test = getml.data.DataFrame.from_pandas(         tsfresh_test, name=\"tsfresh_test\"     ) <p>tsfresh does not contain built-in machine learning algorithms. In order to ensure a fair comparison, we use the exact same machine learning algorithm we have also used for getML: An XGBoost regressor with all hyperparameters set to their default values.</p> <p>In order to do so, we load the tsfresh features into the getML engine.</p> In\u00a0[37]: Copied! <pre>if not RUN_TSFRESH:\n    data_tsfresh_training = load_or_retrieve(\n        \"https://static.getml.com/datasets/air_pollution/tsfresh/tsfresh_training.csv\"\n    )\n    data_tsfresh_test = load_or_retrieve(\n        \"https://static.getml.com/datasets/air_pollution/tsfresh/tsfresh_test.csv\"\n    )\n</pre> if not RUN_TSFRESH:     data_tsfresh_training = load_or_retrieve(         \"https://static.getml.com/datasets/air_pollution/tsfresh/tsfresh_training.csv\"     )     data_tsfresh_test = load_or_retrieve(         \"https://static.getml.com/datasets/air_pollution/tsfresh/tsfresh_test.csv\"     ) <pre>Loading 'tsfresh_training' from disk (project folder).\n\nLoading 'tsfresh_test' from disk (project folder).\n\n</pre> <p>As usual, we need to set roles:</p> In\u00a0[38]: Copied! <pre>def set_roles_tsfresh(df):\n    df.set_role([\"date\"], getml.data.roles.time_stamp)\n    df.set_role([\"pm2.5\"], getml.data.roles.target)\n    df.set_role([\"date\"], getml.data.roles.time_stamp)\n    df.set_role(df.roles.unused, getml.data.roles.numerical)\n    df.set_role([\"id\"], getml.data.roles.unused_float)\n    return df\n\ndf_tsfresh_training = set_roles_tsfresh(data_tsfresh_training)\ndf_tsfresh_test = set_roles_tsfresh(data_tsfresh_test)\n</pre> def set_roles_tsfresh(df):     df.set_role([\"date\"], getml.data.roles.time_stamp)     df.set_role([\"pm2.5\"], getml.data.roles.target)     df.set_role([\"date\"], getml.data.roles.time_stamp)     df.set_role(df.roles.unused, getml.data.roles.numerical)     df.set_role([\"id\"], getml.data.roles.unused_float)     return df  df_tsfresh_training = set_roles_tsfresh(data_tsfresh_training) df_tsfresh_test = set_roles_tsfresh(data_tsfresh_test) <p>In this case, our pipeline is very simple. It only consists of a single XGBoostRegressor.</p> In\u00a0[39]: Copied! <pre>predictor = getml.predictors.XGBoostRegressor()\n\npipe6 = getml.pipeline.Pipeline(\n    tags=[\"tsfresh\", \"memory: 1d\", \"simple features\"], predictors=[predictor]\n)\n\npipe6\n</pre> predictor = getml.predictors.XGBoostRegressor()  pipe6 = getml.pipeline.Pipeline(     tags=[\"tsfresh\", \"memory: 1d\", \"simple features\"], predictors=[predictor] )  pipe6 Out[39]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['tsfresh', 'memory: 1d', 'simple features'])</pre> In\u00a0[40]: Copied! <pre>pipe6.check(df_tsfresh_training)\n</pre> pipe6.check(df_tsfresh_training) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\n</pre> In\u00a0[41]: Copied! <pre>pipe6.fit(df_tsfresh_training)\n</pre> pipe6.fit(df_tsfresh_training) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:06, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:5.970352\n\n</pre> Out[41]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['tsfresh', 'memory: 1d', 'simple features'])</pre> In\u00a0[42]: Copied! <pre>pipe6.score(df_tsfresh_test)\n</pre> pipe6.score(df_tsfresh_test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[42]: date time           set used         target      mae     rmse rsquared 0 2024-02-21 14:58:26 tsfresh_training pm2.5 40.8062 57.7874 0.6106 1 2024-02-21 14:58:26 tsfresh_test pm2.5 46.698 65.9163 0.5105 In\u00a0[43]: Copied! <pre>pipe1.features\n</pre> pipe1.features Out[43]: target name         correlation  importance 0 pm2.5 feature_1_1 0.7269 0.18463717 1 pm2.5 feature_1_2 0.7046 0.11726964 2 pm2.5 feature_1_3 0.7158 0.08975971 3 pm2.5 feature_1_4 0.6811 0.01235796 4 pm2.5 feature_1_5 0.7363 0.27688485 ... ... ... ... 11 pm2.5 temp -0.2112 0.00403082 12 pm2.5 pres 0.0811 0.00672836 13 pm2.5 iws -0.2166 0.00111994 14 pm2.5 is 0.0045 0.00006808 15 pm2.5 ir -0.0541 0.00060757 In\u00a0[44]: Copied! <pre>pipe1.features.sort(by=\"importances\")[0].sql\n</pre> pipe1.features.sort(by=\"importances\")[0].sql Out[44]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_5\";\n\nCREATE TABLE \"FEATURE_1_5\" AS\nSELECT SUM( \n    CASE\n        WHEN ( t2.\"iws\" &gt; 2.996864 ) AND ( t1.\"date\" - t2.\"date\" &gt; 111439.618138 ) THEN COALESCE( t1.\"dewp\" - 1.514736211828887, 0.0 ) * 0.03601656954671504 + COALESCE( t1.\"temp\" - 11.89228926884439, 0.0 ) * -0.04267662247010789 + COALESCE( t1.\"is\" - 0.06612999800412481, 0.0 ) * -0.04725594429771846 + COALESCE( t1.\"ir\" - 0.2116958286208502, 0.0 ) * -0.06321612715292692 + COALESCE( t1.\"pres\" - 1016.466458208569, 0.0 ) * -0.01400320405937972 + COALESCE( t1.\"iws\" - 25.06146098064165, 0.0 ) * -0.001201539145976257 + COALESCE( t1.\"date\" - 1326377672.037789, 0.0 ) * -1.91747588977566e-07 + COALESCE( t2.\"dewp\" - 1.668379064426448, 0.0 ) * 0.001731759164268386 + COALESCE( t2.\"is\" - 0.06086063096820984, 0.0 ) * 0.06061734539826681 + COALESCE( t2.\"ir\" - 0.2111990813489665, 0.0 ) * -0.05403702019138458 + COALESCE( t2.\"temp\" - 12.06001450501632, 0.0 ) * -0.007673283821278228 + COALESCE( t2.\"pres\" - 1016.398404448205, 0.0 ) * 0.002389770661357365 + COALESCE( t2.\"iws\" - 25.00605463556588, 0.0 ) * 0.0006834212113433441 + COALESCE( t2.\"date\" - 1326341256.690439, 0.0 ) * 1.915110364308629e-07 + -3.8304206632990674e-02\n        WHEN ( t2.\"iws\" &gt; 2.996864 ) AND ( t1.\"date\" - t2.\"date\" &lt;= 111439.618138 OR t1.\"date\" IS NULL OR t2.\"date\" IS NULL ) THEN COALESCE( t1.\"dewp\" - 1.514736211828887, 0.0 ) * -0.09710953675504476 + COALESCE( t1.\"temp\" - 11.89228926884439, 0.0 ) * 0.1898035531135342 + COALESCE( t1.\"is\" - 0.06612999800412481, 0.0 ) * 0.2216653278054844 + COALESCE( t1.\"ir\" - 0.2116958286208502, 0.0 ) * 0.2229778643018364 + COALESCE( t1.\"pres\" - 1016.466458208569, 0.0 ) * 0.0137232972180141 + COALESCE( t1.\"iws\" - 25.06146098064165, 0.0 ) * 0.008757818778673529 + COALESCE( t1.\"date\" - 1326377672.037789, 0.0 ) * 3.828784685924325e-05 + COALESCE( t2.\"dewp\" - 1.668379064426448, 0.0 ) * 0.01390967975925731 + COALESCE( t2.\"is\" - 0.06086063096820984, 0.0 ) * -0.05439808805172083 + COALESCE( t2.\"ir\" - 0.2111990813489665, 0.0 ) * -0.1708184586046546 + COALESCE( t2.\"temp\" - 12.06001450501632, 0.0 ) * 0.04153068423706643 + COALESCE( t2.\"pres\" - 1016.398404448205, 0.0 ) * 0.06692208362422453 + COALESCE( t2.\"iws\" - 25.00605463556588, 0.0 ) * 0.0003737878687475554 + COALESCE( t2.\"date\" - 1326341256.690439, 0.0 ) * -3.82854279001483e-05 + -2.6193156234554085e+00\n        WHEN ( t2.\"iws\" &lt;= 2.996864 OR t2.\"iws\" IS NULL ) AND ( t1.\"dewp\" &gt; 11.000000 ) THEN COALESCE( t1.\"dewp\" - 1.514736211828887, 0.0 ) * 0.1393137945465734 + COALESCE( t1.\"temp\" - 11.89228926884439, 0.0 ) * -0.01214102681155058 + COALESCE( t1.\"is\" - 0.06612999800412481, 0.0 ) * -0.1338390047625948 + COALESCE( t1.\"ir\" - 0.2116958286208502, 0.0 ) * -0.0162258602121815 + COALESCE( t1.\"pres\" - 1016.466458208569, 0.0 ) * 0.001699285730186932 + COALESCE( t1.\"iws\" - 25.06146098064165, 0.0 ) * 0.003371800239128447 + COALESCE( t1.\"date\" - 1326377672.037789, 0.0 ) * -1.911896389633282e-07 + COALESCE( t2.\"dewp\" - 1.668379064426448, 0.0 ) * -0.06644334047600213 + COALESCE( t2.\"is\" - 0.06086063096820984, 0.0 ) * -0.1417763462234966 + COALESCE( t2.\"ir\" - 0.2111990813489665, 0.0 ) * -0.1305274026025503 + COALESCE( t2.\"temp\" - 12.06001450501632, 0.0 ) * -0.1337481445687078 + COALESCE( t2.\"pres\" - 1016.398404448205, 0.0 ) * -0.0159175033671927 + COALESCE( t2.\"iws\" - 25.00605463556588, 0.0 ) * 0.0526624167554332 + COALESCE( t2.\"date\" - 1326341256.690439, 0.0 ) * 1.855001999924372e-07 + 1.4765736620125673e+00\n        WHEN ( t2.\"iws\" &lt;= 2.996864 OR t2.\"iws\" IS NULL ) AND ( t1.\"dewp\" &lt;= 11.000000 OR t1.\"dewp\" IS NULL ) THEN COALESCE( t1.\"dewp\" - 1.514736211828887, 0.0 ) * 0.04638612658210784 + COALESCE( t1.\"temp\" - 11.89228926884439, 0.0 ) * -0.02616592034638174 + COALESCE( t1.\"is\" - 0.06612999800412481, 0.0 ) * -0.04279224385040904 + COALESCE( t1.\"ir\" - 0.2116958286208502, 0.0 ) * -0.02539472146735003 + COALESCE( t1.\"pres\" - 1016.466458208569, 0.0 ) * -0.0271755357448101 + COALESCE( t1.\"iws\" - 25.06146098064165, 0.0 ) * -0.002779614164530073 + COALESCE( t1.\"date\" - 1326377672.037789, 0.0 ) * -1.127852653091244e-06 + COALESCE( t2.\"dewp\" - 1.668379064426448, 0.0 ) * -0.009599325629289402 + COALESCE( t2.\"is\" - 0.06086063096820984, 0.0 ) * -0.2440324127160611 + COALESCE( t2.\"ir\" - 0.2111990813489665, 0.0 ) * -0.1198017640418239 + COALESCE( t2.\"temp\" - 12.06001450501632, 0.0 ) * -0.0723450470366292 + COALESCE( t2.\"pres\" - 1016.398404448205, 0.0 ) * -0.006849322627387147 + COALESCE( t2.\"iws\" - 25.00605463556588, 0.0 ) * -0.4129622526694541 + COALESCE( t2.\"date\" - 1326341256.690439, 0.0 ) * 1.129731320846992e-06 + -9.4331291663918275e+00\n        ELSE NULL\n    END\n) AS \"feature_1_5\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"POPULATION__STAGING_TABLE_2\" t2\nON 1 = 1\nWHERE t2.\"date\" &lt;= t1.\"date\"\nAND ( t2.\"date__7_000000_days\" &gt; t1.\"date\" OR t2.\"date__7_000000_days\" IS NULL )\nGROUP BY t1.rowid;\n</pre> <p>This is a typical RelMT feature, where the aggregation (<code>SUM</code> in this case) is applied conditionally \u2013 the conditions are learned by <code>RelMT</code> \u2013 to a set of linear models, whose weights are, again, learned by <code>RelMT</code>.</p> In\u00a0[45]: Copied! <pre># Creates a folder named air_pollution_pipeline containing the SQL code\npipe1.features.to_sql().save(\"air_pollution_pipeline\")\n</pre> # Creates a folder named air_pollution_pipeline containing the SQL code pipe1.features.to_sql().save(\"air_pollution_pipeline\") <p>We have seen that getML outperforms tsfresh by more than 10 percentage points in terms of R-squared. We now want to analyze why that is.</p> <p>There are two possible hypotheses:</p> <ul> <li>getML outperforms featuretools and tsfresh, because it using feature learning and is able to produce more complex features</li> <li>getML outperforms featuretools and tsfresh, because it makes better use of memory and is able to look back further.</li> </ul> <p>Let's summarize our findings:</p> In\u00a0[46]: Copied! <pre>pipes = [pipe1, pipe2, pipe3, pipe4, pipe5, pipe6]\n\ncomparison = pd.DataFrame(\n    dict(\n        tool=[pipe.tags[0] for pipe in pipes],\n        memory=[pipe.tags[1].split()[1] for pipe in pipes],\n        feature_complexity=[pipe.tags[2].split()[0] for pipe in pipes],\n        rsquared=[f\"{pipe.rsquared:.1%}\" for pipe in pipes],\n        rmse=[f\"{pipe.rmse:.3}\" for pipe in pipes],\n    )\n)\n\ncomparison\n</pre> pipes = [pipe1, pipe2, pipe3, pipe4, pipe5, pipe6]  comparison = pd.DataFrame(     dict(         tool=[pipe.tags[0] for pipe in pipes],         memory=[pipe.tags[1].split()[1] for pipe in pipes],         feature_complexity=[pipe.tags[2].split()[0] for pipe in pipes],         rsquared=[f\"{pipe.rsquared:.1%}\" for pipe in pipes],         rmse=[f\"{pipe.rmse:.3}\" for pipe in pipes],     ) )  comparison Out[46]: tool memory feature_complexity rsquared rmse 0 getML: RelMT 7d complex 63.1% 57.5 1 getML: RelMT 1d complex 49.0% 66.9 2 getML: FastProp 7d simple 56.2% 62.6 3 getML: FastProp 1d simple 54.6% 63.4 4 featuretools 1d simple 53.7% 64.3 5 tsfresh 1d simple 51.0% 65.9 <p>The summary table shows that combination of both of our hypotheses explains why getML outperforms featuretools and tsfresh. Complex features do better than simple features with a memory of one day. With a memory of seven days, simple features actually get worse. But when you look back seven days and allow more complex features, you get good results.</p> <p>This suggests that getML outperforms featuretools and tsfresh, because it can make more efficient use of memory and thus look back further. Because RelMT uses feature learning and can build more complex features it can make better use of the greater look-back window.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/air_pollution/#air-pollution-why-feature-learning-is-better-than-simple-propositionalization","title":"Air Pollution - Why feature learning is better than simple propositionalization\u00b6","text":"<p>In this notebook we will compare getML to featuretools and tsfresh, both of which open-source libraries for feature engineering. We find that advanced algorithms featured in getML yield significantly better predictions on this dataset. We then discuss why that is.</p> <p>Summary:</p> <ul> <li>Prediction type: Regression model</li> <li>Domain: Air pollution</li> <li>Prediction target: pm 2.5 concentration</li> <li>Source data: Multivariate time series</li> <li>Population size: 41757</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/air_pollution/#background","title":"Background\u00b6","text":"<p>Many data scientists and AutoML tools use propositionalization methods for feature engineering. These propositionalization methods usually work as follows:</p> <ul> <li>Generate a large number of hard-coded features,</li> <li>Use feature selection to pick a percentage of these features.</li> </ul> <p>By contrast, getML contains approaches for feature learning: Feature learning adapts machine learning approaches such as decision trees or gradient boosting to the problem of extracting features from relational data and time series.</p> <p>In this notebook, we will benchmark getML against featuretools and tsfresh. Both of these libaries use propositionalization approaches for feature engineering.</p> <p>As our example dataset, we use a publicly available dataset on air pollution in Beijing, China: Beijing PM2.5 Data. The data set has been originally used in the following study:</p> <p>Liang, X., Zou, T., Guo, B., Li, S., Zhang, H., Zhang, S., Huang, H. and Chen, S. X. (2015). Assessing Beijing's PM2.5 pollution: severity, weather impact, APEC and winter heating. Proceedings of the Royal Society A, 471, 20150257.</p> <p>We find that getML significantly outperforms featuretools and tsfresh in terms of predictive accuracy ( see Discussion ). Our findings indicate that getML's feature learning algorithms are better at adapting to data sets and are also more scalable due to their lower memory requirement.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/air_pollution/#analysis","title":"Analysis\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/air_pollution/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/air_pollution/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>Downloading the raw data from the  UCI Machine Learning Repository into a prediction ready format takes time. To get to the getML model building as fast as possible, we prepared the data for you and excluded the code from this notebook.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/air_pollution/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/air_pollution/#21-pipeline-1-complex-features-7-days","title":"2.1 Pipeline 1: Complex features, 7 days\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/air_pollution/#22-pipeline-2-complex-features-1-day","title":"2.2 Pipeline 2: Complex features, 1 day\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/air_pollution/#23-pipeline-3-simple-features-7-days","title":"2.3 Pipeline 3: Simple features, 7 days\u00b6","text":"<p>For our third experiment, we will learn simple features and allow a memory of up to seven days.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/air_pollution/#24-pipeline-4-simple-features-1-day","title":"2.4 Pipeline 4: Simple features, 1 day\u00b6","text":"<p>For our fourth experiment, we will learn simple features and allow a memory of up to one day.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/air_pollution/#25-using-featuretools","title":"2.5 Using featuretools\u00b6","text":"<p>To make things a bit easier, we have written high-level wrappers around featuretools and tsfresh which we placed in a separate module (<code>utils</code>).</p>","boost":0.8},{"location":"examples/enterprise-notebooks/air_pollution/#26-using-tsfresh","title":"2.6 Using tsfresh\u00b6","text":"<p>Next, we construct features with tsfresh. tsfresh is based on pandas and rely\\ies on explicit copies for meny operations. This leads to an excessive memory consumption that renders tsfresh nearly unusable for real-world scenarios. Remeber, this is a relatively small data set.</p> <p>To limit the memory consumption, we undertake the following steps:</p> <ul> <li>We limit ourselves to a memory of 1 day from any point in time. This is necessary, because tsfresh duplicates records for every time stamp. That means that looking back 7 days instead of one day, the memory consumption would be  seven times as high.</li> <li>We extract only tsfresh's <code>MinimalFCParameters</code> and <code>IndexBasedFCParameters</code> (the latter is a superset of <code>TimeBasedFCParameters</code>).</li> </ul> <p>In order to make sure that tsfresh's features can be compared to getML's features, we also do the following:</p> <ul> <li>We apply tsfresh's built-in feature selection algorithm.</li> <li>Of the remaining features, we only keep the 40 features most correlated with the target (in terms of the absolute value of the correlation).</li> <li>We add the original columns as additional features.</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/air_pollution/#27-studying-features","title":"2.7 Studying features\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/air_pollution/#28-productionization","title":"2.8 Productionization\u00b6","text":"<p>It is possible to productionize the pipeline by transpiling the features into production-ready SQL code. Please also refer to getML's <code>sqlite3</code> module.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/air_pollution/#3-discussion","title":"3. Discussion\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/air_pollution/#4-conclusion","title":"4. Conclusion\u00b6","text":"<p>We have compared getML's feature learning algorithms to tsfresh's brute-force feature engineering approaches on a data set related to air pollution in China. We found that getML significantly outperforms featuretools and tsfresh. These results are consistent with the view that feature learning can yield significant improvements over simple propositionalization approaches.</p> <p>However, there are other datasets on which simple propositionalization performs well. Our suggestion is therefore to think of algorithms like <code>FastProp</code> and <code>RelMT</code> as tools in a toolbox. If a simple tool like <code>FastProp</code> gets the job done, then use that. But when you need more advanced approaches, like <code>RelMT</code>, you should have them at your disposal as well.</p> <p>You are encouraged to reproduce these results.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/atherosclerosis/","title":"<span class=\"ntitle\">atherosclerosis.ipynb</span> <span class=\"ndesc\">Disease lethality prediction</span>","text":"<p>Let's get started with the analysis and set up your session:</p> In\u00a0[1]: Copied! <pre>import os\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image, Markdown\nimport matplotlib.pyplot as plt\n%matplotlib inline  \n\nimport getml\n\ngetml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.engine.set_project('atherosclerosis')\n</pre> import os from pathlib import Path import numpy as np import pandas as pd from IPython.display import Image, Markdown import matplotlib.pyplot as plt %matplotlib inline    import getml  getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.engine.set_project('atherosclerosis') <pre>getML engine is already running.\n\nConnected to project 'atherosclerosis'\n</pre> In\u00a0[2]: Copied! <pre>population, contr = getml.datasets.load_atherosclerosis()\n</pre> population, contr = getml.datasets.load_atherosclerosis() <pre>\nLoading population...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nLoading contr...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n</pre> <p>Data visualization</p> <p>The original data (image below) model is condensed into 2 tables:</p> <ul> <li>A population table population_{train/test/validate}, based on <code>death</code> table</li> <li>A peripheral table: <code>contr</code>.</li> </ul> <p>Death: population table</p> <ul> <li>Reference Date: Period of time from 1976 to 1999</li> <li>Target: If the patient dies within one year after each reference date</li> </ul> In\u00a0[3]: Copied! <pre>population\n</pre> population Out[3]:  name REFERENCE_DATE ENTRY_DATE      ICO TARGET KONSKUP     STAV        VZDELANI    ZODPOV      TELAKTZA    AKTPOZAM    DOPRAVA     DOPRATRV    ALKOHOL     BOLHR       BOLDK       DUSNOST     RARISK      OBEZRISK    KOURRISK    HTRISK      CHOLRISK    MOC               AGE PARTICIPATION     VYSKA      VAHA     SYST1    DIAST1     SYST2    DIAST2      TRIC     SUBSC     CHLST     TRIGL   KOURENI  DOBAKOUR  BYVKURAK    PIVOMN    VINOMN     LIHMN      KAVA       CAJ      CUKR       ROKNAR     ROKVSTUP     MESVSTUP           IM           HT          ICT       DIABET       HYPLIP        DUMMY         YEAR PIVO7         PIVO10        PIVO12        VINO          LIHOV         IML           HTD           HTL           ICTL          DIABD         DIABL         HYPLD         HYPLL         IMTRV         HTTRV         ICTTRV        DIABTRV       HYPLTRV       DENUMR        MESUMR        ROKUMR        PRICUMR       DEATH_DATE     role     time_stamp time_stamp join_key target categorical categorical categorical categorical categorical categorical categorical categorical categorical categorical categorical categorical categorical categorical categorical categorical categorical categorical numerical     numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string  unit     time stamp time stamp 0 1977-01-01 1976-10-01 10001 0 4 1 3 1 1 2 3 6 2 1 1 1 0 0 1 0 0 1 48 -1899 169 71 120 85 120 90 4 12 209 86 4 10 nan 1 4 9 2 4 3 29 1976 10 2 2 2 2 2 1 1977 NULL NULL NULL NULL 12.0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 1 1978-01-01 1976-10-01 10001 0 4 1 3 1 1 2 3 6 2 1 1 1 0 0 1 0 0 1 49 -1898 169 71 120 85 120 90 4 12 209 86 4 10 nan 1 4 9 2 4 3 29 1976 10 2 2 2 2 2 1 1978 NULL NULL NULL NULL 12.0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 2 1979-01-01 1976-10-01 10001 0 4 1 3 1 1 2 3 6 2 1 1 1 0 0 1 0 0 1 50 -1897 169 71 120 85 120 90 4 12 209 86 4 10 nan 1 4 9 2 4 3 29 1976 10 2 2 2 2 2 1 1979 NULL NULL NULL NULL 12.0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 3 1980-01-01 1976-10-01 10001 0 4 1 3 1 1 2 3 6 2 1 1 1 0 0 1 0 0 1 51 -1896 169 71 120 85 120 90 4 12 209 86 4 10 nan 1 4 9 2 4 3 29 1976 10 2 2 2 2 2 1 1980 NULL NULL NULL NULL 12.0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 4 1981-01-01 1976-10-01 10001 0 4 1 3 1 1 2 3 6 2 1 1 1 0 0 1 0 0 1 52 -1895 169 71 120 85 120 90 4 12 209 86 4 10 nan 1 4 9 2 4 3 29 1976 10 2 2 2 2 2 1 1981 NULL NULL NULL NULL 12.0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 28428 1995-01-01 1977-03-01 30065 0 5 2 2 3 1 2 1 5 2 3 2 2 0 0 1 0 0 1 67 -1882 179 69 110 80 120 80 10 18 235 733 4 10 nan 3 4 7 1 5 2 28 1977 3 2 2 2 2 2 1 1995 NULL 9.0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 28429 1996-01-01 1977-03-01 30065 0 5 2 2 3 1 2 1 5 2 3 2 2 0 0 1 0 0 1 68 -1881 179 69 110 80 120 80 10 18 235 733 4 10 nan 3 4 7 1 5 2 28 1977 3 2 2 2 2 2 1 1996 NULL 9.0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 28430 1997-01-01 1977-03-01 30065 0 5 2 2 3 1 2 1 5 2 3 2 2 0 0 1 0 0 1 69 -1880 179 69 110 80 120 80 10 18 235 733 4 10 nan 3 4 7 1 5 2 28 1977 3 2 2 2 2 2 1 1997 NULL 9.0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 28431 1998-01-01 1977-03-01 30065 0 5 2 2 3 1 2 1 5 2 3 2 2 0 0 1 0 0 1 70 -1879 179 69 110 80 120 80 10 18 235 733 4 10 nan 3 4 7 1 5 2 28 1977 3 2 2 2 2 2 1 1998 NULL 9.0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 28432 1999-01-01 1977-03-01 30065 0 5 2 2 3 1 2 1 5 2 3 2 2 0 0 1 0 0 1 71 -1878 179 69 110 80 120 80 10 18 235 733 4 10 nan 3 4 7 1 5 2 28 1977 3 2 2 2 2 2 1 1999 NULL 9.0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL <p>     28433 rows x 76 columns     memory usage: 16.09 MB     name: population     type: getml.DataFrame </p> <p>Contr: peripheral table</p> In\u00a0[4]: Copied! <pre>contr\n</pre> contr Out[4]:  name CONTROL_DATE      ICO ZMTELAKT    AKTPOZAM    ZMDIET      LEKTLAK     ZMKOUR      POCCIG      PRACNES     JINAONE     BOLHR       BOLDK       DUSN        HODNSK      HYPERD      HYPCHL      HYPTGL           HMOT     CHLST       HDL     HDLMG       LDL       ROKVYS       MESVYS       PORADK     ZMCHARZA          MOC LEKCHOL       SRDCE         HYPERT        CEVMOZ        DIAB          HODN0         ROK0          HODN1         ROK1          HODN2         ROK2          HODN3         ROK3          HODN4         ROK4          HODN11        ROK11         HODN12        ROK12         HODN13        ROK13         HODN14        ROK14         HODN15        ROK15         HODN21        ROK21         HODN23        ROK23         SYST          DIAST         TRIC          SUBSC         HYPERSD       HYPERS        CHLSTMG       TRIGL         TRIGLMG       GLYKEMIE      KYSMOC         role   time_stamp join_key categorical categorical categorical categorical categorical categorical categorical categorical categorical categorical categorical categorical categorical categorical categorical numerical numerical numerical numerical numerical unused_float unused_float unused_float unused_float unused_float unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string unused_string  unit   time stamp 0 1977-09-01 10001 3 2 1 2 2 0.0 1 2.0 1 1 1 2 2.0 2.0 2.0 71 5.61 nan nan nan 1977 9 1 20 1 NULL NULL NULL NULL NULL 0.0 0.0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 130.0 90.0 4.0 12.0 2.0 2.0 217.0 1.22 108.0 NULL NULL 1 1979-01-01 10001 1 1 1 2 1 0.0 2 1 2 1 2 2.0 2.0 2.0 72 6 nan nan nan 1979 1 2 20 3 NULL NULL NULL NULL NULL 0.0 0.0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 140.0 90.0 4.0 11.0 2.0 2.0 232.0 4.4 389.0 NULL NULL 2 1980-04-01 10001 2 1 1 2 1 0.0 2 2 1 1 2 2.0 2.0 2.0 71 6.23 nan nan nan 1980 4 3 20 1 NULL NULL NULL NULL NULL 0.0 0.0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 130.0 90.0 5.0 22.0 2.0 2.0 241.0 1.51 134.0 NULL NULL 3 1982-01-01 10001 2 1 1 2 1 0.0 1 2.0 1 1 1 2 2.0 2.0 2.0 74 5.2 nan nan nan 1982 1 4 20 1 NULL NULL NULL NULL NULL 0.0 0.0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 150.0 100.0 9.0 18.0 2.0 2.0 201.0 1.42 126.0 NULL NULL 4 1983-02-01 10001 2 2 1 2 1 0.0 2 1 2 1 2 2.0 2.0 2.0 73 6.08 1.47 57 4.15 1983 2 5 20 1 NULL NULL NULL NULL NULL 0.0 0.0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 165.0 105.0 7.0 15.0 2.0 2.0 235.0 0.99 88.0 NULL NULL ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 10567 1987-10-01 20358 2 2 1 2 1 0.0 2 4 1 1 3 2.0 2.0 2.0 85 4.89 0.98 38 3.11 1987 10 1 20 1 NULL NULL NULL NULL NULL 0.0 0.0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 120.0 80.0 NULL NULL 2.0 2.0 189.0 1.74 154.0 NULL NULL 10568 1987-11-01 20359 2 2 1 2 1 14.0 2 1 1 1 3 2.0 2.0 2.0 80 4.5 0.98 38 3.16 1987 11 1 20 1 NULL NULL NULL NULL NULL 0.0 0.0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 100.0 70.0 NULL NULL 2.0 2.0 174.0 0.79 70.0 NULL NULL 10569 1987-10-01 20360 2 2 5 2 1 0.0 2 2 1 2 3 2.0 2.0 2.0 95 5.51 1.29 50 3.68 1987 10 1 40 1 NULL NULL NULL NULL NULL 0.0 0.0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 120.0 80.0 NULL NULL 2.0 2.0 213.0 1.18 104.0 NULL NULL 10570 1987-10-01 20362 1 2 1 2 1 0.0 2 6 4 1 3 2.0 2.0 2.0 56 4.89 3 116 1.51 1987 10 1 40 1 NULL NULL NULL NULL NULL 0.0 0.0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 140.0 85.0 NULL NULL 2.0 2.0 189.0 0.82 73.0 NULL NULL 10571 1978-09-01 30037 2 1 1 2 3 15.0 2 4 1 2 4 2.0 2.0 1.0 72 5.61 nan nan nan 1978 9 1 20 1 NULL NULL NULL NULL NULL 0.0 0.0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 125.0 90.0 9.0 17.0 2.0 2.0 217.0 NULL NULL NULL NULL <p>     10572 rows x 67 columns     memory usage: 5.82 MB     name: contr     type: getml.DataFrame </p> In\u00a0[5]: Copied! <pre>split = getml.data.split.random(train=0.7, test=0.3)\n</pre> split = getml.data.split.random(train=0.7, test=0.3) In\u00a0[6]: Copied! <pre>star_schema = getml.data.StarSchema(population, split=split)\n\nstar_schema.join(\n    contr,\n    on=\"ICO\",\n    time_stamps=(\"REFERENCE_DATE\", \"CONTROL_DATE\")\n)\n\nstar_schema\n</pre> star_schema = getml.data.StarSchema(population, split=split)  star_schema.join(     contr,     on=\"ICO\",     time_stamps=(\"REFERENCE_DATE\", \"CONTROL_DATE\") )  star_schema Out[6]: data model diagram contrpopulationICO = ICOCONTROL_DATE &lt;= REFERENCE_DATE staging data frames staging table               0 population POPULATION__STAGING_TABLE_1 1 contr CONTR__STAGING_TABLE_2 container population subset name        rows type 0 test population 8557 View 1 train population 19876 View peripheral name   rows type      0 contr 10572 DataFrame In\u00a0[7]: Copied! <pre>relmt = getml.feature_learning.RelMT(\n    num_features=30,\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    num_threads=1\n)\n\nrelboost = getml.feature_learning.Relboost(\n    num_features=60,\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    min_num_samples=500,\n    max_depth=2,\n    num_threads=1\n)\n\nfeature_selector = getml.predictors.XGBoostClassifier()\n\nxgboost = getml.predictors.XGBoostClassifier(\n    max_depth=5,\n    reg_lambda=100.0,\n    learning_rate=0.1\n)\n</pre> relmt = getml.feature_learning.RelMT(     num_features=30,     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     num_threads=1 )  relboost = getml.feature_learning.Relboost(     num_features=60,     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     min_num_samples=500,     max_depth=2,     num_threads=1 )  feature_selector = getml.predictors.XGBoostClassifier()  xgboost = getml.predictors.XGBoostClassifier(     max_depth=5,     reg_lambda=100.0,     learning_rate=0.1 ) In\u00a0[8]: Copied! <pre>pipe1 = getml.pipeline.Pipeline(\n    tags=[\"relmt\"],\n    data_model=star_schema.data_model,\n    feature_learners=relmt,\n    feature_selectors=feature_selector,\n    share_selected_features=0.8,\n    predictors=xgboost,\n    include_categorical=True\n)\n\npipe1\n</pre> pipe1 = getml.pipeline.Pipeline(     tags=[\"relmt\"],     data_model=star_schema.data_model,     feature_learners=relmt,     feature_selectors=feature_selector,     share_selected_features=0.8,     predictors=xgboost,     include_categorical=True )  pipe1 Out[8]: <pre>Pipeline(data_model='population',\n         feature_learners=['RelMT'],\n         feature_selectors=['XGBoostClassifier'],\n         include_categorical=True,\n         loss_function='CrossEntropyLoss',\n         peripheral=['contr'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=[],\n         share_selected_features=0.8,\n         tags=['relmt'])</pre> In\u00a0[9]: Copied! <pre>pipe2 = getml.pipeline.Pipeline(\n    tags=[\"relboost\"],\n    data_model=star_schema.data_model,\n    feature_learners=relboost,\n    feature_selectors=feature_selector,\n    share_selected_features=0.8,    \n    predictors=xgboost,\n    include_categorical=True\n)\n\npipe2\n</pre> pipe2 = getml.pipeline.Pipeline(     tags=[\"relboost\"],     data_model=star_schema.data_model,     feature_learners=relboost,     feature_selectors=feature_selector,     share_selected_features=0.8,         predictors=xgboost,     include_categorical=True )  pipe2 Out[9]: <pre>Pipeline(data_model='population',\n         feature_learners=['Relboost'],\n         feature_selectors=['XGBoostClassifier'],\n         include_categorical=True,\n         loss_function='CrossEntropyLoss',\n         peripheral=['contr'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=[],\n         share_selected_features=0.8,\n         tags=['relboost'])</pre> <p>We begin with RelMT. Features generated by RelMT suffer from quadratic complexity. Luckily, the number of columns is not too high, so it is still manageble.</p> <p>It always a good idea to check before fitting. There is one minor warning: This warning means that about 12% of patients never appear in CONTR (presumably because they never showed up to their health check-ups).</p> In\u00a0[10]: Copied! <pre>pipe1.check(star_schema.train)\n</pre> pipe1.check(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 1 issues labeled INFO and 0 issues labeled WARNING.\n</pre> Out[10]: type label                  message                          0 INFO FOREIGN KEYS NOT FOUND When joining POPULATION__STAGING_TABLE_1 and CONTR__STAGING_TABLE_2 over 'ICO' and 'ICO', there are no corresponding entries for 12.291205% of entries in 'ICO' in 'POPULATION__STAGING_TABLE_1'. You might want to double-check your join keys. In\u00a0[11]: Copied! <pre>pipe1.fit(star_schema.train)\n</pre> pipe1.fit(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 1 issues labeled INFO and 0 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelMT: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 04:39, remaining: 00:00]          \nRelMT: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nXGBoost: Training as feature selector... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:03, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:04, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:4m:47.297086\n\n</pre> Out[11]: <pre>Pipeline(data_model='population',\n         feature_learners=['RelMT'],\n         feature_selectors=['XGBoostClassifier'],\n         include_categorical=True,\n         loss_function='CrossEntropyLoss',\n         peripheral=['contr'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=[],\n         share_selected_features=0.8,\n         tags=['relmt', 'container-9DTKIA'])</pre> <p>Let's see how well Relboost does. This algorithm has linear complexity in the number of columns.</p> In\u00a0[12]: Copied! <pre>pipe2.fit(star_schema.train)\n</pre> pipe2.fit(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 1 issues labeled INFO and 0 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:19, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \nXGBoost: Training as feature selector... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:03, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:25.559479\n\n</pre> Out[12]: <pre>Pipeline(data_model='population',\n         feature_learners=['Relboost'],\n         feature_selectors=['XGBoostClassifier'],\n         include_categorical=True,\n         loss_function='CrossEntropyLoss',\n         peripheral=['contr'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=[],\n         share_selected_features=0.8,\n         tags=['relboost', 'container-9DTKIA'])</pre> <p>Note that this runs through in under a minute. This demonstrates the power of computational complexity theory. If we had more columns, the difference between these two algorithms would become even more noticable.</p> In\u00a0[13]: Copied! <pre>pipe1.score(star_schema.test)\n</pre> pipe1.score(star_schema.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelMT: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\n</pre> Out[13]: date time           set used target accuracy     auc cross entropy 0 2024-02-21 14:59:13 train TARGET 0.9887 0.8768 0.05199 1 2024-02-21 14:59:39 test TARGET 0.9867 0.7068 0.06838 In\u00a0[14]: Copied! <pre>pipe2.score(star_schema.test)\n</pre> pipe2.score(star_schema.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\n</pre> Out[14]: date time           set used target accuracy     auc cross entropy 0 2024-02-21 14:59:38 train TARGET 0.9878 0.8881 0.05407 1 2024-02-21 14:59:40 test TARGET 0.9867 0.7129 0.06743 In\u00a0[15]: Copied! <pre>names, correlations = pipe2.features.correlations()\n\nplt.subplots(figsize=(20, 10))\n\nplt.bar(names, correlations)\n\nplt.title(\"feature correlations\")\nplt.grid(True)\nplt.xlabel(\"features\")\nplt.ylabel(\"correlations\")\nplt.xticks(rotation='vertical')\n\nplt.show()\n</pre> names, correlations = pipe2.features.correlations()  plt.subplots(figsize=(20, 10))  plt.bar(names, correlations)  plt.title(\"feature correlations\") plt.grid(True) plt.xlabel(\"features\") plt.ylabel(\"correlations\") plt.xticks(rotation='vertical')  plt.show() In\u00a0[16]: Copied! <pre>names, importances = pipe2.features.importances()\n\nplt.subplots(figsize=(20, 10))\n\nplt.bar(names, importances)\n\nplt.title(\"feature importances\")\nplt.grid(True)\nplt.xlabel(\"features\")\nplt.ylabel(\"importances\")\nplt.xticks(rotation='vertical')\n\nplt.show()\n</pre> names, importances = pipe2.features.importances()  plt.subplots(figsize=(20, 10))  plt.bar(names, importances)  plt.title(\"feature importances\") plt.grid(True) plt.xlabel(\"features\") plt.ylabel(\"importances\") plt.xticks(rotation='vertical')  plt.show() <p>As we can see from these figures we need many features to get a good result. No single feature is very correlated with the target and the feature importance is not concentrated on a small number of features (as is often the case with other data sets).</p> <p>This implies that the many columns in the data set are actually needed. The reason we emphasize that is that we sometimes see data sets with many columns, but after analyzing them we find and only a handful of these columns are actually needed. This is not one of these times.</p> <p>The most important features look like this:</p> In\u00a0[17]: Copied! <pre>pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name]\n</pre> pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name] Out[17]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_2\";\n\nCREATE TABLE \"FEATURE_1_2\" AS\nSELECT SUM( \n    CASE\n        WHEN ( t2.\"zmkour\" IN ( '7' ) ) AND ( t1.\"reference_date\" &gt; 852038400.000000 ) THEN COALESCE( t1.\"age\" - 58.55541532813217, 0.0 ) * 0.6992075915192288 + COALESCE( t1.\"participation\" - -1887.414410279945, 0.0 ) * -0.3722162052975991 + COALESCE( t1.\"vyska\" - 174.626548875631, 0.0 ) * 0.002072665325539906 + COALESCE( t1.\"vaha\" - 80.15500229463056, 0.0 ) * 0.7135789936249097 + COALESCE( t1.\"syst1\" - 132.0390087195962, 0.0 ) * 0.6204509957556447 + COALESCE( t1.\"diast1\" - 83.58329508949059, 0.0 ) * -0.2852251406284317 + COALESCE( t1.\"syst2\" - 129.4263423588802, 0.0 ) * 0.5416045970884422 + COALESCE( t1.\"diast2\" - 83.2121385956861, 0.0 ) * -0.7691376230609942 + COALESCE( t1.\"tric\" - 9.410853602569986, 0.0 ) * -0.4921979178739925 + COALESCE( t1.\"subsc\" - 18.22670949977054, 0.0 ) * 0.2235547808836321 + COALESCE( t1.\"chlst\" - 232.7238412115649, 0.0 ) * -0.2305456390285043 + COALESCE( t1.\"trigl\" - 134.5320100963745, 0.0 ) * -0.08732404144815611 + COALESCE( t1.\"koureni\" - 3.382973841211565, 0.0 ) * 3.578257813757872 + COALESCE( t1.\"dobakour\" - 6.960188159706287, 0.0 ) * 3.420605423460954 + COALESCE( t1.\"byvkurak\" - 1.689421753097751, 0.0 ) * 1.468999926745384 + COALESCE( t1.\"pivomn\" - 1.800826067003213, 0.0 ) * 2.006077827378471 + COALESCE( t1.\"vinomn\" - 4.298990362551629, 0.0 ) * 1.509295328270136 + COALESCE( t1.\"lihmn\" - 6.948141349242772, 0.0 ) * 1.689453613870151 + COALESCE( t1.\"kava\" - 1.919343735658559, 0.0 ) * 15.33082510657582 + COALESCE( t1.\"caj\" - 4.731642955484167, 0.0 ) * 10.97404738286759 + COALESCE( t1.\"cukr\" - 4.495869664983938, 0.0 ) * 1.130351262906143 + COALESCE( t1.\"reference_date\" - 614493572.4644332, 0.0 ) * -1.551420482850177e-08 + COALESCE( t1.\"entry_date\" - 230484053.9697109, 0.0 ) * -2.196562174142811e-07 + COALESCE( t2.\"hmot\" - 81.27387640449439, 0.0 ) * 0.2535134048176608 + COALESCE( t2.\"hdlmg\" - 35.46147672552167, 0.0 ) * -0.04134495275193414 + COALESCE( t2.\"chlst\" - 5.892755818619529, 0.0 ) * -10.72183117055155 + COALESCE( t2.\"hdl\" - 0.9149819422150931, 0.0 ) * -1.580592374193315 + COALESCE( t2.\"ldl\" - 2.463498194221503, 0.0 ) * -1.709624186115308 + COALESCE( t2.\"control_date\" - 533301113.6436597, 0.0 ) * -6.539468045584373e-08 + 6.5967578763439345e+00\n        WHEN ( t2.\"zmkour\" IN ( '7' ) ) AND ( t1.\"reference_date\" &lt;= 852038400.000000 OR t1.\"reference_date\" IS NULL ) THEN COALESCE( t1.\"age\" - 58.55541532813217, 0.0 ) * 7.009110879137228 + COALESCE( t1.\"participation\" - -1887.414410279945, 0.0 ) * 13.11693059229238 + COALESCE( t1.\"vyska\" - 174.626548875631, 0.0 ) * 1.269893142479568 + COALESCE( t1.\"vaha\" - 80.15500229463056, 0.0 ) * -3.162664037823355 + COALESCE( t1.\"syst1\" - 132.0390087195962, 0.0 ) * 0.3787421899628546 + COALESCE( t1.\"diast1\" - 83.58329508949059, 0.0 ) * 1.971210134455975 + COALESCE( t1.\"syst2\" - 129.4263423588802, 0.0 ) * 0.6635424419434129 + COALESCE( t1.\"diast2\" - 83.2121385956861, 0.0 ) * -2.939270936123821 + COALESCE( t1.\"tric\" - 9.410853602569986, 0.0 ) * 2.383223854209837 + COALESCE( t1.\"subsc\" - 18.22670949977054, 0.0 ) * 1.498266105418248 + COALESCE( t1.\"chlst\" - 232.7238412115649, 0.0 ) * -0.0517718309107393 + COALESCE( t1.\"trigl\" - 134.5320100963745, 0.0 ) * 0.2276783275568421 + COALESCE( t1.\"koureni\" - 3.382973841211565, 0.0 ) * -4.23254041935678 + COALESCE( t1.\"dobakour\" - 6.960188159706287, 0.0 ) * 5.719150573247517 + COALESCE( t1.\"byvkurak\" - 1.689421753097751, 0.0 ) * 2.70196864001066 + COALESCE( t1.\"pivomn\" - 1.800826067003213, 0.0 ) * 18.79720042999637 + COALESCE( t1.\"vinomn\" - 4.298990362551629, 0.0 ) * -10.93785280575887 + COALESCE( t1.\"lihmn\" - 6.948141349242772, 0.0 ) * -13.52764654128384 + COALESCE( t1.\"kava\" - 1.919343735658559, 0.0 ) * 15.62103938182056 + COALESCE( t1.\"caj\" - 4.731642955484167, 0.0 ) * -8.797933019568511 + COALESCE( t1.\"cukr\" - 4.495869664983938, 0.0 ) * -3.757872625493459 + COALESCE( t1.\"reference_date\" - 614493572.4644332, 0.0 ) * -6.356934996127425e-07 + COALESCE( t1.\"entry_date\" - 230484053.9697109, 0.0 ) * 5.640187513549882e-07 + COALESCE( t2.\"hmot\" - 81.27387640449439, 0.0 ) * 3.023356785615304 + COALESCE( t2.\"hdlmg\" - 35.46147672552167, 0.0 ) * -0.7240192100118329 + COALESCE( t2.\"chlst\" - 5.892755818619529, 0.0 ) * -4.024059582855235 + COALESCE( t2.\"hdl\" - 0.9149819422150931, 0.0 ) * -29.58919968380793 + COALESCE( t2.\"ldl\" - 2.463498194221503, 0.0 ) * 1.778411316422035 + COALESCE( t2.\"control_date\" - 533301113.6436597, 0.0 ) * 2.163236433930246e-07 + -7.9015375976371462e+00\n        WHEN ( t2.\"zmkour\" NOT IN ( '7' ) OR t2.\"zmkour\" IS NULL ) AND ( t1.\"htrisk\" IN ( '6' ) ) THEN COALESCE( t1.\"age\" - 58.55541532813217, 0.0 ) * -3.397114537185319 + COALESCE( t1.\"participation\" - -1887.414410279945, 0.0 ) * -8.217148782839798 + COALESCE( t1.\"vyska\" - 174.626548875631, 0.0 ) * 0.8989975149085871 + COALESCE( t1.\"vaha\" - 80.15500229463056, 0.0 ) * -1.237315019051493 + COALESCE( t1.\"syst1\" - 132.0390087195962, 0.0 ) * -0.9928276153180301 + COALESCE( t1.\"diast1\" - 83.58329508949059, 0.0 ) * -2.788494436812396 + COALESCE( t1.\"syst2\" - 129.4263423588802, 0.0 ) * -0.4818397334334769 + COALESCE( t1.\"diast2\" - 83.2121385956861, 0.0 ) * 6.971936650721264 + COALESCE( t1.\"tric\" - 9.410853602569986, 0.0 ) * -3.694055970730235 + COALESCE( t1.\"subsc\" - 18.22670949977054, 0.0 ) * -5.857003884016523 + COALESCE( t1.\"chlst\" - 232.7238412115649, 0.0 ) * 2.110663998001987 + COALESCE( t1.\"trigl\" - 134.5320100963745, 0.0 ) * 0.1191325417763164 + COALESCE( t1.\"koureni\" - 3.382973841211565, 0.0 ) * -19.83587241194438 + COALESCE( t1.\"dobakour\" - 6.960188159706287, 0.0 ) * -0.6739075204128018 + COALESCE( t1.\"byvkurak\" - 1.689421753097751, 0.0 ) * -2.401197249091638e-07 + COALESCE( t1.\"pivomn\" - 1.800826067003213, 0.0 ) * 43.42691366138832 + COALESCE( t1.\"vinomn\" - 4.298990362551629, 0.0 ) * -188.452548812023 + COALESCE( t1.\"lihmn\" - 6.948141349242772, 0.0 ) * 149.8317616555433 + COALESCE( t1.\"kava\" - 1.919343735658559, 0.0 ) * -7.92533116285952 + COALESCE( t1.\"caj\" - 4.731642955484167, 0.0 ) * 7.371945970513856 + COALESCE( t1.\"cukr\" - 4.495869664983938, 0.0 ) * -6.522018830745494 + COALESCE( t1.\"reference_date\" - 614493572.4644332, 0.0 ) * 3.705953810711698e-07 + COALESCE( t1.\"entry_date\" - 230484053.9697109, 0.0 ) * -1.783593808548376e-06 + COALESCE( t2.\"hmot\" - 81.27387640449439, 0.0 ) * -0.03696650252475624 + COALESCE( t2.\"hdlmg\" - 35.46147672552167, 0.0 ) * -0.8978599033836492 + COALESCE( t2.\"chlst\" - 5.892755818619529, 0.0 ) * -2.613193795898457 + COALESCE( t2.\"hdl\" - 0.9149819422150931, 0.0 ) * 33.10188686443065 + COALESCE( t2.\"ldl\" - 2.463498194221503, 0.0 ) * 2.062126624888299 + COALESCE( t2.\"control_date\" - 533301113.6436597, 0.0 ) * -5.886932876564901e-10 + -1.0081350693584685e+02\n        WHEN ( t2.\"zmkour\" NOT IN ( '7' ) OR t2.\"zmkour\" IS NULL ) AND ( t1.\"htrisk\" NOT IN ( '6' ) OR t1.\"htrisk\" IS NULL ) THEN COALESCE( t1.\"age\" - 58.55541532813217, 0.0 ) * 0.002419801161207402 + COALESCE( t1.\"participation\" - -1887.414410279945, 0.0 ) * -0.06277853150672247 + COALESCE( t1.\"vyska\" - 174.626548875631, 0.0 ) * 0.002571038648386481 + COALESCE( t1.\"vaha\" - 80.15500229463056, 0.0 ) * -0.005578860882900542 + COALESCE( t1.\"syst1\" - 132.0390087195962, 0.0 ) * -0.001057898151656378 + COALESCE( t1.\"diast1\" - 83.58329508949059, 0.0 ) * 0.004014557721064409 + COALESCE( t1.\"syst2\" - 129.4263423588802, 0.0 ) * 0.001873173639385572 + COALESCE( t1.\"diast2\" - 83.2121385956861, 0.0 ) * 0.0002124825706128939 + COALESCE( t1.\"tric\" - 9.410853602569986, 0.0 ) * -0.002640732624754488 + COALESCE( t1.\"subsc\" - 18.22670949977054, 0.0 ) * 0.0001474535382830381 + COALESCE( t1.\"chlst\" - 232.7238412115649, 0.0 ) * -0.0001256948379644612 + COALESCE( t1.\"trigl\" - 134.5320100963745, 0.0 ) * -3.474245058868025e-05 + COALESCE( t1.\"koureni\" - 3.382973841211565, 0.0 ) * 0.0132708577852937 + COALESCE( t1.\"dobakour\" - 6.960188159706287, 0.0 ) * 0.002454283595539609 + COALESCE( t1.\"byvkurak\" - 1.689421753097751, 0.0 ) * -0.0003505117806351746 + COALESCE( t1.\"pivomn\" - 1.800826067003213, 0.0 ) * 0.02562835710639392 + COALESCE( t1.\"vinomn\" - 4.298990362551629, 0.0 ) * -0.005171308330700418 + COALESCE( t1.\"lihmn\" - 6.948141349242772, 0.0 ) * -0.004672335054095883 + COALESCE( t1.\"kava\" - 1.919343735658559, 0.0 ) * 0.02581134186668081 + COALESCE( t1.\"caj\" - 4.731642955484167, 0.0 ) * -0.01154259733921239 + COALESCE( t1.\"cukr\" - 4.495869664983938, 0.0 ) * 0.002194525846818011 + COALESCE( t1.\"reference_date\" - 614493572.4644332, 0.0 ) * 3.411798694161753e-09 + COALESCE( t1.\"entry_date\" - 230484053.9697109, 0.0 ) * -2.075596685480708e-09 + COALESCE( t2.\"hmot\" - 81.27387640449439, 0.0 ) * 0.004478272836447239 + COALESCE( t2.\"hdlmg\" - 35.46147672552167, 0.0 ) * -0.002438061446690539 + COALESCE( t2.\"chlst\" - 5.892755818619529, 0.0 ) * -0.02099226608599071 + COALESCE( t2.\"hdl\" - 0.9149819422150931, 0.0 ) * 0.03134631902567388 + COALESCE( t2.\"ldl\" - 2.463498194221503, 0.0 ) * 0.02487247493931399 + COALESCE( t2.\"control_date\" - 533301113.6436597, 0.0 ) * -2.655718509874403e-09 + -2.7439158467228797e-01\n        ELSE NULL\n    END\n) AS \"feature_1_2\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"CONTR__STAGING_TABLE_2\" t2\nON t1.\"ico\" = t2.\"ico\"\nWHERE t2.\"control_date\" &lt;= t1.\"reference_date\"\nGROUP BY t1.rowid;\n</pre> In\u00a0[18]: Copied! <pre>pipe2.features.to_sql()[pipe2.features.sort(by=\"importances\")[1].name]\n</pre> pipe2.features.to_sql()[pipe2.features.sort(by=\"importances\")[1].name] Out[18]: <pre>\n</pre> In\u00a0[19]: Copied! <pre># Creates a folder named atherosclerosis_pipeline containing\n# the SQL code.\npipe2.features.to_sql().save(\"atherosclerosis_pipeline\", remove=True)\n</pre> # Creates a folder named atherosclerosis_pipeline containing # the SQL code. pipe2.features.to_sql().save(\"atherosclerosis_pipeline\", remove=True) In\u00a0[20]: Copied! <pre>pipe2.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"atherosclerosis_pipeline_spark\", remove=True)\n</pre> pipe2.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"atherosclerosis_pipeline_spark\", remove=True)","boost":0.8},{"location":"examples/enterprise-notebooks/atherosclerosis/#atherosclerosis-disease-lethality-prediction","title":"Atherosclerosis - Disease lethality prediction\u00b6","text":"<p>With this notebook we give a brief introduction to feature engineering on relational data with many columns. We discuss why feature engineering on such data is particularly challenging and what we can do to overcome these problems.</p> <p>Summary:</p> <ul> <li>Prediction type: Binary classification</li> <li>Domain: Health</li> <li>Prediction target: Mortality within one year</li> <li>Source data: 146 columns in 2 tables, 22 MB</li> <li>Population size: 28433</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/atherosclerosis/#feature-engineering-and-the-curse-of-dimensionality","title":"Feature engineering and the curse of dimensionality\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/atherosclerosis/#the-problem","title":"The problem\u00b6","text":"<p>To illustrate the point, we give a simplified example based on the real data used in the analysis below. When we engineer features from relational data, we usually write something like this:</p> <pre>SELECT AVG(t2.HDL)\nFROM population_training t1\nLEFT JOIN contr t2\nON t1.ICO = t2.ICO\nWHERE t1.AGE &gt;= 60 AND t1.ALOKOHOL IN ('1', '2')\nGROUP BY t1.ICO;\n</pre> <p>Think about that for a second. This feature aggregates high-density lipoprotein (HDL) cholesterol values recorded during control dates conditional on age and alcohol consumption. We arbitrarily chose both, the column to aggregate over (HDL)  and the set of columns to construct conditions on (AGE and ALKOHOL) out of a greater set of 146 columns.</p> <p>Every column that we have can either be aggregated (here HDL) or it can be used for our conditions (here AGE and ALKOHOL). That means if we have n columns to aggregate, we can potentially build conditions for $n$ other columns. In other words, the computational complexity is $n^2$ in the number of columns.</p> <p>Note that this problem occurs regardless of whether you automate feature engineering or you do it by hand. The size of the search space is $n^2$ in the number of columns in either case, unless you can rule something out a-priori.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/atherosclerosis/#the-solution","title":"The solution\u00b6","text":"<p>So when we have relational data sets with many columns, what do we do? The answer is to write different features. Specifically, suppose we had features like this:</p> <pre>SELECT AVG(\n    CASE WHEN t1.AGE &gt;= THEN weight1\n    CASE WHEN t1.ALKOHOL IN ('1', '2') THEN weight2\n    END\n)\nFROM population_training t1\nLEFT JOIN contr t2\nON t1.ICO = t2.ICO\nGROUP BY t1.ICO;\n</pre> <p>weight1 and weight2 are learnable weights. An algorithm that generates features like this can only use columns for conditions, it is not allowed to aggregate columns \u2013 and it doesn't need to do so.</p> <p>That means the computational complexity is linear instead of quadratic. For data sets with a large number of columns this can make all the difference in the world. For instance, if you have 100 columns the size of the search space of the second approach is only 1% of the size of the search space of the first one.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/atherosclerosis/#background","title":"Background\u00b6","text":"<p>To illustrate the problem of dimensionality in predictive analytics on relational data, we use the STULONG 1 dataset. It is a longitudinal study of atherosclerosis patients.</p> <p>One of its defining features is that it contains many columns, which makes it a good candidate to illustrate the problem discussed in this notebook.</p> <p>The are some academic studies related to this dataset:</p> <ul> <li>https://www.researchgate.net/publication/228572841_Mining_episode_rules_in_STULONG_dataset</li> <li>https://citeseerx.ist.psu.edu/doc_view/pid/3a9cb05b77b631b6fcbe253eb93e053ba8c0719c</li> </ul> <p>The way these studies handle the large number of columns in the data set is to divide the columns into subgroups and then handling each subgroup separately. Even though this is one way to overcome the curse of dimensionality, it is not a very satisfying approach. We would like to be able to handle a large number of columns at once.</p> <p>The analysis is based on the STULONG 1 dataset. It is publicly available and can be downloaded the the CTU Prague Relational Learning Repository (Now residing at relational-data.org.).</p>","boost":0.8},{"location":"examples/enterprise-notebooks/atherosclerosis/#analysis","title":"Analysis\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/atherosclerosis/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/atherosclerosis/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>Downloading the raw data and convert it into a prediction ready format takes time. To get to the getML model building as fast as possible, we prepared the data for you and excluded the code from this notebook. It is made available in the example notebook featuring the full analysis.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/atherosclerosis/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"<p>The <code>getml.datasets.load_atherosclerosis</code> method took care of the entire data lifting:</p> <ul> <li>Downloads csv's from our servers in python</li> <li>Converts csv's to getML DataFrames</li> <li>Sets roles to columns inside getML DataFrames</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/atherosclerosis/#13-define-relational-model","title":"1.3 Define relational model\u00b6","text":"<p>To start with relational learning, we need to specify an abstract data model. Here, we use the high-level star schema API that allows us to define the abstract data model and construct a container with the concrete data at one-go. While a simple <code>StarSchema</code> indeed works in many cases, it is not sufficient for more complex data models like schoflake schemas, where you would have to define the data model and construct the container in separate steps, by utilzing getML's full-fledged data model and container APIs respectively.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/atherosclerosis/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"<p>We loaded the data, defined the roles, units and the abstract data model. Next, we create a getML pipeline for relational learning.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/atherosclerosis/#21-fitting-the-pipeline","title":"2.1 Fitting the pipeline\u00b6","text":"<p>To illustrate the problem of computational complexity, we fit two getML pipelines using different feature learning algorithms:</p> <p>The first pipeline uses <code>RelMT</code> and the second pipeline uses <code>Relboost</code>. To demonstrate the power of feature ensembling, we build a third pipeline that uses both algorithms. At the end, we compare the runtime and predictive accuracies of all three pipelines.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/atherosclerosis/#22-model-evaluation","title":"2.2 Model evaluation\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/atherosclerosis/#23-studying-the-features","title":"2.3 Studying the features\u00b6","text":"<p>It is always a good idea to study the features generated by the algorithms.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/atherosclerosis/#24-productionization","title":"2.4 Productionization\u00b6","text":"<p>It is possible to productionize the pipeline by transpiling the features into production-ready SQL code. Please also refer to getML's <code>sqlite3</code> and <code>spark</code> modules.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/atherosclerosis/#3-conclusion","title":"3. Conclusion\u00b6","text":"<p>The runtime benchmark between RelMT &amp; Relboost on the same dataset demonstrates the problem of computational complexity in practice. We find that RelMT takes four times as long as Relboost to learn fewer features.</p> <p>The purpose of this notebook has been to illustrate the problem of the curse of dimensionality when engineering features from datasets with many columns.</p> <p>The most important thing to remember is that this problem exists regardless of whether you engineer your features manually or using algorithms. Whether you like it or not: If you write your features in the traditional way, your search space grows quadratically with the number of columns.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/baseball/","title":"<span class=\"ntitle\">baseball.ipynb</span> <span class=\"ndesc\">Predicting players' salary</span>","text":"<p>Let's get started with the analysis and set up your session:</p> In\u00a0[1]: Copied! <pre>import copy\nimport os\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\nfrom pathlib import Path\n\nfrom urllib import request\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline  \n\nimport featuretools\nimport woodwork as ww\nimport getml\n\ngetml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.engine.set_project('baseball')\n</pre> import copy import os os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\" from pathlib import Path  from urllib import request  import numpy as np import pandas as pd from IPython.display import Image import matplotlib.pyplot as plt %matplotlib inline    import featuretools import woodwork as ww import getml  getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.engine.set_project('baseball') <pre>getML engine is already running.\n\nConnected to project 'baseball'\n</pre> In\u00a0[2]: Copied! <pre>conn = getml.database.connect_mysql(\n    host=\"db.relational-data.org\",\n    dbname=\"lahman_2014\",\n    port=3306,\n    user=\"guest\",\n    password=\"relational\"\n)\n\nconn\n</pre> conn = getml.database.connect_mysql(     host=\"db.relational-data.org\",     dbname=\"lahman_2014\",     port=3306,     user=\"guest\",     password=\"relational\" )  conn Out[2]: <pre>Connection(dbname='lahman_2014',\n           dialect='mysql',\n           host='db.relational-data.org',\n           port=3306)</pre> In\u00a0[3]: Copied! <pre>def load_if_needed(name):\n    \"\"\"\n    Loads the data from the relational learning\n    repository, if the data frame has not already\n    been loaded.\n    \"\"\"\n    if not getml.data.exists(name):\n        data_frame = getml.data.DataFrame.from_db(\n            name=name,\n            table_name=name,\n            conn=conn\n        )\n        data_frame.save()\n    else:\n        data_frame = getml.data.load_data_frame(name)\n    return data_frame\n</pre> def load_if_needed(name):     \"\"\"     Loads the data from the relational learning     repository, if the data frame has not already     been loaded.     \"\"\"     if not getml.data.exists(name):         data_frame = getml.data.DataFrame.from_db(             name=name,             table_name=name,             conn=conn         )         data_frame.save()     else:         data_frame = getml.data.load_data_frame(name)     return data_frame In\u00a0[4]: Copied! <pre>allstarfull = load_if_needed(\"allstarfull\")\nawardsplayers = load_if_needed(\"awardsplayers\")\nawardsshareplayers = load_if_needed(\"awardsshareplayers\")\nbatting = load_if_needed(\"batting\")\nbattingpost = load_if_needed(\"battingpost\")\nfielding = load_if_needed(\"fielding\")\nfieldingpost = load_if_needed(\"fieldingpost\")\npitching = load_if_needed(\"pitching\")\npitchingpost = load_if_needed(\"pitchingpost\")\nsalaries = load_if_needed(\"salaries\")\n</pre> allstarfull = load_if_needed(\"allstarfull\") awardsplayers = load_if_needed(\"awardsplayers\") awardsshareplayers = load_if_needed(\"awardsshareplayers\") batting = load_if_needed(\"batting\") battingpost = load_if_needed(\"battingpost\") fielding = load_if_needed(\"fielding\") fieldingpost = load_if_needed(\"fieldingpost\") pitching = load_if_needed(\"pitching\") pitchingpost = load_if_needed(\"pitchingpost\") salaries = load_if_needed(\"salaries\") In\u00a0[5]: Copied! <pre>allstarfull\n</pre> allstarfull Out[5]: name       yearID      gameNum           GP  startingPos playerID      gameID        teamID        lgID          role unused_float unused_float unused_float unused_float unused_string unused_string unused_string unused_string 0 1955 0 1 nan aaronha01 NLS195507120 ML1 NL 1 1956 0 1 nan aaronha01 ALS195607100 ML1 NL 2 1957 0 1 9 aaronha01 NLS195707090 ML1 NL 3 1958 0 1 9 aaronha01 ALS195807080 ML1 NL 4 1959 1 1 9 aaronha01 NLS195907070 ML1 NL ... ... ... ... ... ... ... ... 4826 1978 0 1 9 ziskri01 NLS197807110 TEX AL 4827 2002 0 1 nan zitoba01 NLS200207090 OAK AL 4828 2003 0 0 nan zitoba01 ALS200307150 OAK AL 4829 2006 0 1 nan zitoba01 NLS200607110 OAK AL 4830 2009 0 1 nan zobribe01 NLS200907140 TBA AL <p>     4831 rows x 8 columns     memory usage: 0.45 MB     name: allstarfull     type: getml.DataFrame </p> In\u00a0[6]: Copied! <pre>awardsplayers\n</pre> awardsplayers Out[6]: name       yearID playerID      awardID                          lgID          tie           notes         role unused_float unused_string unused_string                    unused_string unused_string unused_string 0 1877 bondto01 Pitching Triple Crown NL NULL NULL 1 1878 hinespa01 Triple Crown NL NULL NULL 2 1884 heckegu01 Pitching Triple Crown AA NULL NULL 3 1884 radboch01 Pitching Triple Crown NL NULL NULL 4 1887 oneilti01 Triple Crown AA NULL NULL ... ... ... ... ... ... 5790 2012 larocad01 Silver Slugger NL NULL 1B 5791 2012 cabremi01 Triple Crown NL NULL NULL 5792 2012 cabremi01 TSN Major League Player of the Y... ML NULL NULL 5793 2012 verlaju01 TSN Pitcher of the Year AL Y NULL 5794 2012 dickera01 TSN Pitcher of the Year NL NULL NULL <p>     5795 rows x 6 columns     memory usage: 0.48 MB     name: awardsplayers     type: getml.DataFrame </p> In\u00a0[7]: Copied! <pre>awardsshareplayers\n</pre> awardsshareplayers Out[7]: name       yearID    pointsWon    pointsMax   votesFirst awardID            lgID          playerID      role unused_float unused_float unused_float unused_float unused_string      unused_string unused_string 0 1956 1 16 1 Cy Young ML fordwh01 1 1956 4 16 4 Cy Young ML maglisa01 2 1956 10 16 10 Cy Young ML newcodo01 3 1956 1 16 1 Cy Young ML spahnwa01 4 1957 1 16 1 Cy Young ML donovdi01 ... ... ... ... ... ... ... 6284 2006 1 160 0 Rookie of the Year NL willijo03 6285 2006 101 160 10 Rookie of the Year NL zimmery01 6286 2008 3 140 0 Rookie of the Year AL devinjo01 6287 2008 158 160 31 Rookie of the Year NL sotoge01 6288 2008 9 160 0 Rookie of the Year NL volqued01 <p>     6289 rows x 7 columns     memory usage: 0.47 MB     name: awardsshareplayers     type: getml.DataFrame </p> In\u00a0[8]: Copied! <pre>batting\n</pre> batting Out[8]:  name       yearID        stint            G    G_batting           AB            R            H           2B           3B           HR          RBI           SB           CS           BB           SO          IBB          HBP           SH           SF         GIDP        G_old playerID      teamID        lgID           role unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_string unused_string unused_string 0 2004 1 11 11 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 11 aardsda01 SFN NL 1 2006 1 45 43 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 45 aardsda01 CHN NL 2 2007 1 25 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 aardsda01 CHA AL 3 2008 1 47 5 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 5 aardsda01 BOS AL 4 2009 1 73 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan aardsda01 SEA AL ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 92348 1959 1 6 6 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 6 zuverge01 BAL AL 92349 1910 1 27 27 87 7 16 5 0 0 5 1 nan 11 nan nan 1 1 nan nan 27 zwilldu01 CHA AL 92350 1914 1 154 154 592 91 185 38 8 16 95 21 nan 46 68 nan 1 10 nan nan 154 zwilldu01 CHF FL 92351 1915 1 150 150 548 65 157 32 7 13 94 24 nan 67 65 nan 2 18 nan nan 150 zwilldu01 CHF FL 92352 1916 1 35 35 53 4 6 1 0 1 8 0 nan 4 6 nan 0 2 nan nan 35 zwilldu01 CHN NL <p>     92353 rows x 24 columns     memory usage: 19.29 MB     name: batting     type: getml.DataFrame </p> In\u00a0[9]: Copied! <pre>battingpost\n</pre> battingpost Out[9]: name       yearID            G           AB            R            H           2B           3B           HR          RBI           SB           CS           BB           SO          IBB          HBP           SH           SF         GIDP round         playerID      teamID        lgID          role unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_string unused_string unused_string unused_string 0 1884 1 2 0 1 0 0 0 0 0 nan 0 0 0 nan nan nan nan WS becanbu01 NY4 AA 1 1884 3 10 1 0 0 0 0 0 0 nan 0 1 0 nan nan nan nan WS bradyst01 NY4 AA 2 1884 3 10 2 1 0 0 0 1 0 nan 1 1 0 nan nan nan nan WS carrocl01 PRO NL 3 1884 3 9 3 4 0 1 1 2 0 nan 0 3 0 nan nan nan nan WS dennyje01 PRO NL 4 1884 3 10 0 3 1 0 0 0 1 nan 0 3 0 nan nan nan nan WS esterdu01 NY4 AA ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 9793 2012 2 5 1 1 0 0 0 0 0 0 0 2 nan nan nan nan nan WS theriry01 SFN NL 9794 2012 1 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan WS valvejo01 DET AL 9795 2012 1 1 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan WS verlaju01 DET AL 9796 2012 1 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan WS vogelry01 SFN NL 9797 2012 1 2 0 1 0 0 0 1 0 0 0 1 nan nan nan nan nan WS zitoba01 SFN NL <p>     9798 rows x 22 columns     memory usage: 1.93 MB     name: battingpost     type: getml.DataFrame </p> In\u00a0[10]: Copied! <pre>fielding\n</pre> fielding Out[10]:   name       yearID        stint            G           GS      InnOuts           PO            A            E           DP           PB           WP           SB           CS           ZR playerID      teamID        lgID          POS             role unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_string unused_string unused_string unused_string 0 2004 1 11 0 32 0 0 0 0 nan nan nan nan nan aardsda01 SFN NL P 1 2006 1 45 0 159 1 5 0 1 nan nan nan nan nan aardsda01 CHN NL P 2 2007 1 25 0 97 2 4 1 0 nan nan nan nan nan aardsda01 CHA AL P 3 2008 1 47 0 146 3 6 0 0 nan nan nan nan nan aardsda01 BOS AL P 4 2009 1 73 0 214 2 5 0 1 0 nan 0 0 nan aardsda01 SEA AL P ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 137970 1910 1 27 nan nan 45 2 3 1 nan nan nan nan nan zwilldu01 CHA AL OF 137971 1914 1 154 nan nan 340 15 14 3 nan nan nan nan nan zwilldu01 CHF FL OF 137972 1915 1 3 nan nan 3 0 0 0 nan nan nan nan nan zwilldu01 CHF FL 1B 137973 1915 1 148 nan nan 356 20 8 6 nan nan nan nan nan zwilldu01 CHF FL OF 137974 1916 1 10 nan nan 11 0 0 0 nan nan nan nan nan zwilldu01 CHN NL OF <p>     137975 rows x 18 columns     memory usage: 22.57 MB     name: fielding     type: getml.DataFrame </p> In\u00a0[11]: Copied! <pre>fieldingpost\n</pre> fieldingpost Out[11]:  name       yearID            G           GS      InnOuts           PO            A            E           DP           TP           PB           SB           CS playerID      teamID        lgID          round         POS            role unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_string unused_string unused_string unused_string unused_string 0 1957 7 7 186 11 0 0 0 0 nan nan nan aaronha01 ML1 NL WS CF 1 1958 1 1 21 2 0 0 0 0 nan nan nan aaronha01 ML1 NL WS CF 2 1958 7 6 168 13 0 0 0 0 nan nan nan aaronha01 ML1 NL WS RF 3 1969 3 3 78 5 1 0 0 0 nan nan nan aaronha01 ATL NL NLCS RF 4 1979 2 0 15 0 1 0 0 0 nan 0 0 aasedo01 CAL AL ALCS P ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 10341 2006 1 1 24 0 1 0 0 0 nan 0 1 zitoba01 OAK AL ALDS2 P 10342 2012 1 1 23 0 1 0 0 0 0 0 0 zitoba01 SFN NL NLCS P 10343 2012 1 1 8 0 0 0 0 0 0 0 0 zitoba01 SFN NL NLDS2 P 10344 2012 1 1 17 0 0 0 0 0 0 0 0 zitoba01 SFN NL WS P 10345 1946 1 0 6 0 0 0 0 0 nan 0 0 zuberbi01 BOS AL WS P <p>     10346 rows x 17 columns     memory usage: 1.65 MB     name: fieldingpost     type: getml.DataFrame </p> In\u00a0[12]: Copied! <pre>pitching\n</pre> pitching Out[12]:  name       yearID        stint            W            L            G           GS           CG          SHO           SV       IPouts            H           ER           HR           BB           SO        BAOpp          ERA          IBB           WP          HBP           BK          BFP           GF            R           SH           SF         GIDP playerID      teamID        lgID           role unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_string unused_string unused_string 0 2004 1 1 0 11 0 0 0 0 32 20 8 1 10 5 0 6 0 0 2 0 61 5 8 nan nan nan aardsda01 SFN NL 1 2006 1 3 0 45 0 0 0 0 159 41 24 9 28 49 nan 4 0 1 1 0 225 9 25 nan nan nan aardsda01 CHN NL 2 2007 1 2 1 25 0 0 0 0 97 39 23 4 17 36 nan 6 3 2 1 0 151 7 24 nan nan nan aardsda01 CHA AL 3 2008 1 4 2 47 0 0 0 0 146 49 30 4 35 49 nan 5 2 3 5 0 228 7 32 nan nan nan aardsda01 BOS AL 4 2009 1 3 6 73 0 0 0 38 214 49 20 4 34 80 nan 2 3 2 0 0 296 53 23 nan nan nan aardsda01 SEA AL ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 39356 1955 2 4 3 28 5 0 0 4 259 80 21 5 17 31 0 2 1 2 4 0 333 16 28 nan nan nan zuverge01 BAL AL 39357 1956 1 7 6 62 0 0 0 16 292 112 45 6 34 33 0 4 9 1 3 1 432 40 52 nan nan nan zuverge01 BAL AL 39358 1957 1 10 6 56 0 0 0 9 338 105 31 9 39 36 0 2 13 1 4 0 475 37 37 nan nan nan zuverge01 BAL AL 39359 1958 1 2 2 45 0 0 0 7 207 74 26 4 17 22 0 3 3 2 6 0 294 23 29 nan nan nan zuverge01 BAL AL 39360 1959 1 0 1 6 0 0 0 0 39 15 6 1 6 1 0 4 0 1 0 0 55 1 7 nan nan nan zuverge01 BAL AL <p>     39361 rows x 30 columns     memory usage: 10.11 MB     name: pitching     type: getml.DataFrame </p> In\u00a0[13]: Copied! <pre>pitchingpost\n</pre> pitchingpost Out[13]: name       yearID            W            L            G           GS           CG          SHO           SV       IPouts            H           ER           HR           BB           SO        BAOpp          ERA          IBB           WP          HBP           BK          BFP           GF            R           SH           SF         GIDP playerID      round         teamID        lgID          role unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_string unused_string unused_string unused_string 0 1979 1 0 2 0 0 0 0 15 4 1 0 2 6 0 1 1 0 0 0 20 2 1 0 1 0 aasedo01 ALCS CAL AL 1 1975 0 0 1 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 3 1 0 0 0 0 abbotgl01 ALCS OAK AL 2 2000 0 1 1 1 0 0 0 15 3 3 1 3 3 0 5 0 0 0 0 21 0 3 0 0 0 abbotpa01 ALCS SEA AL 3 2000 1 0 1 1 0 0 0 17 5 1 0 3 1 0 1 0 0 1 0 25 0 2 0 1 1 abbotpa01 ALDS2 SEA AL 4 2001 0 0 1 1 0 0 0 15 0 0 0 8 2 0 0 0 0 0 0 21 0 0 1 0 0 abbotpa01 ALCS SEA AL ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 4192 2006 1 0 1 1 0 0 0 24 4 1 1 3 1 0 1 0 0 0 0 30 0 1 1 0 0 zitoba01 ALDS2 OAK AL 4193 2012 1 0 1 1 0 0 0 23 6 0 0 1 6 0 0 1 0 0 nan 29 0 0 nan nan 1 zitoba01 NLCS SFN NL 4194 2012 0 0 1 1 0 0 0 7 4 2 1 4 4 0 6 0 0 0 nan 16 0 2 nan nan 0 zitoba01 NLDS2 SFN NL 4195 2012 1 0 1 1 0 0 0 17 6 1 0 1 3 0 1 0 0 0 nan 23 0 1 nan nan 1 zitoba01 WS SFN NL 4196 1946 0 0 1 0 0 0 0 6 3 1 0 1 1 0 4 1 0 0 0 9 0 1 0 0 1 zuberbi01 WS BOS AL <p>     4197 rows x 30 columns     memory usage: 1.10 MB     name: pitchingpost     type: getml.DataFrame </p> In\u00a0[14]: Copied! <pre>salaries\n</pre> salaries Out[14]:  name       yearID       salary teamID        lgID          playerID       role unused_float unused_float unused_string unused_string unused_string 0 1985 870000 ATL NL barkele01 1 1985 550000 ATL NL bedrost01 2 1985 545000 ATL NL benedbr01 3 1985 633333 ATL NL campri01 4 1985 625000 ATL NL ceronri01 ... ... ... ... ... 23106 2012 750000 WAS NL tracych01 23107 2012 4000000 WAS NL wangch01 23108 2012 13571428 WAS NL werthja01 23109 2012 2300000 WAS NL zimmejo02 23110 2012 12000000 WAS NL zimmery01 <p>     23111 rows x 5 columns     memory usage: 1.31 MB     name: salaries     type: getml.DataFrame </p> <p>getML requires that we define roles for each of the columns.</p> In\u00a0[15]: Copied! <pre>allstarfull[\"year\"] = allstarfull[\"yearID\"].as_str().as_ts([\"%Y\"])\n\nallstarfull.set_role([\"playerID\"], getml.data.roles.join_key)\nallstarfull.set_role(allstarfull.roles.unused_string, getml.data.roles.categorical)\nallstarfull.set_role(allstarfull.roles.unused_float, getml.data.roles.numerical)\nallstarfull.set_role(\"year\", getml.data.roles.time_stamp)\n\nallstarfull.set_role(\"yearID\", getml.data.roles.unused_float)\n\nallstarfull\n</pre> allstarfull[\"year\"] = allstarfull[\"yearID\"].as_str().as_ts([\"%Y\"])  allstarfull.set_role([\"playerID\"], getml.data.roles.join_key) allstarfull.set_role(allstarfull.roles.unused_string, getml.data.roles.categorical) allstarfull.set_role(allstarfull.roles.unused_float, getml.data.roles.numerical) allstarfull.set_role(\"year\", getml.data.roles.time_stamp)  allstarfull.set_role(\"yearID\", getml.data.roles.unused_float)  allstarfull Out[15]: name                        year  playerID gameID       teamID      lgID          gameNum        GP startingPos       yearID role                  time_stamp  join_key categorical  categorical categorical numerical numerical   numerical unused_float unit time stamp, comparison only 0 1955-01-01 aaronha01 NLS195507120 ML1 NL 0 1 nan 1955 1 1956-01-01 aaronha01 ALS195607100 ML1 NL 0 1 nan 1956 2 1957-01-01 aaronha01 NLS195707090 ML1 NL 0 1 9 1957 3 1958-01-01 aaronha01 ALS195807080 ML1 NL 0 1 9 1958 4 1959-01-01 aaronha01 NLS195907070 ML1 NL 1 1 9 1959 ... ... ... ... ... ... ... ... ... 4826 1978-01-01 ziskri01 NLS197807110 TEX AL 0 1 9 1978 4827 2002-01-01 zitoba01 NLS200207090 OAK AL 0 1 nan 2002 4828 2003-01-01 zitoba01 ALS200307150 OAK AL 0 0 nan 2003 4829 2006-01-01 zitoba01 NLS200607110 OAK AL 0 1 nan 2006 4830 2009-01-01 zobribe01 NLS200907140 TBA AL 0 1 nan 2009 <p>     4831 rows x 9 columns     memory usage: 0.27 MB     name: allstarfull     type: getml.DataFrame </p> In\u00a0[16]: Copied! <pre>awardsplayers[\"year\"] = awardsplayers[\"yearID\"].as_str().as_ts([\"%Y\"])\n\nawardsplayers.set_role([\"playerID\"], getml.data.roles.join_key)\nawardsplayers.set_role([\"awardID\", \"lgID\", \"notes\"], getml.data.roles.categorical)\nawardsplayers.set_role(\"year\", getml.data.roles.time_stamp)\n\nawardsplayers\n</pre> awardsplayers[\"year\"] = awardsplayers[\"yearID\"].as_str().as_ts([\"%Y\"])  awardsplayers.set_role([\"playerID\"], getml.data.roles.join_key) awardsplayers.set_role([\"awardID\", \"lgID\", \"notes\"], getml.data.roles.categorical) awardsplayers.set_role(\"year\", getml.data.roles.time_stamp)  awardsplayers Out[16]: name                        year  playerID awardID                          lgID        notes             yearID tie           role                  time_stamp  join_key categorical                      categorical categorical unused_float unused_string unit time stamp, comparison only 0 1877-01-01 bondto01 Pitching Triple Crown NL NULL 1877 NULL 1 1878-01-01 hinespa01 Triple Crown NL NULL 1878 NULL 2 1884-01-01 heckegu01 Pitching Triple Crown AA NULL 1884 NULL 3 1884-01-01 radboch01 Pitching Triple Crown NL NULL 1884 NULL 4 1887-01-01 oneilti01 Triple Crown AA NULL 1887 NULL ... ... ... ... ... ... ... 5790 2012-01-01 larocad01 Silver Slugger NL 1B 2012 NULL 5791 2012-01-01 cabremi01 Triple Crown NL NULL 2012 NULL 5792 2012-01-01 cabremi01 TSN Major League Player of the Y... ML NULL 2012 NULL 5793 2012-01-01 verlaju01 TSN Pitcher of the Year AL NULL 2012 Y 5794 2012-01-01 dickera01 TSN Pitcher of the Year NL NULL 2012 NULL <p>     5795 rows x 7 columns     memory usage: 0.24 MB     name: awardsplayers     type: getml.DataFrame </p> In\u00a0[17]: Copied! <pre>awardsshareplayers[\"year\"] = awardsshareplayers[\"yearID\"].as_str().as_ts([\"%Y\"])\n\nawardsshareplayers.set_role([\"playerID\"], getml.data.roles.join_key)\nawardsshareplayers.set_role(awardsshareplayers.roles.unused_float, getml.data.roles.numerical)\nawardsshareplayers.set_role(awardsshareplayers.roles.unused_string, getml.data.roles.categorical)\nawardsshareplayers.set_role(\"yearID\", getml.data.roles.unused_float)\nawardsshareplayers.set_role(\"year\", getml.data.roles.time_stamp)\n\nawardsshareplayers\n</pre> awardsshareplayers[\"year\"] = awardsshareplayers[\"yearID\"].as_str().as_ts([\"%Y\"])  awardsshareplayers.set_role([\"playerID\"], getml.data.roles.join_key) awardsshareplayers.set_role(awardsshareplayers.roles.unused_float, getml.data.roles.numerical) awardsshareplayers.set_role(awardsshareplayers.roles.unused_string, getml.data.roles.categorical) awardsshareplayers.set_role(\"yearID\", getml.data.roles.unused_float) awardsshareplayers.set_role(\"year\", getml.data.roles.time_stamp)  awardsshareplayers Out[17]: name                        year  playerID awardID            lgID        pointsWon pointsMax votesFirst       yearID role                  time_stamp  join_key categorical        categorical numerical numerical  numerical unused_float unit time stamp, comparison only 0 1956-01-01 fordwh01 Cy Young ML 1 16 1 1956 1 1956-01-01 maglisa01 Cy Young ML 4 16 4 1956 2 1956-01-01 newcodo01 Cy Young ML 10 16 10 1956 3 1956-01-01 spahnwa01 Cy Young ML 1 16 1 1956 4 1957-01-01 donovdi01 Cy Young ML 1 16 1 1957 ... ... ... ... ... ... ... ... 6284 2006-01-01 willijo03 Rookie of the Year NL 1 160 0 2006 6285 2006-01-01 zimmery01 Rookie of the Year NL 101 160 10 2006 6286 2008-01-01 devinjo01 Rookie of the Year AL 3 140 0 2008 6287 2008-01-01 sotoge01 Rookie of the Year NL 158 160 31 2008 6288 2008-01-01 volqued01 Rookie of the Year NL 9 160 0 2008 <p>     6289 rows x 8 columns     memory usage: 0.33 MB     name: awardsshareplayers     type: getml.DataFrame </p> In\u00a0[18]: Copied! <pre>batting[\"year\"] = batting[\"yearID\"].as_str().as_ts([\"%Y\"])\n\nbatting.set_role([\"playerID\", \"teamID\"], getml.data.roles.join_key)\nbatting.set_role(batting.roles.unused_float, getml.data.roles.numerical)\nbatting.set_role(batting.roles.unused_string, getml.data.roles.categorical)\nbatting.set_role(\"yearID\", getml.data.roles.unused_float)\nbatting.set_role(\"year\", getml.data.roles.time_stamp)\n\nbatting\n</pre> batting[\"year\"] = batting[\"yearID\"].as_str().as_ts([\"%Y\"])  batting.set_role([\"playerID\", \"teamID\"], getml.data.roles.join_key) batting.set_role(batting.roles.unused_float, getml.data.roles.numerical) batting.set_role(batting.roles.unused_string, getml.data.roles.categorical) batting.set_role(\"yearID\", getml.data.roles.unused_float) batting.set_role(\"year\", getml.data.roles.time_stamp)  batting Out[18]:  name                        year  playerID   teamID lgID            stint         G G_batting        AB         R         H        2B        3B        HR       RBI        SB        CS        BB        SO       IBB       HBP        SH        SF      GIDP     G_old       yearID  role                  time_stamp  join_key join_key categorical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical unused_float  unit time stamp, comparison only 0 2004-01-01 aardsda01 SFN NL 1 11 11 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 11 2004 1 2006-01-01 aardsda01 CHN NL 1 45 43 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 45 2006 2 2007-01-01 aardsda01 CHA AL 1 25 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2007 3 2008-01-01 aardsda01 BOS AL 1 47 5 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 5 2008 4 2009-01-01 aardsda01 SEA AL 1 73 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan 2009 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 92348 1959-01-01 zuverge01 BAL AL 1 6 6 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 6 1959 92349 1910-01-01 zwilldu01 CHA AL 1 27 27 87 7 16 5 0 0 5 1 nan 11 nan nan 1 1 nan nan 27 1910 92350 1914-01-01 zwilldu01 CHF FL 1 154 154 592 91 185 38 8 16 95 21 nan 46 68 nan 1 10 nan nan 154 1914 92351 1915-01-01 zwilldu01 CHF FL 1 150 150 548 65 157 32 7 13 94 24 nan 67 65 nan 2 18 nan nan 150 1915 92352 1916-01-01 zwilldu01 CHN NL 1 35 35 53 4 6 1 0 1 8 0 nan 4 6 nan 0 2 nan nan 35 1916 <p>     92353 rows x 25 columns     memory usage: 17.36 MB     name: batting     type: getml.DataFrame </p> In\u00a0[19]: Copied! <pre>battingpost[\"year\"] = battingpost[\"yearID\"].as_str().as_ts([\"%Y\"])\n\nbattingpost.set_role([\"playerID\", \"teamID\"], getml.data.roles.join_key)\nbattingpost.set_role(battingpost.roles.unused_float, getml.data.roles.numerical)\nbattingpost.set_role(battingpost.roles.unused_string, getml.data.roles.categorical)\nbattingpost.set_role(\"yearID\", getml.data.roles.unused_float)\nbattingpost.set_role(\"year\", getml.data.roles.time_stamp)\n\nbattingpost\n</pre> battingpost[\"year\"] = battingpost[\"yearID\"].as_str().as_ts([\"%Y\"])  battingpost.set_role([\"playerID\", \"teamID\"], getml.data.roles.join_key) battingpost.set_role(battingpost.roles.unused_float, getml.data.roles.numerical) battingpost.set_role(battingpost.roles.unused_string, getml.data.roles.categorical) battingpost.set_role(\"yearID\", getml.data.roles.unused_float) battingpost.set_role(\"year\", getml.data.roles.time_stamp)  battingpost Out[19]: name                        year  playerID   teamID round       lgID                G        AB         R         H        2B        3B        HR       RBI        SB        CS        BB        SO       IBB       HBP        SH        SF      GIDP       yearID role                  time_stamp  join_key join_key categorical categorical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical unused_float unit time stamp, comparison only 0 1884-01-01 becanbu01 NY4 WS AA 1 2 0 1 0 0 0 0 0 nan 0 0 0 nan nan nan nan 1884 1 1884-01-01 bradyst01 NY4 WS AA 3 10 1 0 0 0 0 0 0 nan 0 1 0 nan nan nan nan 1884 2 1884-01-01 carrocl01 PRO WS NL 3 10 2 1 0 0 0 1 0 nan 1 1 0 nan nan nan nan 1884 3 1884-01-01 dennyje01 PRO WS NL 3 9 3 4 0 1 1 2 0 nan 0 3 0 nan nan nan nan 1884 4 1884-01-01 esterdu01 NY4 WS AA 3 10 0 3 1 0 0 0 1 nan 0 3 0 nan nan nan nan 1884 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 9793 2012-01-01 theriry01 SFN WS NL 2 5 1 1 0 0 0 0 0 0 0 2 nan nan nan nan nan 2012 9794 2012-01-01 valvejo01 DET WS AL 1 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan 2012 9795 2012-01-01 verlaju01 DET WS AL 1 1 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan 2012 9796 2012-01-01 vogelry01 SFN WS NL 1 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan 2012 9797 2012-01-01 zitoba01 SFN WS NL 1 2 0 1 0 0 0 1 0 0 0 1 nan nan nan nan nan 2012 <p>     9798 rows x 23 columns     memory usage: 1.65 MB     name: battingpost     type: getml.DataFrame </p> In\u00a0[20]: Copied! <pre>fielding[\"year\"] = fielding[\"yearID\"].as_str().as_ts([\"%Y\"])\n\nfielding.set_role([\"playerID\", \"teamID\"], getml.data.roles.join_key)\nfielding.set_role([\"stint\", \"G\",\"GS\",\"InnOuts\", \"PO\", \"A\", \"E\", \"DP\"], getml.data.roles.numerical)\nfielding.set_role(fielding.roles.unused_string, getml.data.roles.categorical)\nfielding.set_role(\"year\", getml.data.roles.time_stamp)\n\nfielding\n</pre> fielding[\"year\"] = fielding[\"yearID\"].as_str().as_ts([\"%Y\"])  fielding.set_role([\"playerID\", \"teamID\"], getml.data.roles.join_key) fielding.set_role([\"stint\", \"G\",\"GS\",\"InnOuts\", \"PO\", \"A\", \"E\", \"DP\"], getml.data.roles.numerical) fielding.set_role(fielding.roles.unused_string, getml.data.roles.categorical) fielding.set_role(\"year\", getml.data.roles.time_stamp)  fielding Out[20]:   name                        year  playerID   teamID lgID        POS             stint         G        GS   InnOuts        PO         A         E        DP       yearID           PB           WP           SB           CS           ZR   role                  time_stamp  join_key join_key categorical categorical numerical numerical numerical numerical numerical numerical numerical numerical unused_float unused_float unused_float unused_float unused_float unused_float   unit time stamp, comparison only 0 2004-01-01 aardsda01 SFN NL P 1 11 0 32 0 0 0 0 2004 nan nan nan nan nan 1 2006-01-01 aardsda01 CHN NL P 1 45 0 159 1 5 0 1 2006 nan nan nan nan nan 2 2007-01-01 aardsda01 CHA AL P 1 25 0 97 2 4 1 0 2007 nan nan nan nan nan 3 2008-01-01 aardsda01 BOS AL P 1 47 0 146 3 6 0 0 2008 nan nan nan nan nan 4 2009-01-01 aardsda01 SEA AL P 1 73 0 214 2 5 0 1 2009 0 nan 0 0 nan ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 137970 1910-01-01 zwilldu01 CHA AL OF 1 27 nan nan 45 2 3 1 1910 nan nan nan nan nan 137971 1914-01-01 zwilldu01 CHF FL OF 1 154 nan nan 340 15 14 3 1914 nan nan nan nan nan 137972 1915-01-01 zwilldu01 CHF FL 1B 1 3 nan nan 3 0 0 0 1915 nan nan nan nan nan 137973 1915-01-01 zwilldu01 CHF FL OF 1 148 nan nan 356 20 8 6 1915 nan nan nan nan nan 137974 1916-01-01 zwilldu01 CHN NL OF 1 10 nan nan 11 0 0 0 1916 nan nan nan nan nan <p>     137975 rows x 19 columns     memory usage: 18.76 MB     name: fielding     type: getml.DataFrame </p> In\u00a0[21]: Copied! <pre>fieldingpost[\"year\"] = fieldingpost[\"yearID\"].as_str().as_ts([\"%Y\"])\n\nfieldingpost.set_role([\"playerID\", \"teamID\"], getml.data.roles.join_key)\nfieldingpost.set_role([\"G\", \"GS\", \"InnOuts\", \"PO\", \"A\", \"E\", \"DP\", \"TP\", \"SB\", \"CS\"], getml.data.roles.numerical)\nfieldingpost.set_role(fieldingpost.roles.unused_string, getml.data.roles.categorical)\nfieldingpost.set_role(\"year\", getml.data.roles.time_stamp)\n\nfieldingpost\n</pre> fieldingpost[\"year\"] = fieldingpost[\"yearID\"].as_str().as_ts([\"%Y\"])  fieldingpost.set_role([\"playerID\", \"teamID\"], getml.data.roles.join_key) fieldingpost.set_role([\"G\", \"GS\", \"InnOuts\", \"PO\", \"A\", \"E\", \"DP\", \"TP\", \"SB\", \"CS\"], getml.data.roles.numerical) fieldingpost.set_role(fieldingpost.roles.unused_string, getml.data.roles.categorical) fieldingpost.set_role(\"year\", getml.data.roles.time_stamp)  fieldingpost Out[21]:  name                        year  playerID   teamID lgID        round       POS                 G        GS   InnOuts        PO         A         E        DP        TP        SB        CS       yearID           PB  role                  time_stamp  join_key join_key categorical categorical categorical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical unused_float unused_float  unit time stamp, comparison only 0 1957-01-01 aaronha01 ML1 NL WS CF 7 7 186 11 0 0 0 0 nan nan 1957 nan 1 1958-01-01 aaronha01 ML1 NL WS CF 1 1 21 2 0 0 0 0 nan nan 1958 nan 2 1958-01-01 aaronha01 ML1 NL WS RF 7 6 168 13 0 0 0 0 nan nan 1958 nan 3 1969-01-01 aaronha01 ATL NL NLCS RF 3 3 78 5 1 0 0 0 nan nan 1969 nan 4 1979-01-01 aasedo01 CAL AL ALCS P 2 0 15 0 1 0 0 0 0 0 1979 nan ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 10341 2006-01-01 zitoba01 OAK AL ALDS2 P 1 1 24 0 1 0 0 0 0 1 2006 nan 10342 2012-01-01 zitoba01 SFN NL NLCS P 1 1 23 0 1 0 0 0 0 0 2012 0 10343 2012-01-01 zitoba01 SFN NL NLDS2 P 1 1 8 0 0 0 0 0 0 0 2012 0 10344 2012-01-01 zitoba01 SFN NL WS P 1 1 17 0 0 0 0 0 0 0 2012 0 10345 1946-01-01 zuberbi01 BOS AL WS P 1 0 6 0 0 0 0 0 0 0 1946 nan <p>     10346 rows x 18 columns     memory usage: 1.28 MB     name: fieldingpost     type: getml.DataFrame </p> In\u00a0[22]: Copied! <pre>pitching[\"year\"] = pitching[\"yearID\"].as_str().as_ts([\"%Y\"])\n\npitching.set_role([\"playerID\", \"teamID\"], getml.data.roles.join_key)\npitching.set_role([\n    \"stint\", \n    \"W\", \n    \"L\", \n    \"G\", \n    \"GS\", \n    \"CG\", \n    \"SHO\", \n    \"SV\", \n    \"IPouts\", \n    \"H\", \n    \"ER\", \n    \"HR\", \n    \"BB\", \n    \"SO\", \n    \"BAOpp\", \n    \"ERA\", \n    \"IBB\", \n    \"WP\", \n    \"HBP\", \n    \"BK\", \n    \"BFP\", \n    \"GF\", \n    \"R\"], getml.data.roles.numerical)\npitching.set_role(pitching.roles.unused_string, getml.data.roles.categorical)\npitching.set_role(\"yearID\", getml.data.roles.unused_float)\npitching.set_role(\"year\", getml.data.roles.time_stamp)\n\npitching\n</pre> pitching[\"year\"] = pitching[\"yearID\"].as_str().as_ts([\"%Y\"])  pitching.set_role([\"playerID\", \"teamID\"], getml.data.roles.join_key) pitching.set_role([     \"stint\",      \"W\",      \"L\",      \"G\",      \"GS\",      \"CG\",      \"SHO\",      \"SV\",      \"IPouts\",      \"H\",      \"ER\",      \"HR\",      \"BB\",      \"SO\",      \"BAOpp\",      \"ERA\",      \"IBB\",      \"WP\",      \"HBP\",      \"BK\",      \"BFP\",      \"GF\",      \"R\"], getml.data.roles.numerical) pitching.set_role(pitching.roles.unused_string, getml.data.roles.categorical) pitching.set_role(\"yearID\", getml.data.roles.unused_float) pitching.set_role(\"year\", getml.data.roles.time_stamp)  pitching Out[22]:  name                        year  playerID   teamID lgID            stint         W         L         G        GS        CG       SHO        SV    IPouts         H        ER        HR        BB        SO     BAOpp       ERA       IBB        WP       HBP        BK       BFP        GF         R       yearID           SH           SF         GIDP  role                  time_stamp  join_key join_key categorical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical unused_float unused_float unused_float unused_float  unit time stamp, comparison only 0 2004-01-01 aardsda01 SFN NL 1 1 0 11 0 0 0 0 32 20 8 1 10 5 0 6 0 0 2 0 61 5 8 2004 nan nan nan 1 2006-01-01 aardsda01 CHN NL 1 3 0 45 0 0 0 0 159 41 24 9 28 49 nan 4 0 1 1 0 225 9 25 2006 nan nan nan 2 2007-01-01 aardsda01 CHA AL 1 2 1 25 0 0 0 0 97 39 23 4 17 36 nan 6 3 2 1 0 151 7 24 2007 nan nan nan 3 2008-01-01 aardsda01 BOS AL 1 4 2 47 0 0 0 0 146 49 30 4 35 49 nan 5 2 3 5 0 228 7 32 2008 nan nan nan 4 2009-01-01 aardsda01 SEA AL 1 3 6 73 0 0 0 38 214 49 20 4 34 80 nan 2 3 2 0 0 296 53 23 2009 nan nan nan ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 39356 1955-01-01 zuverge01 BAL AL 2 4 3 28 5 0 0 4 259 80 21 5 17 31 0 2 1 2 4 0 333 16 28 1955 nan nan nan 39357 1956-01-01 zuverge01 BAL AL 1 7 6 62 0 0 0 16 292 112 45 6 34 33 0 4 9 1 3 1 432 40 52 1956 nan nan nan 39358 1957-01-01 zuverge01 BAL AL 1 10 6 56 0 0 0 9 338 105 31 9 39 36 0 2 13 1 4 0 475 37 37 1957 nan nan nan 39359 1958-01-01 zuverge01 BAL AL 1 2 2 45 0 0 0 7 207 74 26 4 17 22 0 3 3 2 6 0 294 23 29 1958 nan nan nan 39360 1959-01-01 zuverge01 BAL AL 1 0 1 6 0 0 0 0 39 15 6 1 6 1 0 4 0 1 0 0 55 1 7 1959 nan nan nan <p>     39361 rows x 31 columns     memory usage: 9.29 MB     name: pitching     type: getml.DataFrame </p> In\u00a0[23]: Copied! <pre>pitchingpost[\"year\"] = pitchingpost[\"yearID\"].as_str().as_ts([\"%Y\"])\n\npitchingpost.set_role([\"playerID\", \"teamID\"], getml.data.roles.join_key)\npitchingpost.set_role(pitchingpost.roles.unused_float, getml.data.roles.numerical)\npitchingpost.set_role(pitchingpost.roles.unused_string, getml.data.roles.categorical)\npitchingpost.set_role(\"yearID\", getml.data.roles.unused_float)\npitchingpost.set_role(\"year\", getml.data.roles.time_stamp)\n\npitchingpost\n</pre> pitchingpost[\"year\"] = pitchingpost[\"yearID\"].as_str().as_ts([\"%Y\"])  pitchingpost.set_role([\"playerID\", \"teamID\"], getml.data.roles.join_key) pitchingpost.set_role(pitchingpost.roles.unused_float, getml.data.roles.numerical) pitchingpost.set_role(pitchingpost.roles.unused_string, getml.data.roles.categorical) pitchingpost.set_role(\"yearID\", getml.data.roles.unused_float) pitchingpost.set_role(\"year\", getml.data.roles.time_stamp)  pitchingpost Out[23]: name                        year  playerID   teamID round       lgID                W         L         G        GS        CG       SHO        SV    IPouts         H        ER        HR        BB        SO     BAOpp       ERA       IBB        WP       HBP        BK       BFP        GF         R        SH        SF      GIDP       yearID role                  time_stamp  join_key join_key categorical categorical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical unused_float unit time stamp, comparison only 0 1979-01-01 aasedo01 CAL ALCS AL 1 0 2 0 0 0 0 15 4 1 0 2 6 0 1 1 0 0 0 20 2 1 0 1 0 1979 1 1975-01-01 abbotgl01 OAK ALCS AL 0 0 1 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 3 1 0 0 0 0 1975 2 2000-01-01 abbotpa01 SEA ALCS AL 0 1 1 1 0 0 0 15 3 3 1 3 3 0 5 0 0 0 0 21 0 3 0 0 0 2000 3 2000-01-01 abbotpa01 SEA ALDS2 AL 1 0 1 1 0 0 0 17 5 1 0 3 1 0 1 0 0 1 0 25 0 2 0 1 1 2000 4 2001-01-01 abbotpa01 SEA ALCS AL 0 0 1 1 0 0 0 15 0 0 0 8 2 0 0 0 0 0 0 21 0 0 1 0 0 2001 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 4192 2006-01-01 zitoba01 OAK ALDS2 AL 1 0 1 1 0 0 0 24 4 1 1 3 1 0 1 0 0 0 0 30 0 1 1 0 0 2006 4193 2012-01-01 zitoba01 SFN NLCS NL 1 0 1 1 0 0 0 23 6 0 0 1 6 0 0 1 0 0 nan 29 0 0 nan nan 1 2012 4194 2012-01-01 zitoba01 SFN NLDS2 NL 0 0 1 1 0 0 0 7 4 2 1 4 4 0 6 0 0 0 nan 16 0 2 nan nan 0 2012 4195 2012-01-01 zitoba01 SFN WS NL 1 0 1 1 0 0 0 17 6 1 0 1 3 0 1 0 0 0 nan 23 0 1 nan nan 1 2012 4196 1946-01-01 zuberbi01 BOS WS AL 0 0 1 0 0 0 0 6 3 1 0 1 1 0 4 1 0 0 0 9 0 1 0 0 1 1946 <p>     4197 rows x 31 columns     memory usage: 0.97 MB     name: pitchingpost     type: getml.DataFrame </p> In\u00a0[24]: Copied! <pre>salaries[\"year\"] = salaries[\"yearID\"].as_str().as_ts([\"%Y\"])\nsalaries[\"teamIDCat\"] = salaries[\"teamID\"]\n\nsalaries.set_role([\"playerID\", \"teamID\"], getml.data.roles.join_key)\nsalaries.set_role([\"lgID\", \"teamIDCat\"], getml.data.roles.categorical)\nsalaries.set_role(\"yearID\", getml.data.roles.numerical)\nsalaries.set_role(\"salary\", getml.data.roles.target)\nsalaries.set_role(\"year\", getml.data.roles.time_stamp)\n\nsalaries\n</pre> salaries[\"year\"] = salaries[\"yearID\"].as_str().as_ts([\"%Y\"]) salaries[\"teamIDCat\"] = salaries[\"teamID\"]  salaries.set_role([\"playerID\", \"teamID\"], getml.data.roles.join_key) salaries.set_role([\"lgID\", \"teamIDCat\"], getml.data.roles.categorical) salaries.set_role(\"yearID\", getml.data.roles.numerical) salaries.set_role(\"salary\", getml.data.roles.target) salaries.set_role(\"year\", getml.data.roles.time_stamp)  salaries Out[24]:  name                        year  playerID   teamID   salary lgID        teamIDCat      yearID  role                  time_stamp  join_key join_key   target categorical categorical numerical  unit time stamp, comparison only 0 1985-01-01 barkele01 ATL 870000 NL ATL 1985 1 1985-01-01 bedrost01 ATL 550000 NL ATL 1985 2 1985-01-01 benedbr01 ATL 545000 NL ATL 1985 3 1985-01-01 campri01 ATL 633333 NL ATL 1985 4 1985-01-01 ceronri01 ATL 625000 NL ATL 1985 ... ... ... ... ... ... ... 23106 2012-01-01 tracych01 WAS 750000 NL WAS 2012 23107 2012-01-01 wangch01 WAS 4000000 NL WAS 2012 23108 2012-01-01 werthja01 WAS 13571428 NL WAS 2012 23109 2012-01-01 zimmejo02 WAS 2300000 NL WAS 2012 23110 2012-01-01 zimmery01 WAS 12000000 NL WAS 2012 <p>     23111 rows x 7 columns     memory usage: 0.92 MB     name: salaries     type: getml.DataFrame </p> In\u00a0[25]: Copied! <pre>split = getml.data.split.random(train=0.8, test=0.2)\n</pre> split = getml.data.split.random(train=0.8, test=0.2) In\u00a0[26]: Copied! <pre>star_schema = getml.data.StarSchema(population=salaries, split=split)\n\nstar_schema.join(\n    allstarfull,\n    on=\"playerID\",\n    time_stamps=\"year\",\n    horizon=getml.data.time.days(1),\n)\n\nstar_schema.join(\n    awardsplayers,\n    on=\"playerID\",\n    time_stamps=\"year\",\n    horizon=getml.data.time.days(1),\n)\n\nstar_schema.join(\n    awardsshareplayers,\n    on=\"playerID\",\n    time_stamps=\"year\",\n    horizon=getml.data.time.days(1),\n)\n\nstar_schema.join(\n    batting,\n    on=\"playerID\",\n    time_stamps=\"year\",\n    horizon=getml.data.time.days(1),\n)\n\nstar_schema.join(\n    battingpost,\n    on=\"playerID\",\n    time_stamps=\"year\",\n    horizon=getml.data.time.days(1),\n)\n\nstar_schema.join(\n    fielding,\n    on=\"playerID\",\n    time_stamps=\"year\",\n    horizon=getml.data.time.days(1),\n)\n\nstar_schema.join(\n    fieldingpost,\n    on=\"playerID\",\n    time_stamps=\"year\",\n    horizon=getml.data.time.days(1),\n)\n\nstar_schema.join(\n    pitching,\n    on=\"playerID\",\n    time_stamps=\"year\",\n    horizon=getml.data.time.days(1),\n)\n\nstar_schema.join(\n    pitchingpost,\n    on=\"playerID\",\n    time_stamps=\"year\",\n    horizon=getml.data.time.days(1),\n)\n\nstar_schema\n</pre> star_schema = getml.data.StarSchema(population=salaries, split=split)  star_schema.join(     allstarfull,     on=\"playerID\",     time_stamps=\"year\",     horizon=getml.data.time.days(1), )  star_schema.join(     awardsplayers,     on=\"playerID\",     time_stamps=\"year\",     horizon=getml.data.time.days(1), )  star_schema.join(     awardsshareplayers,     on=\"playerID\",     time_stamps=\"year\",     horizon=getml.data.time.days(1), )  star_schema.join(     batting,     on=\"playerID\",     time_stamps=\"year\",     horizon=getml.data.time.days(1), )  star_schema.join(     battingpost,     on=\"playerID\",     time_stamps=\"year\",     horizon=getml.data.time.days(1), )  star_schema.join(     fielding,     on=\"playerID\",     time_stamps=\"year\",     horizon=getml.data.time.days(1), )  star_schema.join(     fieldingpost,     on=\"playerID\",     time_stamps=\"year\",     horizon=getml.data.time.days(1), )  star_schema.join(     pitching,     on=\"playerID\",     time_stamps=\"year\",     horizon=getml.data.time.days(1), )  star_schema.join(     pitchingpost,     on=\"playerID\",     time_stamps=\"year\",     horizon=getml.data.time.days(1), )  star_schema Out[26]: data model diagram allstarfullawardsplayersawardsshareplayersbattingbattingpostfieldingfieldingpostpitchingpitchingpostsalariesplayerID = playerIDyear &lt;= yearHorizon: 1.0 daysplayerID = playerIDyear &lt;= yearHorizon: 1.0 daysplayerID = playerIDyear &lt;= yearHorizon: 1.0 daysplayerID = playerIDyear &lt;= yearHorizon: 1.0 daysplayerID = playerIDyear &lt;= yearHorizon: 1.0 daysplayerID = playerIDyear &lt;= yearHorizon: 1.0 daysplayerID = playerIDyear &lt;= yearHorizon: 1.0 daysplayerID = playerIDyear &lt;= yearHorizon: 1.0 daysplayerID = playerIDyear &lt;= yearHorizon: 1.0 days staging data frames        staging table                    0 salaries SALARIES__STAGING_TABLE_1 1 allstarfull ALLSTARFULL__STAGING_TABLE_2 2 awardsplayers AWARDSPLAYERS__STAGING_TABLE_3 3 awardsshareplayers AWARDSSHAREPLAYERS__STAGING_TABLE_4 4 batting BATTING__STAGING_TABLE_5 5 battingpost BATTINGPOST__STAGING_TABLE_6 6 fielding FIELDING__STAGING_TABLE_7 7 fieldingpost FIELDINGPOST__STAGING_TABLE_8 8 pitching PITCHING__STAGING_TABLE_9 9 pitchingpost PITCHINGPOST__STAGING_TABLE_10 container population subset name      rows type 0 test salaries 4539 View 1 train salaries 18572 View peripheral name                 rows type      0 allstarfull 4831 DataFrame 1 awardsplayers 5795 DataFrame 2 awardsshareplayers 6289 DataFrame 3 batting 92353 DataFrame 4 battingpost 9798 DataFrame 5 fielding 137975 DataFrame 6 fieldingpost 10346 DataFrame 7 pitching 39361 DataFrame 8 pitchingpost 4197 DataFrame <p>Set-up the feature learner &amp; predictor</p> In\u00a0[27]: Copied! <pre>mapping = getml.preprocessors.Mapping()\n\nfast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    num_threads=1,\n    num_features=700,\n)\n\nrelboost = getml.feature_learning.Relboost(\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    num_threads=1,\n    max_depth=8,\n)\n\npredictor = getml.predictors.XGBoostRegressor(n_jobs=1)\n</pre> mapping = getml.preprocessors.Mapping()  fast_prop = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.SquareLoss,     num_threads=1,     num_features=700, )  relboost = getml.feature_learning.Relboost(     loss_function=getml.feature_learning.loss_functions.SquareLoss,     num_threads=1,     max_depth=8, )  predictor = getml.predictors.XGBoostRegressor(n_jobs=1) <p>Build the pipeline</p> In\u00a0[28]: Copied! <pre>pipe1 = getml.pipeline.Pipeline(\n    tags=['fast_prop'],\n    data_model=star_schema.data_model,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    predictors=[predictor],\n    include_categorical=True,\n)\n\npipe1\n</pre> pipe1 = getml.pipeline.Pipeline(     tags=['fast_prop'],     data_model=star_schema.data_model,     preprocessors=[mapping],     feature_learners=[fast_prop],     predictors=[predictor],     include_categorical=True, )  pipe1 Out[28]: <pre>Pipeline(data_model='salaries',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=['allstarfull', 'awardsplayers', 'awardsshareplayers', 'batting',\n                     'battingpost', 'fielding', 'fieldingpost', 'pitching', 'pitchingpost'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['fast_prop'])</pre> In\u00a0[29]: Copied! <pre>pipe2 = getml.pipeline.Pipeline(\n    tags=['relboost'],\n    data_model=star_schema.data_model,\n    preprocessors=[mapping],\n    feature_learners=[relboost],\n    predictors=[predictor],    \n    include_categorical=True,\n)\n\npipe2\n</pre> pipe2 = getml.pipeline.Pipeline(     tags=['relboost'],     data_model=star_schema.data_model,     preprocessors=[mapping],     feature_learners=[relboost],     predictors=[predictor],         include_categorical=True, )  pipe2 Out[29]: <pre>Pipeline(data_model='salaries',\n         feature_learners=['Relboost'],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=['allstarfull', 'awardsplayers', 'awardsshareplayers', 'batting',\n                     'battingpost', 'fielding', 'fieldingpost', 'pitching', 'pitchingpost'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['relboost'])</pre> In\u00a0[30]: Copied! <pre>pipe1.check(star_schema.train)\n</pre> pipe1.check(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 9 issues labeled INFO and 0 issues labeled WARNING.\n</pre> Out[30]: type label                  message                          0 INFO FOREIGN KEYS NOT FOUND When joining SALARIES__STAGING_TABLE_1 and ALLSTARFULL__STAGING_TABLE_2 over 'playerID' and 'playerID', there are no corresponding entries for 64.710317% of entries in 'playerID' in 'SALARIES__STAGING_TABLE_1'. You might want to double-check your join keys. 1 INFO FOREIGN KEYS NOT FOUND When joining SALARIES__STAGING_TABLE_1 and AWARDSPLAYERS__STAGING_TABLE_3 over 'playerID' and 'playerID', there are no corresponding entries for 75.376911% of entries in 'playerID' in 'SALARIES__STAGING_TABLE_1'. You might want to double-check your join keys. 2 INFO FOREIGN KEYS NOT FOUND When joining SALARIES__STAGING_TABLE_1 and AWARDSSHAREPLAYERS__STAGING_TABLE_4 over 'playerID' and 'playerID', there are no corresponding entries for 62.459617% of entries in 'playerID' in 'SALARIES__STAGING_TABLE_1'. You might want to double-check your join keys. 3 INFO FOREIGN KEYS NOT FOUND When joining SALARIES__STAGING_TABLE_1 and BATTING__STAGING_TABLE_5 over 'playerID' and 'playerID', there are no corresponding entries for 8.765884% of entries in 'playerID' in 'SALARIES__STAGING_TABLE_1'. You might want to double-check your join keys. 4 INFO FOREIGN KEYS NOT FOUND When joining SALARIES__STAGING_TABLE_1 and BATTINGPOST__STAGING_TABLE_6 over 'playerID' and 'playerID', there are no corresponding entries for 41.018738% of entries in 'playerID' in 'SALARIES__STAGING_TABLE_1'. You might want to double-check your join keys. 5 INFO FOREIGN KEYS NOT FOUND When joining SALARIES__STAGING_TABLE_1 and FIELDING__STAGING_TABLE_7 over 'playerID' and 'playerID', there are no corresponding entries for 19.270946% of entries in 'playerID' in 'SALARIES__STAGING_TABLE_1'. You might want to double-check your join keys. 6 INFO FOREIGN KEYS NOT FOUND When joining SALARIES__STAGING_TABLE_1 and FIELDINGPOST__STAGING_TABLE_8 over 'playerID' and 'playerID', there are no corresponding entries for 38.369589% of entries in 'playerID' in 'SALARIES__STAGING_TABLE_1'. You might want to double-check your join keys. 7 INFO FOREIGN KEYS NOT FOUND When joining SALARIES__STAGING_TABLE_1 and PITCHING__STAGING_TABLE_9 over 'playerID' and 'playerID', there are no corresponding entries for 54.862158% of entries in 'playerID' in 'SALARIES__STAGING_TABLE_1'. You might want to double-check your join keys. 8 INFO FOREIGN KEYS NOT FOUND When joining SALARIES__STAGING_TABLE_1 and PITCHINGPOST__STAGING_TABLE_10 over 'playerID' and 'playerID', there are no corresponding entries for 73.589274% of entries in 'playerID' in 'SALARIES__STAGING_TABLE_1'. You might want to double-check your join keys. In\u00a0[31]: Copied! <pre>pipe1.fit(star_schema.train)\n</pre> pipe1.fit(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\nThe pipeline check generated 9 issues labeled INFO and 0 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 3080 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:14, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:25, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:47.314969\n\n</pre> Out[31]: <pre>Pipeline(data_model='salaries',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=['allstarfull', 'awardsplayers', 'awardsshareplayers', 'batting',\n                     'battingpost', 'fielding', 'fieldingpost', 'pitching', 'pitchingpost'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['fast_prop', 'container-Y43LHa'])</pre> In\u00a0[32]: Copied! <pre>pipe2.check(star_schema.train)\n</pre> pipe2.check(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 9 issues labeled INFO and 0 issues labeled WARNING.\n</pre> Out[32]: type label                  message                          0 INFO FOREIGN KEYS NOT FOUND When joining SALARIES__STAGING_TABLE_1 and ALLSTARFULL__STAGING_TABLE_2 over 'playerID' and 'playerID', there are no corresponding entries for 64.710317% of entries in 'playerID' in 'SALARIES__STAGING_TABLE_1'. You might want to double-check your join keys. 1 INFO FOREIGN KEYS NOT FOUND When joining SALARIES__STAGING_TABLE_1 and AWARDSPLAYERS__STAGING_TABLE_3 over 'playerID' and 'playerID', there are no corresponding entries for 75.376911% of entries in 'playerID' in 'SALARIES__STAGING_TABLE_1'. You might want to double-check your join keys. 2 INFO FOREIGN KEYS NOT FOUND When joining SALARIES__STAGING_TABLE_1 and AWARDSSHAREPLAYERS__STAGING_TABLE_4 over 'playerID' and 'playerID', there are no corresponding entries for 62.459617% of entries in 'playerID' in 'SALARIES__STAGING_TABLE_1'. You might want to double-check your join keys. 3 INFO FOREIGN KEYS NOT FOUND When joining SALARIES__STAGING_TABLE_1 and BATTING__STAGING_TABLE_5 over 'playerID' and 'playerID', there are no corresponding entries for 8.765884% of entries in 'playerID' in 'SALARIES__STAGING_TABLE_1'. You might want to double-check your join keys. 4 INFO FOREIGN KEYS NOT FOUND When joining SALARIES__STAGING_TABLE_1 and BATTINGPOST__STAGING_TABLE_6 over 'playerID' and 'playerID', there are no corresponding entries for 41.018738% of entries in 'playerID' in 'SALARIES__STAGING_TABLE_1'. You might want to double-check your join keys. 5 INFO FOREIGN KEYS NOT FOUND When joining SALARIES__STAGING_TABLE_1 and FIELDING__STAGING_TABLE_7 over 'playerID' and 'playerID', there are no corresponding entries for 19.270946% of entries in 'playerID' in 'SALARIES__STAGING_TABLE_1'. You might want to double-check your join keys. 6 INFO FOREIGN KEYS NOT FOUND When joining SALARIES__STAGING_TABLE_1 and FIELDINGPOST__STAGING_TABLE_8 over 'playerID' and 'playerID', there are no corresponding entries for 38.369589% of entries in 'playerID' in 'SALARIES__STAGING_TABLE_1'. You might want to double-check your join keys. 7 INFO FOREIGN KEYS NOT FOUND When joining SALARIES__STAGING_TABLE_1 and PITCHING__STAGING_TABLE_9 over 'playerID' and 'playerID', there are no corresponding entries for 54.862158% of entries in 'playerID' in 'SALARIES__STAGING_TABLE_1'. You might want to double-check your join keys. 8 INFO FOREIGN KEYS NOT FOUND When joining SALARIES__STAGING_TABLE_1 and PITCHINGPOST__STAGING_TABLE_10 over 'playerID' and 'playerID', there are no corresponding entries for 73.589274% of entries in 'playerID' in 'SALARIES__STAGING_TABLE_1'. You might want to double-check your join keys. In\u00a0[33]: Copied! <pre>pipe2.fit(star_schema.train)\n</pre> pipe2.fit(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 9 issues labeled INFO and 0 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:48, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:04, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:04, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:56.172085\n\n</pre> Out[33]: <pre>Pipeline(data_model='salaries',\n         feature_learners=['Relboost'],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=['allstarfull', 'awardsplayers', 'awardsshareplayers', 'batting',\n                     'battingpost', 'fielding', 'fieldingpost', 'pitching', 'pitchingpost'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['relboost', 'container-Y43LHa'])</pre> In\u00a0[34]: Copied! <pre>fastprop_score = pipe1.score(star_schema.test)\nfastprop_score\n</pre> fastprop_score = pipe1.score(star_schema.test) fastprop_score <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[34]: date time           set used target          mae          rmse rsquared 0 2024-02-21 14:55:51 train salary 693195.922 1251354.7312 0.8222 1 2024-02-21 14:56:51 test salary 765292.554 1402960.9303 0.788 In\u00a0[35]: Copied! <pre>relboost_score = pipe2.score(star_schema.test)\nrelboost_score\n</pre> relboost_score = pipe2.score(star_schema.test) relboost_score <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\n</pre> Out[35]: date time           set used target          mae          rmse rsquared 0 2024-02-21 14:56:47 train salary 461650.473 798183.2356 0.9276 1 2024-02-21 14:56:53 test salary 668857.0542 1229058.3564 0.8371 In\u00a0[36]: Copied! <pre>population_train_pd = star_schema.train.population.to_pandas()\npopulation_test_pd = star_schema.test.population.to_pandas()\n</pre> population_train_pd = star_schema.train.population.to_pandas() population_test_pd = star_schema.test.population.to_pandas() In\u00a0[37]: Copied! <pre>allstarfull_pd = allstarfull.drop(allstarfull.roles.unused).to_pandas()\nawardsplayers_pd = awardsplayers.drop(awardsplayers.roles.unused).to_pandas()\nawardsshareplayers_pd = awardsshareplayers.drop(awardsshareplayers.roles.unused).to_pandas()\nbatting_pd = batting.drop(batting.roles.unused).to_pandas()\nbattingpost_pd = battingpost.drop(battingpost.roles.unused).to_pandas()\nfielding_pd = fielding.drop(fielding.roles.unused).to_pandas()\nfieldingpost_pd = fieldingpost.drop(fieldingpost.roles.unused).to_pandas()\npitching_pd = pitching.drop(pitching.roles.unused).to_pandas()\npitchingpost_pd = pitchingpost.drop(pitchingpost.roles.unused).to_pandas()\n</pre> allstarfull_pd = allstarfull.drop(allstarfull.roles.unused).to_pandas() awardsplayers_pd = awardsplayers.drop(awardsplayers.roles.unused).to_pandas() awardsshareplayers_pd = awardsshareplayers.drop(awardsshareplayers.roles.unused).to_pandas() batting_pd = batting.drop(batting.roles.unused).to_pandas() battingpost_pd = battingpost.drop(battingpost.roles.unused).to_pandas() fielding_pd = fielding.drop(fielding.roles.unused).to_pandas() fieldingpost_pd = fieldingpost.drop(fieldingpost.roles.unused).to_pandas() pitching_pd = pitching.drop(pitching.roles.unused).to_pandas() pitchingpost_pd = pitchingpost.drop(pitchingpost.roles.unused).to_pandas() <p>featuretools requires that we manually define a primary key and then join on that primary key. Therefore, we need some manual data preparation.</p> In\u00a0[38]: Copied! <pre>population_train_pd[\"id\"] = population_train_pd.index\npopulation_train_pd\n</pre> population_train_pd[\"id\"] = population_train_pd.index population_train_pd Out[38]: lgID teamIDCat playerID teamID yearID salary year id 0 NL ATL barkele01 ATL 1985.0 870000.0 1985-01-01 0 1 NL ATL bedrost01 ATL 1985.0 550000.0 1985-01-01 1 2 NL ATL benedbr01 ATL 1985.0 545000.0 1985-01-01 2 3 NL ATL ceronri01 ATL 1985.0 625000.0 1985-01-01 3 4 NL ATL chambch01 ATL 1985.0 800000.0 1985-01-01 4 ... ... ... ... ... ... ... ... ... 18567 NL WAS strasst01 WAS 2012.0 4875000.0 2012-01-01 18567 18568 NL WAS tracych01 WAS 2012.0 750000.0 2012-01-01 18568 18569 NL WAS wangch01 WAS 2012.0 4000000.0 2012-01-01 18569 18570 NL WAS werthja01 WAS 2012.0 13571428.0 2012-01-01 18570 18571 NL WAS zimmery01 WAS 2012.0 12000000.0 2012-01-01 18571 <p>18572 rows \u00d7 8 columns</p> In\u00a0[39]: Copied! <pre>population_test_pd[\"id\"] = population_test_pd.index\npopulation_test_pd\n</pre> population_test_pd[\"id\"] = population_test_pd.index population_test_pd Out[39]: lgID teamIDCat playerID teamID yearID salary year id 0 NL ATL campri01 ATL 1985.0 633333.0 1985-01-01 0 1 NL ATL dedmoje01 ATL 1985.0 150000.0 1985-01-01 1 2 NL ATL hornebo01 ATL 1985.0 1500000.0 1985-01-01 2 3 AL BAL dempsri01 BAL 1985.0 512500.0 1985-01-01 3 4 AL BAL martide01 BAL 1985.0 560000.0 1985-01-01 4 ... ... ... ... ... ... ... ... ... 4534 NL WAS desmoia01 WAS 2012.0 512500.0 2012-01-01 4534 4535 NL WAS espinda01 WAS 2012.0 506000.0 2012-01-01 4535 4536 NL WAS gorzeto01 WAS 2012.0 3000000.0 2012-01-01 4536 4537 NL WAS matthry01 WAS 2012.0 481000.0 2012-01-01 4537 4538 NL WAS zimmejo02 WAS 2012.0 2300000.0 2012-01-01 4538 <p>4539 rows \u00d7 8 columns</p> In\u00a0[40]: Copied! <pre>def prepare_peripheral(peripheral_pd, train_or_test):\n    \"\"\"\n    Helper function that imitates the behavior of \n    the data model defined above.\n    \"\"\"\n    peripheral_new = peripheral_pd.merge(\n        train_or_test[[\"id\", \"playerID\", \"year\"]],\n        on=\"playerID\"\n    )\n\n    peripheral_new = peripheral_new[\n        peripheral_new[\"year_x\"] &lt; peripheral_new[\"year_y\"]\n    ]\n\n    del peripheral_new[\"year_x\"]\n    del peripheral_new[\"year_y\"]\n    del peripheral_new[\"playerID\"]\n\n    return peripheral_new\n</pre> def prepare_peripheral(peripheral_pd, train_or_test):     \"\"\"     Helper function that imitates the behavior of      the data model defined above.     \"\"\"     peripheral_new = peripheral_pd.merge(         train_or_test[[\"id\", \"playerID\", \"year\"]],         on=\"playerID\"     )      peripheral_new = peripheral_new[         peripheral_new[\"year_x\"] &lt; peripheral_new[\"year_y\"]     ]      del peripheral_new[\"year_x\"]     del peripheral_new[\"year_y\"]     del peripheral_new[\"playerID\"]      return peripheral_new In\u00a0[41]: Copied! <pre>allstarfull_train_pd = prepare_peripheral(allstarfull_pd, population_train_pd)\nallstarfull_test_pd = prepare_peripheral(allstarfull_pd, population_test_pd)\nallstarfull_train_pd\n</pre> allstarfull_train_pd = prepare_peripheral(allstarfull_pd, population_train_pd) allstarfull_test_pd = prepare_peripheral(allstarfull_pd, population_test_pd) allstarfull_train_pd Out[41]: gameID teamID lgID gameNum GP startingPos id 1 NLS198607150 BAL AL 0.0 1.0 NaN 1051 2 NLS198607150 BAL AL 0.0 1.0 NaN 1569 3 NLS198607150 BAL AL 0.0 1.0 NaN 2426 11 NLS200407130 PHI NL 0.0 1.0 NaN 13684 12 NLS200407130 PHI NL 0.0 1.0 NaN 14357 ... ... ... ... ... ... ... ... 20490 NLS200607110 OAK AL 0.0 1.0 NaN 17789 20491 NLS200607110 OAK AL 0.0 1.0 NaN 18456 20494 NLS200907140 TBA AL 0.0 1.0 NaN 17158 20495 NLS200907140 TBA AL 0.0 1.0 NaN 17836 20496 NLS200907140 TBA AL 0.0 1.0 NaN 18502 <p>11557 rows \u00d7 7 columns</p> In\u00a0[42]: Copied! <pre>awardsplayers_train_pd = prepare_peripheral(awardsplayers_pd, population_train_pd)\nawardsplayers_test_pd = prepare_peripheral(awardsplayers_pd, population_test_pd)\nawardsplayers_train_pd\n</pre> awardsplayers_train_pd = prepare_peripheral(awardsplayers_pd, population_train_pd) awardsplayers_test_pd = prepare_peripheral(awardsplayers_pd, population_test_pd) awardsplayers_train_pd Out[42]: awardID lgID notes id 0 TSN Player of the Year AL None 4683 1 TSN Player of the Year AL None 5856 2 TSN Player of the Year AL None 8922 3 TSN Player of the Year AL None 9727 4 Rookie of the Year NL None 120 ... ... ... ... ... 26203 Hank Aaron Award AL None 18529 26209 Silver Slugger AL OF 18529 26213 NLCS MVP NL None 17779 26228 Roberto Clemente Award ML None 17310 26245 ALCS MVP AL None 18506 <p>15639 rows \u00d7 4 columns</p> In\u00a0[43]: Copied! <pre>awardsshareplayers_train_pd = prepare_peripheral(awardsshareplayers_pd, population_train_pd)\nawardsshareplayers_test_pd = prepare_peripheral(awardsshareplayers_pd, population_test_pd)\nawardsshareplayers_train_pd\n</pre> awardsshareplayers_train_pd = prepare_peripheral(awardsshareplayers_pd, population_train_pd) awardsshareplayers_test_pd = prepare_peripheral(awardsshareplayers_pd, population_test_pd) awardsshareplayers_train_pd Out[43]: awardID lgID pointsWon pointsMax votesFirst id 0 Cy Young NL 1.0 24.0 1.0 254 1 Cy Young NL 1.0 24.0 1.0 614 2 Cy Young NL 15.0 120.0 1.0 254 3 Cy Young NL 15.0 120.0 1.0 614 4 Cy Young NL 10.0 120.0 0.0 254 ... ... ... ... ... ... ... 24216 Rookie of the Year AL 3.0 140.0 0.0 16986 24217 Rookie of the Year AL 3.0 140.0 0.0 18315 24218 Rookie of the Year NL 9.0 160.0 0.0 16703 24219 Rookie of the Year NL 9.0 160.0 0.0 17370 24220 Rookie of the Year NL 9.0 160.0 0.0 18404 <p>15527 rows \u00d7 6 columns</p> In\u00a0[44]: Copied! <pre>batting_train_pd = prepare_peripheral(batting_pd, population_train_pd)\nbatting_test_pd = prepare_peripheral(batting_pd, population_test_pd)\nbatting_train_pd\n</pre> batting_train_pd = prepare_peripheral(batting_pd, population_train_pd) batting_test_pd = prepare_peripheral(batting_pd, population_test_pd) batting_train_pd Out[44]: lgID teamID stint G G_batting AB R H 2B 3B ... CS BB SO IBB HBP SH SF GIDP G_old id 1 NL SFN 1.0 11.0 11.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 11.0 14656 2 NL SFN 1.0 11.0 11.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 11.0 16389 3 NL SFN 1.0 11.0 11.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 11.0 17069 4 NL SFN 1.0 11.0 11.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 11.0 17743 5 NL SFN 1.0 11.0 11.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 11.0 18263 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 208118 NL ATL 1.0 11.0 11.0 25.0 2.0 5.0 1.0 0.0 ... 0.0 2.0 3.0 0.0 0.0 0.0 0.0 0.0 11.0 2109 208119 NL ATL 1.0 81.0 81.0 190.0 16.0 48.0 8.0 1.0 ... 0.0 16.0 14.0 1.0 0.0 4.0 0.0 3.0 81.0 2109 208120 AL NYA 1.0 21.0 21.0 48.0 2.0 4.0 1.0 0.0 ... 0.0 5.0 4.0 0.0 0.0 4.0 0.0 1.0 21.0 2109 208121 AL NYA 1.0 14.0 14.0 34.0 2.0 6.0 0.0 0.0 ... 0.0 0.0 4.0 0.0 0.0 2.0 0.0 1.0 14.0 2109 208122 AL CLE 1.0 51.0 51.0 130.0 9.0 30.0 5.0 1.0 ... 0.0 8.0 13.0 0.0 0.0 8.0 0.0 3.0 51.0 2109 <p>105808 rows \u00d7 23 columns</p> In\u00a0[45]: Copied! <pre>battingpost_train_pd = prepare_peripheral(battingpost_pd, population_train_pd)\nbattingpost_test_pd = prepare_peripheral(battingpost_pd, population_test_pd)\nbattingpost_train_pd\n</pre> battingpost_train_pd = prepare_peripheral(battingpost_pd, population_train_pd) battingpost_test_pd = prepare_peripheral(battingpost_pd, population_test_pd) battingpost_train_pd Out[45]: round lgID teamID G AB R H 2B 3B HR ... SB CS BB SO IBB HBP SH SF GIDP id 0 WS NL SLN 1.0 1.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 301 1 WS NL SLN 1.0 1.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 533 2 WS NL SLN 2.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 301 3 WS NL SLN 2.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 533 4 NLCS NL PHI 1.0 2.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 301 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 49525 NLDS2 NL ARI 3.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 NaN NaN NaN NaN NaN 17924 49530 NLDS2 NL ARI 1.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 1.0 0.0 NaN NaN NaN NaN NaN 17926 49544 WS AL TEX 3.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 NaN NaN NaN NaN NaN 18503 49549 WS AL TEX 3.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 NaN NaN NaN NaN NaN 18508 49566 WS AL TEX 2.0 5.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 2.0 NaN NaN NaN NaN NaN 18516 <p>22893 rows \u00d7 21 columns</p> In\u00a0[46]: Copied! <pre>fielding_train_pd = prepare_peripheral(fielding_pd, population_train_pd)\nfielding_test_pd = prepare_peripheral(fielding_pd, population_test_pd)\nfielding_train_pd\n</pre> fielding_train_pd = prepare_peripheral(fielding_pd, population_train_pd) fielding_test_pd = prepare_peripheral(fielding_pd, population_test_pd) fielding_train_pd Out[46]: lgID POS teamID stint G GS InnOuts PO A E DP id 1 NL P SFN 1.0 11.0 0.0 32.0 0.0 0.0 0.0 0.0 14656 2 NL P SFN 1.0 11.0 0.0 32.0 0.0 0.0 0.0 0.0 16389 3 NL P SFN 1.0 11.0 0.0 32.0 0.0 0.0 0.0 0.0 17069 4 NL P SFN 1.0 11.0 0.0 32.0 0.0 0.0 0.0 0.0 17743 5 NL P SFN 1.0 11.0 0.0 32.0 0.0 0.0 0.0 0.0 18263 ... ... ... ... ... ... ... ... ... ... ... ... ... 361010 AL SS NYA 1.0 21.0 18.0 453.0 30.0 54.0 3.0 12.0 2109 361011 AL 2B NYA 1.0 7.0 6.0 168.0 17.0 18.0 0.0 5.0 2109 361012 AL 3B NYA 1.0 1.0 0.0 3.0 0.0 0.0 0.0 0.0 2109 361013 AL SS NYA 1.0 6.0 1.0 70.0 3.0 7.0 0.0 0.0 2109 361014 AL SS CLE 1.0 49.0 44.0 1139.0 77.0 112.0 8.0 21.0 2109 <p>182144 rows \u00d7 12 columns</p> In\u00a0[47]: Copied! <pre>fieldingpost_train_pd = prepare_peripheral(fieldingpost_pd, population_train_pd)\nfieldingpost_test_pd = prepare_peripheral(fieldingpost_pd, population_test_pd)\nfieldingpost_train_pd\n</pre> fieldingpost_train_pd = prepare_peripheral(fieldingpost_pd, population_train_pd) fieldingpost_test_pd = prepare_peripheral(fieldingpost_pd, population_test_pd) fieldingpost_train_pd Out[47]: lgID round POS teamID G GS InnOuts PO A E DP TP SB CS id 0 AL ALCS P CAL 2.0 0.0 15.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 454 1 AL ALCS P CAL 2.0 0.0 15.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1051 2 AL ALCS P CAL 2.0 0.0 15.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1569 3 AL ALCS P CAL 2.0 0.0 15.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 2426 11 NL NLCS 2B FLO 2.0 2.0 51.0 4.0 1.0 0.0 0.0 0.0 NaN NaN 8818 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 58087 AL ALCS P OAK 1.0 1.0 11.0 0.0 2.0 0.0 0.0 0.0 0.0 0.0 17789 58088 AL ALCS P OAK 1.0 1.0 11.0 0.0 2.0 0.0 0.0 0.0 0.0 0.0 18456 58095 AL ALDS2 P OAK 1.0 1.0 24.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 17114 58096 AL ALDS2 P OAK 1.0 1.0 24.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 17789 58097 AL ALDS2 P OAK 1.0 1.0 24.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 18456 <p>27491 rows \u00d7 15 columns</p> In\u00a0[48]: Copied! <pre>pitching_train_pd = prepare_peripheral(pitching_pd, population_train_pd)\npitching_test_pd = prepare_peripheral(pitching_pd, population_test_pd)\npitching_train_pd\n</pre> pitching_train_pd = prepare_peripheral(pitching_pd, population_train_pd) pitching_test_pd = prepare_peripheral(pitching_pd, population_test_pd) pitching_train_pd Out[48]: lgID teamID stint W L G GS CG SHO SV ... BAOpp ERA IBB WP HBP BK BFP GF R id 1 NL SFN 1.0 1.0 0.0 11.0 0.0 0.0 0.0 0.0 ... 0.0 6.0 0.0 0.0 2.0 0.0 61.0 5.0 8.0 14656 2 NL SFN 1.0 1.0 0.0 11.0 0.0 0.0 0.0 0.0 ... 0.0 6.0 0.0 0.0 2.0 0.0 61.0 5.0 8.0 16389 3 NL SFN 1.0 1.0 0.0 11.0 0.0 0.0 0.0 0.0 ... 0.0 6.0 0.0 0.0 2.0 0.0 61.0 5.0 8.0 17069 4 NL SFN 1.0 1.0 0.0 11.0 0.0 0.0 0.0 0.0 ... 0.0 6.0 0.0 0.0 2.0 0.0 61.0 5.0 8.0 17743 5 NL SFN 1.0 1.0 0.0 11.0 0.0 0.0 0.0 0.0 ... 0.0 6.0 0.0 0.0 2.0 0.0 61.0 5.0 8.0 18263 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 91802 NL SFN 1.0 10.0 13.0 33.0 33.0 1.0 0.0 0.0 ... NaN 4.0 8.0 2.0 8.0 2.0 818.0 0.0 89.0 17789 91803 NL SFN 1.0 10.0 13.0 33.0 33.0 1.0 0.0 0.0 ... NaN 4.0 8.0 2.0 8.0 2.0 818.0 0.0 89.0 18456 91811 NL SFN 1.0 9.0 14.0 34.0 33.0 1.0 0.0 0.0 ... NaN 4.0 7.0 7.0 7.0 0.0 848.0 1.0 97.0 17789 91812 NL SFN 1.0 9.0 14.0 34.0 33.0 1.0 0.0 0.0 ... NaN 4.0 7.0 7.0 7.0 0.0 848.0 1.0 97.0 18456 91821 NL SFN 1.0 3.0 4.0 13.0 9.0 0.0 0.0 0.0 ... NaN 5.0 1.0 1.0 0.0 0.0 225.0 3.0 35.0 18456 <p>45669 rows \u00d7 26 columns</p> In\u00a0[49]: Copied! <pre>pitchingpost_train_pd = prepare_peripheral(pitchingpost_pd, population_train_pd)\npitchingpost_test_pd = prepare_peripheral(pitchingpost_pd, population_test_pd)\npitchingpost_train_pd\n</pre> pitchingpost_train_pd = prepare_peripheral(pitchingpost_pd, population_train_pd) pitchingpost_test_pd = prepare_peripheral(pitchingpost_pd, population_test_pd) pitchingpost_train_pd Out[49]: round lgID teamID W L G GS CG SHO SV ... WP HBP BK BFP GF R SH SF GIDP id 0 ALCS AL CAL 1.0 0.0 2.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 20.0 2.0 1.0 0.0 1.0 0.0 454 1 ALCS AL CAL 1.0 0.0 2.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 20.0 2.0 1.0 0.0 1.0 0.0 1051 2 ALCS AL CAL 1.0 0.0 2.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 20.0 2.0 1.0 0.0 1.0 0.0 1569 3 ALCS AL CAL 1.0 0.0 2.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 20.0 2.0 1.0 0.0 1.0 0.0 2426 7 ALCS AL SEA 0.0 1.0 1.0 1.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 21.0 0.0 3.0 0.0 0.0 0.0 11801 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 24239 ALCS AL OAK 0.0 1.0 1.0 1.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 21.0 0.0 5.0 0.0 0.0 0.0 17789 24240 ALCS AL OAK 0.0 1.0 1.0 1.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 21.0 0.0 5.0 0.0 0.0 0.0 18456 24247 ALDS2 AL OAK 1.0 0.0 1.0 1.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 30.0 0.0 1.0 1.0 0.0 0.0 17114 24248 ALDS2 AL OAK 1.0 0.0 1.0 1.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 30.0 0.0 1.0 1.0 0.0 0.0 17789 24249 ALDS2 AL OAK 1.0 0.0 1.0 1.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 30.0 0.0 1.0 1.0 0.0 0.0 18456 <p>11364 rows \u00d7 29 columns</p> In\u00a0[50]: Copied! <pre>population_train_pd\n</pre> population_train_pd Out[50]: lgID teamIDCat playerID teamID yearID salary year id 0 NL ATL barkele01 ATL 1985.0 870000.0 1985-01-01 0 1 NL ATL bedrost01 ATL 1985.0 550000.0 1985-01-01 1 2 NL ATL benedbr01 ATL 1985.0 545000.0 1985-01-01 2 3 NL ATL ceronri01 ATL 1985.0 625000.0 1985-01-01 3 4 NL ATL chambch01 ATL 1985.0 800000.0 1985-01-01 4 ... ... ... ... ... ... ... ... ... 18567 NL WAS strasst01 WAS 2012.0 4875000.0 2012-01-01 18567 18568 NL WAS tracych01 WAS 2012.0 750000.0 2012-01-01 18568 18569 NL WAS wangch01 WAS 2012.0 4000000.0 2012-01-01 18569 18570 NL WAS werthja01 WAS 2012.0 13571428.0 2012-01-01 18570 18571 NL WAS zimmery01 WAS 2012.0 12000000.0 2012-01-01 18571 <p>18572 rows \u00d7 8 columns</p> In\u00a0[51]: Copied! <pre>def add_index(df):\n    df.insert(0, \"index\", range(len(df)))\n\npopulation_pd_logical_types = {\n    'lgID': ww.logical_types.Categorical,\n    'teamIDCat': ww.logical_types.Categorical,\n    'playerID': ww.logical_types.Categorical,\n    'teamID': ww.logical_types.Categorical,\n    'yearID': ww.logical_types.Integer,\n    'salary': ww.logical_types.IntegerNullable,\n    'year': ww.logical_types.Datetime,\n    'id': ww.logical_types.Integer\n}\npopulation_test_pd.ww.init(logical_types=population_pd_logical_types, index='id', name='population')\npopulation_train_pd.ww.init(logical_types=population_pd_logical_types, index='id', name='population')\n\nadd_index(allstarfull_test_pd)\nadd_index(allstarfull_train_pd)\nallstarfull_pd_logical_types = {\n    'index': ww.logical_types.Integer,\n    'gameID': ww.logical_types.Categorical,\n    'teamID': ww.logical_types.Categorical,\n    'lgID': ww.logical_types.Categorical,\n    'gameNum': ww.logical_types.Categorical,\n    'GP': ww.logical_types.Categorical,\n    'startingPos': ww.logical_types.Categorical,\n    'id': ww.logical_types.Integer\n}\nallstarfull_test_pd.ww.init(logical_types=allstarfull_pd_logical_types, index='index', name='allstarfull')\nallstarfull_train_pd.ww.init(logical_types=allstarfull_pd_logical_types, index='index', name='allstarfull')\n\nadd_index(awardsplayers_test_pd)\nadd_index(awardsplayers_train_pd)\nawardsplayers_pd_logical_types = {\n    'index': ww.logical_types.Integer,\n    'awardID': ww.logical_types.Categorical,\n    'lgID': ww.logical_types.Categorical,\n    'notes': ww.logical_types.Categorical,\n    'id': ww.logical_types.Integer\n}\nawardsplayers_test_pd.ww.init(logical_types=awardsplayers_pd_logical_types, index='index', name='awardsplayers')\nawardsplayers_train_pd.ww.init(logical_types=awardsplayers_pd_logical_types, index='index', name='awardsplayers')\n\nadd_index(awardsshareplayers_test_pd)\nadd_index(awardsshareplayers_train_pd)\nawardsshareplayers_pd_logical_types = {\n    'index': ww.logical_types.Integer,\n    'awardID': ww.logical_types.Categorical,\n    'lgID': ww.logical_types.Categorical,\n    'pointsWon': ww.logical_types.IntegerNullable,\n    'pointsMax': ww.logical_types.Categorical,\n    'votesFirst': ww.logical_types.IntegerNullable,\n    'id': ww.logical_types.Integer\n}\nawardsshareplayers_test_pd.ww.init(logical_types=awardsshareplayers_pd_logical_types, index='index', name='awardsshareplayers')\nawardsshareplayers_train_pd.ww.init(logical_types=awardsshareplayers_pd_logical_types, index='index', name='awardsshareplayers')\n\nadd_index(batting_test_pd)\nadd_index(batting_train_pd)\nbatting_pd_logical_types = {\n    'index': ww.logical_types.Integer,\n    'lgID': ww.logical_types.Categorical,\n    'teamID': ww.logical_types.Categorical,\n    'stint': ww.logical_types.Categorical,\n    'G': ww.logical_types.IntegerNullable,\n    'G_batting': ww.logical_types.IntegerNullable,\n    'AB': ww.logical_types.IntegerNullable,\n    'R': ww.logical_types.IntegerNullable,\n    'H': ww.logical_types.IntegerNullable,\n    '2B': ww.logical_types.IntegerNullable,\n    '3B': ww.logical_types.IntegerNullable,\n    'HR': ww.logical_types.IntegerNullable,\n    'RBI': ww.logical_types.IntegerNullable,\n    'SB': ww.logical_types.IntegerNullable,\n    'CS': ww.logical_types.IntegerNullable,\n    'BB': ww.logical_types.IntegerNullable,\n    'SO': ww.logical_types.IntegerNullable,\n    'IBB': ww.logical_types.IntegerNullable,\n    'HBP': ww.logical_types.IntegerNullable,\n    'SH': ww.logical_types.IntegerNullable,\n    'SF': ww.logical_types.IntegerNullable,\n    'GIDP': ww.logical_types.IntegerNullable,\n    'G_old': ww.logical_types.IntegerNullable,\n    'id': ww.logical_types.Integer\n}\nbatting_test_pd.ww.init(logical_types=batting_pd_logical_types, index='index', name='batting')\nbatting_train_pd.ww.init(logical_types=batting_pd_logical_types, index='index', name='batting')\n\nadd_index(battingpost_test_pd)\nadd_index(battingpost_train_pd)\nbattingpost_pd_logical_types = {\n    'index': ww.logical_types.Integer,\n    'round': ww.logical_types.Categorical,\n    'lgID': ww.logical_types.Categorical,\n    'teamID': ww.logical_types.Categorical,\n    'G': ww.logical_types.IntegerNullable,\n    'AB': ww.logical_types.IntegerNullable,\n    'R': ww.logical_types.IntegerNullable,\n    'H': ww.logical_types.IntegerNullable,\n    '2B': ww.logical_types.IntegerNullable,\n    '3B': ww.logical_types.IntegerNullable,\n    'HR': ww.logical_types.IntegerNullable,\n    'RBI': ww.logical_types.IntegerNullable,\n    'SB': ww.logical_types.IntegerNullable,\n    'CS': ww.logical_types.IntegerNullable,\n    'BB': ww.logical_types.IntegerNullable,\n    'SO': ww.logical_types.IntegerNullable,\n    'IBB': ww.logical_types.IntegerNullable,\n    'HBP': ww.logical_types.IntegerNullable,\n    'SH': ww.logical_types.IntegerNullable,\n    'SF': ww.logical_types.IntegerNullable,\n    'GIDP': ww.logical_types.IntegerNullable,\n    'id': ww.logical_types.Integer\n}\nbattingpost_test_pd.ww.init(logical_types=battingpost_pd_logical_types, index='index', name='battingpost')\nbattingpost_train_pd.ww.init(logical_types=battingpost_pd_logical_types, index='index', name='battingpost')\n\nadd_index(fielding_test_pd)\nadd_index(fielding_train_pd)\nfielding_pd_logical_types = {\n    'index': ww.logical_types.Integer,\n    'lgID': ww.logical_types.Categorical,\n    'POS': ww.logical_types.Categorical,\n    'teamID': ww.logical_types.Categorical,\n    'stint': ww.logical_types.Categorical,\n    'G': ww.logical_types.IntegerNullable,\n    'GS': ww.logical_types.IntegerNullable,\n    'InnOuts': ww.logical_types.IntegerNullable,\n    'PO': ww.logical_types.IntegerNullable,\n    'A': ww.logical_types.IntegerNullable,\n    'E': ww.logical_types.IntegerNullable,\n    'DP': ww.logical_types.IntegerNullable,\n    'id': ww.logical_types.Integer\n}\nfielding_test_pd.ww.init(logical_types=fielding_pd_logical_types, index='index', name='fielding')\nfielding_train_pd.ww.init(logical_types=fielding_pd_logical_types, index='index', name='fielding')\n\nadd_index(fieldingpost_test_pd)\nadd_index(fieldingpost_train_pd)\nfieldingpost_pd_logical_types = {\n    'index': ww.logical_types.Integer,\n    'lgID': ww.logical_types.Categorical,\n    'round': ww.logical_types.Categorical,\n    'POS': ww.logical_types.Categorical,\n    'teamID': ww.logical_types.Categorical,\n    'G': ww.logical_types.IntegerNullable,\n    'GS': ww.logical_types.IntegerNullable,\n    'InnOuts': ww.logical_types.IntegerNullable,\n    'PO': ww.logical_types.IntegerNullable,\n    'A': ww.logical_types.IntegerNullable,\n    'E': ww.logical_types.IntegerNullable,\n    'DP': ww.logical_types.IntegerNullable,\n    'TP': ww.logical_types.IntegerNullable,\n    'SB': ww.logical_types.IntegerNullable,\n    'CS': ww.logical_types.IntegerNullable,\n    'id': ww.logical_types.Integer\n}\nfieldingpost_test_pd.ww.init(logical_types=fieldingpost_pd_logical_types, index='index', name='fieldingpost')\nfieldingpost_train_pd.ww.init(logical_types=fieldingpost_pd_logical_types, index='index', name='fieldingpost')\n\nadd_index(pitching_test_pd)\nadd_index(pitching_train_pd)\npitching_pd_logical_types = {\n    'index': ww.logical_types.Integer,\n    'lgID': ww.logical_types.Categorical,\n    'teamID': ww.logical_types.Categorical,\n    'stint': ww.logical_types.Categorical,\n    'W': ww.logical_types.IntegerNullable,\n    'L': ww.logical_types.IntegerNullable,\n    'G': ww.logical_types.IntegerNullable,\n    'GS': ww.logical_types.IntegerNullable,\n    'CG': ww.logical_types.IntegerNullable,\n    'SHO': ww.logical_types.IntegerNullable,\n    'SV': ww.logical_types.IntegerNullable,\n    'IPouts': ww.logical_types.IntegerNullable,\n    'H': ww.logical_types.IntegerNullable,\n    'ER': ww.logical_types.IntegerNullable,\n    'HR': ww.logical_types.IntegerNullable,\n    'BB': ww.logical_types.IntegerNullable,\n    'SO': ww.logical_types.IntegerNullable,\n    'BAOpp': ww.logical_types.IntegerNullable,\n    'ERA': ww.logical_types.IntegerNullable,\n    'IBB': ww.logical_types.IntegerNullable,\n    'WP': ww.logical_types.IntegerNullable,\n    'HBP': ww.logical_types.IntegerNullable,\n    'BK': ww.logical_types.IntegerNullable,\n    'BFP': ww.logical_types.IntegerNullable,\n    'GF': ww.logical_types.IntegerNullable,\n    'R': ww.logical_types.IntegerNullable,\n    'id': ww.logical_types.Integer\n}\npitching_test_pd.ww.init(logical_types=pitching_pd_logical_types, index='index', name='pitching')\npitching_train_pd.ww.init(logical_types=pitching_pd_logical_types, index='index', name='pitching')\n\nadd_index(pitchingpost_test_pd)\nadd_index(pitchingpost_train_pd)\npitchingpost_pd_logical_types = {\n    'index': ww.logical_types.Integer,\n    'round': ww.logical_types.Categorical,\n    'lgID': ww.logical_types.Categorical,\n    'teamID': ww.logical_types.Categorical,\n    'W': ww.logical_types.IntegerNullable,\n    'L': ww.logical_types.IntegerNullable,\n    'G': ww.logical_types.IntegerNullable,\n    'GS': ww.logical_types.IntegerNullable,\n    'CG': ww.logical_types.IntegerNullable,\n    'SHO': ww.logical_types.IntegerNullable,\n    'SV': ww.logical_types.IntegerNullable,\n    'IPouts': ww.logical_types.IntegerNullable,\n    'H': ww.logical_types.IntegerNullable,\n    'ER': ww.logical_types.IntegerNullable,\n    'HR': ww.logical_types.IntegerNullable,\n    'BB': ww.logical_types.IntegerNullable,\n    'SO': ww.logical_types.IntegerNullable,\n    'BAOpp': ww.logical_types.IntegerNullable,\n    'ERA': ww.logical_types.IntegerNullable,\n    'IBB': ww.logical_types.IntegerNullable,\n    'WP': ww.logical_types.IntegerNullable,\n    'HBP': ww.logical_types.IntegerNullable,\n    'BK': ww.logical_types.IntegerNullable,\n    'BFP': ww.logical_types.IntegerNullable,\n    'GF': ww.logical_types.IntegerNullable,\n    'R': ww.logical_types.Double,\n    'SH': ww.logical_types.IntegerNullable,\n    'SF': ww.logical_types.IntegerNullable,\n    'GIDP': ww.logical_types.IntegerNullable,\n    'id': ww.logical_types.Integer\n}\npitchingpost_test_pd.ww.init(logical_types=pitchingpost_pd_logical_types, index='index', name='pitchingpost')\npitchingpost_train_pd.ww.init(logical_types=pitchingpost_pd_logical_types, index='index', name='pitchingpost')\n</pre> def add_index(df):     df.insert(0, \"index\", range(len(df)))  population_pd_logical_types = {     'lgID': ww.logical_types.Categorical,     'teamIDCat': ww.logical_types.Categorical,     'playerID': ww.logical_types.Categorical,     'teamID': ww.logical_types.Categorical,     'yearID': ww.logical_types.Integer,     'salary': ww.logical_types.IntegerNullable,     'year': ww.logical_types.Datetime,     'id': ww.logical_types.Integer } population_test_pd.ww.init(logical_types=population_pd_logical_types, index='id', name='population') population_train_pd.ww.init(logical_types=population_pd_logical_types, index='id', name='population')  add_index(allstarfull_test_pd) add_index(allstarfull_train_pd) allstarfull_pd_logical_types = {     'index': ww.logical_types.Integer,     'gameID': ww.logical_types.Categorical,     'teamID': ww.logical_types.Categorical,     'lgID': ww.logical_types.Categorical,     'gameNum': ww.logical_types.Categorical,     'GP': ww.logical_types.Categorical,     'startingPos': ww.logical_types.Categorical,     'id': ww.logical_types.Integer } allstarfull_test_pd.ww.init(logical_types=allstarfull_pd_logical_types, index='index', name='allstarfull') allstarfull_train_pd.ww.init(logical_types=allstarfull_pd_logical_types, index='index', name='allstarfull')  add_index(awardsplayers_test_pd) add_index(awardsplayers_train_pd) awardsplayers_pd_logical_types = {     'index': ww.logical_types.Integer,     'awardID': ww.logical_types.Categorical,     'lgID': ww.logical_types.Categorical,     'notes': ww.logical_types.Categorical,     'id': ww.logical_types.Integer } awardsplayers_test_pd.ww.init(logical_types=awardsplayers_pd_logical_types, index='index', name='awardsplayers') awardsplayers_train_pd.ww.init(logical_types=awardsplayers_pd_logical_types, index='index', name='awardsplayers')  add_index(awardsshareplayers_test_pd) add_index(awardsshareplayers_train_pd) awardsshareplayers_pd_logical_types = {     'index': ww.logical_types.Integer,     'awardID': ww.logical_types.Categorical,     'lgID': ww.logical_types.Categorical,     'pointsWon': ww.logical_types.IntegerNullable,     'pointsMax': ww.logical_types.Categorical,     'votesFirst': ww.logical_types.IntegerNullable,     'id': ww.logical_types.Integer } awardsshareplayers_test_pd.ww.init(logical_types=awardsshareplayers_pd_logical_types, index='index', name='awardsshareplayers') awardsshareplayers_train_pd.ww.init(logical_types=awardsshareplayers_pd_logical_types, index='index', name='awardsshareplayers')  add_index(batting_test_pd) add_index(batting_train_pd) batting_pd_logical_types = {     'index': ww.logical_types.Integer,     'lgID': ww.logical_types.Categorical,     'teamID': ww.logical_types.Categorical,     'stint': ww.logical_types.Categorical,     'G': ww.logical_types.IntegerNullable,     'G_batting': ww.logical_types.IntegerNullable,     'AB': ww.logical_types.IntegerNullable,     'R': ww.logical_types.IntegerNullable,     'H': ww.logical_types.IntegerNullable,     '2B': ww.logical_types.IntegerNullable,     '3B': ww.logical_types.IntegerNullable,     'HR': ww.logical_types.IntegerNullable,     'RBI': ww.logical_types.IntegerNullable,     'SB': ww.logical_types.IntegerNullable,     'CS': ww.logical_types.IntegerNullable,     'BB': ww.logical_types.IntegerNullable,     'SO': ww.logical_types.IntegerNullable,     'IBB': ww.logical_types.IntegerNullable,     'HBP': ww.logical_types.IntegerNullable,     'SH': ww.logical_types.IntegerNullable,     'SF': ww.logical_types.IntegerNullable,     'GIDP': ww.logical_types.IntegerNullable,     'G_old': ww.logical_types.IntegerNullable,     'id': ww.logical_types.Integer } batting_test_pd.ww.init(logical_types=batting_pd_logical_types, index='index', name='batting') batting_train_pd.ww.init(logical_types=batting_pd_logical_types, index='index', name='batting')  add_index(battingpost_test_pd) add_index(battingpost_train_pd) battingpost_pd_logical_types = {     'index': ww.logical_types.Integer,     'round': ww.logical_types.Categorical,     'lgID': ww.logical_types.Categorical,     'teamID': ww.logical_types.Categorical,     'G': ww.logical_types.IntegerNullable,     'AB': ww.logical_types.IntegerNullable,     'R': ww.logical_types.IntegerNullable,     'H': ww.logical_types.IntegerNullable,     '2B': ww.logical_types.IntegerNullable,     '3B': ww.logical_types.IntegerNullable,     'HR': ww.logical_types.IntegerNullable,     'RBI': ww.logical_types.IntegerNullable,     'SB': ww.logical_types.IntegerNullable,     'CS': ww.logical_types.IntegerNullable,     'BB': ww.logical_types.IntegerNullable,     'SO': ww.logical_types.IntegerNullable,     'IBB': ww.logical_types.IntegerNullable,     'HBP': ww.logical_types.IntegerNullable,     'SH': ww.logical_types.IntegerNullable,     'SF': ww.logical_types.IntegerNullable,     'GIDP': ww.logical_types.IntegerNullable,     'id': ww.logical_types.Integer } battingpost_test_pd.ww.init(logical_types=battingpost_pd_logical_types, index='index', name='battingpost') battingpost_train_pd.ww.init(logical_types=battingpost_pd_logical_types, index='index', name='battingpost')  add_index(fielding_test_pd) add_index(fielding_train_pd) fielding_pd_logical_types = {     'index': ww.logical_types.Integer,     'lgID': ww.logical_types.Categorical,     'POS': ww.logical_types.Categorical,     'teamID': ww.logical_types.Categorical,     'stint': ww.logical_types.Categorical,     'G': ww.logical_types.IntegerNullable,     'GS': ww.logical_types.IntegerNullable,     'InnOuts': ww.logical_types.IntegerNullable,     'PO': ww.logical_types.IntegerNullable,     'A': ww.logical_types.IntegerNullable,     'E': ww.logical_types.IntegerNullable,     'DP': ww.logical_types.IntegerNullable,     'id': ww.logical_types.Integer } fielding_test_pd.ww.init(logical_types=fielding_pd_logical_types, index='index', name='fielding') fielding_train_pd.ww.init(logical_types=fielding_pd_logical_types, index='index', name='fielding')  add_index(fieldingpost_test_pd) add_index(fieldingpost_train_pd) fieldingpost_pd_logical_types = {     'index': ww.logical_types.Integer,     'lgID': ww.logical_types.Categorical,     'round': ww.logical_types.Categorical,     'POS': ww.logical_types.Categorical,     'teamID': ww.logical_types.Categorical,     'G': ww.logical_types.IntegerNullable,     'GS': ww.logical_types.IntegerNullable,     'InnOuts': ww.logical_types.IntegerNullable,     'PO': ww.logical_types.IntegerNullable,     'A': ww.logical_types.IntegerNullable,     'E': ww.logical_types.IntegerNullable,     'DP': ww.logical_types.IntegerNullable,     'TP': ww.logical_types.IntegerNullable,     'SB': ww.logical_types.IntegerNullable,     'CS': ww.logical_types.IntegerNullable,     'id': ww.logical_types.Integer } fieldingpost_test_pd.ww.init(logical_types=fieldingpost_pd_logical_types, index='index', name='fieldingpost') fieldingpost_train_pd.ww.init(logical_types=fieldingpost_pd_logical_types, index='index', name='fieldingpost')  add_index(pitching_test_pd) add_index(pitching_train_pd) pitching_pd_logical_types = {     'index': ww.logical_types.Integer,     'lgID': ww.logical_types.Categorical,     'teamID': ww.logical_types.Categorical,     'stint': ww.logical_types.Categorical,     'W': ww.logical_types.IntegerNullable,     'L': ww.logical_types.IntegerNullable,     'G': ww.logical_types.IntegerNullable,     'GS': ww.logical_types.IntegerNullable,     'CG': ww.logical_types.IntegerNullable,     'SHO': ww.logical_types.IntegerNullable,     'SV': ww.logical_types.IntegerNullable,     'IPouts': ww.logical_types.IntegerNullable,     'H': ww.logical_types.IntegerNullable,     'ER': ww.logical_types.IntegerNullable,     'HR': ww.logical_types.IntegerNullable,     'BB': ww.logical_types.IntegerNullable,     'SO': ww.logical_types.IntegerNullable,     'BAOpp': ww.logical_types.IntegerNullable,     'ERA': ww.logical_types.IntegerNullable,     'IBB': ww.logical_types.IntegerNullable,     'WP': ww.logical_types.IntegerNullable,     'HBP': ww.logical_types.IntegerNullable,     'BK': ww.logical_types.IntegerNullable,     'BFP': ww.logical_types.IntegerNullable,     'GF': ww.logical_types.IntegerNullable,     'R': ww.logical_types.IntegerNullable,     'id': ww.logical_types.Integer } pitching_test_pd.ww.init(logical_types=pitching_pd_logical_types, index='index', name='pitching') pitching_train_pd.ww.init(logical_types=pitching_pd_logical_types, index='index', name='pitching')  add_index(pitchingpost_test_pd) add_index(pitchingpost_train_pd) pitchingpost_pd_logical_types = {     'index': ww.logical_types.Integer,     'round': ww.logical_types.Categorical,     'lgID': ww.logical_types.Categorical,     'teamID': ww.logical_types.Categorical,     'W': ww.logical_types.IntegerNullable,     'L': ww.logical_types.IntegerNullable,     'G': ww.logical_types.IntegerNullable,     'GS': ww.logical_types.IntegerNullable,     'CG': ww.logical_types.IntegerNullable,     'SHO': ww.logical_types.IntegerNullable,     'SV': ww.logical_types.IntegerNullable,     'IPouts': ww.logical_types.IntegerNullable,     'H': ww.logical_types.IntegerNullable,     'ER': ww.logical_types.IntegerNullable,     'HR': ww.logical_types.IntegerNullable,     'BB': ww.logical_types.IntegerNullable,     'SO': ww.logical_types.IntegerNullable,     'BAOpp': ww.logical_types.IntegerNullable,     'ERA': ww.logical_types.IntegerNullable,     'IBB': ww.logical_types.IntegerNullable,     'WP': ww.logical_types.IntegerNullable,     'HBP': ww.logical_types.IntegerNullable,     'BK': ww.logical_types.IntegerNullable,     'BFP': ww.logical_types.IntegerNullable,     'GF': ww.logical_types.IntegerNullable,     'R': ww.logical_types.Double,     'SH': ww.logical_types.IntegerNullable,     'SF': ww.logical_types.IntegerNullable,     'GIDP': ww.logical_types.IntegerNullable,     'id': ww.logical_types.Integer } pitchingpost_test_pd.ww.init(logical_types=pitchingpost_pd_logical_types, index='index', name='pitchingpost') pitchingpost_train_pd.ww.init(logical_types=pitchingpost_pd_logical_types, index='index', name='pitchingpost') In\u00a0[52]: Copied! <pre>dataframes_train = {\n    \"population\" : (population_train_pd, ),\n    \"allstarfull\": (allstarfull_train_pd, ),\n    \"awardsplayers\": (awardsplayers_train_pd, ),\n    \"awardsshareplayers\": (awardsshareplayers_train_pd, ),\n    \"batting\": (batting_train_pd, ),\n    \"battingpost\": (battingpost_train_pd, ),\n    \"fielding\": (fielding_train_pd, ),\n    \"fieldingpost\": (fieldingpost_train_pd, ),\n    \"pitching\": (pitching_train_pd, ),\n    \"pitchingpost\": (pitchingpost_train_pd, ),\n}\n</pre> dataframes_train = {     \"population\" : (population_train_pd, ),     \"allstarfull\": (allstarfull_train_pd, ),     \"awardsplayers\": (awardsplayers_train_pd, ),     \"awardsshareplayers\": (awardsshareplayers_train_pd, ),     \"batting\": (batting_train_pd, ),     \"battingpost\": (battingpost_train_pd, ),     \"fielding\": (fielding_train_pd, ),     \"fieldingpost\": (fieldingpost_train_pd, ),     \"pitching\": (pitching_train_pd, ),     \"pitchingpost\": (pitchingpost_train_pd, ), } In\u00a0[53]: Copied! <pre>dataframes_test = {\n    \"population\" : (population_test_pd, ),\n    \"allstarfull\": (allstarfull_test_pd, ),\n    \"awardsplayers\": (awardsplayers_test_pd, ),\n    \"awardsshareplayers\": (awardsshareplayers_test_pd, ),\n    \"batting\": (batting_test_pd, ),\n    \"battingpost\": (battingpost_test_pd, ),\n    \"fielding\": (fielding_test_pd, ),\n    \"fieldingpost\": (fieldingpost_test_pd, ),\n    \"pitching\": (pitching_test_pd, ),\n    \"pitchingpost\": (pitchingpost_test_pd, ),\n}\n</pre> dataframes_test = {     \"population\" : (population_test_pd, ),     \"allstarfull\": (allstarfull_test_pd, ),     \"awardsplayers\": (awardsplayers_test_pd, ),     \"awardsshareplayers\": (awardsshareplayers_test_pd, ),     \"batting\": (batting_test_pd, ),     \"battingpost\": (battingpost_test_pd, ),     \"fielding\": (fielding_test_pd, ),     \"fieldingpost\": (fieldingpost_test_pd, ),     \"pitching\": (pitching_test_pd, ),     \"pitchingpost\": (pitchingpost_test_pd, ), } In\u00a0[54]: Copied! <pre>relationships = [\n    (\"population\", \"id\", \"allstarfull\", \"id\"),\n    (\"population\", \"id\", \"awardsplayers\", \"id\"),\n    (\"population\", \"id\", \"awardsshareplayers\", \"id\"),\n    (\"population\", \"id\", \"batting\", \"id\"),\n    (\"population\", \"id\", \"battingpost\", \"id\"),\n    (\"population\", \"id\", \"fielding\", \"id\"),\n    (\"population\", \"id\", \"fieldingpost\", \"id\"),\n    (\"population\", \"id\", \"pitching\", \"id\"),\n    (\"population\", \"id\", \"pitchingpost\", \"id\"),\n]\n</pre> relationships = [     (\"population\", \"id\", \"allstarfull\", \"id\"),     (\"population\", \"id\", \"awardsplayers\", \"id\"),     (\"population\", \"id\", \"awardsshareplayers\", \"id\"),     (\"population\", \"id\", \"batting\", \"id\"),     (\"population\", \"id\", \"battingpost\", \"id\"),     (\"population\", \"id\", \"fielding\", \"id\"),     (\"population\", \"id\", \"fieldingpost\", \"id\"),     (\"population\", \"id\", \"pitching\", \"id\"),     (\"population\", \"id\", \"pitchingpost\", \"id\"), ] In\u00a0[55]: Copied! <pre>featuretools_train_pd = featuretools.dfs(\n    dataframes=dataframes_train,\n    relationships=relationships,\n    target_dataframe_name=\"population\")[0]\n</pre> featuretools_train_pd = featuretools.dfs(     dataframes=dataframes_train,     relationships=relationships,     target_dataframe_name=\"population\")[0] In\u00a0[56]: Copied! <pre>featuretools_test_pd = featuretools.dfs(\n    dataframes=dataframes_test,\n    relationships=relationships,\n    target_dataframe_name=\"population\")[0]\n</pre> featuretools_test_pd = featuretools.dfs(     dataframes=dataframes_test,     relationships=relationships,     target_dataframe_name=\"population\")[0] In\u00a0[57]: Copied! <pre>featuretools_train_pd.replace([np.inf, -np.inf], np.nan, inplace=True)\nfeaturetools_test_pd.replace([np.inf, -np.inf], np.nan, inplace=True)\n</pre> featuretools_train_pd.replace([np.inf, -np.inf], np.nan, inplace=True) featuretools_test_pd.replace([np.inf, -np.inf], np.nan, inplace=True) In\u00a0[58]: Copied! <pre>featuretools_train = getml.data.DataFrame.from_pandas(featuretools_train_pd, \"featuretools_train\")\nfeaturetools_test = getml.data.DataFrame.from_pandas(featuretools_test_pd, \"featuretools_test\")\n</pre> featuretools_train = getml.data.DataFrame.from_pandas(featuretools_train_pd, \"featuretools_train\") featuretools_test = getml.data.DataFrame.from_pandas(featuretools_test_pd, \"featuretools_test\") In\u00a0[59]: Copied! <pre>featuretools_train.set_role(\"salary\", getml.data.roles.target)\nfeaturetools_train.set_role(featuretools_train.roles.unused_float, getml.data.roles.numerical)\nfeaturetools_train.set_role(featuretools_train.roles.unused_string, getml.data.roles.categorical)\n\nfeaturetools_train\n</pre> featuretools_train.set_role(\"salary\", getml.data.roles.target) featuretools_train.set_role(featuretools_train.roles.unused_float, getml.data.roles.numerical) featuretools_train.set_role(featuretools_train.roles.unused_string, getml.data.roles.categorical)  featuretools_train Out[59]:  name   salary lgID        teamIDCat   playerID    teamID      COUNT(allstarfull) MODE(allstarfull.GP) MODE(allstarfull.gameID) MODE(allstarfull.gameNum) MODE(allstarfull.lgID) MODE(allstarfull.startingPos) MODE(allstarfull.teamID) NUM_UNIQUE(allstarfull.GP) NUM_UNIQUE(allstarfull.gameID) NUM_UNIQUE(allstarfull.gameNum) NUM_UNIQUE(allstarfull.lgID) NUM_UNIQUE(allstarfull.startingPos) NUM_UNIQUE(allstarfull.teamID) COUNT(awardsplayers) MODE(awardsplayers.awardID) MODE(awardsplayers.lgID) MODE(awardsplayers.notes) NUM_UNIQUE(awardsplayers.awardID) NUM_UNIQUE(awardsplayers.lgID) NUM_UNIQUE(awardsplayers.notes) COUNT(awardsshareplayers) MODE(awardsshareplayers.awardID) MODE(awardsshareplayers.lgID) MODE(awardsshareplayers.pointsMax) NUM_UNIQUE(awardsshareplayers.awardID) NUM_UNIQUE(awardsshareplayers.lgID) NUM_UNIQUE(awardsshareplayers.pointsMax) COUNT(batting) MODE(batting.lgID) MODE(batting.stint) MODE(batting.teamID) NUM_UNIQUE(batting.lgID) NUM_UNIQUE(batting.stint) NUM_UNIQUE(batting.teamID) COUNT(battingpost) MODE(battingpost.lgID) MODE(battingpost.round) MODE(battingpost.teamID) NUM_UNIQUE(battingpost.lgID) NUM_UNIQUE(battingpost.round) NUM_UNIQUE(battingpost.teamID) COUNT(fielding) MODE(fielding.POS) MODE(fielding.lgID) MODE(fielding.stint) MODE(fielding.teamID) NUM_UNIQUE(fielding.POS) NUM_UNIQUE(fielding.lgID) NUM_UNIQUE(fielding.stint) NUM_UNIQUE(fielding.teamID) COUNT(fieldingpost) MODE(fieldingpost.POS) MODE(fieldingpost.lgID) MODE(fieldingpost.round) MODE(fieldingpost.teamID) NUM_UNIQUE(fieldingpost.POS) NUM_UNIQUE(fieldingpost.lgID) NUM_UNIQUE(fieldingpost.round) NUM_UNIQUE(fieldingpost.teamID) COUNT(pitching) MODE(pitching.lgID) MODE(pitching.stint) MODE(pitching.teamID) NUM_UNIQUE(pitching.lgID) NUM_UNIQUE(pitching.stint) NUM_UNIQUE(pitching.teamID) COUNT(pitchingpost) MODE(pitchingpost.lgID) MODE(pitchingpost.round) MODE(pitchingpost.teamID) NUM_UNIQUE(pitchingpost.lgID) NUM_UNIQUE(pitchingpost.round) NUM_UNIQUE(pitchingpost.teamID) DAY(year)   MONTH(year) WEEKDAY(year) YEAR(year)     yearID MAX(awardsshareplayers.pointsWon) MAX(awardsshareplayers.votesFirst) MEAN(awardsshareplayers.pointsWon) MEAN(awardsshareplayers.votesFirst) MIN(awardsshareplayers.pointsWon) MIN(awardsshareplayers.votesFirst) SKEW(awardsshareplayers.pointsWon) SKEW(awardsshareplayers.votesFirst) STD(awardsshareplayers.pointsWon) STD(awardsshareplayers.votesFirst) SUM(awardsshareplayers.pointsWon) SUM(awardsshareplayers.votesFirst) MAX(batting.2B) MAX(batting.3B) MAX(batting.AB) MAX(batting.BB) MAX(batting.CS) MAX(batting.G) MAX(batting.GIDP) MAX(batting.G_batting) MAX(batting.G_old) MAX(batting.H) MAX(batting.HBP) MAX(batting.HR) MAX(batting.IBB) MAX(batting.R) MAX(batting.RBI) MAX(batting.SB) MAX(batting.SF) MAX(batting.SH) MAX(batting.SO) MEAN(batting.2B) MEAN(batting.3B) MEAN(batting.AB) MEAN(batting.BB) MEAN(batting.CS) MEAN(batting.G) MEAN(batting.GIDP) MEAN(batting.G_batting) MEAN(batting.G_old) MEAN(batting.H) MEAN(batting.HBP) MEAN(batting.HR) MEAN(batting.IBB) MEAN(batting.R) MEAN(batting.RBI) MEAN(batting.SB) MEAN(batting.SF) MEAN(batting.SH) MEAN(batting.SO) MIN(batting.2B) MIN(batting.3B) MIN(batting.AB) MIN(batting.BB) MIN(batting.CS) MIN(batting.G) MIN(batting.GIDP) MIN(batting.G_batting) MIN(batting.G_old) MIN(batting.H) MIN(batting.HBP) MIN(batting.HR) MIN(batting.IBB) MIN(batting.R) MIN(batting.RBI) MIN(batting.SB) MIN(batting.SF) MIN(batting.SH) MIN(batting.SO) SKEW(batting.2B) SKEW(batting.3B) SKEW(batting.AB) SKEW(batting.BB) SKEW(batting.CS) SKEW(batting.G) SKEW(batting.GIDP) SKEW(batting.G_batting) SKEW(batting.G_old) SKEW(batting.H) SKEW(batting.HBP) SKEW(batting.HR) SKEW(batting.IBB) SKEW(batting.R) SKEW(batting.RBI) SKEW(batting.SB) SKEW(batting.SF) SKEW(batting.SH) SKEW(batting.SO) STD(batting.2B) STD(batting.3B) STD(batting.AB) STD(batting.BB) STD(batting.CS) STD(batting.G) STD(batting.GIDP) STD(batting.G_batting) STD(batting.G_old) STD(batting.H) STD(batting.HBP) STD(batting.HR) STD(batting.IBB) STD(batting.R) STD(batting.RBI) STD(batting.SB) STD(batting.SF) STD(batting.SH) STD(batting.SO) SUM(batting.2B) SUM(batting.3B) SUM(batting.AB) SUM(batting.BB) SUM(batting.CS) SUM(batting.G) SUM(batting.GIDP) SUM(batting.G_batting) SUM(batting.G_old) SUM(batting.H) SUM(batting.HBP) SUM(batting.HR) SUM(batting.IBB) SUM(batting.R) SUM(batting.RBI) SUM(batting.SB) SUM(batting.SF) SUM(batting.SH) SUM(batting.SO) MAX(battingpost.2B) MAX(battingpost.3B) MAX(battingpost.AB) MAX(battingpost.BB) MAX(battingpost.CS) MAX(battingpost.G) MAX(battingpost.GIDP) MAX(battingpost.H) MAX(battingpost.HBP) MAX(battingpost.HR) MAX(battingpost.IBB) MAX(battingpost.R) MAX(battingpost.RBI) MAX(battingpost.SB) MAX(battingpost.SF) MAX(battingpost.SH) MAX(battingpost.SO) MEAN(battingpost.2B) MEAN(battingpost.3B) MEAN(battingpost.AB) MEAN(battingpost.BB) MEAN(battingpost.CS) MEAN(battingpost.G) MEAN(battingpost.GIDP) MEAN(battingpost.H) MEAN(battingpost.HBP) MEAN(battingpost.HR) MEAN(battingpost.IBB) MEAN(battingpost.R) MEAN(battingpost.RBI) MEAN(battingpost.SB) MEAN(battingpost.SF) MEAN(battingpost.SH) MEAN(battingpost.SO) MIN(battingpost.2B) MIN(battingpost.3B) MIN(battingpost.AB) MIN(battingpost.BB) MIN(battingpost.CS) MIN(battingpost.G) MIN(battingpost.GIDP) MIN(battingpost.H) MIN(battingpost.HBP) MIN(battingpost.HR) MIN(battingpost.IBB) MIN(battingpost.R) MIN(battingpost.RBI) MIN(battingpost.SB) MIN(battingpost.SF) MIN(battingpost.SH) MIN(battingpost.SO) SKEW(battingpost.2B) SKEW(battingpost.3B) SKEW(battingpost.AB) SKEW(battingpost.BB) SKEW(battingpost.CS) SKEW(battingpost.G) SKEW(battingpost.GIDP) SKEW(battingpost.H) SKEW(battingpost.HBP) SKEW(battingpost.HR) SKEW(battingpost.IBB) SKEW(battingpost.R) SKEW(battingpost.RBI) SKEW(battingpost.SB) SKEW(battingpost.SF) SKEW(battingpost.SH) SKEW(battingpost.SO) STD(battingpost.2B) STD(battingpost.3B) STD(battingpost.AB) STD(battingpost.BB) STD(battingpost.CS) STD(battingpost.G) STD(battingpost.GIDP) STD(battingpost.H) STD(battingpost.HBP) STD(battingpost.HR) STD(battingpost.IBB) STD(battingpost.R) STD(battingpost.RBI) STD(battingpost.SB) STD(battingpost.SF) STD(battingpost.SH) STD(battingpost.SO) SUM(battingpost.2B) SUM(battingpost.3B) SUM(battingpost.AB) SUM(battingpost.BB) SUM(battingpost.CS) SUM(battingpost.G) SUM(battingpost.GIDP) SUM(battingpost.H) SUM(battingpost.HBP) SUM(battingpost.HR) SUM(battingpost.IBB) SUM(battingpost.R) SUM(battingpost.RBI) SUM(battingpost.SB) SUM(battingpost.SF) SUM(battingpost.SH) SUM(battingpost.SO) MAX(fielding.A) MAX(fielding.DP) MAX(fielding.E) MAX(fielding.G) MAX(fielding.GS) MAX(fielding.InnOuts) MAX(fielding.PO) MEAN(fielding.A) MEAN(fielding.DP) MEAN(fielding.E) MEAN(fielding.G) MEAN(fielding.GS) MEAN(fielding.InnOuts) MEAN(fielding.PO) MIN(fielding.A) MIN(fielding.DP) MIN(fielding.E) MIN(fielding.G) MIN(fielding.GS) MIN(fielding.InnOuts) MIN(fielding.PO) SKEW(fielding.A) SKEW(fielding.DP) SKEW(fielding.E) SKEW(fielding.G) SKEW(fielding.GS) SKEW(fielding.InnOuts) SKEW(fielding.PO) STD(fielding.A) STD(fielding.DP) STD(fielding.E) STD(fielding.G) STD(fielding.GS) STD(fielding.InnOuts) STD(fielding.PO) SUM(fielding.A) SUM(fielding.DP) SUM(fielding.E) SUM(fielding.G) SUM(fielding.GS) SUM(fielding.InnOuts) SUM(fielding.PO) MAX(fieldingpost.A) MAX(fieldingpost.CS) MAX(fieldingpost.DP) MAX(fieldingpost.E) MAX(fieldingpost.G) MAX(fieldingpost.GS) MAX(fieldingpost.InnOuts) MAX(fieldingpost.PO) MAX(fieldingpost.SB) MAX(fieldingpost.TP) MEAN(fieldingpost.A) MEAN(fieldingpost.CS) MEAN(fieldingpost.DP) MEAN(fieldingpost.E) MEAN(fieldingpost.G) MEAN(fieldingpost.GS) MEAN(fieldingpost.InnOuts) MEAN(fieldingpost.PO) MEAN(fieldingpost.SB) MEAN(fieldingpost.TP) MIN(fieldingpost.A) MIN(fieldingpost.CS) MIN(fieldingpost.DP) MIN(fieldingpost.E) MIN(fieldingpost.G) MIN(fieldingpost.GS) MIN(fieldingpost.InnOuts) MIN(fieldingpost.PO) MIN(fieldingpost.SB) MIN(fieldingpost.TP) SKEW(fieldingpost.A) SKEW(fieldingpost.CS) SKEW(fieldingpost.DP) SKEW(fieldingpost.E) SKEW(fieldingpost.G) SKEW(fieldingpost.GS) SKEW(fieldingpost.InnOuts) SKEW(fieldingpost.PO) SKEW(fieldingpost.SB) SKEW(fieldingpost.TP) STD(fieldingpost.A) STD(fieldingpost.CS) STD(fieldingpost.DP) STD(fieldingpost.E) STD(fieldingpost.G) STD(fieldingpost.GS) STD(fieldingpost.InnOuts) STD(fieldingpost.PO) STD(fieldingpost.SB) STD(fieldingpost.TP) SUM(fieldingpost.A) SUM(fieldingpost.CS) SUM(fieldingpost.DP) SUM(fieldingpost.E) SUM(fieldingpost.G) SUM(fieldingpost.GS) SUM(fieldingpost.InnOuts) SUM(fieldingpost.PO) SUM(fieldingpost.SB) SUM(fieldingpost.TP) MAX(pitching.BAOpp) MAX(pitching.BB) MAX(pitching.BFP) MAX(pitching.BK) MAX(pitching.CG) MAX(pitching.ER) MAX(pitching.ERA) MAX(pitching.G) MAX(pitching.GF) MAX(pitching.GS) MAX(pitching.H) MAX(pitching.HBP) MAX(pitching.HR) MAX(pitching.IBB) MAX(pitching.IPouts) MAX(pitching.L) MAX(pitching.R) MAX(pitching.SHO) MAX(pitching.SO) MAX(pitching.SV) MAX(pitching.W) MAX(pitching.WP) MEAN(pitching.BAOpp) MEAN(pitching.BB) MEAN(pitching.BFP) MEAN(pitching.BK) MEAN(pitching.CG) MEAN(pitching.ER) MEAN(pitching.ERA) MEAN(pitching.G) MEAN(pitching.GF) MEAN(pitching.GS) MEAN(pitching.H) MEAN(pitching.HBP) MEAN(pitching.HR) MEAN(pitching.IBB) MEAN(pitching.IPouts) MEAN(pitching.L) MEAN(pitching.R) MEAN(pitching.SHO) MEAN(pitching.SO) MEAN(pitching.SV) MEAN(pitching.W) MEAN(pitching.WP) MIN(pitching.BAOpp) MIN(pitching.BB) MIN(pitching.BFP) MIN(pitching.BK) MIN(pitching.CG) MIN(pitching.ER) MIN(pitching.ERA) MIN(pitching.G) MIN(pitching.GF) MIN(pitching.GS) MIN(pitching.H) MIN(pitching.HBP) MIN(pitching.HR) MIN(pitching.IBB) MIN(pitching.IPouts) MIN(pitching.L) MIN(pitching.R) MIN(pitching.SHO) MIN(pitching.SO) MIN(pitching.SV) MIN(pitching.W) MIN(pitching.WP) SKEW(pitching.BAOpp) SKEW(pitching.BB) SKEW(pitching.BFP) SKEW(pitching.BK) SKEW(pitching.CG) SKEW(pitching.ER) SKEW(pitching.ERA) SKEW(pitching.G) SKEW(pitching.GF) SKEW(pitching.GS) SKEW(pitching.H) SKEW(pitching.HBP) SKEW(pitching.HR) SKEW(pitching.IBB) SKEW(pitching.IPouts) SKEW(pitching.L) SKEW(pitching.R) SKEW(pitching.SHO) SKEW(pitching.SO) SKEW(pitching.SV) SKEW(pitching.W) SKEW(pitching.WP) STD(pitching.BAOpp) STD(pitching.BB) STD(pitching.BFP) STD(pitching.BK) STD(pitching.CG) STD(pitching.ER) STD(pitching.ERA) STD(pitching.G) STD(pitching.GF) STD(pitching.GS) STD(pitching.H) STD(pitching.HBP) STD(pitching.HR) STD(pitching.IBB) STD(pitching.IPouts) STD(pitching.L) STD(pitching.R) STD(pitching.SHO) STD(pitching.SO) STD(pitching.SV) STD(pitching.W) STD(pitching.WP) SUM(pitching.BAOpp) SUM(pitching.BB) SUM(pitching.BFP) SUM(pitching.BK) SUM(pitching.CG) SUM(pitching.ER) SUM(pitching.ERA) SUM(pitching.G) SUM(pitching.GF) SUM(pitching.GS) SUM(pitching.H) SUM(pitching.HBP) SUM(pitching.HR) SUM(pitching.IBB) SUM(pitching.IPouts) SUM(pitching.L) SUM(pitching.R) SUM(pitching.SHO) SUM(pitching.SO) SUM(pitching.SV) SUM(pitching.W) SUM(pitching.WP) MAX(pitchingpost.BAOpp) MAX(pitchingpost.BB) MAX(pitchingpost.BFP) MAX(pitchingpost.BK) MAX(pitchingpost.CG) MAX(pitchingpost.ER) MAX(pitchingpost.ERA) MAX(pitchingpost.G) MAX(pitchingpost.GF) MAX(pitchingpost.GIDP) MAX(pitchingpost.GS) MAX(pitchingpost.H) MAX(pitchingpost.HBP) MAX(pitchingpost.HR) MAX(pitchingpost.IBB) MAX(pitchingpost.IPouts) MAX(pitchingpost.L) MAX(pitchingpost.R) MAX(pitchingpost.SF) MAX(pitchingpost.SH) MAX(pitchingpost.SHO) MAX(pitchingpost.SO) MAX(pitchingpost.SV) MAX(pitchingpost.W) MAX(pitchingpost.WP) MEAN(pitchingpost.BAOpp) MEAN(pitchingpost.BB) MEAN(pitchingpost.BFP) MEAN(pitchingpost.BK) MEAN(pitchingpost.CG) MEAN(pitchingpost.ER) MEAN(pitchingpost.ERA) MEAN(pitchingpost.G) MEAN(pitchingpost.GF) MEAN(pitchingpost.GIDP) MEAN(pitchingpost.GS) MEAN(pitchingpost.H) MEAN(pitchingpost.HBP) MEAN(pitchingpost.HR) MEAN(pitchingpost.IBB) MEAN(pitchingpost.IPouts) MEAN(pitchingpost.L) MEAN(pitchingpost.R) MEAN(pitchingpost.SF) MEAN(pitchingpost.SH) MEAN(pitchingpost.SHO) MEAN(pitchingpost.SO) MEAN(pitchingpost.SV) MEAN(pitchingpost.W) MEAN(pitchingpost.WP) MIN(pitchingpost.BAOpp) MIN(pitchingpost.BB) MIN(pitchingpost.BFP) MIN(pitchingpost.BK) MIN(pitchingpost.CG) MIN(pitchingpost.ER) MIN(pitchingpost.ERA) MIN(pitchingpost.G) MIN(pitchingpost.GF) MIN(pitchingpost.GIDP) MIN(pitchingpost.GS) MIN(pitchingpost.H) MIN(pitchingpost.HBP) MIN(pitchingpost.HR) MIN(pitchingpost.IBB) MIN(pitchingpost.IPouts) MIN(pitchingpost.L) MIN(pitchingpost.R) MIN(pitchingpost.SF) MIN(pitchingpost.SH) MIN(pitchingpost.SHO) MIN(pitchingpost.SO) MIN(pitchingpost.SV) MIN(pitchingpost.W) MIN(pitchingpost.WP) SKEW(pitchingpost.BAOpp) SKEW(pitchingpost.BB) SKEW(pitchingpost.BFP) SKEW(pitchingpost.BK) SKEW(pitchingpost.CG) SKEW(pitchingpost.ER) SKEW(pitchingpost.ERA) SKEW(pitchingpost.G) SKEW(pitchingpost.GF) SKEW(pitchingpost.GIDP) SKEW(pitchingpost.GS) SKEW(pitchingpost.H) SKEW(pitchingpost.HBP) SKEW(pitchingpost.HR) SKEW(pitchingpost.IBB) SKEW(pitchingpost.IPouts) SKEW(pitchingpost.L) SKEW(pitchingpost.R) SKEW(pitchingpost.SF) SKEW(pitchingpost.SH) SKEW(pitchingpost.SHO) SKEW(pitchingpost.SO) SKEW(pitchingpost.SV) SKEW(pitchingpost.W) SKEW(pitchingpost.WP) STD(pitchingpost.BAOpp) STD(pitchingpost.BB) STD(pitchingpost.BFP) STD(pitchingpost.BK) STD(pitchingpost.CG) STD(pitchingpost.ER) STD(pitchingpost.ERA) STD(pitchingpost.G) STD(pitchingpost.GF) STD(pitchingpost.GIDP) STD(pitchingpost.GS) STD(pitchingpost.H) STD(pitchingpost.HBP) STD(pitchingpost.HR) STD(pitchingpost.IBB) STD(pitchingpost.IPouts) STD(pitchingpost.L) STD(pitchingpost.R) STD(pitchingpost.SF) STD(pitchingpost.SH) STD(pitchingpost.SHO) STD(pitchingpost.SO) STD(pitchingpost.SV) STD(pitchingpost.W) STD(pitchingpost.WP) SUM(pitchingpost.BAOpp) SUM(pitchingpost.BB) SUM(pitchingpost.BFP) SUM(pitchingpost.BK) SUM(pitchingpost.CG) SUM(pitchingpost.ER) SUM(pitchingpost.ERA) SUM(pitchingpost.G) SUM(pitchingpost.GF) SUM(pitchingpost.GIDP) SUM(pitchingpost.GS) SUM(pitchingpost.H) SUM(pitchingpost.HBP) SUM(pitchingpost.HR) SUM(pitchingpost.IBB) SUM(pitchingpost.IPouts) SUM(pitchingpost.L) SUM(pitchingpost.R) SUM(pitchingpost.SF) SUM(pitchingpost.SH) SUM(pitchingpost.SHO) SUM(pitchingpost.SO) SUM(pitchingpost.SV) SUM(pitchingpost.W) SUM(pitchingpost.WP)  role   target categorical categorical categorical categorical categorical        categorical          categorical              categorical               categorical            categorical                   categorical              categorical                categorical                    categorical                     categorical                  categorical                      categorical                    categorical          categorical                 categorical              categorical               categorical                      categorical                    categorical                     categorical               categorical                      categorical                   categorical                      categorical                      categorical                      categorical                      categorical    categorical        categorical         categorical          categorical              categorical               categorical                categorical        categorical            categorical             categorical              categorical                  categorical                   categorical                    categorical     categorical        categorical         categorical          categorical           categorical              categorical               categorical                categorical                 categorical         categorical            categorical             categorical              categorical               categorical                  categorical                   categorical                    categorical                     categorical     categorical         categorical          categorical           categorical               categorical                categorical                 categorical         categorical             categorical              categorical               categorical                   categorical                    categorical                     categorical categorical categorical   categorical numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical       numerical       numerical       numerical       numerical       numerical      numerical         numerical              numerical          numerical      numerical        numerical       numerical        numerical      numerical        numerical       numerical       numerical       numerical       numerical        numerical        numerical        numerical        numerical        numerical       numerical          numerical               numerical           numerical       numerical         numerical        numerical         numerical       numerical         numerical        numerical        numerical        numerical        numerical       numerical       numerical       numerical       numerical       numerical      numerical         numerical              numerical          numerical      numerical        numerical       numerical        numerical      numerical        numerical       numerical       numerical       numerical       numerical        numerical        numerical        numerical        numerical        numerical       numerical          numerical               numerical           numerical       numerical         numerical        numerical         numerical       numerical         numerical        numerical        numerical        numerical        numerical       numerical       numerical       numerical       numerical       numerical      numerical         numerical              numerical          numerical      numerical        numerical       numerical        numerical      numerical        numerical       numerical       numerical       numerical       numerical       numerical       numerical       numerical       numerical       numerical      numerical         numerical              numerical          numerical      numerical        numerical       numerical        numerical      numerical        numerical       numerical       numerical       numerical       numerical           numerical           numerical           numerical           numerical           numerical          numerical             numerical          numerical            numerical           numerical            numerical          numerical            numerical           numerical           numerical           numerical           numerical            numerical            numerical            numerical            numerical            numerical           numerical              numerical           numerical             numerical            numerical             numerical           numerical             numerical            numerical            numerical            numerical            numerical           numerical           numerical           numerical           numerical           numerical          numerical             numerical          numerical            numerical           numerical            numerical          numerical            numerical           numerical           numerical           numerical           numerical            numerical            numerical            numerical            numerical            numerical           numerical              numerical           numerical             numerical            numerical             numerical           numerical             numerical            numerical            numerical            numerical            numerical           numerical           numerical           numerical           numerical           numerical          numerical             numerical          numerical            numerical           numerical            numerical          numerical            numerical           numerical           numerical           numerical           numerical           numerical           numerical           numerical           numerical           numerical          numerical             numerical          numerical            numerical           numerical            numerical          numerical            numerical           numerical           numerical           numerical           numerical       numerical        numerical       numerical       numerical        numerical             numerical        numerical        numerical         numerical        numerical        numerical         numerical              numerical         numerical       numerical        numerical       numerical       numerical        numerical             numerical        numerical        numerical         numerical        numerical        numerical         numerical              numerical         numerical       numerical        numerical       numerical       numerical        numerical             numerical        numerical       numerical        numerical       numerical       numerical        numerical             numerical        numerical           numerical            numerical            numerical           numerical           numerical            numerical                 numerical            numerical            numerical            numerical            numerical             numerical             numerical            numerical            numerical             numerical                  numerical             numerical             numerical             numerical           numerical            numerical            numerical           numerical           numerical            numerical                 numerical            numerical            numerical            numerical            numerical             numerical             numerical            numerical            numerical             numerical                  numerical             numerical             numerical             numerical           numerical            numerical            numerical           numerical           numerical            numerical                 numerical            numerical            numerical            numerical           numerical            numerical            numerical           numerical           numerical            numerical                 numerical            numerical            numerical            numerical           numerical        numerical         numerical        numerical        numerical        numerical         numerical       numerical        numerical        numerical       numerical         numerical        numerical         numerical            numerical       numerical       numerical         numerical        numerical        numerical       numerical        numerical            numerical         numerical          numerical         numerical         numerical         numerical          numerical        numerical         numerical         numerical        numerical          numerical         numerical          numerical             numerical        numerical        numerical          numerical         numerical         numerical        numerical         numerical           numerical        numerical         numerical        numerical        numerical        numerical         numerical       numerical        numerical        numerical       numerical         numerical        numerical         numerical            numerical       numerical       numerical         numerical        numerical        numerical       numerical        numerical            numerical         numerical          numerical         numerical         numerical         numerical          numerical        numerical         numerical         numerical        numerical          numerical         numerical          numerical             numerical        numerical        numerical          numerical         numerical         numerical        numerical         numerical           numerical        numerical         numerical        numerical        numerical        numerical         numerical       numerical        numerical        numerical       numerical         numerical        numerical         numerical            numerical       numerical       numerical         numerical        numerical        numerical       numerical        numerical           numerical        numerical         numerical        numerical        numerical        numerical         numerical       numerical        numerical        numerical       numerical         numerical        numerical         numerical            numerical       numerical       numerical         numerical        numerical        numerical       numerical        numerical               numerical            numerical             numerical            numerical            numerical            numerical             numerical           numerical            numerical              numerical            numerical           numerical             numerical            numerical             numerical                numerical           numerical           numerical            numerical            numerical             numerical            numerical            numerical           numerical            numerical                numerical             numerical              numerical             numerical             numerical             numerical              numerical            numerical             numerical               numerical             numerical            numerical              numerical             numerical              numerical                 numerical            numerical            numerical             numerical             numerical              numerical             numerical             numerical            numerical             numerical               numerical            numerical             numerical            numerical            numerical            numerical             numerical           numerical            numerical              numerical            numerical           numerical             numerical            numerical             numerical                numerical           numerical           numerical            numerical            numerical             numerical            numerical            numerical           numerical            numerical                numerical             numerical              numerical             numerical             numerical             numerical              numerical            numerical             numerical               numerical             numerical            numerical              numerical             numerical              numerical                 numerical            numerical            numerical             numerical             numerical              numerical             numerical             numerical            numerical             numerical               numerical            numerical             numerical            numerical            numerical            numerical             numerical           numerical            numerical              numerical            numerical           numerical             numerical            numerical             numerical                numerical           numerical           numerical            numerical            numerical             numerical            numerical            numerical           numerical            numerical               numerical            numerical             numerical            numerical            numerical            numerical             numerical           numerical            numerical              numerical            numerical           numerical             numerical            numerical             numerical                numerical           numerical           numerical            numerical            numerical             numerical            numerical            numerical           numerical            numerical 0 870000 NL ATL barkele01 ATL 1 1 ALS198108090 0 AL NULL CLE 1 1 1 1 0 1 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 10 AL 1 CLE 2 2 3 0 NULL NULL NULL NULL NULL NULL 10 P AL 1 CLE 1 2 2 3 0 NULL NULL NULL NULL NULL NULL NULL NULL 10 AL 1 CLE 2 2 3 0 NULL NULL NULL NULL NULL NULL 1 1 1 1985 1985 nan nan nan nan nan nan nan nan nan nan 0 0 1 0 38 6 0 36 1 21 36 2 0 0 0 2 1 0 0 4 19 0.5 0 23 3 0 21.7 0.5 2.7 21.7 1.5 0 0 0 1.5 0.5 0 0 3 12 0 0 8 0 0 2 0 0 2 1 0 0 0 1 0 0 0 2 5 nan nan nan nan nan -0.6626 nan 2.773 -0.6626 nan nan nan nan nan nan nan nan nan nan 0.7071 0 21.2132 4.2426 0 11.1759 0.7071 6.7007 11.1759 0.7071 0 0 0 0.7071 0.7071 0 0 1.4142 9.8995 1 0 46 6 0 217 1 27 217 3 0 0 0 3 1 0 0 6 24 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 34 3 2 36 36 739 23 16.2 1.2 1.2 21.7 16.5 361.9 7.1 2 0 0 2 0 45 0 0.4213 0.6014 -0.4725 -0.6626 0.0872 0.3367 1.8029 10.0863 0.9189 0.9189 11.1759 13.0491 249.7009 6.4713 162 12 12 217 165 3619 71 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 92 1052 1 10 114 5 36 15 36 237 3 17 3 739 13 127 3 187 4 19 14 0 45.9 514.1 0.2 3.5 56.1 3.3 21.7 2.5 16.5 115.1 1.8 8 1.8 361.9 6.6 61.3 0.7 90.6 0.5 7 6.2 0 6 56 0 0 4 2 2 0 0 7 0 0 0 45 0 4 0 7 0 1 1 0 0.3966 0.2943 1.7788 0.7966 0.08345 0.2342 -0.6626 2.2162 0.0872 0.09068 -0.6014 0.2625 -0.6606 0.3367 -0.009864 0.1508 1.7178 0.3541 2.8525 0.9886 0.6971 0 29.7263 351.6881 0.4216 4.0069 39.7309 0.9487 11.1759 4.9497 13.0491 78.3162 0.9189 6.7987 1.1353 249.7009 4.5019 43.5917 0.9487 63.9986 1.2693 6.0369 4.0497 0 459 5141 2 35 561 33 217 25 165 1151 18 80 18 3619 66 613 7 906 5 70 62 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 550000 NL ATL bedrost01 ATL 0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 1 Rookie of the Year NL 120 1 1 1 4 NL 1 ATL 1 1 1 1 NL NLCS ATL 1 1 1 4 P NL 1 ATL 1 1 1 1 1 P NL NLCS ATL 1 1 1 1 4 NL 1 ATL 1 1 1 1 NL NLCS ATL 1 1 1 1 1 1 1985 1985 4 0 4 0 4 0 nan nan nan nan 4 0 0 0 26 1 0 70 0 70 70 2 0 0 0 0 0 0 0 4 9 0 0 16 0.25 0 47.25 0 47.25 47.25 1.25 0 0 0 0 0 0 0 1.5 5.75 0 0 2 0 0 15 0 15 15 0 0 0 0 0 0 0 0 0 1 0 0 -1.1105 2. 0 -0.7352 0 -0.7352 -0.7352 -0.8546 0 0 0 0 0 0 0 1.5396 -1.1985 0 0 10.0995 0.5 0 25.1048 0 25.1048 25.1048 0.9574 0 0 0 0 0 0 0 1.7321 3.4034 0 0 64 1 0 189 0 189 189 5 0 0 0 0 0 0 0 6 23 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 16 2 1 70 4 413 12 10 1.25 0.5 47.25 2.25 274.25 4.5 2 0 0 15 1 73 1 -0.6325 -0.8546 0 -0.7352 0.3704 -0.9572 1.5966 6.3246 0.9574 0.5774 25.1048 1.5 150.163 5.1962 40 5 2 189 9 1097 18 0 0 0 0 2 0 3 0 0 0 0 0 0 0 2 0 3 0 0 0 0 0 0 0 2 0 3 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 2 0 3 0 0 0 0 57 567 0 0 48 4 70 52 4 102 4 11 8 413 10 50 0 123 19 9 4 0 39 380.5 0 0 29.75 2.75 47.25 28.75 2.25 70.5 2.5 6.25 5 274.25 6 31.5 0 81.75 10.25 6.75 1.5 0 15 106 0 0 12 2 15 5 1 15 1 2 2 73 2 14 0 9 0 1 0 0 -0.6325 -0.9498 0 0 0.06631 0.8546 -0.7352 -0.0778 0.3704 -1.128 0 0.3579 0 -0.9572 0 0.1248 0 -1.353 -0.5695 -1.9137 0.8546 0 18.9737 205.4629 0 0 15.9243 0.9574 25.1048 19.2072 1.5 40.7145 1.7321 3.7749 2.4495 150.163 3.266 16.0935 0 51.7518 7.8049 3.8622 1.9149 0 156 1522 0 0 119 11 189 115 9 282 10 25 20 1097 24 126 0 327 41 27 6 0 1 7 0 0 2 18 2 0 0 0 3 0 0 0 3 0 2 1 0 0 2 0 0 1 0 1 7 0 0 2 18 2 0 0 0 3 0 0 0 3 0 2 1 0 0 2 0 0 1 0 1 7 0 0 2 18 2 0 0 0 3 0 0 0 3 0 2 1 0 0 2 0 0 1 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 1 7 0 0 2 18 2 0 0 0 3 0 0 0 3 0 2 1 0 0 2 0 0 1 2 545000 NL ATL benedbr01 ATL 2 1 ALS198108090 0 NL NULL ATL 1 2 1 1 0 1 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 7 NL 1 ATL 1 1 1 1 NL NLCS ATL 1 1 1 7 C NL 1 ATL 1 1 1 1 1 C NL NLCS ATL 1 1 1 1 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 1 1 1 1985 1985 nan nan nan nan nan nan nan nan nan nan 0 0 14 1 423 61 4 134 12 134 134 126 3 5 16 43 44 4 4 13 40 10.1429 0.7143 288.4286 33.1429 2.2857 93.5714 7.7143 93.5714 93.5714 73.7143 1.2857 2.2857 6.4286 23.4286 28.1429 1.5714 2.2857 4.5714 24.2857 2 0 52 6 0 22 0 22 22 13 0 0 2 3 1 0 0 0 6 -1.6097 -1.2296 -1.1677 0.08628 -0.7065 -1.2212 -1.1445 -1.2212 -1.2212 -0.4194 0.6817 0.0508 1.3403 -0.08253 -0.8802 1.0788 -0.7065 1.2214 -0.1462 4.0591 0.488 126.3683 16.0979 1.3801 37.3624 4.2706 37.3624 37.3624 36.5227 1.2536 1.8898 4.9952 13.1891 15.6677 1.3973 1.3801 4.4293 11.3242 71 5 2019 232 16 655 54 655 655 516 9 16 45 164 197 11 16 32 170 1 0 8 2 0 3 0 2 0 0 0 1 0 0 0 0 1 1 0 8 2 0 3 0 2 0 0 0 1 0 0 0 0 1 1 0 8 2 0 3 0 2 0 0 0 1 0 0 0 0 1 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 1 0 8 2 0 3 0 2 0 0 0 1 0 0 0 0 1 91 12 7 134 128 3320 738 57 5.7143 5.4286 93.5714 88 2309.8571 453.5714 14 1 1 22 16 423 81 -0.4571 0.4045 -1.7671 -1.2212 -1.3323 -1.3386 -0.6846 28.1721 3.9881 2.1492 37.3624 37.0945 976.072 208.6032 399 40 38 655 616 16169 3175 2 0 0 0 3 3 76 16 1 0 2 0 0 0 3 3 76 16 1 0 2 0 0 0 3 3 76 16 1 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 2 0 0 0 3 3 76 16 1 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 625000 NL ATL ceronri01 ATL 0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 1 TSN All-Star AL C 1 1 1 1 MVP AL 392 1 1 1 10 AL 1 NYA 1 1 3 4 AL ALCS NYA 1 3 1 0 NULL NULL NULL NULL NULL NULL NULL NULL 4 C AL ALCS NYA 1 1 3 1 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 1 1 1 1985 1985 77 1 77 1 77 1 nan nan nan nan 77 1 30 4 519 37 4 147 14 147 147 144 6 14 2 70 85 1 10 8 56 10.3 1.2 229.8 15.4 1.4 69.4 6.1 69.4 69.4 54.8 1.1 3.6 0.5 22.9 26.1 0.3 2.3 3.1 23.7 0 0 12 0 0 7 0 7 7 2 0 0 0 1 0 0 0 0 0 1.1975 1.0006 0.3925 0.5283 0.4414 0.2422 0.3565 0.2422 0.2422 0.7775 2.7111 1.8908 1.1785 1.201 1.4726 1.0351 1.4583 0.828 0.3548 10.4142 1.6865 173.4735 12.4651 1.5776 48.9335 4.7246 48.9335 48.9335 46.0019 1.792 4.2479 0.7071 21.8553 26.9008 0.483 3.401 2.2828 17.1985 103 12 2298 154 14 694 61 694 694 548 11 36 5 229 261 3 23 31 237 2 0 21 4 0 6 1 6 1 1 1 2 5 0 0 1 2 0.75 0 15.25 1 0 4.25 0.25 3.75 0.25 0.75 0.25 1.25 2.5 0 0 0.25 1.25 0 0 10 0 0 3 0 1 0 0 0 1 0 0 0 0 0 0.8546 0 0.158 2. 0 0.3704 2. -0.7133 2. -2. 2. 2. 0 0 0 2. -0.8546 0.9574 0 5.1235 2 0 1.5 0.5 2.0616 0.5 0.5 0.5 0.5 2.0817 0 0 0.5 0.9574 3 0 61 4 0 17 1 15 1 3 1 5 10 0 0 1 5 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 4 5 1 1 6 6 153 42 6 0 2.75 2 0.25 0.25 4.25 4.25 110.25 30.25 3 0 1 0 0 0 3 3 75 14 1 0 -0.3704 1.1903 2. 2. 0.3704 0.3704 0.1702 -0.3427 1.1903 0 1.5 2.1602 0.5 0.5 1.5 1.5 39.6768 14.0564 2.1602 0 11 8 1 1 17 17 441 121 12 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 800000 NL ATL chambch01 ATL 1 1 NLS197607130 0 AL NULL NYA 1 1 1 1 0 1 3 Gold Glove AL 1B 3 1 1 4 MVP AL 336 2 2 3 15 AL 1 NYA 2 2 3 7 AL ALCS NYA 2 3 2 0 NULL NULL NULL NULL NULL NULL NULL NULL 7 1B AL ALCS NYA 1 2 3 2 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 1 1 1 1985 1985 71 11 21.25 2.75 1 0 1.8957 2. 33.4701 5.5 85 11 38 6 641 63 8 162 22 162 162 188 5 20 15 90 96 7 6 4 83 25.1333 2.8 485.2 39.9333 2.2 131.7333 10.6 131.7333 131.7333 135.4 1.8 12 6.0667 58.8667 62.4667 2.6667 3.6 1.2667 58.6 4 0 67 5 0 17 1 17 17 22 0 0 1 8 7 0 0 0 5 -0.8293 0.3528 -1.7276 -0.4725 1.414 -2.2755 0.7222 -2.2755 -2.2755 -1.3168 0.6688 -0.2249 0.5322 -0.7309 -0.5715 0.7373 -0.5448 0.7649 -1.3754 8.9192 1.7403 145.3912 16.0911 2.4842 37.2721 4.9828 37.2721 37.2721 42.6946 1.8205 6.0356 4.8176 20.9995 24.1805 2.2573 1.9928 1.2228 19.7042 377 42 7278 599 33 1976 159 1976 1976 2031 27 180 91 883 937 40 54 19 879 2 1 24 3 0 6 1 11 1 2 2 5 8 2 1 0 4 0.5714 0.1429 16.2857 0.7143 0 4.2857 0.4286 4.5714 0.1429 0.4286 0.4286 1.7143 2.1429 0.2857 0.1429 0 2 0 0 10 0 0 3 0 0 0 0 0 0 0 0 0 0 0 1.1145 2.6458 0.3056 1.7836 0 0.2489 0.3742 0.504 2.6458 1.7598 1.7598 1.0961 1.5735 2.6458 2.6458 0 0.3928 0.7868 0.378 5.0238 1.1127 0 1.1127 0.5345 3.8668 0.378 0.7868 0.7868 1.976 2.9681 0.7559 0.378 0 1.5275 4 1 114 5 0 30 3 32 1 3 3 12 15 2 1 0 14 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 6 nan 6 1 6 6 168 55 nan 0 3.2857 nan 2.5714 0.2857 4.2857 4.2857 114 34.7143 nan 0 1 nan 0 0 3 3 76 17 nan 0 0.0508 nan 0.6545 1.2296 0.2489 0.2489 0.528 0.5058 nan 0 1.8898 nan 1.9881 0.488 1.1127 1.1127 32.3471 13.5119 nan 0 23 0 18 2 30 30 798 243 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 18567 4875000 NL WAS strasst01 WAS 0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 1 1 6 2012 2012 nan nan nan nan nan nan nan nan nan nan 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 18568 750000 NL WAS tracych01 WAS 0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 8 NL 1 ARI 1 2 3 0 NULL NULL NULL NULL NULL NULL 24 1B NL 1 ARI 6 1 2 3 0 NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 1 1 6 2012 2012 nan nan nan nan nan nan nan nan nan nan 0 0 41 4 597 54 3 154 11 154 154 168 8 27 7 91 80 5 6 1 129 20.125 1.125 310.5 27 0.625 96.625 6.25 96.625 121.2 86.25 2.25 9.875 3.125 38 41.625 1.375 3.25 0.375 54.125 2 0 44 5 0 28 0 28 76 11 0 0 0 5 5 0 0 0 15 0.2672 1.0284 0.168 0.1675 1.9604 -0.1779 -0.1983 -0.1779 -0.6095 0.2779 1.5549 1.0695 0.04936 0.8056 0.05028 1.2615 -0.3143 0.6441 1.3153 13.5376 1.6421 197.8253 17.5987 1.0607 47.9403 4.3342 47.9403 36.2726 59.6436 2.8158 9.1875 2.4165 30.4959 26.5219 1.8468 2.1213 0.5175 36.3492 161 9 2484 216 5 773 50 773 606 690 18 79 25 304 333 11 26 3 433 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 260 75 25 147 143 3834 706 39.1 12.4 3.7 30.9583 32.9 862.9 118.95 0 0 0 1 0 3 2 2.5607 1.974 2.7175 1.7042 1.4604 1.5566 2.1377 77.8716 20.7171 7.4063 41.8345 41.7081 1103.0638 201.6588 782 248 74 743 658 17258 2379 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 18569 4000000 NL WAS wangch01 WAS 0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 2 Cy Young AL 140 2 1 2 6 AL 1 NYA 2 1 2 0 NULL NULL NULL NULL NULL NULL 6 P AL 1 NYA 1 2 1 2 3 P AL ALDS2 NYA 1 1 2 1 6 AL 1 NYA 2 1 2 3 AL ALDS2 NYA 1 2 1 1 1 6 2012 2012 51 0 26.5 0 2 0 nan nan 34.6482 0 53 0 0 0 19 1 0 34 1 11 34 1 0 0 0 1 1 0 0 1 10 0 0 5.5 0.1667 0 20 0.1667 2.8333 13 0.1667 0 0 0 0.1667 0.1667 0 0 0.1667 3 0 0 1 0 0 11 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 2.3279 2.4495 0 0.7801 2.4495 2.3973 0.9459 2.4495 0 0 0 2.4495 2.4495 0 0 2.4495 1.7227 0 0 6.6858 0.4082 0 9.6954 0.4082 4.0208 13.7659 0.4082 0 0 0 0.4082 0.4082 0 0 0.4082 3.6878 0 0 33 1 0 120 1 17 65 1 0 0 0 1 1 0 0 1 18 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 42 4 1 34 33 654 15 23.3333 1.8333 0.5 20 19.1667 366.5 7.1667 6 0 0 11 9 126 3 0.0138 0.02609 0 0.7801 0.6705 0.4743 0.9105 16.4398 1.7224 0.5477 9.6954 10.0083 215.9896 4.916 140 11 3 120 115 2199 43 2 1 0 1 2 2 20 0 0 0 1.3333 1 0 0.3333 1.3333 1.3333 19 0 0 0 1 1 0 0 1 1 17 0 0 0 1.7321 0 0 1.7321 1.7321 1.7321 -1.7321 0 0 0 0.5774 0 0 0.5774 0.5774 0.5774 1.7321 0 0 0 4 3 0 1 4 4 57 0 0 0 nan 59 900 1 2 88 9 34 2 33 233 8 12 4 654 7 92 1 104 1 19 9 nan 35 513.5 0.3333 0.6667 56.3333 4.5 20 0.5 19.1667 128 3.6667 8.1667 1.6667 366.5 4.8333 59.8333 0.1667 55.8333 0.1667 9.8333 3.8333 nan 13 206 0 0 28 3 11 0 9 66 1 4 0 126 2 35 0 25 0 1 0 nan 0.2052 0.5187 0.9682 0.8573 0.5011 2.188 0.7801 1.5367 0.6705 0.8268 0.9639 -0.2683 0.84 0.4743 -0.6384 0.6152 2.4495 0.7902 2.4495 0.4422 0.7815 nan 17.9666 288.1553 0.5164 0.8165 23.6192 2.2583 9.6954 0.8367 10.0083 71.1337 2.7325 2.6394 1.5055 215.9896 1.9408 23.1553 0.4082 29.9294 0.4082 7.5741 3.1885 0 210 3081 2 4 338 27 120 3 115 768 22 49 10 2199 29 359 1 335 1 59 23 0 4 34 0 0 12 19 2 0 1 2 14 2 3 0 20 2 12 0 2 0 4 0 1 0 0 1.6667 30 0 0 5.3333 8 1.3333 0 0.6667 1.3333 9.3333 1 1.6667 0 19 1 6.3333 0 0.6667 0 2.3333 0 0.3333 0 0 0 27 0 0 1 1 1 0 0 1 6 0 1 0 17 0 3 0 0 0 1 0 0 0 0 1.2933 1.1521 0 0 1.5078 1.5454 1.7321 0 -1.7321 1.7321 1.2933 0 1.7321 0 -1.7321 0 1.6523 0 1.7321 0 0.9352 0 1.7321 0 0 2.0817 3.6056 0 0 5.8595 9.6437 0.5774 0 0.5774 0.5774 4.1633 1 1.1547 0 1.7321 1 4.9329 0 1.1547 0 1.5275 0 0.5774 0 0 5 90 0 0 16 24 4 0 2 4 28 3 5 0 57 3 19 0 2 0 7 0 1 0 18570 13571428 NL WAS werthja01 WAS 1 1 NLS200907140 0 NL NULL PHI 1 1 1 1 0 1 0 NULL NULL NULL NULL NULL NULL 2 MVP NL 448 1 1 1 9 NL 1 PHI 2 1 4 10 NL NLCS PHI 1 4 2 35 CF NL 1 PHI 6 2 1 4 12 RF NL NLDS2 PHI 2 1 4 2 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 1 1 6 2012 2012 52 0 31 0 10 0 nan nan 29.6985 0 62 0 46 3 571 91 3 159 11 159 150 164 10 36 8 106 99 20 9 2 160 18.2222 1.7778 342.2222 48.3333 1.5556 102.7778 5.4444 102.7778 87.1429 90.4444 4.5556 15.5556 2.5556 55.7778 51.5556 10.6667 3.1111 0.4444 98.5556 2 0 46 3 0 15 0 15 15 10 0 0 0 4 6 1 0 0 11 0.9258 -0.1885 -0.3723 -0.2173 0.09246 -0.6935 0.1026 -0.6935 -0.4489 -0.276 0.05138 0.3008 0.9426 -0.174 -0.1046 0.02164 1.1347 1.5007 -0.563 13.6086 1.0929 204.7778 31.4046 1.236 53.6954 4.9526 53.6954 50.5979 55.471 3.504 12.2282 3.0046 35.5239 30.725 7.8581 2.8916 0.7265 55.2746 164 16 3080 435 14 925 49 925 610 814 41 140 23 502 464 96 28 4 887 3 1 21 6 0 6 1 8 1 3 2 5 6 3 1 0 7 0.9 0.2 15.3 2.7 0 4.4 0.3 4.1 0.1 1.3 0.4 3 2.6 0.5 0.1 0 5.2 0 0 3 0 0 2 0 0 0 0 0 0 0 0 0 0 1 1.2043 1.7788 -1.6814 0.3071 0 -0.5435 1.0351 -0.2578 3.1623 -0.04206 1.6577 -0.5031 0.2005 2.2698 3.1623 0 -0.9788 1.1972 0.4216 5.1001 2.0028 0 1.2649 0.483 2.079 0.3162 1.0593 0.6992 1.4907 2.0656 0.9718 0.3162 0 1.9889 9 2 153 27 0 44 3 41 1 13 4 30 26 5 1 0 52 11 4 8 157 152 4138 353 3.4545 0.9697 1.5152 48.6 44.3636 1205.3939 98.2121 0 0 0 1 0 3 1 0.702 1.0714 1.3512 0.969 1.0709 1.0889 1.1254 3.7757 1.2115 2.1083 50.9678 50.33 1351.3185 106.7728 114 32 50 1701 1464 39778 3241 1 nan 1 1 6 6 158 14 nan 0 0.3333 nan 0.1667 0.08333 3.75 3.5833 94.5833 7.0833 nan 0 0 nan 0 0 1 0 2 0 nan 0 0.8124 nan 2.0552 3.4641 -0.5741 -0.794 -0.7422 -0.4648 nan 0 0.4924 nan 0.3892 0.2887 1.8647 2.1515 54.9669 4.3788 nan 0 4 0 2 1 45 43 1135 85 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 18571 12000000 NL WAS zimmery01 WAS 1 1 NLS200907140 0 NL NULL WAS 1 1 1 1 0 1 4 Silver Slugger NL 3B 3 2 1 3 MVP NL 448 2 1 2 7 NL 1 WAS 1 1 1 0 NULL NULL NULL NULL NULL NULL 11 3B NL 1 WAS 3 1 1 1 0 NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 1 1 6 2012 2012 101 10 40.3333 3.3333 2 0 1.5571 1.7321 53.1445 5.7735 121 10 47 5 653 72 8 162 26 162 162 178 4 33 9 110 110 11 9 1 125 30.5714 2 469 48.2857 1.7143 120.7143 15.1429 120.7143 109.2 135.2857 2.1429 18.2857 4.2857 69.5714 71.1429 3.5714 4.4286 0.1429 88.2857 10 0 58 3 0 20 1 20 20 23 0 0 0 6 6 0 1 0 12 -0.3386 0.4601 -1.5444 -1.0907 2.4863 -1.5169 -0.5899 -1.5169 -1.0083 -1.5931 -0.3521 -0.5231 0.09144 -0.8753 -0.812 1.7193 0.844 2.6458 -1.2173 13.0749 1.8257 205.7409 24.8912 2.8115 50.8911 7.9252 50.8911 57.2425 56.1359 1.3452 10.7194 3.2514 35.6878 37.5297 3.5989 2.4398 0.378 40.2977 214 14 3283 338 12 845 106 845 546 947 15 128 30 487 498 25 31 1 618 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 348 39 23 161 161 4295 152 200.5 21.125 12 75.2727 102.625 2709.5 83 4 0 0 1 1 27 3 -0.7079 -0.5983 -0.4494 0.02141 -0.9297 -0.9306 -0.4548 126.256 12.9553 7.8194 71.5222 64.1871 1680.0757 55.9745 1604 169 96 828 821 21676 664 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 <p>     18572 rows x 695 columns     memory usage: 97.24 MB     name: featuretools_train     type: getml.DataFrame </p> In\u00a0[60]: Copied! <pre>featuretools_test.set_role(\"salary\", getml.data.roles.target)\nfeaturetools_test.set_role(featuretools_test.roles.unused_float, getml.data.roles.numerical)\nfeaturetools_test.set_role(featuretools_test.roles.unused_string, getml.data.roles.categorical)\n\nfeaturetools_test\n</pre> featuretools_test.set_role(\"salary\", getml.data.roles.target) featuretools_test.set_role(featuretools_test.roles.unused_float, getml.data.roles.numerical) featuretools_test.set_role(featuretools_test.roles.unused_string, getml.data.roles.categorical)  featuretools_test Out[60]: name  salary lgID        teamIDCat   playerID    teamID      COUNT(allstarfull) MODE(allstarfull.GP) MODE(allstarfull.gameID) MODE(allstarfull.gameNum) MODE(allstarfull.lgID) MODE(allstarfull.startingPos) MODE(allstarfull.teamID) NUM_UNIQUE(allstarfull.GP) NUM_UNIQUE(allstarfull.gameID) NUM_UNIQUE(allstarfull.gameNum) NUM_UNIQUE(allstarfull.lgID) NUM_UNIQUE(allstarfull.startingPos) NUM_UNIQUE(allstarfull.teamID) COUNT(awardsplayers) MODE(awardsplayers.awardID) MODE(awardsplayers.lgID) MODE(awardsplayers.notes) NUM_UNIQUE(awardsplayers.awardID) NUM_UNIQUE(awardsplayers.lgID) NUM_UNIQUE(awardsplayers.notes) COUNT(awardsshareplayers) MODE(awardsshareplayers.awardID) MODE(awardsshareplayers.lgID) MODE(awardsshareplayers.pointsMax) NUM_UNIQUE(awardsshareplayers.awardID) NUM_UNIQUE(awardsshareplayers.lgID) NUM_UNIQUE(awardsshareplayers.pointsMax) COUNT(batting) MODE(batting.lgID) MODE(batting.stint) MODE(batting.teamID) NUM_UNIQUE(batting.lgID) NUM_UNIQUE(batting.stint) NUM_UNIQUE(batting.teamID) COUNT(battingpost) MODE(battingpost.lgID) MODE(battingpost.round) MODE(battingpost.teamID) NUM_UNIQUE(battingpost.lgID) NUM_UNIQUE(battingpost.round) NUM_UNIQUE(battingpost.teamID) COUNT(fielding) MODE(fielding.POS) MODE(fielding.lgID) MODE(fielding.stint) MODE(fielding.teamID) NUM_UNIQUE(fielding.POS) NUM_UNIQUE(fielding.lgID) NUM_UNIQUE(fielding.stint) NUM_UNIQUE(fielding.teamID) COUNT(fieldingpost) MODE(fieldingpost.POS) MODE(fieldingpost.lgID) MODE(fieldingpost.round) MODE(fieldingpost.teamID) NUM_UNIQUE(fieldingpost.POS) NUM_UNIQUE(fieldingpost.lgID) NUM_UNIQUE(fieldingpost.round) NUM_UNIQUE(fieldingpost.teamID) COUNT(pitching) MODE(pitching.lgID) MODE(pitching.stint) MODE(pitching.teamID) NUM_UNIQUE(pitching.lgID) NUM_UNIQUE(pitching.stint) NUM_UNIQUE(pitching.teamID) COUNT(pitchingpost) MODE(pitchingpost.lgID) MODE(pitchingpost.round) MODE(pitchingpost.teamID) NUM_UNIQUE(pitchingpost.lgID) NUM_UNIQUE(pitchingpost.round) NUM_UNIQUE(pitchingpost.teamID) DAY(year)   MONTH(year) WEEKDAY(year) YEAR(year)     yearID MAX(awardsshareplayers.pointsWon) MAX(awardsshareplayers.votesFirst) MEAN(awardsshareplayers.pointsWon) MEAN(awardsshareplayers.votesFirst) MIN(awardsshareplayers.pointsWon) MIN(awardsshareplayers.votesFirst) SKEW(awardsshareplayers.pointsWon) SKEW(awardsshareplayers.votesFirst) STD(awardsshareplayers.pointsWon) STD(awardsshareplayers.votesFirst) SUM(awardsshareplayers.pointsWon) SUM(awardsshareplayers.votesFirst) MAX(batting.2B) MAX(batting.3B) MAX(batting.AB) MAX(batting.BB) MAX(batting.CS) MAX(batting.G) MAX(batting.GIDP) MAX(batting.G_batting) MAX(batting.G_old) MAX(batting.H) MAX(batting.HBP) MAX(batting.HR) MAX(batting.IBB) MAX(batting.R) MAX(batting.RBI) MAX(batting.SB) MAX(batting.SF) MAX(batting.SH) MAX(batting.SO) MEAN(batting.2B) MEAN(batting.3B) MEAN(batting.AB) MEAN(batting.BB) MEAN(batting.CS) MEAN(batting.G) MEAN(batting.GIDP) MEAN(batting.G_batting) MEAN(batting.G_old) MEAN(batting.H) MEAN(batting.HBP) MEAN(batting.HR) MEAN(batting.IBB) MEAN(batting.R) MEAN(batting.RBI) MEAN(batting.SB) MEAN(batting.SF) MEAN(batting.SH) MEAN(batting.SO) MIN(batting.2B) MIN(batting.3B) MIN(batting.AB) MIN(batting.BB) MIN(batting.CS) MIN(batting.G) MIN(batting.GIDP) MIN(batting.G_batting) MIN(batting.G_old) MIN(batting.H) MIN(batting.HBP) MIN(batting.HR) MIN(batting.IBB) MIN(batting.R) MIN(batting.RBI) MIN(batting.SB) MIN(batting.SF) MIN(batting.SH) MIN(batting.SO) SKEW(batting.2B) SKEW(batting.3B) SKEW(batting.AB) SKEW(batting.BB) SKEW(batting.CS) SKEW(batting.G) SKEW(batting.GIDP) SKEW(batting.G_batting) SKEW(batting.G_old) SKEW(batting.H) SKEW(batting.HBP) SKEW(batting.HR) SKEW(batting.IBB) SKEW(batting.R) SKEW(batting.RBI) SKEW(batting.SB) SKEW(batting.SF) SKEW(batting.SH) SKEW(batting.SO) STD(batting.2B) STD(batting.3B) STD(batting.AB) STD(batting.BB) STD(batting.CS) STD(batting.G) STD(batting.GIDP) STD(batting.G_batting) STD(batting.G_old) STD(batting.H) STD(batting.HBP) STD(batting.HR) STD(batting.IBB) STD(batting.R) STD(batting.RBI) STD(batting.SB) STD(batting.SF) STD(batting.SH) STD(batting.SO) SUM(batting.2B) SUM(batting.3B) SUM(batting.AB) SUM(batting.BB) SUM(batting.CS) SUM(batting.G) SUM(batting.GIDP) SUM(batting.G_batting) SUM(batting.G_old) SUM(batting.H) SUM(batting.HBP) SUM(batting.HR) SUM(batting.IBB) SUM(batting.R) SUM(batting.RBI) SUM(batting.SB) SUM(batting.SF) SUM(batting.SH) SUM(batting.SO) MAX(battingpost.2B) MAX(battingpost.3B) MAX(battingpost.AB) MAX(battingpost.BB) MAX(battingpost.CS) MAX(battingpost.G) MAX(battingpost.GIDP) MAX(battingpost.H) MAX(battingpost.HBP) MAX(battingpost.HR) MAX(battingpost.IBB) MAX(battingpost.R) MAX(battingpost.RBI) MAX(battingpost.SB) MAX(battingpost.SF) MAX(battingpost.SH) MAX(battingpost.SO) MEAN(battingpost.2B) MEAN(battingpost.3B) MEAN(battingpost.AB) MEAN(battingpost.BB) MEAN(battingpost.CS) MEAN(battingpost.G) MEAN(battingpost.GIDP) MEAN(battingpost.H) MEAN(battingpost.HBP) MEAN(battingpost.HR) MEAN(battingpost.IBB) MEAN(battingpost.R) MEAN(battingpost.RBI) MEAN(battingpost.SB) MEAN(battingpost.SF) MEAN(battingpost.SH) MEAN(battingpost.SO) MIN(battingpost.2B) MIN(battingpost.3B) MIN(battingpost.AB) MIN(battingpost.BB) MIN(battingpost.CS) MIN(battingpost.G) MIN(battingpost.GIDP) MIN(battingpost.H) MIN(battingpost.HBP) MIN(battingpost.HR) MIN(battingpost.IBB) MIN(battingpost.R) MIN(battingpost.RBI) MIN(battingpost.SB) MIN(battingpost.SF) MIN(battingpost.SH) MIN(battingpost.SO) SKEW(battingpost.2B) SKEW(battingpost.3B) SKEW(battingpost.AB) SKEW(battingpost.BB) SKEW(battingpost.CS) SKEW(battingpost.G) SKEW(battingpost.GIDP) SKEW(battingpost.H) SKEW(battingpost.HBP) SKEW(battingpost.HR) SKEW(battingpost.IBB) SKEW(battingpost.R) SKEW(battingpost.RBI) SKEW(battingpost.SB) SKEW(battingpost.SF) SKEW(battingpost.SH) SKEW(battingpost.SO) STD(battingpost.2B) STD(battingpost.3B) STD(battingpost.AB) STD(battingpost.BB) STD(battingpost.CS) STD(battingpost.G) STD(battingpost.GIDP) STD(battingpost.H) STD(battingpost.HBP) STD(battingpost.HR) STD(battingpost.IBB) STD(battingpost.R) STD(battingpost.RBI) STD(battingpost.SB) STD(battingpost.SF) STD(battingpost.SH) STD(battingpost.SO) SUM(battingpost.2B) SUM(battingpost.3B) SUM(battingpost.AB) SUM(battingpost.BB) SUM(battingpost.CS) SUM(battingpost.G) SUM(battingpost.GIDP) SUM(battingpost.H) SUM(battingpost.HBP) SUM(battingpost.HR) SUM(battingpost.IBB) SUM(battingpost.R) SUM(battingpost.RBI) SUM(battingpost.SB) SUM(battingpost.SF) SUM(battingpost.SH) SUM(battingpost.SO) MAX(fielding.A) MAX(fielding.DP) MAX(fielding.E) MAX(fielding.G) MAX(fielding.GS) MAX(fielding.InnOuts) MAX(fielding.PO) MEAN(fielding.A) MEAN(fielding.DP) MEAN(fielding.E) MEAN(fielding.G) MEAN(fielding.GS) MEAN(fielding.InnOuts) MEAN(fielding.PO) MIN(fielding.A) MIN(fielding.DP) MIN(fielding.E) MIN(fielding.G) MIN(fielding.GS) MIN(fielding.InnOuts) MIN(fielding.PO) SKEW(fielding.A) SKEW(fielding.DP) SKEW(fielding.E) SKEW(fielding.G) SKEW(fielding.GS) SKEW(fielding.InnOuts) SKEW(fielding.PO) STD(fielding.A) STD(fielding.DP) STD(fielding.E) STD(fielding.G) STD(fielding.GS) STD(fielding.InnOuts) STD(fielding.PO) SUM(fielding.A) SUM(fielding.DP) SUM(fielding.E) SUM(fielding.G) SUM(fielding.GS) SUM(fielding.InnOuts) SUM(fielding.PO) MAX(fieldingpost.A) MAX(fieldingpost.CS) MAX(fieldingpost.DP) MAX(fieldingpost.E) MAX(fieldingpost.G) MAX(fieldingpost.GS) MAX(fieldingpost.InnOuts) MAX(fieldingpost.PO) MAX(fieldingpost.SB) MAX(fieldingpost.TP) MEAN(fieldingpost.A) MEAN(fieldingpost.CS) MEAN(fieldingpost.DP) MEAN(fieldingpost.E) MEAN(fieldingpost.G) MEAN(fieldingpost.GS) MEAN(fieldingpost.InnOuts) MEAN(fieldingpost.PO) MEAN(fieldingpost.SB) MEAN(fieldingpost.TP) MIN(fieldingpost.A) MIN(fieldingpost.CS) MIN(fieldingpost.DP) MIN(fieldingpost.E) MIN(fieldingpost.G) MIN(fieldingpost.GS) MIN(fieldingpost.InnOuts) MIN(fieldingpost.PO) MIN(fieldingpost.SB) MIN(fieldingpost.TP) SKEW(fieldingpost.A) SKEW(fieldingpost.CS) SKEW(fieldingpost.DP) SKEW(fieldingpost.E) SKEW(fieldingpost.G) SKEW(fieldingpost.GS) SKEW(fieldingpost.InnOuts) SKEW(fieldingpost.PO) SKEW(fieldingpost.SB) SKEW(fieldingpost.TP) STD(fieldingpost.A) STD(fieldingpost.CS) STD(fieldingpost.DP) STD(fieldingpost.E) STD(fieldingpost.G) STD(fieldingpost.GS) STD(fieldingpost.InnOuts) STD(fieldingpost.PO) STD(fieldingpost.SB) STD(fieldingpost.TP) SUM(fieldingpost.A) SUM(fieldingpost.CS) SUM(fieldingpost.DP) SUM(fieldingpost.E) SUM(fieldingpost.G) SUM(fieldingpost.GS) SUM(fieldingpost.InnOuts) SUM(fieldingpost.PO) SUM(fieldingpost.SB) SUM(fieldingpost.TP) MAX(pitching.BAOpp) MAX(pitching.BB) MAX(pitching.BFP) MAX(pitching.BK) MAX(pitching.CG) MAX(pitching.ER) MAX(pitching.ERA) MAX(pitching.G) MAX(pitching.GF) MAX(pitching.GS) MAX(pitching.H) MAX(pitching.HBP) MAX(pitching.HR) MAX(pitching.IBB) MAX(pitching.IPouts) MAX(pitching.L) MAX(pitching.R) MAX(pitching.SHO) MAX(pitching.SO) MAX(pitching.SV) MAX(pitching.W) MAX(pitching.WP) MEAN(pitching.BAOpp) MEAN(pitching.BB) MEAN(pitching.BFP) MEAN(pitching.BK) MEAN(pitching.CG) MEAN(pitching.ER) MEAN(pitching.ERA) MEAN(pitching.G) MEAN(pitching.GF) MEAN(pitching.GS) MEAN(pitching.H) MEAN(pitching.HBP) MEAN(pitching.HR) MEAN(pitching.IBB) MEAN(pitching.IPouts) MEAN(pitching.L) MEAN(pitching.R) MEAN(pitching.SHO) MEAN(pitching.SO) MEAN(pitching.SV) MEAN(pitching.W) MEAN(pitching.WP) MIN(pitching.BAOpp) MIN(pitching.BB) MIN(pitching.BFP) MIN(pitching.BK) MIN(pitching.CG) MIN(pitching.ER) MIN(pitching.ERA) MIN(pitching.G) MIN(pitching.GF) MIN(pitching.GS) MIN(pitching.H) MIN(pitching.HBP) MIN(pitching.HR) MIN(pitching.IBB) MIN(pitching.IPouts) MIN(pitching.L) MIN(pitching.R) MIN(pitching.SHO) MIN(pitching.SO) MIN(pitching.SV) MIN(pitching.W) MIN(pitching.WP) SKEW(pitching.BAOpp) SKEW(pitching.BB) SKEW(pitching.BFP) SKEW(pitching.BK) SKEW(pitching.CG) SKEW(pitching.ER) SKEW(pitching.ERA) SKEW(pitching.G) SKEW(pitching.GF) SKEW(pitching.GS) SKEW(pitching.H) SKEW(pitching.HBP) SKEW(pitching.HR) SKEW(pitching.IBB) SKEW(pitching.IPouts) SKEW(pitching.L) SKEW(pitching.R) SKEW(pitching.SHO) SKEW(pitching.SO) SKEW(pitching.SV) SKEW(pitching.W) SKEW(pitching.WP) STD(pitching.BAOpp) STD(pitching.BB) STD(pitching.BFP) STD(pitching.BK) STD(pitching.CG) STD(pitching.ER) STD(pitching.ERA) STD(pitching.G) STD(pitching.GF) STD(pitching.GS) STD(pitching.H) STD(pitching.HBP) STD(pitching.HR) STD(pitching.IBB) STD(pitching.IPouts) STD(pitching.L) STD(pitching.R) STD(pitching.SHO) STD(pitching.SO) STD(pitching.SV) STD(pitching.W) STD(pitching.WP) SUM(pitching.BAOpp) SUM(pitching.BB) SUM(pitching.BFP) SUM(pitching.BK) SUM(pitching.CG) SUM(pitching.ER) SUM(pitching.ERA) SUM(pitching.G) SUM(pitching.GF) SUM(pitching.GS) SUM(pitching.H) SUM(pitching.HBP) SUM(pitching.HR) SUM(pitching.IBB) SUM(pitching.IPouts) SUM(pitching.L) SUM(pitching.R) SUM(pitching.SHO) SUM(pitching.SO) SUM(pitching.SV) SUM(pitching.W) SUM(pitching.WP) MAX(pitchingpost.BAOpp) MAX(pitchingpost.BB) MAX(pitchingpost.BFP) MAX(pitchingpost.BK) MAX(pitchingpost.CG) MAX(pitchingpost.ER) MAX(pitchingpost.ERA) MAX(pitchingpost.G) MAX(pitchingpost.GF) MAX(pitchingpost.GIDP) MAX(pitchingpost.GS) MAX(pitchingpost.H) MAX(pitchingpost.HBP) MAX(pitchingpost.HR) MAX(pitchingpost.IBB) MAX(pitchingpost.IPouts) MAX(pitchingpost.L) MAX(pitchingpost.R) MAX(pitchingpost.SF) MAX(pitchingpost.SH) MAX(pitchingpost.SHO) MAX(pitchingpost.SO) MAX(pitchingpost.SV) MAX(pitchingpost.W) MAX(pitchingpost.WP) MEAN(pitchingpost.BAOpp) MEAN(pitchingpost.BB) MEAN(pitchingpost.BFP) MEAN(pitchingpost.BK) MEAN(pitchingpost.CG) MEAN(pitchingpost.ER) MEAN(pitchingpost.ERA) MEAN(pitchingpost.G) MEAN(pitchingpost.GF) MEAN(pitchingpost.GIDP) MEAN(pitchingpost.GS) MEAN(pitchingpost.H) MEAN(pitchingpost.HBP) MEAN(pitchingpost.HR) MEAN(pitchingpost.IBB) MEAN(pitchingpost.IPouts) MEAN(pitchingpost.L) MEAN(pitchingpost.R) MEAN(pitchingpost.SF) MEAN(pitchingpost.SH) MEAN(pitchingpost.SHO) MEAN(pitchingpost.SO) MEAN(pitchingpost.SV) MEAN(pitchingpost.W) MEAN(pitchingpost.WP) MIN(pitchingpost.BAOpp) MIN(pitchingpost.BB) MIN(pitchingpost.BFP) MIN(pitchingpost.BK) MIN(pitchingpost.CG) MIN(pitchingpost.ER) MIN(pitchingpost.ERA) MIN(pitchingpost.G) MIN(pitchingpost.GF) MIN(pitchingpost.GIDP) MIN(pitchingpost.GS) MIN(pitchingpost.H) MIN(pitchingpost.HBP) MIN(pitchingpost.HR) MIN(pitchingpost.IBB) MIN(pitchingpost.IPouts) MIN(pitchingpost.L) MIN(pitchingpost.R) MIN(pitchingpost.SF) MIN(pitchingpost.SH) MIN(pitchingpost.SHO) MIN(pitchingpost.SO) MIN(pitchingpost.SV) MIN(pitchingpost.W) MIN(pitchingpost.WP) SKEW(pitchingpost.BAOpp) SKEW(pitchingpost.BB) SKEW(pitchingpost.BFP) SKEW(pitchingpost.BK) SKEW(pitchingpost.CG) SKEW(pitchingpost.ER) SKEW(pitchingpost.ERA) SKEW(pitchingpost.G) SKEW(pitchingpost.GF) SKEW(pitchingpost.GIDP) SKEW(pitchingpost.GS) SKEW(pitchingpost.H) SKEW(pitchingpost.HBP) SKEW(pitchingpost.HR) SKEW(pitchingpost.IBB) SKEW(pitchingpost.IPouts) SKEW(pitchingpost.L) SKEW(pitchingpost.R) SKEW(pitchingpost.SF) SKEW(pitchingpost.SH) SKEW(pitchingpost.SHO) SKEW(pitchingpost.SO) SKEW(pitchingpost.SV) SKEW(pitchingpost.W) SKEW(pitchingpost.WP) STD(pitchingpost.BAOpp) STD(pitchingpost.BB) STD(pitchingpost.BFP) STD(pitchingpost.BK) STD(pitchingpost.CG) STD(pitchingpost.ER) STD(pitchingpost.ERA) STD(pitchingpost.G) STD(pitchingpost.GF) STD(pitchingpost.GIDP) STD(pitchingpost.GS) STD(pitchingpost.H) STD(pitchingpost.HBP) STD(pitchingpost.HR) STD(pitchingpost.IBB) STD(pitchingpost.IPouts) STD(pitchingpost.L) STD(pitchingpost.R) STD(pitchingpost.SF) STD(pitchingpost.SH) STD(pitchingpost.SHO) STD(pitchingpost.SO) STD(pitchingpost.SV) STD(pitchingpost.W) STD(pitchingpost.WP) SUM(pitchingpost.BAOpp) SUM(pitchingpost.BB) SUM(pitchingpost.BFP) SUM(pitchingpost.BK) SUM(pitchingpost.CG) SUM(pitchingpost.ER) SUM(pitchingpost.ERA) SUM(pitchingpost.G) SUM(pitchingpost.GF) SUM(pitchingpost.GIDP) SUM(pitchingpost.GS) SUM(pitchingpost.H) SUM(pitchingpost.HBP) SUM(pitchingpost.HR) SUM(pitchingpost.IBB) SUM(pitchingpost.IPouts) SUM(pitchingpost.L) SUM(pitchingpost.R) SUM(pitchingpost.SF) SUM(pitchingpost.SH) SUM(pitchingpost.SHO) SUM(pitchingpost.SO) SUM(pitchingpost.SV) SUM(pitchingpost.W) SUM(pitchingpost.WP) role  target categorical categorical categorical categorical categorical        categorical          categorical              categorical               categorical            categorical                   categorical              categorical                categorical                    categorical                     categorical                  categorical                      categorical                    categorical          categorical                 categorical              categorical               categorical                      categorical                    categorical                     categorical               categorical                      categorical                   categorical                      categorical                      categorical                      categorical                      categorical    categorical        categorical         categorical          categorical              categorical               categorical                categorical        categorical            categorical             categorical              categorical                  categorical                   categorical                    categorical     categorical        categorical         categorical          categorical           categorical              categorical               categorical                categorical                 categorical         categorical            categorical             categorical              categorical               categorical                  categorical                   categorical                    categorical                     categorical     categorical         categorical          categorical           categorical               categorical                categorical                 categorical         categorical             categorical              categorical               categorical                   categorical                    categorical                     categorical categorical categorical   categorical numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical       numerical       numerical       numerical       numerical       numerical      numerical         numerical              numerical          numerical      numerical        numerical       numerical        numerical      numerical        numerical       numerical       numerical       numerical       numerical        numerical        numerical        numerical        numerical        numerical       numerical          numerical               numerical           numerical       numerical         numerical        numerical         numerical       numerical         numerical        numerical        numerical        numerical        numerical       numerical       numerical       numerical       numerical       numerical      numerical         numerical              numerical          numerical      numerical        numerical       numerical        numerical      numerical        numerical       numerical       numerical       numerical       numerical        numerical        numerical        numerical        numerical        numerical       numerical          numerical               numerical           numerical       numerical         numerical        numerical         numerical       numerical         numerical        numerical        numerical        numerical        numerical       numerical       numerical       numerical       numerical       numerical      numerical         numerical              numerical          numerical      numerical        numerical       numerical        numerical      numerical        numerical       numerical       numerical       numerical       numerical       numerical       numerical       numerical       numerical       numerical      numerical         numerical              numerical          numerical      numerical        numerical       numerical        numerical      numerical        numerical       numerical       numerical       numerical       numerical           numerical           numerical           numerical           numerical           numerical          numerical             numerical          numerical            numerical           numerical            numerical          numerical            numerical           numerical           numerical           numerical           numerical            numerical            numerical            numerical            numerical            numerical           numerical              numerical           numerical             numerical            numerical             numerical           numerical             numerical            numerical            numerical            numerical            numerical           numerical           numerical           numerical           numerical           numerical          numerical             numerical          numerical            numerical           numerical            numerical          numerical            numerical           numerical           numerical           numerical           numerical            numerical            numerical            numerical            numerical            numerical           numerical              numerical           numerical             numerical            numerical             numerical           numerical             numerical            numerical            numerical            numerical            numerical           numerical           numerical           numerical           numerical           numerical          numerical             numerical          numerical            numerical           numerical            numerical          numerical            numerical           numerical           numerical           numerical           numerical           numerical           numerical           numerical           numerical           numerical          numerical             numerical          numerical            numerical           numerical            numerical          numerical            numerical           numerical           numerical           numerical           numerical       numerical        numerical       numerical       numerical        numerical             numerical        numerical        numerical         numerical        numerical        numerical         numerical              numerical         numerical       numerical        numerical       numerical       numerical        numerical             numerical        numerical        numerical         numerical        numerical        numerical         numerical              numerical         numerical       numerical        numerical       numerical       numerical        numerical             numerical        numerical       numerical        numerical       numerical       numerical        numerical             numerical        numerical           numerical            numerical            numerical           numerical           numerical            numerical                 numerical            numerical            numerical            numerical            numerical             numerical             numerical            numerical            numerical             numerical                  numerical             numerical             numerical             numerical           numerical            numerical            numerical           numerical           numerical            numerical                 numerical            numerical            numerical            numerical            numerical             numerical             numerical            numerical            numerical             numerical                  numerical             numerical             numerical             numerical           numerical            numerical            numerical           numerical           numerical            numerical                 numerical            numerical            numerical            numerical           numerical            numerical            numerical           numerical           numerical            numerical                 numerical            numerical            numerical            numerical           numerical        numerical         numerical        numerical        numerical        numerical         numerical       numerical        numerical        numerical       numerical         numerical        numerical         numerical            numerical       numerical       numerical         numerical        numerical        numerical       numerical        numerical            numerical         numerical          numerical         numerical         numerical         numerical          numerical        numerical         numerical         numerical        numerical          numerical         numerical          numerical             numerical        numerical        numerical          numerical         numerical         numerical        numerical         numerical           numerical        numerical         numerical        numerical        numerical        numerical         numerical       numerical        numerical        numerical       numerical         numerical        numerical         numerical            numerical       numerical       numerical         numerical        numerical        numerical       numerical        numerical            numerical         numerical          numerical         numerical         numerical         numerical          numerical        numerical         numerical         numerical        numerical          numerical         numerical          numerical             numerical        numerical        numerical          numerical         numerical         numerical        numerical         numerical           numerical        numerical         numerical        numerical        numerical        numerical         numerical       numerical        numerical        numerical       numerical         numerical        numerical         numerical            numerical       numerical       numerical         numerical        numerical        numerical       numerical        numerical           numerical        numerical         numerical        numerical        numerical        numerical         numerical       numerical        numerical        numerical       numerical         numerical        numerical         numerical            numerical       numerical       numerical         numerical        numerical        numerical       numerical        numerical               numerical            numerical             numerical            numerical            numerical            numerical             numerical           numerical            numerical              numerical            numerical           numerical             numerical            numerical             numerical                numerical           numerical           numerical            numerical            numerical             numerical            numerical            numerical           numerical            numerical                numerical             numerical              numerical             numerical             numerical             numerical              numerical            numerical             numerical               numerical             numerical            numerical              numerical             numerical              numerical                 numerical            numerical            numerical             numerical             numerical              numerical             numerical             numerical            numerical             numerical               numerical            numerical             numerical            numerical            numerical            numerical             numerical           numerical            numerical              numerical            numerical           numerical             numerical            numerical             numerical                numerical           numerical           numerical            numerical            numerical             numerical            numerical            numerical           numerical            numerical                numerical             numerical              numerical             numerical             numerical             numerical              numerical            numerical             numerical               numerical             numerical            numerical              numerical             numerical              numerical                 numerical            numerical            numerical             numerical             numerical              numerical             numerical             numerical            numerical             numerical               numerical            numerical             numerical            numerical            numerical            numerical             numerical           numerical            numerical              numerical            numerical           numerical             numerical            numerical             numerical                numerical           numerical           numerical            numerical            numerical             numerical            numerical            numerical           numerical            numerical               numerical            numerical             numerical            numerical            numerical            numerical             numerical           numerical            numerical              numerical            numerical           numerical             numerical            numerical             numerical                numerical           numerical           numerical            numerical            numerical             numerical            numerical            numerical           numerical            numerical 0 633333 NL ATL campri01 ATL 0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 1 MVP NL 336 1 1 1 8 NL 1 ATL 1 1 1 1 NL NLCS ATL 1 1 1 0 NULL NULL NULL NULL NULL NULL NULL NULL 1 P NL NLCS ATL 1 1 1 1 8 NL 1 ATL 1 1 1 1 NL NLCS ATL 1 1 1 1 1 1 1985 1985 9 0 9 0 9 0 nan nan nan nan 9 0 1 0 45 2 0 77 1 77 77 5 2 0 0 3 3 0 1 5 23 0.5 0 20.25 0.375 0 43.5 0.125 43.5 43.5 1.25 0.375 0 0 0.625 0.625 0 0.125 1.625 10 0 0 2 0 0 5 0 5 5 0 0 0 0 0 0 0 0 0 1 0 0 0.5815 1.951 0 -0.4333 2.8284 -0.4333 -0.4333 1.556 1.951 0 0 1.9604 1.6519 0 2.8284 0.9157 0.6749 0.5345 0 18.0297 0.744 0 20.5704 0.3536 20.5704 20.5704 1.8323 0.744 0 0 1.0607 1.1877 0 0.3536 1.8468 8.3837 4 0 162 3 0 348 1 348 348 10 3 0 0 5 5 0 1 13 80 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 1 1 3 0 0 0 0 0 0 0 1 1 3 0 0 0 0 0 0 0 1 1 3 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 1 1 3 0 0 0 0 63 761 2 3 72 6 77 44 21 199 4 18 13 532 13 84 0 69 22 11 8 0 34.375 435 1 0.625 37.125 3 43.5 18.375 7.875 105 2 8 5.875 305.5 5.375 43.5 0 44.75 6.75 6.5 3.125 0 2 46 0 0 8 1 5 1 0 13 0 0 0 34 1 9 0 6 0 0 0 0 -0.3257 -0.3004 0 1.9604 0.3173 0.5543 -0.4333 0.4135 0.6696 0.09749 0.3307 0.6191 0.2945 -0.2618 1.2158 0.1602 0 -0.6459 0.9797 -0.7085 0.693 0 20.3044 222.1345 0.9258 1.0607 22.548 1.6036 20.5704 16.8263 9.7018 55.616 1.5119 6.3696 4.5178 158.5145 3.8891 25.394 0 22.5182 8.7301 3.8545 3.0443 0 275 3480 8 5 297 24 348 147 63 840 16 64 47 2444 43 348 0 358 54 52 25 0 1 8 0 0 4 36 1 0 0 1 4 0 0 0 3 1 4 0 0 0 0 0 0 0 0 1 8 0 0 4 36 1 0 0 1 4 0 0 0 3 1 4 0 0 0 0 0 0 0 0 1 8 0 0 4 36 1 0 0 1 4 0 0 0 3 1 4 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 1 8 0 0 4 36 1 0 0 1 4 0 0 0 3 1 4 0 0 0 0 0 0 0 1 150000 NL ATL dedmoje01 ATL 0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 2 NL 1 ATL 1 1 1 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL NULL NULL 2 NL 1 ATL 1 1 1 0 NULL NULL NULL NULL NULL NULL 1 1 1 1985 1985 nan nan nan nan nan nan nan nan nan nan 0 0 0 0 6 0 0 54 0 54 54 0 0 0 0 0 0 0 0 1 1 0 0 3 0 0 29.5 0 29.5 29.5 0 0 0 0 0 0 0 0 0.5 0.5 0 0 0 0 0 5 0 5 5 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 4.2426 0 0 34.6482 0 34.6482 34.6482 0 0 0 0 0 0 0 0 0.7071 0.7071 0 0 6 0 0 59 0 59 59 0 0 0 0 0 0 0 0 1 1 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 35 354 0 0 34 13 54 19 0 86 2 5 9 243 3 39 0 51 4 4 3 0 17.5 188.5 0 0 20 8 29.5 9.5 0 48 1 3 4.5 127.5 1.5 22.5 0 27 2 2 1.5 0 0 23 0 0 6 3 5 0 0 10 0 1 0 12 0 6 0 3 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 24.7487 234.0523 0 0 19.799 7.0711 34.6482 13.435 0 53.7401 1.4142 2.8284 6.364 163.3417 2.1213 23.3345 0 33.9411 2.8284 2.8284 2.1213 0 35 377 0 0 40 16 59 19 0 96 2 6 9 255 3 45 0 54 4 4 3 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1500000 NL ATL hornebo01 ATL 1 1 NLS198207130 0 NL NULL ATL 1 1 1 1 0 1 1 Rookie of the Year NL NULL 1 1 0 4 MVP NL 336 2 1 2 7 NL 1 ATL 1 1 1 1 NL NLCS ATL 1 1 1 10 3B NL 1 ATL 2 1 1 1 1 3B NL NLCS ATL 1 1 1 1 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 1 1 1 1985 1985 42 12 14 3 1 0 1.609 2. 19.3735 6 56 12 25 1 499 66 5 140 16 140 140 153 4 35 6 85 98 4 9 1 75 16.1429 0.5714 367.2857 33.5714 1.8571 98.4286 9.7143 98.4286 98.4286 103.4286 1.7143 23 3 59.1429 68 1.7143 3.7143 0.1429 51.4286 8 0 113 14 0 32 3 32 32 31 0 3 2 15 19 0 2 0 17 0.3431 -0.3742 -1.0883 1.1061 0.8 -1.0006 -0.02558 -1.0006 -1.0006 -0.8636 0.7065 -0.7731 1.9799 -0.8853 -0.6669 0.05194 2.2203 2.6458 -0.4431 6.466 0.5345 136.8341 18.1462 1.7728 36.0595 4.7509 36.0595 36.0595 40.2859 1.3801 11.5326 1.4142 25.0694 29.676 1.7043 2.43 0.378 20.9353 113 4 2571 235 13 689 68 689 689 724 12 161 21 414 476 12 26 1 360 0 0 11 0 0 3 0 1 0 0 0 0 0 0 0 0 2 0 0 11 0 0 3 0 1 0 0 0 0 0 0 0 0 2 0 0 11 0 0 3 0 1 0 0 0 0 0 0 0 0 2 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 11 0 0 3 0 1 0 0 0 0 0 0 0 0 2 251 32 23 137 137 3519 414 117.9 13 9.3 69.1 67.9 1724.2 88.3 0 0 0 1 0 3 2 -0.05114 0.3443 0.3635 -0.2447 -0.2157 -0.2406 2.6782 91.5829 10.2198 7.15 47.6269 47.7248 1202.7394 119.4609 1179 130 93 691 679 17242 883 7 nan 0 0 3 3 72 1 nan 0 7 nan 0 0 3 3 72 1 nan 0 7 nan 0 0 3 3 72 1 nan 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 7 0 0 0 3 3 72 1 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 512500 AL BAL dempsri01 BAL 0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 2 Babe Ruth Award AL C 2 2 1 0 NULL NULL NULL NULL NULL NULL 17 AL 1 BAL 1 2 3 4 AL ALCS BAL 1 2 1 0 NULL NULL NULL NULL NULL NULL NULL NULL 4 C AL ALCS BAL 1 1 2 1 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 1 1 1 1985 1985 nan nan nan nan nan nan nan nan nan nan 0 0 26 4 441 48 3 136 12 136 136 114 3 11 2 51 41 7 6 7 58 8.7647 0.6471 191.7647 21.8235 0.9412 68.5294 5.5882 68.5294 68.5294 45.8235 0.7059 3.1176 0.5294 20.1176 17.5882 0.9412 1.7647 2.6471 24.1765 0 0 6 1 0 5 0 5 5 0 0 0 0 0 0 0 0 0 0 0.8167 1.9434 0.08724 0.07107 0.9792 -0.068 0.1341 -0.068 -0.068 0.2333 1.2955 0.8822 0.7496 0.3107 0.2151 2.7929 0.8217 0.3938 0.3044 9.2028 1.2217 155.3752 17.9973 1.144 50.2433 4.5008 50.2433 50.2433 38.917 1.1048 3.5335 0.6243 18.1827 16.3404 1.7843 2.1946 2.0899 21.1076 149 11 3260 371 16 1165 95 1165 1165 779 12 53 9 342 299 16 30 45 411 4 0 21 2 0 7 1 6 0 1 2 3 2 1 0 0 3 2 0 14 1.25 0 4.75 0.5 4.25 0 0.25 0.5 2.5 1 0.25 0 0 1.5 0 0 10 1 0 3 0 2 0 0 0 1 0 0 0 0 0 0 0 1.597 2. 0 0.7528 0 -0.7528 0 2. 2. -2. 0 2. 0 0 0 1.633 0 4.8305 0.5 0 1.7078 0.5774 1.7078 0 0.5 1 1 1.1547 0.5 0 0 1.291 8 0 56 5 0 19 2 17 0 1 2 10 4 1 0 0 6 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 5 3 1 0 7 6 165 38 4 0 3 1.75 0.25 0 4.75 4.5 120 26 1.25 0 1 1 0 0 3 3 84 10 0 0 0 0.8546 2. 0 0.7528 0 0.7636 -0.9764 1.6585 0 1.8257 0.9574 0.5 0 1.7078 1.291 33.6749 11.6905 1.893 0 12 7 1 0 19 18 480 104 5 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 560000 AL BAL martide01 BAL 0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 2 Cy Young AL 140 2 1 2 9 AL 1 BAL 1 1 1 1 AL WS BAL 1 1 1 9 P AL 1 BAL 1 1 1 1 2 P AL ALCS BAL 1 1 2 1 9 AL 1 BAL 1 1 1 2 AL ALCS BAL 1 2 1 1 1 1 1985 1985 3 0 3 0 3 0 nan nan 0 0 6 0 0 0 0 0 0 42 0 1 42 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 31.4444 0 0.1111 31.3333 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 4 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan -1.6547 nan 3 -1.6041 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 12.0531 nan 0.3333 12.114 nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 283 0 1 282 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 59 8 5 42 39 877 27 33.2222 3.3333 1.4444 31.3333 23.5556 529.4444 15.1111 4 0 0 4 2 83 3 -0.2338 0.6857 1.8192 -1.6041 -0.1645 -0.209 0.01254 18.0331 2.5 1.5092 12.114 13.277 257.65 8.5065 299 30 13 282 212 4765 136 1 0 1 0 2 1 25 2 2 0 0.5 0 0.5 0 1.5 1 15.5 1 1 0 0 0 0 0 1 1 6 0 0 0 nan nan nan nan nan nan nan nan nan nan 0.7071 0 0.7071 0 0.7071 0 13.435 1.4142 1.4142 0 1 0 1 0 3 2 31 2 2 0 0 93 1206 2 18 119 5 42 19 39 279 8 30 6 877 16 129 3 142 4 16 13 0 57.5556 746.8889 0.5556 7.3333 79.1111 3.5556 31.3333 3.8889 23.5556 178.6667 3.3333 17.5556 2.3333 529.4444 9.1111 87.1111 1 87.5556 0.5556 10.5556 5.6667 0 8 106 0 1 8 2 4 0 2 23 0 1 0 83 2 8 0 18 0 1 0 0 -0.4575 -0.3514 1.0143 0.7717 -0.8476 0.2704 -1.6041 2.1216 -0.1645 -0.5972 0.772 -0.3131 0.6624 -0.209 0.1397 -1.0248 0.5249 -0.4028 2.6953 -0.5712 0.2488 0 26.9774 357.8325 0.7265 6.0828 36.4535 1.0138 12.114 6.2738 13.277 83.4116 2.7386 9.8249 2.1794 257.65 5.0607 39.9889 1.2247 40.5682 1.3333 5.5703 4.1833 0 518 6722 5 66 712 32 282 35 212 1608 30 158 21 4765 82 784 9 788 5 95 51 0 0 32 0 0 4 18 2 1 2 1 8 1 1 0 25 0 4 0 0 0 4 0 0 0 0 0 21 0 0 3.5 10.5 1.5 0.5 1.5 1 7 0.5 1 0 15.5 0 3.5 0 0 0 2 0 0 0 0 0 10 0 0 3 3 1 0 1 1 6 0 1 0 6 0 3 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 15.5563 0 0 0.7071 10.6066 0.7071 0.7071 0.7071 0 1.4142 0.7071 0 0 13.435 0 0.7071 0 0 0 2.8284 0 0 0 0 0 42 0 0 7 21 3 1 3 2 14 1 2 0 31 0 7 0 0 0 4 0 0 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 4534 512500 NL WAS desmoia01 WAS 0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 1 1 6 2012 2012 nan nan nan nan nan nan nan nan nan nan 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4535 506000 NL WAS espinda01 WAS 0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 1 1 6 2012 2012 nan nan nan nan nan nan nan nan nan nan 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4536 3000000 NL WAS gorzeto01 WAS 0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 8 NL 1 PIT 1 2 3 0 NULL NULL NULL NULL NULL NULL 8 P NL 1 PIT 1 1 2 3 0 NULL NULL NULL NULL NULL NULL NULL NULL 8 NL 1 PIT 1 2 3 0 NULL NULL NULL NULL NULL NULL 1 1 6 2012 2012 nan nan nan nan nan nan nan nan nan nan 0 0 0 0 63 2 0 32 2 29 29 5 1 0 0 2 3 0 0 6 31 0 0 24.125 1 0 18.375 0.375 17.75 18.4 2.125 0.125 0 0 0.75 1.5 0 0 2.875 11.375 0 0 1 0 0 3 0 3 3 0 0 0 0 0 0 0 0 0 1 0 0 0.8142 0 0 -0.01019 1.951 -0.06458 -0.4709 0.07359 2.8284 0 0 0.6154 0 0 0 0.3404 0.9901 0 0 20.6705 0.9258 0 10.862 0.744 10.1805 11.393 1.9594 0.3536 0 0 0.8864 1.1952 0 0 2.4749 10.0134 0 0 193 8 0 147 3 142 92 17 1 0 0 6 12 0 0 23 91 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 22 4 3 32 32 605 12 11.125 0.75 1.25 18.5 13.75 248.625 4.125 1 0 0 3 0 18 0 0.01566 2.2936 0.3864 -0.002143 0.3057 0.5535 1.3775 8.2191 1.3887 1.0351 11.0065 11.1963 202.2452 3.7583 89 6 10 148 110 1989 33 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 nan 70 874 1 1 87 12 32 3 32 214 11 20 5 605 10 90 1 135 1 14 5 nan 36.25 364.75 0.375 0.125 42.125 5.25 18.5 0.75 13.75 84.625 3.125 9.25 1.75 248.625 5.375 44.5 0.125 63.25 0.125 5 2.375 nan 3 32 0 0 5 3 3 0 0 6 0 0 0 18 1 5 0 3 0 0 0 nan 0.1519 0.5036 0.6441 2.8284 0.2703 2.1617 -0.002143 1.3554 0.3057 0.6789 1.5012 0.1874 0.5779 0.5535 -0.04201 0.1678 2.8284 0.2401 2.8284 1.4367 0.1642 nan 28.9815 294.2529 0.5175 0.3536 31.2841 2.9155 11.0065 1.165 11.1963 71.6757 3.7961 7.8513 2.0529 202.2452 3.7393 32.6321 0.3536 49.5025 0.3536 4.2426 2.3867 0 290 2918 3 1 337 42 148 6 110 677 25 74 14 1989 43 356 1 506 1 40 19 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4537 481000 NL WAS matthry01 WAS 0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 1 1 6 2012 2012 nan nan nan nan nan nan nan nan nan nan 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4538 2300000 NL WAS zimmejo02 WAS 0 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 0 NULL NULL NULL NULL NULL NULL 1 1 6 2012 2012 nan nan nan nan nan nan nan nan nan nan 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 <p>     4539 rows x 695 columns     memory usage: 23.77 MB     name: featuretools_test     type: getml.DataFrame </p> <p>We train an untuned XGBoostRegressor on top of featuretools' features, just like we have done for getML's features.</p> <p>Since some of featuretools features are categorical, we allow the pipeline to include these features as well. Other features contain NaN values, which is why we also apply getML's Imputation preprocessor.</p> In\u00a0[61]: Copied! <pre>imputation = getml.preprocessors.Imputation()\n\npredictor = getml.predictors.XGBoostRegressor(n_jobs=1)\n\npipe3 = getml.pipeline.Pipeline(\n    tags=['featuretools'],\n    preprocessors=[imputation],\n    predictors=[predictor],\n    include_categorical=True,\n)\n\npipe3\n</pre> imputation = getml.preprocessors.Imputation()  predictor = getml.predictors.XGBoostRegressor(n_jobs=1)  pipe3 = getml.pipeline.Pipeline(     tags=['featuretools'],     preprocessors=[imputation],     predictors=[predictor],     include_categorical=True, )  pipe3 Out[61]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=['Imputation'],\n         share_selected_features=0.5,\n         tags=['featuretools'])</pre> In\u00a0[62]: Copied! <pre>pipe3.fit(featuretools_train)\n</pre> pipe3.fit(featuretools_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 1 issues labeled INFO and 14 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:28, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:35.0652\n\n</pre> Out[62]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=['Imputation'],\n         share_selected_features=0.5,\n         tags=['featuretools'])</pre> In\u00a0[63]: Copied! <pre>featuretools_score = pipe3.score(featuretools_test)\nfeaturetools_score\n</pre> featuretools_score = pipe3.score(featuretools_test) featuretools_score <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[63]: date time           set used           target          mae          rmse rsquared 0 2024-02-21 14:59:48 featuretools_train salary 716216.5012 1297813.3733 0.8115 1 2024-02-21 14:59:50 featuretools_test salary 782064.7582 1435156.4642 0.7795 In\u00a0[64]: Copied! <pre>pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name]\n</pre> pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name] Out[64]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_29\";\n\nCREATE TABLE \"FEATURE_1_29\" AS\nSELECT LAST( t2.\"gameid__mapping_1_target_1_avg\", t2.\"year__1_000000_days\" ) AS \"feature_1_29\",\n       t1.rowid AS rownum\nFROM \"SALARIES__STAGING_TABLE_1\" t1\nINNER JOIN \"ALLSTARFULL__STAGING_TABLE_2\" t2\nON t1.\"playerid\" = t2.\"playerid\"\nWHERE t2.\"year__1_000000_days\" &lt;= t1.\"year\"\nGROUP BY t1.rowid;\n</pre> In\u00a0[65]: Copied! <pre>pipe2.features.to_sql()[pipe2.features.sort(by=\"importances\")[0].name]\n</pre> pipe2.features.to_sql()[pipe2.features.sort(by=\"importances\")[0].name] Out[65]: <pre>-- The size of the SQL code for FEATURE_1_1 is 139592 characters, which is greater than the size_threshold of 50000!\n-- To display very long features like this anyway, increase the size_threshold or set the size_threshold to None.\nDROP TABLE IF EXISTS \"FEATURE_1_1\";\n\nCREATE TABLE \"FEATURE_1_1\";\n</pre> In\u00a0[66]: Copied! <pre># Creates a folder named baseball_pipeline containing\n# the SQL code.\npipe2.features.to_sql().save(\"baseball_pipeline\")\n</pre> # Creates a folder named baseball_pipeline containing # the SQL code. pipe2.features.to_sql().save(\"baseball_pipeline\") In\u00a0[67]: Copied! <pre># Creates a folder named baseball_pipeline_spark containing\n# the SQL code.\npipe2.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"baseball_pipeline_spark\")\n</pre> # Creates a folder named baseball_pipeline_spark containing # the SQL code. pipe2.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"baseball_pipeline_spark\") In\u00a0[68]: Copied! <pre>scores = [fastprop_score, relboost_score, featuretools_score]\npd.DataFrame(data={\n    'Name': ['getML: FastProp', 'getML: Relboost', 'featuretools'],\n    'R-squared': [f'{score.rsquared:.2%}' for score in scores],\n    'RMSE': [f'{score.rmse:,.0f}' for score in scores],\n    'MAE': [f'{score.mae:,.0f}' for score in scores]\n})\n</pre> scores = [fastprop_score, relboost_score, featuretools_score] pd.DataFrame(data={     'Name': ['getML: FastProp', 'getML: Relboost', 'featuretools'],     'R-squared': [f'{score.rsquared:.2%}' for score in scores],     'RMSE': [f'{score.rmse:,.0f}' for score in scores],     'MAE': [f'{score.mae:,.0f}' for score in scores] }) Out[68]: Name R-squared RMSE MAE 0 getML: FastProp 78.80% 1,402,961 765,293 1 getML: Relboost 83.71% 1,229,058 668,857 2 featuretools 77.95% 1,435,156 782,065 <p>As we can see, Relboost outperforms FastProp and both algorithms outperform featuretools according to all three measures.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/baseball/#baseball-predicting-players-salary","title":"Baseball - Predicting players' salary\u00b6","text":"<p>In this notebook, we will benchmark several of getML's feature learning algorithms against featuretools using a dataset related to baseball players' salary.</p> <p>Summary:</p> <ul> <li>Prediction type: Regression model</li> <li>Domain: Sports</li> <li>Prediction target: Salaries</li> <li>Population size: 23111</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/baseball/#background","title":"Background\u00b6","text":"<p>In the late 1990s, the Oakland Athletics began focusing on the idea of sabermetrics, using statistical methods to identify undervalued baseball players. This was done to compensate for the fact that the team had a significantly smaller budget than most other teams in its league. Under its general manager Billy Beane, the Oakland Athletics became the first team in over 100 years to win 20 consecutive games in a row, despite still being significantly disadvantaged in terms of its budget. After this remarkable success, the use of sabermetrics quickly became the norm in baseball. These events have been documented in a bestselling book and a movie, both called Moneyball.</p> <p>In this notebook, we will demonstrate that relational learning can be used for sabermetrics. Specifically, we will develop a model to predict players' salary using getML's statistical relational learning algorithms. Such predictions can then be used to identify undervalued players.</p> <p>The dataset has been downloaded from the CTU Prague relational learning repository (Motl and Schulte, 2015) (Now residing at relational-data.org.).</p> <p>We will benchmark getML 's feature learning algorithms against featuretools, an open-source implementation of the propositionalization algorithm, similar to getML's FastProp.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/baseball/#analysis","title":"Analysis\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/baseball/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/baseball/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>We begin by downloading the data:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/baseball/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/baseball/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"<p>We loaded the data and defined the roles and units. Next, we create a getML pipeline for relational learning.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/baseball/#21-define-relational-model","title":"2.1 Define relational model\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/baseball/#22-getml-pipeline","title":"2.2 getML pipeline\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/baseball/#23-model-training","title":"2.3 Model training\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/baseball/#24-model-evaluation","title":"2.4 Model evaluation\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/baseball/#25-featuretools","title":"2.5 featuretools\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/baseball/#26-features","title":"2.6 Features\u00b6","text":"<p>The most important features look as follows:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/baseball/#27-productionization","title":"2.7 Productionization\u00b6","text":"<p>It is possible to productionize the pipeline by transpiling the features into production-ready SQL code.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/baseball/#28-discussion","title":"2.8 Discussion\u00b6","text":"<p>For a more convenient overview, we summarize our results into a table.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/baseball/#3-conclusion","title":"3. Conclusion\u00b6","text":"<p>We have demonstrated how statistical relational learning can be used for sabermetrics. We have also shown that getML outperforms featuretools on this dataset.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/baseball/#references","title":"References\u00b6","text":"<p>Motl, Jan, and Oliver Schulte. \"The CTU prague relational learning repository.\" arXiv preprint arXiv:1511.03086 (2015).</p>","boost":0.8},{"location":"examples/enterprise-notebooks/consumer_expenditures/","title":"<span class=\"ntitle\">consumer_expenditures.ipynb</span> <span class=\"ndesc\">Why relational learning matters</span>","text":"In\u00a0[1]: Copied! <pre>import datetime\nimport os\nfrom pathlib import Path\nfrom urllib import request\nimport time\nimport zipfile\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport getml\n</pre> import datetime import os from pathlib import Path from urllib import request import time import zipfile  import matplotlib.pyplot as plt %matplotlib inline  import getml In\u00a0[2]: Copied! <pre>getml.engine.launch(in_memory=False, home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.engine.set_project(\"consumer_expenditures\")\n</pre> getml.engine.launch(in_memory=False, home_directory=Path.home(), allow_remote_ips=True, token='token') getml.engine.set_project(\"consumer_expenditures\") <pre>getML engine is already running.\nLoading pipelines... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:52, remaining: 00:00]          \n\nConnected to project 'consumer_expenditures'\n</pre> In\u00a0[3]: Copied! <pre>conn = getml.database.connect_mysql(\n    host=\"db.relational-data.org\",\n    dbname=\"ConsumerExpenditures\",\n    port=3306,\n    user=\"guest\",\n    password=\"relational\"\n)\n\nconn\n</pre> conn = getml.database.connect_mysql(     host=\"db.relational-data.org\",     dbname=\"ConsumerExpenditures\",     port=3306,     user=\"guest\",     password=\"relational\" )  conn Out[3]: <pre>Connection(dbname='ConsumerExpenditures',\n           dialect='mysql',\n           host='db.relational-data.org',\n           port=3306)</pre> In\u00a0[4]: Copied! <pre>def load_if_needed(name):\n    \"\"\"\n    Loads the data from the relational learning\n    repository, if the data frame has not already\n    been loaded.\n    \"\"\"\n    if getml.data.exists(name):\n        return getml.data.load_data_frame(name)\n    data_frame = getml.data.DataFrame.from_db(\n        name=name,\n        table_name=name,\n        conn=conn\n    )\n    data_frame.save()\n    return data_frame\n</pre> def load_if_needed(name):     \"\"\"     Loads the data from the relational learning     repository, if the data frame has not already     been loaded.     \"\"\"     if getml.data.exists(name):         return getml.data.load_data_frame(name)     data_frame = getml.data.DataFrame.from_db(         name=name,         table_name=name,         conn=conn     )     data_frame.save()     return data_frame In\u00a0[5]: Copied! <pre>households = load_if_needed(\"HOUSEHOLDS\")\nhousehold_members = load_if_needed(\"HOUSEHOLD_MEMBERS\")\nexpenditures = load_if_needed(\"EXPENDITURES\")\n</pre> households = load_if_needed(\"HOUSEHOLDS\") household_members = load_if_needed(\"HOUSEHOLD_MEMBERS\") expenditures = load_if_needed(\"EXPENDITURES\") In\u00a0[6]: Copied! <pre>households\n</pre> households Out[6]:  name         YEAR  INCOME_RANK INCOME_RANK_1 INCOME_RANK_2 INCOME_RANK_3 INCOME_RANK_4 INCOME_RANK_5 INCOME_RANK_MEAN      AGE_REF HOUSEHOLD_ID   role unused_float unused_float  unused_float  unused_float  unused_float  unused_float  unused_float     unused_float unused_float unused_string 0 2015 0.3044 0.1448 0.1427 0.1432 0.1422 0.1382 0.127 66 03111041 1 2015 0.3063 0.1462 0.1444 0.1446 0.1435 0.1395 0.1283 66 03111042 2 2015 0.6931 0.6222 0.6204 0.623 0.6131 0.6123 0.6207 48 03111051 3 2015 0.6926 0.6216 0.6198 0.6224 0.6125 0.6117 0.6201 48 03111052 4 2015 0.2817 0.113 0.1128 0.1098 0.1116 0.1092 0.0951 37 03111061 ... ... ... ... ... ... ... ... ... ... 56807 2019 0.4828 0.4106 0.3603 0.3958 0.377 0.3984 0.3769 67 04362582 56808 2019 0.6644 0.5975 0.6026 0.5949 0.596 0.6002 0.6 52 04362661 56809 2019 0.6639 0.597 0.6021 0.5944 0.5955 0.5997 0.5995 52 04362662 56810 2019 0.162 0.05217 0.03955 0.04507 0.04607 0.02436 0.03558 72 04362671 56811 2019 0.1616 0.03925 0.05741 0.04595 0.03789 0.05746 0.03931 72 04362672 <p>     56812 rows x 10 columns     memory usage: 5.06 MB     name: HOUSEHOLDS     type: getml.DataFrame </p> In\u00a0[7]: Copied! <pre>household_members\n</pre> household_members Out[7]:   name         YEAR          AGE HOUSEHOLD_ID  MARITAL       SEX           WORK_STATUS     role unused_float unused_float unused_string unused_string unused_string unused_string 0 2015 66 03111041 1 1 NULL 1 2015 66 03111042 1 1 NULL 2 2015 56 03111091 1 1 NULL 3 2015 56 03111092 1 1 NULL 4 2015 50 03111111 1 1 1 ... ... ... ... ... ... 137350 2019 22 04362422 5 2 NULL 137351 2019 11 04362431 5 2 NULL 137352 2019 11 04362432 5 2 NULL 137353 2019 72 04362671 5 2 NULL 137354 2019 72 04362672 5 2 NULL <p>     137355 rows x 6 columns     memory usage: 8.59 MB     name: HOUSEHOLD_MEMBERS     type: getml.DataFrame </p> In\u00a0[8]: Copied! <pre>expenditures\n</pre> expenditures Out[8]:    name         YEAR        MONTH         COST         GIFT  IS_TRAINING EXPENDITURE_ID HOUSEHOLD_ID  PRODUCT_CODE     role unused_float unused_float unused_float unused_float unused_float unused_string  unused_string unused_string 0 2015 1 3.89 0 1 1 03111041 010210 1 2015 1 4.66 0 1 10 03111041 120310 2 2015 2 9.79 0 1 100 03111051 190211 3 2015 2 2.95 0 1 1000 03111402 040510 4 2015 1 2.12 0 1 10000 03114161 190321 ... ... ... ... ... ... ... ... 2020629 2017 6 1.99 0 1 999995 03708582 150110 2020630 2017 6 3.619 0 1 999996 03708582 150110 2020631 2017 6 5.2727 0 1 999997 03708582 150211 2020632 2017 6 4.6894 0 1 999998 03708582 150310 2020633 2017 6 5.7177 0 1 999999 03708582 160310 <p>     2020634 rows x 8 columns     memory usage: 176.70 MB     name: EXPENDITURES     type: getml.DataFrame </p> In\u00a0[9]: Copied! <pre>households.set_role(\"HOUSEHOLD_ID\", getml.data.roles.join_key)\nhouseholds.set_role(households.roles.unused_float, getml.data.roles.numerical)\n\nhouseholds\n</pre> households.set_role(\"HOUSEHOLD_ID\", getml.data.roles.join_key) households.set_role(households.roles.unused_float, getml.data.roles.numerical)  households Out[9]:  name HOUSEHOLD_ID      YEAR INCOME_RANK INCOME_RANK_1 INCOME_RANK_2 INCOME_RANK_3 INCOME_RANK_4 INCOME_RANK_5 INCOME_RANK_MEAN   AGE_REF  role     join_key numerical   numerical     numerical     numerical     numerical     numerical     numerical        numerical numerical 0 03111041 2015 0.3044 0.1448 0.1427 0.1432 0.1422 0.1382 0.127 66 1 03111042 2015 0.3063 0.1462 0.1444 0.1446 0.1435 0.1395 0.1283 66 2 03111051 2015 0.6931 0.6222 0.6204 0.623 0.6131 0.6123 0.6207 48 3 03111052 2015 0.6926 0.6216 0.6198 0.6224 0.6125 0.6117 0.6201 48 4 03111061 2015 0.2817 0.113 0.1128 0.1098 0.1116 0.1092 0.0951 37 ... ... ... ... ... ... ... ... ... ... 56807 04362582 2019 0.4828 0.4106 0.3603 0.3958 0.377 0.3984 0.3769 67 56808 04362661 2019 0.6644 0.5975 0.6026 0.5949 0.596 0.6002 0.6 52 56809 04362662 2019 0.6639 0.597 0.6021 0.5944 0.5955 0.5997 0.5995 52 56810 04362671 2019 0.162 0.05217 0.03955 0.04507 0.04607 0.02436 0.03558 72 56811 04362672 2019 0.1616 0.03925 0.05741 0.04595 0.03789 0.05746 0.03931 72 <p>     56812 rows x 10 columns     memory usage: 4.32 MB     name: HOUSEHOLDS     type: getml.DataFrame </p> In\u00a0[10]: Copied! <pre>household_members.set_role(\"HOUSEHOLD_ID\", getml.data.roles.join_key)\nhousehold_members.set_role(household_members.roles.unused_float, getml.data.roles.numerical)\nhousehold_members.set_role(household_members.roles.unused_string, getml.data.roles.categorical)\n\nhousehold_members\n</pre> household_members.set_role(\"HOUSEHOLD_ID\", getml.data.roles.join_key) household_members.set_role(household_members.roles.unused_float, getml.data.roles.numerical) household_members.set_role(household_members.roles.unused_string, getml.data.roles.categorical)  household_members Out[10]:   name HOUSEHOLD_ID MARITAL     SEX         WORK_STATUS      YEAR       AGE   role     join_key categorical categorical categorical numerical numerical 0 03111041 1 1 NULL 2015 66 1 03111042 1 1 NULL 2015 66 2 03111091 1 1 NULL 2015 56 3 03111092 1 1 NULL 2015 56 4 03111111 1 1 1 2015 50 ... ... ... ... ... ... 137350 04362422 5 2 NULL 2019 22 137351 04362431 5 2 NULL 2019 11 137352 04362432 5 2 NULL 2019 11 137353 04362671 5 2 NULL 2019 72 137354 04362672 5 2 NULL 2019 72 <p>     137355 rows x 6 columns     memory usage: 4.40 MB     name: HOUSEHOLD_MEMBERS     type: getml.DataFrame </p> In\u00a0[11]: Copied! <pre>year = expenditures[\"YEAR\"]\nmonth = expenditures[\"MONTH\"]\n\nts_strings = year + \"/\" + month\n\nexpenditures[\"TIME_STAMP\"] = ts_strings.as_ts([\"%Y/%n\"])\n</pre> year = expenditures[\"YEAR\"] month = expenditures[\"MONTH\"]  ts_strings = year + \"/\" + month  expenditures[\"TIME_STAMP\"] = ts_strings.as_ts([\"%Y/%n\"]) In\u00a0[12]: Copied! <pre>expenditures.set_role(\"HOUSEHOLD_ID\", getml.data.roles.join_key)\nexpenditures.set_role(\"GIFT\", getml.data.roles.target)\nexpenditures.set_role(\"COST\", getml.data.roles.numerical)\nexpenditures.set_role([\"PRODUCT_CODE\", \"MONTH\", \"YEAR\"], getml.data.roles.categorical)\nexpenditures.set_role(\"TIME_STAMP\", getml.data.roles.time_stamp)\n\nexpenditures.set_subroles(\"PRODUCT_CODE\", getml.data.subroles.include.substring)\n\nexpenditures\n</pre> expenditures.set_role(\"HOUSEHOLD_ID\", getml.data.roles.join_key) expenditures.set_role(\"GIFT\", getml.data.roles.target) expenditures.set_role(\"COST\", getml.data.roles.numerical) expenditures.set_role([\"PRODUCT_CODE\", \"MONTH\", \"YEAR\"], getml.data.roles.categorical) expenditures.set_role(\"TIME_STAMP\", getml.data.roles.time_stamp)  expenditures.set_subroles(\"PRODUCT_CODE\", getml.data.subroles.include.substring)  expenditures Out[12]:      name                  TIME_STAMP HOUSEHOLD_ID   GIFT MONTH       YEAR        PRODUCT_CODE      COST  IS_TRAINING EXPENDITURE_ID      role                  time_stamp     join_key target categorical categorical categorical  numerical unused_float unused_string       unit time stamp, comparison only subroles: - include substring    0 2015-01-01 03111041 0 1 2015 010210 3.89 1 1 1 2015-01-01 03111041 0 1 2015 120310 4.66 1 10 2 2015-02-01 03111051 0 2 2015 190211 9.79 1 100 3 2015-02-01 03111402 0 2 2015 040510 2.95 1 1000 4 2015-01-01 03114161 0 1 2015 190321 2.12 1 10000 ... ... ... ... ... ... ... ... ... 2020629 2017-06-01 03708582 0 6 2017 150110 1.99 1 999995 2020630 2017-06-01 03708582 0 6 2017 150110 3.619 1 999996 2020631 2017-06-01 03708582 0 6 2017 150211 5.2727 1 999997 2020632 2017-06-01 03708582 0 6 2017 150310 4.6894 1 999998 2020633 2017-06-01 03708582 0 6 2017 160310 5.7177 1 999999 <p>     2020634 rows x 9 columns     memory usage: 128.21 MB     name: EXPENDITURES     type: getml.DataFrame </p> In\u00a0[13]: Copied! <pre>split = expenditures.rowid.as_str().update(expenditures.IS_TRAINING == 1, \"train\").update(expenditures.IS_TRAINING == 0, \"test\")\nsplit\n</pre> split = expenditures.rowid.as_str().update(expenditures.IS_TRAINING == 1, \"train\").update(expenditures.IS_TRAINING == 0, \"test\") split Out[13]: 0 train 1 train 2 train 3 train 4 train ... <p>     2020634 rows          type: StringColumnView </p> In\u00a0[14]: Copied! <pre>star_schema = getml.data.StarSchema(alias=\"POPULATION\", population=expenditures, split=split)\n\nstar_schema.join(\n    expenditures,\n    on=\"HOUSEHOLD_ID\",\n    time_stamps=\"TIME_STAMP\"\n)\n\nstar_schema.join(\n    households,\n    on=\"HOUSEHOLD_ID\",\n    relationship=getml.data.relationship.many_to_one,\n)\n\nstar_schema.join(\n    household_members,\n    on=\"HOUSEHOLD_ID\",\n)\n\nstar_schema\n</pre> star_schema = getml.data.StarSchema(alias=\"POPULATION\", population=expenditures, split=split)  star_schema.join(     expenditures,     on=\"HOUSEHOLD_ID\",     time_stamps=\"TIME_STAMP\" )  star_schema.join(     households,     on=\"HOUSEHOLD_ID\",     relationship=getml.data.relationship.many_to_one, )  star_schema.join(     household_members,     on=\"HOUSEHOLD_ID\", )  star_schema Out[14]: data model diagram EXPENDITURESHOUSEHOLDSHOUSEHOLD_MEMBERSPOPULATIONHOUSEHOLD_ID = HOUSEHOLD_IDTIME_STAMP &lt;= TIME_STAMPHOUSEHOLD_ID = HOUSEHOLD_IDRelationship: many-to-oneHOUSEHOLD_ID = HOUSEHOLD_ID staging data frames            staging table                    0 POPULATION, HOUSEHOLDS POPULATION__STAGING_TABLE_1 1 EXPENDITURES EXPENDITURES__STAGING_TABLE_2 2 HOUSEHOLD_MEMBERS HOUSEHOLD_MEMBERS__STAGING_TABLE_3 container population subset name            rows type 0 test EXPENDITURES 387583 View 1 train EXPENDITURES 1633051 View peripheral name                 rows type      0 EXPENDITURES 2020634 DataFrame 1 HOUSEHOLDS 56812 DataFrame 2 HOUSEHOLD_MEMBERS 137355 DataFrame In\u00a0[15]: Copied! <pre>ucc1 = getml.preprocessors.Substring(0, 1)\nucc2 = getml.preprocessors.Substring(0, 2)\nucc3 = getml.preprocessors.Substring(0, 3)\nucc4 = getml.preprocessors.Substring(0, 4)\nucc5 = getml.preprocessors.Substring(0, 5)\n\nmapping = getml.preprocessors.Mapping(multithreading=False)\n\nfast_prop = getml.feature_learning.FastProp(\n    aggregation=getml.feature_learning.FastProp.agg_sets.All,\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    num_threads=1,\n    sampling_factor=0.1,\n    num_features=100,\n)\n\nrelboost = getml.feature_learning.Relboost(\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    num_threads=1,\n    num_features=20,\n)\n\nfeature_selector = getml.predictors.XGBoostClassifier()\n\npredictor = getml.predictors.XGBoostClassifier(\n    booster=\"gbtree\",\n    n_estimators=100,\n    max_depth=7,\n    reg_lambda=0.0,\n    n_jobs=1\n)\n</pre> ucc1 = getml.preprocessors.Substring(0, 1) ucc2 = getml.preprocessors.Substring(0, 2) ucc3 = getml.preprocessors.Substring(0, 3) ucc4 = getml.preprocessors.Substring(0, 4) ucc5 = getml.preprocessors.Substring(0, 5)  mapping = getml.preprocessors.Mapping(multithreading=False)  fast_prop = getml.feature_learning.FastProp(     aggregation=getml.feature_learning.FastProp.agg_sets.All,     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     num_threads=1,     sampling_factor=0.1,     num_features=100, )  relboost = getml.feature_learning.Relboost(     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     num_threads=1,     num_features=20, )  feature_selector = getml.predictors.XGBoostClassifier()  predictor = getml.predictors.XGBoostClassifier(     booster=\"gbtree\",     n_estimators=100,     max_depth=7,     reg_lambda=0.0,     n_jobs=1 ) In\u00a0[16]: Copied! <pre>pipe1 = getml.pipeline.Pipeline(\n    tags=[\"FastProp\"],\n    data_model=star_schema.data_model,\n    share_selected_features=0.4,\n    preprocessors=[mapping],\n    feature_learners=fast_prop,\n    feature_selectors=feature_selector,\n    predictors=predictor\n)\n\npipe1\n</pre> pipe1 = getml.pipeline.Pipeline(     tags=[\"FastProp\"],     data_model=star_schema.data_model,     share_selected_features=0.4,     preprocessors=[mapping],     feature_learners=fast_prop,     feature_selectors=feature_selector,     predictors=predictor )  pipe1 Out[16]: <pre>Pipeline(data_model='POPULATION',\n         feature_learners=['FastProp'],\n         feature_selectors=['XGBoostClassifier'],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['EXPENDITURES', 'HOUSEHOLDS', 'HOUSEHOLD_MEMBERS'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.4,\n         tags=['FastProp'])</pre> In\u00a0[17]: Copied! <pre>pipe2 = getml.pipeline.Pipeline(\n    tags=[\"Relboost\"],\n    data_model=star_schema.data_model,\n    share_selected_features=0.9,\n    preprocessors=[ucc1, ucc2, ucc3, ucc4, ucc5, mapping],\n    feature_learners=relboost,\n    feature_selectors=feature_selector,\n    predictors=predictor\n)\n\npipe2\n</pre> pipe2 = getml.pipeline.Pipeline(     tags=[\"Relboost\"],     data_model=star_schema.data_model,     share_selected_features=0.9,     preprocessors=[ucc1, ucc2, ucc3, ucc4, ucc5, mapping],     feature_learners=relboost,     feature_selectors=feature_selector,     predictors=predictor )  pipe2 Out[17]: <pre>Pipeline(data_model='POPULATION',\n         feature_learners=['Relboost'],\n         feature_selectors=['XGBoostClassifier'],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['EXPENDITURES', 'HOUSEHOLDS', 'HOUSEHOLD_MEMBERS'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Substring', 'Substring', 'Substring', 'Substring', 'Substring',\n                        'Mapping'],\n         share_selected_features=0.9,\n         tags=['Relboost'])</pre> In\u00a0[18]: Copied! <pre>pipe3 = getml.pipeline.Pipeline(\n    tags=[\"FastProp\", \"Relboost\"],\n    data_model=star_schema.data_model,\n    share_selected_features=0.2,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop, relboost],\n    feature_selectors=feature_selector,\n    predictors=predictor\n)\n\npipe3\n</pre> pipe3 = getml.pipeline.Pipeline(     tags=[\"FastProp\", \"Relboost\"],     data_model=star_schema.data_model,     share_selected_features=0.2,     preprocessors=[mapping],     feature_learners=[fast_prop, relboost],     feature_selectors=feature_selector,     predictors=predictor )  pipe3 Out[18]: <pre>Pipeline(data_model='POPULATION',\n         feature_learners=['FastProp', 'Relboost'],\n         feature_selectors=['XGBoostClassifier'],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['EXPENDITURES', 'HOUSEHOLDS', 'HOUSEHOLD_MEMBERS'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.2,\n         tags=['FastProp', 'Relboost'])</pre> <p><code>.fit(...)</code> will automatically call <code>.check(...)</code>, but it is always a good idea to call <code>.check(...)</code> separately, so we still have time for some last-minute fixes.</p> In\u00a0[19]: Copied! <pre>pipe1.check(star_schema.train)\n</pre> pipe1.check(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:20, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\nOK.\n</pre> In\u00a0[20]: Copied! <pre>pipe1.fit(star_schema.train)\n</pre> pipe1.fit(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 418 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:40, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 01:54, remaining: 00:00]          \nXGBoost: Training as feature selector... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 06:58, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 07:29, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:17m:4.359167\n\n</pre> Out[20]: <pre>Pipeline(data_model='POPULATION',\n         feature_learners=['FastProp'],\n         feature_selectors=['XGBoostClassifier'],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['EXPENDITURES', 'HOUSEHOLDS', 'HOUSEHOLD_MEMBERS'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.4,\n         tags=['FastProp', 'container-2IKXQ4'])</pre> In\u00a0[21]: Copied! <pre>pipe2.check(star_schema.train)\n</pre> pipe2.check(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:55, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\n</pre> In\u00a0[22]: Copied! <pre>pipe2.fit(star_schema.train)\n</pre> pipe2.fit(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:05, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:07, remaining: 00:00]          \nRelboost: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 08:34, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 06:02, remaining: 00:00]          \nXGBoost: Training as feature selector... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 02:55, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 06:32, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:24m:12.258032\n\n</pre> Out[22]: <pre>Pipeline(data_model='POPULATION',\n         feature_learners=['Relboost'],\n         feature_selectors=['XGBoostClassifier'],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['EXPENDITURES', 'HOUSEHOLDS', 'HOUSEHOLD_MEMBERS'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Substring', 'Substring', 'Substring', 'Substring', 'Substring',\n                        'Mapping'],\n         share_selected_features=0.9,\n         tags=['Relboost', 'container-2IKXQ4'])</pre> In\u00a0[23]: Copied! <pre>pipe3.check(star_schema.train)\n</pre> pipe3.check(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\n</pre> In\u00a0[24]: Copied! <pre>pipe3.fit(star_schema.train)\n</pre> pipe3.fit(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRetrieving features (because a similar feature learner has already been fitted)... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 05:33, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 01:36, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 05:17, remaining: 00:00]          \nXGBoost: Training as feature selector... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 06:30, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 03:50, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:22m:47.586342\n\n</pre> Out[24]: <pre>Pipeline(data_model='POPULATION',\n         feature_learners=['FastProp', 'Relboost'],\n         feature_selectors=['XGBoostClassifier'],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['EXPENDITURES', 'HOUSEHOLDS', 'HOUSEHOLD_MEMBERS'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.2,\n         tags=['FastProp', 'Relboost', 'container-2IKXQ4'])</pre> In\u00a0[25]: Copied! <pre>pipe1.score(star_schema.test)\n</pre> pipe1.score(star_schema.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:05, remaining: 00:00]          \n\n</pre> Out[25]: date time           set used target accuracy     auc cross entropy 0 2024-02-21 16:40:54 train GIFT 0.9827 0.9351 0.06029 1 2024-02-21 17:29:07 test GIFT 0.9804 0.8658 0.07702 In\u00a0[26]: Copied! <pre>pipe2.score(star_schema.test)\n</pre> pipe2.score(star_schema.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:04, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 01:41, remaining: 00:00]          \n\n</pre> Out[26]: date time           set used target accuracy     auc cross entropy 0 2024-02-21 17:06:09 train GIFT 0.9823 0.9205 0.0637 1 2024-02-21 17:30:53 test GIFT 0.9806 0.8619 0.07703 In\u00a0[27]: Copied! <pre>pipe3.score(star_schema.test)\n</pre> pipe3.score(star_schema.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:03, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:06, remaining: 00:00]          \n\n</pre> Out[27]: date time           set used target accuracy     auc cross entropy 0 2024-02-21 17:28:59 train GIFT 0.9825 0.9325 0.0611 1 2024-02-21 17:31:04 test GIFT 0.9806 0.868 0.07648 In\u00a0[28]: Copied! <pre>LENGTH=50\n\nnames, correlations = pipe1.features.correlations()\n\nplt.subplots(figsize=(20, 10))\n\nplt.bar(names[:LENGTH], correlations[:LENGTH])\n\nplt.title(\"feature correlations\")\nplt.grid(True)\nplt.xlabel(\"features\")\nplt.ylabel(\"correlations\")\nplt.xticks(rotation='vertical')\n\nplt.show()\n</pre> LENGTH=50  names, correlations = pipe1.features.correlations()  plt.subplots(figsize=(20, 10))  plt.bar(names[:LENGTH], correlations[:LENGTH])  plt.title(\"feature correlations\") plt.grid(True) plt.xlabel(\"features\") plt.ylabel(\"correlations\") plt.xticks(rotation='vertical')  plt.show() In\u00a0[29]: Copied! <pre>LENGTH=50\n\nnames, correlations = pipe2.features.correlations()\n\nplt.subplots(figsize=(20, 10))\n\nplt.bar(names[:LENGTH], correlations[:LENGTH])\n\nplt.title(\"feature correlations\")\nplt.grid(True)\nplt.xlabel(\"features\")\nplt.ylabel(\"correlations\")\nplt.xticks(rotation='vertical')\n\nplt.show()\n</pre> LENGTH=50  names, correlations = pipe2.features.correlations()  plt.subplots(figsize=(20, 10))  plt.bar(names[:LENGTH], correlations[:LENGTH])  plt.title(\"feature correlations\") plt.grid(True) plt.xlabel(\"features\") plt.ylabel(\"correlations\") plt.xticks(rotation='vertical')  plt.show() <p>We can express the features in SQLite3:</p> <p>Because getML uses a feature learning approach, the concept of feature importances can also be carried over to the individual columns.</p> In\u00a0[30]: Copied! <pre>names, importances = pipe1.columns.importances()\n\nplt.subplots(figsize=(20, 10))\n\nplt.bar(names, importances)\n\nplt.title(\"column importances\")\nplt.grid(True)\nplt.xlabel(\"columns\")\nplt.ylabel(\"importance\")\nplt.xticks(rotation='vertical')\n\nplt.show()\n</pre> names, importances = pipe1.columns.importances()  plt.subplots(figsize=(20, 10))  plt.bar(names, importances)  plt.title(\"column importances\") plt.grid(True) plt.xlabel(\"columns\") plt.ylabel(\"importance\") plt.xticks(rotation='vertical')  plt.show() In\u00a0[31]: Copied! <pre>names, importances = pipe2.columns.importances()\n\nplt.subplots(figsize=(20, 10))\n\nplt.bar(names, importances)\n\nplt.title(\"column importances\")\nplt.grid(True)\nplt.xlabel(\"columns\")\nplt.ylabel(\"importance\")\nplt.xticks(rotation='vertical')\n\nplt.show()\n</pre> names, importances = pipe2.columns.importances()  plt.subplots(figsize=(20, 10))  plt.bar(names, importances)  plt.title(\"column importances\") plt.grid(True) plt.xlabel(\"columns\") plt.ylabel(\"importance\") plt.xticks(rotation='vertical')  plt.show() <p>The most important features look as follows:</p> In\u00a0[32]: Copied! <pre>pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name]\n</pre> pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name] Out[32]: <pre>DROP TABLE IF EXISTS \"PRODUCT_CODE__MAPPING_TARGET_1_AVG\";\n\nCREATE TABLE \"PRODUCT_CODE__MAPPING_TARGET_1_AVG\"(\"key\" TEXT, \"value\" REAL);\n\nINSERT INTO \"PRODUCT_CODE__MAPPING_TARGET_1_AVG\" (\"key\", \"value\")\nVALUES('410901', 0.5265553869499241),\n      ('410140', 0.5248618784530387),\n      ('004190', 0.5073846153846154),\n      ('410120', 0.5013123359580053),\n      ('410110', 0.4444444444444444),\n      ('004100', 0.3336306868867083),\n      ('390110', 0.3132530120481928),\n      ('390120', 0.3067484662576687),\n      ('410130', 0.2967448902346707),\n      ('370110', 0.2948717948717949),\n      ('370212', 0.2944444444444445),\n      ('370220', 0.2920353982300885),\n      ('680140', 0.288135593220339),\n      ('390322', 0.2795918367346939),\n      ('390321', 0.2764227642276423),\n      ('370901', 0.271948608137045),\n      ('390210', 0.2579837194740138),\n      ('370125', 0.2519157088122606),\n      ('390310', 0.2443181818181818),\n      ('390223', 0.2344706911636046),\n      ('390230', 0.2238442822384428),\n      ('370211', 0.2185714285714286),\n      ('370314', 0.2182952182952183),\n      ('400220', 0.2164179104477612),\n      ('610110', 0.2162868883078072),\n      ('360320', 0.2151898734177215),\n      ('590220', 0.2075471698113208),\n      ('370213', 0.2015968063872255),\n      ('400210', 0.1944764096662831),\n      ('430120', 0.194006309148265),\n      ('320130', 0.1899441340782123),\n      ('390901', 0.1797752808988764),\n      ('330410', 0.1751831107281344),\n      ('380410', 0.1386392811296534),\n      ('590230', 0.13469068128426),\n      ('360350', 0.1321279554937413),\n      ('360210', 0.1305555555555556),\n      ('290420', 0.1282051282051282),\n      ('280220', 0.1231884057971015),\n      ('320903', 0.1229724632214259),\n      ('360420', 0.1222091656874266),\n      ('005000', 0.1219512195121951),\n      ('660900', 0.1205479452054795),\n      ('320345', 0.1176205497972059),\n      ('610902', 0.1162790697674419),\n      ('660110', 0.111731843575419),\n      ('600900', 0.1111111111111111),\n      ('670110', 0.1111111111111111),\n      ('320233', 0.1108969866853539),\n      ('610230', 0.11),\n      ('660210', 0.1097922848664688),\n      ('610901', 0.1097560975609756),\n      ('380510', 0.1081081081081081),\n      ('290310', 0.1044776119402985),\n      ('280120', 0.1030640668523677),\n      ('380901', 0.1010141987829615),\n      ('320521', 0.1009174311926606),\n      ('360330', 0.1004366812227074),\n      ('360311', 0.09981167608286252),\n      ('430110', 0.09863945578231292),\n      ('300320', 0.0975609756097561),\n      ('360312', 0.09716599190283401),\n      ('660000', 0.09413886384129846),\n      ('600430', 0.09302325581395349),\n      ('380110', 0.09302325581395349),\n      ('310231', 0.09090909090909091),\n      ('004000', 0.08723998758149643),\n      ('600410', 0.08408408408408409),\n      ('310210', 0.08333333333333333),\n      ('340120', 0.08333333333333333),\n      ('430130', 0.08226221079691516),\n      ('600210', 0.08190476190476191),\n      ('380315', 0.08014981273408239),\n      ('610120', 0.07865168539325842),\n      ('620610', 0.07755102040816327),\n      ('360513', 0.07722969606377678),\n      ('280140', 0.07646356033452807),\n      ('320380', 0.07645788336933046),\n      ('620213', 0.07375643224699828),\n      ('620510', 0.07370393504059962),\n      ('380430', 0.07358390682901006),\n      ('310316', 0.07329842931937172),\n      ('310232', 0.0726950354609929),\n      ('200210', 0.07258064516129033),\n      ('530110', 0.07235621521335807),\n      ('380320', 0.07142857142857142),\n      ('640420', 0.06923076923076923),\n      ('620214', 0.0689900426742532),\n      ('610130', 0.06882591093117409),\n      ('290410', 0.06748466257668712),\n      ('380420', 0.06734816596512327),\n      ('400310', 0.0672059738643435),\n      ('690117', 0.06666666666666667),\n      ('610903', 0.06578947368421052),\n      ('310220', 0.06555863342566944),\n      ('320330', 0.06554307116104868),\n      ('400110', 0.06538692261547691),\n      ('640120', 0.06442953020134229),\n      ('690230', 0.0641025641025641),\n      ('620330', 0.06329113924050633),\n      ('420115', 0.06281407035175879),\n      ('380311', 0.0625),\n      ('310340', 0.06231454005934718),\n      ('320370', 0.06196746707978312),\n      ('380340', 0.06157635467980296),\n      ('380210', 0.06014492753623189),\n      ('620112', 0.05970149253731343),\n      ('340110', 0.05929824561403509),\n      ('320901', 0.05747126436781609),\n      ('280110', 0.05726600985221675),\n      ('290120', 0.05673758865248227),\n      ('320150', 0.05652173913043478),\n      ('240220', 0.05647840531561462),\n      ('340907', 0.05555555555555555),\n      ('600310', 0.05521472392638037),\n      ('320221', 0.05381727158948686),\n      ('320522', 0.05371900826446281),\n      ('620913', 0.05333333333333334),\n      ('340510', 0.052734375),\n      ('640130', 0.05263157894736842),\n      ('310332', 0.05128205128205128),\n      ('320232', 0.05029013539651837),\n      ('380333', 0.0501577287066246),\n      ('690118', 0.05),\n      ('670903', 0.04895104895104895),\n      ('320905', 0.04766031195840555),\n      ('320627', 0.04761904761904762),\n      ('320902', 0.04666666666666667),\n      ('690110', 0.04666666666666667),\n      ('150110', 0.04635643740546312),\n      ('620221', 0.04615384615384616),\n      ('670901', 0.04597701149425287),\n      ('001000', 0.04587155963302753),\n      ('670310', 0.04553734061930783),\n      ('340610', 0.04444444444444445),\n      ('200410', 0.04397394136807817),\n      ('300900', 0.04375),\n      ('610320', 0.04300578034682081),\n      ('300110', 0.0425531914893617),\n      ('002000', 0.0418848167539267),\n      ('680220', 0.04184704184704185),\n      ('570901', 0.04081632653061224),\n      ('280210', 0.04081632653061224),\n      ('600420', 0.04044489383215369),\n      ('320420', 0.0400890868596882),\n      ('290440', 0.038860103626943),\n      ('200310', 0.03872966692486444),\n      ('310900', 0.0380952380952381),\n      ('520550', 0.03773584905660377),\n      ('690116', 0.03773584905660377),\n      ('020410', 0.03773262762506403),\n      ('440130', 0.03759398496240601),\n      ('380902', 0.03571428571428571),\n      ('550320', 0.03547297297297297),\n      ('290110', 0.03539823008849557),\n      ('590210', 0.03476151980598222),\n      ('320904', 0.03454231433506045),\n      ('490311', 0.03448275862068965),\n      ('620310', 0.03422053231939164),\n      ('220000', 0.03418803418803419),\n      ('320120', 0.03355704697986577),\n      ('240310', 0.03343949044585987),\n      ('310351', 0.03333333333333333),\n      ('640310', 0.03329679364209372),\n      ('670902', 0.03174603174603174),\n      ('680903', 0.03137789904502047),\n      ('310140', 0.0308641975308642),\n      ('620420', 0.03061224489795918),\n      ('630220', 0.03052325581395349),\n      ('330610', 0.03022860381636123),\n      ('330510', 0.02971188475390156),\n      ('180620', 0.02942668696093353),\n      ('240900', 0.02941176470588235),\n      ('550330', 0.02935420743639922),\n      ('320610', 0.02929427430093209),\n      ('620710', 0.02877697841726619),\n      ('290320', 0.02877697841726619),\n      ('200111', 0.02867072111207646),\n      ('240320', 0.02842928216062544),\n      ('310352', 0.02838427947598253),\n      ('320410', 0.02791625124626122),\n      ('300218', 0.02777777777777778),\n      ('320110', 0.02768166089965398),\n      ('620121', 0.02765208647561589),\n      ('340210', 0.02722323049001815),\n      ('240210', 0.02707581227436823),\n      ('440150', 0.02702702702702703),\n      ('320140', 0.02697022767075306),\n      ('640220', 0.02683461117196057),\n      ('640410', 0.026232741617357),\n      ('310335', 0.02593659942363112),\n      ('490315', 0.02564102564102564),\n      ('340901', 0.02542372881355932),\n      ('610310', 0.02461584365209608),\n      ('680110', 0.02362204724409449),\n      ('340903', 0.0234375),\n      ('480213', 0.0231811697574893),\n      ('320430', 0.02272727272727273),\n      ('230000', 0.02272727272727273),\n      ('640210', 0.02267002518891688),\n      ('550310', 0.02246796559592768),\n      ('490110', 0.02173913043478261),\n      ('620410', 0.02165087956698241),\n      ('340913', 0.02127659574468085),\n      ('340906', 0.02127659574468085),\n      ('590110', 0.0209366391184573),\n      ('620810', 0.02090592334494774),\n      ('020710', 0.02085600290170475),\n      ('620926', 0.02076875387476751),\n      ('480212', 0.02055622732769045),\n      ('020510', 0.0202097074243193),\n      ('650210', 0.02016868353502017),\n      ('530510', 0.02005730659025788),\n      ('520310', 0.02),\n      ('480110', 0.01970443349753695),\n      ('550110', 0.0194300518134715),\n      ('650110', 0.0190424374319913),\n      ('320511', 0.01829268292682927),\n      ('240120', 0.01818181818181818),\n      ('040610', 0.01785714285714286),\n      ('170531', 0.0177293934681182),\n      ('550210', 0.01761658031088083),\n      ('290430', 0.01748251748251748),\n      ('002100', 0.01715481171548117),\n      ('150310', 0.01708217913204063),\n      ('560310', 0.01682692307692308),\n      ('640110', 0.01674500587544066),\n      ('640430', 0.01648351648351648),\n      ('570000', 0.01633393829401089),\n      ('240110', 0.0162052667116813),\n      ('690119', 0.01618122977346278),\n      ('630110', 0.0158344666796192),\n      ('330310', 0.01570146818923328),\n      ('020820', 0.01567783584383646),\n      ('130320', 0.0156165858912224),\n      ('630210', 0.0155902004454343),\n      ('020610', 0.01553829078801332),\n      ('010120', 0.01547231270358306),\n      ('180310', 0.01535880227155395),\n      ('550410', 0.01529571719918423),\n      ('360110', 0.01515151515151515),\n      ('620114', 0.01492537313432836),\n      ('440210', 0.01488095238095238),\n      ('470220', 0.01478743068391867),\n      ('620111', 0.01471389645776567),\n      ('330210', 0.01441871961769795),\n      ('140320', 0.01423487544483986),\n      ('340520', 0.01411100658513641),\n      ('560210', 0.01355661881977671),\n      ('530311', 0.01341184167484462),\n      ('330110', 0.01330895052321447),\n      ('050900', 0.0131578947368421),\n      ('250900', 0.01309707241910632),\n      ('690120', 0.01305483028720627),\n      ('490300', 0.01298701298701299),\n      ('180320', 0.01298701298701299),\n      ('170533', 0.01296982530439386),\n      ('540000', 0.01271259233808624),\n      ('170510', 0.01269971323228185),\n      ('620930', 0.01252609603340292),\n      ('340410', 0.01241642788920726),\n      ('270000', 0.01241039905852145),\n      ('520110', 0.01237964236588721),\n      ('560400', 0.01210898082744702),\n      ('180612', 0.01201452919810003),\n      ('620320', 0.01185770750988142),\n      ('470211', 0.01179941002949852),\n      ('180520', 0.01179574732267577),\n      ('100410', 0.01164329187615771),\n      ('310331', 0.01162790697674419),\n      ('530412', 0.01158504476040021),\n      ('020810', 0.01154575219713941),\n      ('530210', 0.01152737752161383),\n      ('220110', 0.01149425287356322),\n      ('320630', 0.01142857142857143),\n      ('520531', 0.01112484548825711),\n      ('180710', 0.01103708190322364),\n      ('030810', 0.01092896174863388),\n      ('130310', 0.01086556169429098),\n      ('170210', 0.01082262080178853),\n      ('340620', 0.01075268817204301),\n      ('999900', 0.01062416998671979),\n      ('030210', 0.01055662188099808),\n      ('030510', 0.01044277360066834),\n      ('170110', 0.01034780109226789),\n      ('220210', 0.01027397260273973),\n      ('680902', 0.01025641025641026),\n      ('020310', 0.01021667580910587),\n      ('130212', 0.009969657563935847),\n      ('030710', 0.009891435464414958),\n      ('140420', 0.009844993715961458),\n      ('560330', 0.009771986970684038),\n      ('270210', 0.009420631182289214),\n      ('140220', 0.009351432880844645),\n      ('160320', 0.00933609958506224),\n      ('560110', 0.009322560596643879),\n      ('170520', 0.009291360421578144),\n      ('230110', 0.009202453987730062),\n      ('170310', 0.009154113557358054),\n      ('180110', 0.009134615384615385),\n      ('140210', 0.009130282102305981),\n      ('160212', 0.009098914000587027),\n      ('050410', 0.008833922261484099),\n      ('100210', 0.008741319144525446),\n      ('170532', 0.008554705087798289),\n      ('620912', 0.008553654743390357),\n      ('090210', 0.008506616257088847),\n      ('490000', 0.008489564909798374),\n      ('170410', 0.008431932544539644),\n      ('210210', 0.00823045267489712),\n      ('020620', 0.008152173913043478),\n      ('340310', 0.008032128514056224),\n      ('110410', 0.007990834884720034),\n      ('490312', 0.007977207977207978),\n      ('210110', 0.007972665148063782),\n      ('180420', 0.007866728366496992),\n      ('180220', 0.007703887363853715),\n      ('010210', 0.007637017070979336),\n      ('180510', 0.007588713125267208),\n      ('470111', 0.007556238768484639),\n      ('060310', 0.007518796992481203),\n      ('050310', 0.007514761137949544),\n      ('030610', 0.007317073170731708),\n      ('180611', 0.007287611061195967),\n      ('010320', 0.007257694074414332),\n      ('500110', 0.007106598984771574),\n      ('040510', 0.006984459577440196),\n      ('110310', 0.006973269134982567),\n      ('250220', 0.006944444444444444),\n      ('580000', 0.006857142857142857),\n      ('020210', 0.006824146981627296),\n      ('180210', 0.006806282722513089),\n      ('040410', 0.006790744466800805),\n      ('050110', 0.00675990675990676),\n      ('010110', 0.006644518272425249),\n      ('180410', 0.006634078212290503),\n      ('140230', 0.00663265306122449),\n      ('050210', 0.00662133142448103),\n      ('160310', 0.006574892130675981),\n      ('020110', 0.006501360749924402),\n      ('070110', 0.006377551020408163),\n      ('030310', 0.00625),\n      ('120310', 0.006177540831006178),\n      ('100510', 0.006119326874043855),\n      ('030410', 0.006116207951070336),\n      ('690114', 0.006105834464043419),\n      ('110510', 0.005989518342899925),\n      ('160211', 0.005981308411214953),\n      ('150211', 0.005960568546538285),\n      ('130211', 0.005947955390334572),\n      ('520541', 0.005911778080945885),\n      ('120210', 0.005798018131983976),\n      ('040110', 0.005780346820809248),\n      ('260110', 0.005772763054316453),\n      ('070240', 0.005749668288367979),\n      ('090110', 0.005704227647576519),\n      ('110210', 0.005692403229145104),\n      ('030110', 0.005622410731899783),\n      ('260210', 0.0055542698449433),\n      ('080110', 0.005548549810844893),\n      ('120110', 0.005436931593515224),\n      ('040310', 0.005404077622205846),\n      ('250210', 0.005342831700801425),\n      ('010310', 0.005331627212625293),\n      ('440120', 0.005319148936170213),\n      ('100110', 0.005308219178082192),\n      ('470112', 0.005277044854881266),\n      ('110110', 0.005152378864284149),\n      ('160110', 0.005109489051094891),\n      ('270410', 0.00496031746031746),\n      ('060110', 0.004922542348342262),\n      ('520516', 0.004901960784313725),\n      ('270310', 0.004885574697865775),\n      ('120410', 0.004865350089766607),\n      ('220120', 0.004815409309791332),\n      ('040210', 0.004786324786324786),\n      ('070230', 0.004725554343874954),\n      ('130110', 0.004694835680751174),\n      ('140110', 0.004555336991406978),\n      ('340530', 0.004530011325028313),\n      ('060210', 0.00400114318376679),\n      ('230900', 0.003992015968063872),\n      ('520410', 0.003937007874015748),\n      ('140340', 0.003897369275738876),\n      ('490313', 0.003875968992248062),\n      ('009000', 0.002952029520295203),\n      ('350110', 0.002881844380403458),\n      ('140330', 0.002380952380952381),\n      ('130122', 0.002169197396963124),\n      ('150212', 0.001451378809869376),\n      ('130121', 0.001373626373626374),\n      ('190323', 0.0009389671361502347),\n      ('190311', 0.0008796003096193089),\n      ('200532', 0.0005934718100890207),\n      ('190312', 0.0005761198329252485),\n      ('190314', 0.0004549590536851683),\n      ('190324', 0.0004541326067211626),\n      ('200522', 0.0004464285714285714),\n      ('190212', 0.0004089793692629283),\n      ('190114', 0.0003787878787878788),\n      ('190112', 0.0003610760064993681),\n      ('190322', 0.0002765869174388052),\n      ('190211', 0.0002144925463840132),\n      ('190111', 0.0002058036633052068),\n      ('200512', 0.0001853911753800519),\n      ('190321', 7.427213309566251e-05),\n      ('440140', 0),\n      ('200112', 0),\n      ('620925', 0),\n      ('250110', 0),\n      ('200531', 0),\n      ('310242', 0),\n      ('600130', 0),\n      ('580901', 0),\n      ('200521', 0),\n      ('490316', 0),\n      ('200523', 0),\n      ('190113', 0),\n      ('310241', 0),\n      ('550340', 0),\n      ('450350', 0),\n      ('190214', 0),\n      ('300410', 0),\n      ('530903', 0),\n      ('200513', 0),\n      ('140410', 0),\n      ('002200', 0),\n      ('630900', 0),\n      ('680210', 0),\n      ('290210', 0),\n      ('140310', 0),\n      ('200533', 0),\n      ('440110', 0),\n      ('190313', 0),\n      ('190213', 0),\n      ('270311', 0),\n      ('270900', 0),\n      ('200511', 0);\n\nALTER TABLE \"POPULATION__STAGING_TABLE_1\" ADD COLUMN \"product_code__mapping_target_1_avg\" REAL;\n\nUPDATE \"POPULATION__STAGING_TABLE_1\" SET \"product_code__mapping_target_1_avg\" = 0.0;\n\nUPDATE \"POPULATION__STAGING_TABLE_1\"\nSET \"product_code__mapping_target_1_avg\" = \"PRODUCT_CODE__MAPPING_TARGET_1_AVG\".\"value\"\nFROM \"PRODUCT_CODE__MAPPING_TARGET_1_AVG\"\nWHERE \"POPULATION__STAGING_TABLE_1\".\"product_code\" = \"PRODUCT_CODE__MAPPING_TARGET_1_AVG\".\"key\";\n\nDROP TABLE IF EXISTS \"PRODUCT_CODE__MAPPING_TARGET_1_AVG\";\n</pre> In\u00a0[33]: Copied! <pre>pipe2.features.to_sql()[pipe2.features.sort(by=\"importances\")[0].name]\n</pre> pipe2.features.to_sql()[pipe2.features.sort(by=\"importances\")[0].name] Out[33]: <pre>DROP TABLE IF EXISTS \"PRODUCT_CODE__MAPPING_TARGET_1_AVG\";\n\nCREATE TABLE \"PRODUCT_CODE__MAPPING_TARGET_1_AVG\"(\"key\" TEXT, \"value\" REAL);\n\nINSERT INTO \"PRODUCT_CODE__MAPPING_TARGET_1_AVG\" (\"key\", \"value\")\nVALUES('410901', 0.5265553869499241),\n      ('410140', 0.5248618784530387),\n      ('004190', 0.5073846153846154),\n      ('410120', 0.5013123359580053),\n      ('410110', 0.4444444444444444),\n      ('004100', 0.3336306868867083),\n      ('390110', 0.3132530120481928),\n      ('390120', 0.3067484662576687),\n      ('410130', 0.2967448902346707),\n      ('370110', 0.2948717948717949),\n      ('370212', 0.2944444444444445),\n      ('370220', 0.2920353982300885),\n      ('680140', 0.288135593220339),\n      ('390322', 0.2795918367346939),\n      ('390321', 0.2764227642276423),\n      ('370901', 0.271948608137045),\n      ('390210', 0.2579837194740138),\n      ('370125', 0.2519157088122606),\n      ('390310', 0.2443181818181818),\n      ('390223', 0.2344706911636046),\n      ('390230', 0.2238442822384428),\n      ('370211', 0.2185714285714286),\n      ('370314', 0.2182952182952183),\n      ('400220', 0.2164179104477612),\n      ('610110', 0.2162868883078072),\n      ('360320', 0.2151898734177215),\n      ('590220', 0.2075471698113208),\n      ('370213', 0.2015968063872255),\n      ('400210', 0.1944764096662831),\n      ('430120', 0.194006309148265),\n      ('320130', 0.1899441340782123),\n      ('390901', 0.1797752808988764),\n      ('330410', 0.1751831107281344),\n      ('380410', 0.1386392811296534),\n      ('590230', 0.13469068128426),\n      ('360350', 0.1321279554937413),\n      ('360210', 0.1305555555555556),\n      ('290420', 0.1282051282051282),\n      ('280220', 0.1231884057971015),\n      ('320903', 0.1229724632214259),\n      ('360420', 0.1222091656874266),\n      ('005000', 0.1219512195121951),\n      ('660900', 0.1205479452054795),\n      ('320345', 0.1176205497972059),\n      ('610902', 0.1162790697674419),\n      ('660110', 0.111731843575419),\n      ('600900', 0.1111111111111111),\n      ('670110', 0.1111111111111111),\n      ('320233', 0.1108969866853539),\n      ('610230', 0.11),\n      ('660210', 0.1097922848664688),\n      ('610901', 0.1097560975609756),\n      ('380510', 0.1081081081081081),\n      ('290310', 0.1044776119402985),\n      ('280120', 0.1030640668523677),\n      ('380901', 0.1010141987829615),\n      ('320521', 0.1009174311926606),\n      ('360330', 0.1004366812227074),\n      ('360311', 0.09981167608286252),\n      ('430110', 0.09863945578231292),\n      ('300320', 0.0975609756097561),\n      ('360312', 0.09716599190283401),\n      ('660000', 0.09413886384129846),\n      ('600430', 0.09302325581395349),\n      ('380110', 0.09302325581395349),\n      ('310231', 0.09090909090909091),\n      ('004000', 0.08723998758149643),\n      ('600410', 0.08408408408408409),\n      ('310210', 0.08333333333333333),\n      ('340120', 0.08333333333333333),\n      ('430130', 0.08226221079691516),\n      ('600210', 0.08190476190476191),\n      ('380315', 0.08014981273408239),\n      ('610120', 0.07865168539325842),\n      ('620610', 0.07755102040816327),\n      ('360513', 0.07722969606377678),\n      ('280140', 0.07646356033452807),\n      ('320380', 0.07645788336933046),\n      ('620213', 0.07375643224699828),\n      ('620510', 0.07370393504059962),\n      ('380430', 0.07358390682901006),\n      ('310316', 0.07329842931937172),\n      ('310232', 0.0726950354609929),\n      ('200210', 0.07258064516129033),\n      ('530110', 0.07235621521335807),\n      ('380320', 0.07142857142857142),\n      ('640420', 0.06923076923076923),\n      ('620214', 0.0689900426742532),\n      ('610130', 0.06882591093117409),\n      ('290410', 0.06748466257668712),\n      ('380420', 0.06734816596512327),\n      ('400310', 0.0672059738643435),\n      ('690117', 0.06666666666666667),\n      ('610903', 0.06578947368421052),\n      ('310220', 0.06555863342566944),\n      ('320330', 0.06554307116104868),\n      ('400110', 0.06538692261547691),\n      ('640120', 0.06442953020134229),\n      ('690230', 0.0641025641025641),\n      ('620330', 0.06329113924050633),\n      ('420115', 0.06281407035175879),\n      ('380311', 0.0625),\n      ('310340', 0.06231454005934718),\n      ('320370', 0.06196746707978312),\n      ('380340', 0.06157635467980296),\n      ('380210', 0.06014492753623189),\n      ('620112', 0.05970149253731343),\n      ('340110', 0.05929824561403509),\n      ('320901', 0.05747126436781609),\n      ('280110', 0.05726600985221675),\n      ('290120', 0.05673758865248227),\n      ('320150', 0.05652173913043478),\n      ('240220', 0.05647840531561462),\n      ('340907', 0.05555555555555555),\n      ('600310', 0.05521472392638037),\n      ('320221', 0.05381727158948686),\n      ('320522', 0.05371900826446281),\n      ('620913', 0.05333333333333334),\n      ('340510', 0.052734375),\n      ('640130', 0.05263157894736842),\n      ('310332', 0.05128205128205128),\n      ('320232', 0.05029013539651837),\n      ('380333', 0.0501577287066246),\n      ('690118', 0.05),\n      ('670903', 0.04895104895104895),\n      ('320905', 0.04766031195840555),\n      ('320627', 0.04761904761904762),\n      ('320902', 0.04666666666666667),\n      ('690110', 0.04666666666666667),\n      ('150110', 0.04635643740546312),\n      ('620221', 0.04615384615384616),\n      ('670901', 0.04597701149425287),\n      ('001000', 0.04587155963302753),\n      ('670310', 0.04553734061930783),\n      ('340610', 0.04444444444444445),\n      ('200410', 0.04397394136807817),\n      ('300900', 0.04375),\n      ('610320', 0.04300578034682081),\n      ('300110', 0.0425531914893617),\n      ('002000', 0.0418848167539267),\n      ('680220', 0.04184704184704185),\n      ('570901', 0.04081632653061224),\n      ('280210', 0.04081632653061224),\n      ('600420', 0.04044489383215369),\n      ('320420', 0.0400890868596882),\n      ('290440', 0.038860103626943),\n      ('200310', 0.03872966692486444),\n      ('310900', 0.0380952380952381),\n      ('520550', 0.03773584905660377),\n      ('690116', 0.03773584905660377),\n      ('020410', 0.03773262762506403),\n      ('440130', 0.03759398496240601),\n      ('380902', 0.03571428571428571),\n      ('550320', 0.03547297297297297),\n      ('290110', 0.03539823008849557),\n      ('590210', 0.03476151980598222),\n      ('320904', 0.03454231433506045),\n      ('490311', 0.03448275862068965),\n      ('620310', 0.03422053231939164),\n      ('220000', 0.03418803418803419),\n      ('320120', 0.03355704697986577),\n      ('240310', 0.03343949044585987),\n      ('310351', 0.03333333333333333),\n      ('640310', 0.03329679364209372),\n      ('670902', 0.03174603174603174),\n      ('680903', 0.03137789904502047),\n      ('310140', 0.0308641975308642),\n      ('620420', 0.03061224489795918),\n      ('630220', 0.03052325581395349),\n      ('330610', 0.03022860381636123),\n      ('330510', 0.02971188475390156),\n      ('180620', 0.02942668696093353),\n      ('240900', 0.02941176470588235),\n      ('550330', 0.02935420743639922),\n      ('320610', 0.02929427430093209),\n      ('620710', 0.02877697841726619),\n      ('290320', 0.02877697841726619),\n      ('200111', 0.02867072111207646),\n      ('240320', 0.02842928216062544),\n      ('310352', 0.02838427947598253),\n      ('320410', 0.02791625124626122),\n      ('300218', 0.02777777777777778),\n      ('320110', 0.02768166089965398),\n      ('620121', 0.02765208647561589),\n      ('340210', 0.02722323049001815),\n      ('240210', 0.02707581227436823),\n      ('440150', 0.02702702702702703),\n      ('320140', 0.02697022767075306),\n      ('640220', 0.02683461117196057),\n      ('640410', 0.026232741617357),\n      ('310335', 0.02593659942363112),\n      ('490315', 0.02564102564102564),\n      ('340901', 0.02542372881355932),\n      ('610310', 0.02461584365209608),\n      ('680110', 0.02362204724409449),\n      ('340903', 0.0234375),\n      ('480213', 0.0231811697574893),\n      ('320430', 0.02272727272727273),\n      ('230000', 0.02272727272727273),\n      ('640210', 0.02267002518891688),\n      ('550310', 0.02246796559592768),\n      ('490110', 0.02173913043478261),\n      ('620410', 0.02165087956698241),\n      ('340913', 0.02127659574468085),\n      ('340906', 0.02127659574468085),\n      ('590110', 0.0209366391184573),\n      ('620810', 0.02090592334494774),\n      ('020710', 0.02085600290170475),\n      ('620926', 0.02076875387476751),\n      ('480212', 0.02055622732769045),\n      ('020510', 0.0202097074243193),\n      ('650210', 0.02016868353502017),\n      ('530510', 0.02005730659025788),\n      ('520310', 0.02),\n      ('480110', 0.01970443349753695),\n      ('550110', 0.0194300518134715),\n      ('650110', 0.0190424374319913),\n      ('320511', 0.01829268292682927),\n      ('240120', 0.01818181818181818),\n      ('040610', 0.01785714285714286),\n      ('170531', 0.0177293934681182),\n      ('550210', 0.01761658031088083),\n      ('290430', 0.01748251748251748),\n      ('002100', 0.01715481171548117),\n      ('150310', 0.01708217913204063),\n      ('560310', 0.01682692307692308),\n      ('640110', 0.01674500587544066),\n      ('640430', 0.01648351648351648),\n      ('570000', 0.01633393829401089),\n      ('240110', 0.0162052667116813),\n      ('690119', 0.01618122977346278),\n      ('630110', 0.0158344666796192),\n      ('330310', 0.01570146818923328),\n      ('020820', 0.01567783584383646),\n      ('130320', 0.0156165858912224),\n      ('630210', 0.0155902004454343),\n      ('020610', 0.01553829078801332),\n      ('010120', 0.01547231270358306),\n      ('180310', 0.01535880227155395),\n      ('550410', 0.01529571719918423),\n      ('360110', 0.01515151515151515),\n      ('620114', 0.01492537313432836),\n      ('440210', 0.01488095238095238),\n      ('470220', 0.01478743068391867),\n      ('620111', 0.01471389645776567),\n      ('330210', 0.01441871961769795),\n      ('140320', 0.01423487544483986),\n      ('340520', 0.01411100658513641),\n      ('560210', 0.01355661881977671),\n      ('530311', 0.01341184167484462),\n      ('330110', 0.01330895052321447),\n      ('050900', 0.0131578947368421),\n      ('250900', 0.01309707241910632),\n      ('690120', 0.01305483028720627),\n      ('490300', 0.01298701298701299),\n      ('180320', 0.01298701298701299),\n      ('170533', 0.01296982530439386),\n      ('540000', 0.01271259233808624),\n      ('170510', 0.01269971323228185),\n      ('620930', 0.01252609603340292),\n      ('340410', 0.01241642788920726),\n      ('270000', 0.01241039905852145),\n      ('520110', 0.01237964236588721),\n      ('560400', 0.01210898082744702),\n      ('180612', 0.01201452919810003),\n      ('620320', 0.01185770750988142),\n      ('470211', 0.01179941002949852),\n      ('180520', 0.01179574732267577),\n      ('100410', 0.01164329187615771),\n      ('310331', 0.01162790697674419),\n      ('530412', 0.01158504476040021),\n      ('020810', 0.01154575219713941),\n      ('530210', 0.01152737752161383),\n      ('220110', 0.01149425287356322),\n      ('320630', 0.01142857142857143),\n      ('520531', 0.01112484548825711),\n      ('180710', 0.01103708190322364),\n      ('030810', 0.01092896174863388),\n      ('130310', 0.01086556169429098),\n      ('170210', 0.01082262080178853),\n      ('340620', 0.01075268817204301),\n      ('999900', 0.01062416998671979),\n      ('030210', 0.01055662188099808),\n      ('030510', 0.01044277360066834),\n      ('170110', 0.01034780109226789),\n      ('220210', 0.01027397260273973),\n      ('680902', 0.01025641025641026),\n      ('020310', 0.01021667580910587),\n      ('130212', 0.009969657563935847),\n      ('030710', 0.009891435464414958),\n      ('140420', 0.009844993715961458),\n      ('560330', 0.009771986970684038),\n      ('270210', 0.009420631182289214),\n      ('140220', 0.009351432880844645),\n      ('160320', 0.00933609958506224),\n      ('560110', 0.009322560596643879),\n      ('170520', 0.009291360421578144),\n      ('230110', 0.009202453987730062),\n      ('170310', 0.009154113557358054),\n      ('180110', 0.009134615384615385),\n      ('140210', 0.009130282102305981),\n      ('160212', 0.009098914000587027),\n      ('050410', 0.008833922261484099),\n      ('100210', 0.008741319144525446),\n      ('170532', 0.008554705087798289),\n      ('620912', 0.008553654743390357),\n      ('090210', 0.008506616257088847),\n      ('490000', 0.008489564909798374),\n      ('170410', 0.008431932544539644),\n      ('210210', 0.00823045267489712),\n      ('020620', 0.008152173913043478),\n      ('340310', 0.008032128514056224),\n      ('110410', 0.007990834884720034),\n      ('490312', 0.007977207977207978),\n      ('210110', 0.007972665148063782),\n      ('180420', 0.007866728366496992),\n      ('180220', 0.007703887363853715),\n      ('010210', 0.007637017070979336),\n      ('180510', 0.007588713125267208),\n      ('470111', 0.007556238768484639),\n      ('060310', 0.007518796992481203),\n      ('050310', 0.007514761137949544),\n      ('030610', 0.007317073170731708),\n      ('180611', 0.007287611061195967),\n      ('010320', 0.007257694074414332),\n      ('500110', 0.007106598984771574),\n      ('040510', 0.006984459577440196),\n      ('110310', 0.006973269134982567),\n      ('250220', 0.006944444444444444),\n      ('580000', 0.006857142857142857),\n      ('020210', 0.006824146981627296),\n      ('180210', 0.006806282722513089),\n      ('040410', 0.006790744466800805),\n      ('050110', 0.00675990675990676),\n      ('010110', 0.006644518272425249),\n      ('180410', 0.006634078212290503),\n      ('140230', 0.00663265306122449),\n      ('050210', 0.00662133142448103),\n      ('160310', 0.006574892130675981),\n      ('020110', 0.006501360749924402),\n      ('070110', 0.006377551020408163),\n      ('030310', 0.00625),\n      ('120310', 0.006177540831006178),\n      ('100510', 0.006119326874043855),\n      ('030410', 0.006116207951070336),\n      ('690114', 0.006105834464043419),\n      ('110510', 0.005989518342899925),\n      ('160211', 0.005981308411214953),\n      ('150211', 0.005960568546538285),\n      ('130211', 0.005947955390334572),\n      ('520541', 0.005911778080945885),\n      ('120210', 0.005798018131983976),\n      ('040110', 0.005780346820809248),\n      ('260110', 0.005772763054316453),\n      ('070240', 0.005749668288367979),\n      ('090110', 0.005704227647576519),\n      ('110210', 0.005692403229145104),\n      ('030110', 0.005622410731899783),\n      ('260210', 0.0055542698449433),\n      ('080110', 0.005548549810844893),\n      ('120110', 0.005436931593515224),\n      ('040310', 0.005404077622205846),\n      ('250210', 0.005342831700801425),\n      ('010310', 0.005331627212625293),\n      ('440120', 0.005319148936170213),\n      ('100110', 0.005308219178082192),\n      ('470112', 0.005277044854881266),\n      ('110110', 0.005152378864284149),\n      ('160110', 0.005109489051094891),\n      ('270410', 0.00496031746031746),\n      ('060110', 0.004922542348342262),\n      ('520516', 0.004901960784313725),\n      ('270310', 0.004885574697865775),\n      ('120410', 0.004865350089766607),\n      ('220120', 0.004815409309791332),\n      ('040210', 0.004786324786324786),\n      ('070230', 0.004725554343874954),\n      ('130110', 0.004694835680751174),\n      ('140110', 0.004555336991406978),\n      ('340530', 0.004530011325028313),\n      ('060210', 0.00400114318376679),\n      ('230900', 0.003992015968063872),\n      ('520410', 0.003937007874015748),\n      ('140340', 0.003897369275738876),\n      ('490313', 0.003875968992248062),\n      ('009000', 0.002952029520295203),\n      ('350110', 0.002881844380403458),\n      ('140330', 0.002380952380952381),\n      ('130122', 0.002169197396963124),\n      ('150212', 0.001451378809869376),\n      ('130121', 0.001373626373626374),\n      ('190323', 0.0009389671361502347),\n      ('190311', 0.0008796003096193089),\n      ('200532', 0.0005934718100890207),\n      ('190312', 0.0005761198329252485),\n      ('190314', 0.0004549590536851683),\n      ('190324', 0.0004541326067211626),\n      ('200522', 0.0004464285714285714),\n      ('190212', 0.0004089793692629283),\n      ('190114', 0.0003787878787878788),\n      ('190112', 0.0003610760064993681),\n      ('190322', 0.0002765869174388052),\n      ('190211', 0.0002144925463840132),\n      ('190111', 0.0002058036633052068),\n      ('200512', 0.0001853911753800519),\n      ('190321', 7.427213309566251e-05),\n      ('440140', 0),\n      ('200112', 0),\n      ('620925', 0),\n      ('250110', 0),\n      ('200531', 0),\n      ('310242', 0),\n      ('600130', 0),\n      ('580901', 0),\n      ('200521', 0),\n      ('490316', 0),\n      ('200523', 0),\n      ('190113', 0),\n      ('310241', 0),\n      ('550340', 0),\n      ('450350', 0),\n      ('190214', 0),\n      ('300410', 0),\n      ('530903', 0),\n      ('200513', 0),\n      ('140410', 0),\n      ('002200', 0),\n      ('630900', 0),\n      ('680210', 0),\n      ('290210', 0),\n      ('140310', 0),\n      ('200533', 0),\n      ('440110', 0),\n      ('190313', 0),\n      ('190213', 0),\n      ('270311', 0),\n      ('270900', 0),\n      ('200511', 0);\n\nALTER TABLE \"POPULATION__STAGING_TABLE_1\" ADD COLUMN \"product_code__mapping_target_1_avg\" REAL;\n\nUPDATE \"POPULATION__STAGING_TABLE_1\" SET \"product_code__mapping_target_1_avg\" = 0.0;\n\nUPDATE \"POPULATION__STAGING_TABLE_1\"\nSET \"product_code__mapping_target_1_avg\" = \"PRODUCT_CODE__MAPPING_TARGET_1_AVG\".\"value\"\nFROM \"PRODUCT_CODE__MAPPING_TARGET_1_AVG\"\nWHERE \"POPULATION__STAGING_TABLE_1\".\"product_code\" = \"PRODUCT_CODE__MAPPING_TARGET_1_AVG\".\"key\";\n\nDROP TABLE IF EXISTS \"PRODUCT_CODE__MAPPING_TARGET_1_AVG\";\n</pre> In\u00a0[34]: Copied! <pre>pipe3.features.to_sql()[pipe3.features.sort(by=\"importances\")[0].name]\n</pre> pipe3.features.to_sql()[pipe3.features.sort(by=\"importances\")[0].name] Out[34]: <pre>DROP TABLE IF EXISTS \"PRODUCT_CODE__MAPPING_TARGET_1_AVG\";\n\nCREATE TABLE \"PRODUCT_CODE__MAPPING_TARGET_1_AVG\"(\"key\" TEXT, \"value\" REAL);\n\nINSERT INTO \"PRODUCT_CODE__MAPPING_TARGET_1_AVG\" (\"key\", \"value\")\nVALUES('410901', 0.5265553869499241),\n      ('410140', 0.5248618784530387),\n      ('004190', 0.5073846153846154),\n      ('410120', 0.5013123359580053),\n      ('410110', 0.4444444444444444),\n      ('004100', 0.3336306868867083),\n      ('390110', 0.3132530120481928),\n      ('390120', 0.3067484662576687),\n      ('410130', 0.2967448902346707),\n      ('370110', 0.2948717948717949),\n      ('370212', 0.2944444444444445),\n      ('370220', 0.2920353982300885),\n      ('680140', 0.288135593220339),\n      ('390322', 0.2795918367346939),\n      ('390321', 0.2764227642276423),\n      ('370901', 0.271948608137045),\n      ('390210', 0.2579837194740138),\n      ('370125', 0.2519157088122606),\n      ('390310', 0.2443181818181818),\n      ('390223', 0.2344706911636046),\n      ('390230', 0.2238442822384428),\n      ('370211', 0.2185714285714286),\n      ('370314', 0.2182952182952183),\n      ('400220', 0.2164179104477612),\n      ('610110', 0.2162868883078072),\n      ('360320', 0.2151898734177215),\n      ('590220', 0.2075471698113208),\n      ('370213', 0.2015968063872255),\n      ('400210', 0.1944764096662831),\n      ('430120', 0.194006309148265),\n      ('320130', 0.1899441340782123),\n      ('390901', 0.1797752808988764),\n      ('330410', 0.1751831107281344),\n      ('380410', 0.1386392811296534),\n      ('590230', 0.13469068128426),\n      ('360350', 0.1321279554937413),\n      ('360210', 0.1305555555555556),\n      ('290420', 0.1282051282051282),\n      ('280220', 0.1231884057971015),\n      ('320903', 0.1229724632214259),\n      ('360420', 0.1222091656874266),\n      ('005000', 0.1219512195121951),\n      ('660900', 0.1205479452054795),\n      ('320345', 0.1176205497972059),\n      ('610902', 0.1162790697674419),\n      ('660110', 0.111731843575419),\n      ('600900', 0.1111111111111111),\n      ('670110', 0.1111111111111111),\n      ('320233', 0.1108969866853539),\n      ('610230', 0.11),\n      ('660210', 0.1097922848664688),\n      ('610901', 0.1097560975609756),\n      ('380510', 0.1081081081081081),\n      ('290310', 0.1044776119402985),\n      ('280120', 0.1030640668523677),\n      ('380901', 0.1010141987829615),\n      ('320521', 0.1009174311926606),\n      ('360330', 0.1004366812227074),\n      ('360311', 0.09981167608286252),\n      ('430110', 0.09863945578231292),\n      ('300320', 0.0975609756097561),\n      ('360312', 0.09716599190283401),\n      ('660000', 0.09413886384129846),\n      ('600430', 0.09302325581395349),\n      ('380110', 0.09302325581395349),\n      ('310231', 0.09090909090909091),\n      ('004000', 0.08723998758149643),\n      ('600410', 0.08408408408408409),\n      ('310210', 0.08333333333333333),\n      ('340120', 0.08333333333333333),\n      ('430130', 0.08226221079691516),\n      ('600210', 0.08190476190476191),\n      ('380315', 0.08014981273408239),\n      ('610120', 0.07865168539325842),\n      ('620610', 0.07755102040816327),\n      ('360513', 0.07722969606377678),\n      ('280140', 0.07646356033452807),\n      ('320380', 0.07645788336933046),\n      ('620213', 0.07375643224699828),\n      ('620510', 0.07370393504059962),\n      ('380430', 0.07358390682901006),\n      ('310316', 0.07329842931937172),\n      ('310232', 0.0726950354609929),\n      ('200210', 0.07258064516129033),\n      ('530110', 0.07235621521335807),\n      ('380320', 0.07142857142857142),\n      ('640420', 0.06923076923076923),\n      ('620214', 0.0689900426742532),\n      ('610130', 0.06882591093117409),\n      ('290410', 0.06748466257668712),\n      ('380420', 0.06734816596512327),\n      ('400310', 0.0672059738643435),\n      ('690117', 0.06666666666666667),\n      ('610903', 0.06578947368421052),\n      ('310220', 0.06555863342566944),\n      ('320330', 0.06554307116104868),\n      ('400110', 0.06538692261547691),\n      ('640120', 0.06442953020134229),\n      ('690230', 0.0641025641025641),\n      ('620330', 0.06329113924050633),\n      ('420115', 0.06281407035175879),\n      ('380311', 0.0625),\n      ('310340', 0.06231454005934718),\n      ('320370', 0.06196746707978312),\n      ('380340', 0.06157635467980296),\n      ('380210', 0.06014492753623189),\n      ('620112', 0.05970149253731343),\n      ('340110', 0.05929824561403509),\n      ('320901', 0.05747126436781609),\n      ('280110', 0.05726600985221675),\n      ('290120', 0.05673758865248227),\n      ('320150', 0.05652173913043478),\n      ('240220', 0.05647840531561462),\n      ('340907', 0.05555555555555555),\n      ('600310', 0.05521472392638037),\n      ('320221', 0.05381727158948686),\n      ('320522', 0.05371900826446281),\n      ('620913', 0.05333333333333334),\n      ('340510', 0.052734375),\n      ('640130', 0.05263157894736842),\n      ('310332', 0.05128205128205128),\n      ('320232', 0.05029013539651837),\n      ('380333', 0.0501577287066246),\n      ('690118', 0.05),\n      ('670903', 0.04895104895104895),\n      ('320905', 0.04766031195840555),\n      ('320627', 0.04761904761904762),\n      ('320902', 0.04666666666666667),\n      ('690110', 0.04666666666666667),\n      ('150110', 0.04635643740546312),\n      ('620221', 0.04615384615384616),\n      ('670901', 0.04597701149425287),\n      ('001000', 0.04587155963302753),\n      ('670310', 0.04553734061930783),\n      ('340610', 0.04444444444444445),\n      ('200410', 0.04397394136807817),\n      ('300900', 0.04375),\n      ('610320', 0.04300578034682081),\n      ('300110', 0.0425531914893617),\n      ('002000', 0.0418848167539267),\n      ('680220', 0.04184704184704185),\n      ('570901', 0.04081632653061224),\n      ('280210', 0.04081632653061224),\n      ('600420', 0.04044489383215369),\n      ('320420', 0.0400890868596882),\n      ('290440', 0.038860103626943),\n      ('200310', 0.03872966692486444),\n      ('310900', 0.0380952380952381),\n      ('520550', 0.03773584905660377),\n      ('690116', 0.03773584905660377),\n      ('020410', 0.03773262762506403),\n      ('440130', 0.03759398496240601),\n      ('380902', 0.03571428571428571),\n      ('550320', 0.03547297297297297),\n      ('290110', 0.03539823008849557),\n      ('590210', 0.03476151980598222),\n      ('320904', 0.03454231433506045),\n      ('490311', 0.03448275862068965),\n      ('620310', 0.03422053231939164),\n      ('220000', 0.03418803418803419),\n      ('320120', 0.03355704697986577),\n      ('240310', 0.03343949044585987),\n      ('310351', 0.03333333333333333),\n      ('640310', 0.03329679364209372),\n      ('670902', 0.03174603174603174),\n      ('680903', 0.03137789904502047),\n      ('310140', 0.0308641975308642),\n      ('620420', 0.03061224489795918),\n      ('630220', 0.03052325581395349),\n      ('330610', 0.03022860381636123),\n      ('330510', 0.02971188475390156),\n      ('180620', 0.02942668696093353),\n      ('240900', 0.02941176470588235),\n      ('550330', 0.02935420743639922),\n      ('320610', 0.02929427430093209),\n      ('620710', 0.02877697841726619),\n      ('290320', 0.02877697841726619),\n      ('200111', 0.02867072111207646),\n      ('240320', 0.02842928216062544),\n      ('310352', 0.02838427947598253),\n      ('320410', 0.02791625124626122),\n      ('300218', 0.02777777777777778),\n      ('320110', 0.02768166089965398),\n      ('620121', 0.02765208647561589),\n      ('340210', 0.02722323049001815),\n      ('240210', 0.02707581227436823),\n      ('440150', 0.02702702702702703),\n      ('320140', 0.02697022767075306),\n      ('640220', 0.02683461117196057),\n      ('640410', 0.026232741617357),\n      ('310335', 0.02593659942363112),\n      ('490315', 0.02564102564102564),\n      ('340901', 0.02542372881355932),\n      ('610310', 0.02461584365209608),\n      ('680110', 0.02362204724409449),\n      ('340903', 0.0234375),\n      ('480213', 0.0231811697574893),\n      ('320430', 0.02272727272727273),\n      ('230000', 0.02272727272727273),\n      ('640210', 0.02267002518891688),\n      ('550310', 0.02246796559592768),\n      ('490110', 0.02173913043478261),\n      ('620410', 0.02165087956698241),\n      ('340913', 0.02127659574468085),\n      ('340906', 0.02127659574468085),\n      ('590110', 0.0209366391184573),\n      ('620810', 0.02090592334494774),\n      ('020710', 0.02085600290170475),\n      ('620926', 0.02076875387476751),\n      ('480212', 0.02055622732769045),\n      ('020510', 0.0202097074243193),\n      ('650210', 0.02016868353502017),\n      ('530510', 0.02005730659025788),\n      ('520310', 0.02),\n      ('480110', 0.01970443349753695),\n      ('550110', 0.0194300518134715),\n      ('650110', 0.0190424374319913),\n      ('320511', 0.01829268292682927),\n      ('240120', 0.01818181818181818),\n      ('040610', 0.01785714285714286),\n      ('170531', 0.0177293934681182),\n      ('550210', 0.01761658031088083),\n      ('290430', 0.01748251748251748),\n      ('002100', 0.01715481171548117),\n      ('150310', 0.01708217913204063),\n      ('560310', 0.01682692307692308),\n      ('640110', 0.01674500587544066),\n      ('640430', 0.01648351648351648),\n      ('570000', 0.01633393829401089),\n      ('240110', 0.0162052667116813),\n      ('690119', 0.01618122977346278),\n      ('630110', 0.0158344666796192),\n      ('330310', 0.01570146818923328),\n      ('020820', 0.01567783584383646),\n      ('130320', 0.0156165858912224),\n      ('630210', 0.0155902004454343),\n      ('020610', 0.01553829078801332),\n      ('010120', 0.01547231270358306),\n      ('180310', 0.01535880227155395),\n      ('550410', 0.01529571719918423),\n      ('360110', 0.01515151515151515),\n      ('620114', 0.01492537313432836),\n      ('440210', 0.01488095238095238),\n      ('470220', 0.01478743068391867),\n      ('620111', 0.01471389645776567),\n      ('330210', 0.01441871961769795),\n      ('140320', 0.01423487544483986),\n      ('340520', 0.01411100658513641),\n      ('560210', 0.01355661881977671),\n      ('530311', 0.01341184167484462),\n      ('330110', 0.01330895052321447),\n      ('050900', 0.0131578947368421),\n      ('250900', 0.01309707241910632),\n      ('690120', 0.01305483028720627),\n      ('490300', 0.01298701298701299),\n      ('180320', 0.01298701298701299),\n      ('170533', 0.01296982530439386),\n      ('540000', 0.01271259233808624),\n      ('170510', 0.01269971323228185),\n      ('620930', 0.01252609603340292),\n      ('340410', 0.01241642788920726),\n      ('270000', 0.01241039905852145),\n      ('520110', 0.01237964236588721),\n      ('560400', 0.01210898082744702),\n      ('180612', 0.01201452919810003),\n      ('620320', 0.01185770750988142),\n      ('470211', 0.01179941002949852),\n      ('180520', 0.01179574732267577),\n      ('100410', 0.01164329187615771),\n      ('310331', 0.01162790697674419),\n      ('530412', 0.01158504476040021),\n      ('020810', 0.01154575219713941),\n      ('530210', 0.01152737752161383),\n      ('220110', 0.01149425287356322),\n      ('320630', 0.01142857142857143),\n      ('520531', 0.01112484548825711),\n      ('180710', 0.01103708190322364),\n      ('030810', 0.01092896174863388),\n      ('130310', 0.01086556169429098),\n      ('170210', 0.01082262080178853),\n      ('340620', 0.01075268817204301),\n      ('999900', 0.01062416998671979),\n      ('030210', 0.01055662188099808),\n      ('030510', 0.01044277360066834),\n      ('170110', 0.01034780109226789),\n      ('220210', 0.01027397260273973),\n      ('680902', 0.01025641025641026),\n      ('020310', 0.01021667580910587),\n      ('130212', 0.009969657563935847),\n      ('030710', 0.009891435464414958),\n      ('140420', 0.009844993715961458),\n      ('560330', 0.009771986970684038),\n      ('270210', 0.009420631182289214),\n      ('140220', 0.009351432880844645),\n      ('160320', 0.00933609958506224),\n      ('560110', 0.009322560596643879),\n      ('170520', 0.009291360421578144),\n      ('230110', 0.009202453987730062),\n      ('170310', 0.009154113557358054),\n      ('180110', 0.009134615384615385),\n      ('140210', 0.009130282102305981),\n      ('160212', 0.009098914000587027),\n      ('050410', 0.008833922261484099),\n      ('100210', 0.008741319144525446),\n      ('170532', 0.008554705087798289),\n      ('620912', 0.008553654743390357),\n      ('090210', 0.008506616257088847),\n      ('490000', 0.008489564909798374),\n      ('170410', 0.008431932544539644),\n      ('210210', 0.00823045267489712),\n      ('020620', 0.008152173913043478),\n      ('340310', 0.008032128514056224),\n      ('110410', 0.007990834884720034),\n      ('490312', 0.007977207977207978),\n      ('210110', 0.007972665148063782),\n      ('180420', 0.007866728366496992),\n      ('180220', 0.007703887363853715),\n      ('010210', 0.007637017070979336),\n      ('180510', 0.007588713125267208),\n      ('470111', 0.007556238768484639),\n      ('060310', 0.007518796992481203),\n      ('050310', 0.007514761137949544),\n      ('030610', 0.007317073170731708),\n      ('180611', 0.007287611061195967),\n      ('010320', 0.007257694074414332),\n      ('500110', 0.007106598984771574),\n      ('040510', 0.006984459577440196),\n      ('110310', 0.006973269134982567),\n      ('250220', 0.006944444444444444),\n      ('580000', 0.006857142857142857),\n      ('020210', 0.006824146981627296),\n      ('180210', 0.006806282722513089),\n      ('040410', 0.006790744466800805),\n      ('050110', 0.00675990675990676),\n      ('010110', 0.006644518272425249),\n      ('180410', 0.006634078212290503),\n      ('140230', 0.00663265306122449),\n      ('050210', 0.00662133142448103),\n      ('160310', 0.006574892130675981),\n      ('020110', 0.006501360749924402),\n      ('070110', 0.006377551020408163),\n      ('030310', 0.00625),\n      ('120310', 0.006177540831006178),\n      ('100510', 0.006119326874043855),\n      ('030410', 0.006116207951070336),\n      ('690114', 0.006105834464043419),\n      ('110510', 0.005989518342899925),\n      ('160211', 0.005981308411214953),\n      ('150211', 0.005960568546538285),\n      ('130211', 0.005947955390334572),\n      ('520541', 0.005911778080945885),\n      ('120210', 0.005798018131983976),\n      ('040110', 0.005780346820809248),\n      ('260110', 0.005772763054316453),\n      ('070240', 0.005749668288367979),\n      ('090110', 0.005704227647576519),\n      ('110210', 0.005692403229145104),\n      ('030110', 0.005622410731899783),\n      ('260210', 0.0055542698449433),\n      ('080110', 0.005548549810844893),\n      ('120110', 0.005436931593515224),\n      ('040310', 0.005404077622205846),\n      ('250210', 0.005342831700801425),\n      ('010310', 0.005331627212625293),\n      ('440120', 0.005319148936170213),\n      ('100110', 0.005308219178082192),\n      ('470112', 0.005277044854881266),\n      ('110110', 0.005152378864284149),\n      ('160110', 0.005109489051094891),\n      ('270410', 0.00496031746031746),\n      ('060110', 0.004922542348342262),\n      ('520516', 0.004901960784313725),\n      ('270310', 0.004885574697865775),\n      ('120410', 0.004865350089766607),\n      ('220120', 0.004815409309791332),\n      ('040210', 0.004786324786324786),\n      ('070230', 0.004725554343874954),\n      ('130110', 0.004694835680751174),\n      ('140110', 0.004555336991406978),\n      ('340530', 0.004530011325028313),\n      ('060210', 0.00400114318376679),\n      ('230900', 0.003992015968063872),\n      ('520410', 0.003937007874015748),\n      ('140340', 0.003897369275738876),\n      ('490313', 0.003875968992248062),\n      ('009000', 0.002952029520295203),\n      ('350110', 0.002881844380403458),\n      ('140330', 0.002380952380952381),\n      ('130122', 0.002169197396963124),\n      ('150212', 0.001451378809869376),\n      ('130121', 0.001373626373626374),\n      ('190323', 0.0009389671361502347),\n      ('190311', 0.0008796003096193089),\n      ('200532', 0.0005934718100890207),\n      ('190312', 0.0005761198329252485),\n      ('190314', 0.0004549590536851683),\n      ('190324', 0.0004541326067211626),\n      ('200522', 0.0004464285714285714),\n      ('190212', 0.0004089793692629283),\n      ('190114', 0.0003787878787878788),\n      ('190112', 0.0003610760064993681),\n      ('190322', 0.0002765869174388052),\n      ('190211', 0.0002144925463840132),\n      ('190111', 0.0002058036633052068),\n      ('200512', 0.0001853911753800519),\n      ('190321', 7.427213309566251e-05),\n      ('440140', 0),\n      ('200112', 0),\n      ('620925', 0),\n      ('250110', 0),\n      ('200531', 0),\n      ('310242', 0),\n      ('600130', 0),\n      ('580901', 0),\n      ('200521', 0),\n      ('490316', 0),\n      ('200523', 0),\n      ('190113', 0),\n      ('310241', 0),\n      ('550340', 0),\n      ('450350', 0),\n      ('190214', 0),\n      ('300410', 0),\n      ('530903', 0),\n      ('200513', 0),\n      ('140410', 0),\n      ('002200', 0),\n      ('630900', 0),\n      ('680210', 0),\n      ('290210', 0),\n      ('140310', 0),\n      ('200533', 0),\n      ('440110', 0),\n      ('190313', 0),\n      ('190213', 0),\n      ('270311', 0),\n      ('270900', 0),\n      ('200511', 0);\n\nALTER TABLE \"POPULATION__STAGING_TABLE_1\" ADD COLUMN \"product_code__mapping_target_1_avg\" REAL;\n\nUPDATE \"POPULATION__STAGING_TABLE_1\" SET \"product_code__mapping_target_1_avg\" = 0.0;\n\nUPDATE \"POPULATION__STAGING_TABLE_1\"\nSET \"product_code__mapping_target_1_avg\" = \"PRODUCT_CODE__MAPPING_TARGET_1_AVG\".\"value\"\nFROM \"PRODUCT_CODE__MAPPING_TARGET_1_AVG\"\nWHERE \"POPULATION__STAGING_TABLE_1\".\"product_code\" = \"PRODUCT_CODE__MAPPING_TARGET_1_AVG\".\"key\";\n\nDROP TABLE IF EXISTS \"PRODUCT_CODE__MAPPING_TARGET_1_AVG\";\n</pre> In\u00a0[35]: Copied! <pre># Creates a folder named containing the SQL code.\npipe3.features.to_sql().save(\"consumer_expenditures_pipeline\")\n</pre> # Creates a folder named containing the SQL code. pipe3.features.to_sql().save(\"consumer_expenditures_pipeline\") In\u00a0[36]: Copied! <pre>pipe3.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"consumer_expenditures_spark\")\n</pre> pipe3.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"consumer_expenditures_spark\")","boost":0.8},{"location":"examples/enterprise-notebooks/consumer_expenditures/#consumer-expenditure-why-relational-learning-matters","title":"Consumer expenditure - Why relational learning matters\u00b6","text":"<p>This example demonstrates how powerful a real relational learning algorithm can be. Based on a public-domain dataset on consumer behavior, we use a propostionalization algorithm to predict whether purchases were made as a gift. We show that with relational learning, we can get an AUC of over 90%. The generated features would have been impossible to build by hand or by using brute-force approaches.</p> <p>Summary:</p> <ul> <li>Prediction type: Classification model</li> <li>Domain: Retail</li> <li>Prediction target: If a purchase is a gift</li> <li>Source data: Relational data set, 4 tables</li> <li>Population size: 2.020.634</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/consumer_expenditures/#background","title":"Background\u00b6","text":"<p>Relational learning is one of the most underappreciated fields of machine learning. Even though relational learning is very relevant to many real world data science projects, many data scientists don't even know what relational learning is.</p> <p>There are many subdomains of relational learning, but the most important one is extracting features from relational data: Most business data is relational, meaning that it is spread out over several relational tables. However, most machine learning algorithms require that the data be presented in the form of a single flat table. So we need to extract features from our relational data. Some people also call this data wrangling.</p> <p>Most data scientists we know extract features from relational data manually or by using crude, brute-force approaches (randomly generate thousands of features and then do a feature selection). This is very time-consuming and does not produce good features.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/consumer_expenditures/#the-challenge","title":"The challenge\u00b6","text":"<p>The Consumer Expenditure Data Set is a public domain data set provided by the American Bureau of Labor Statistics (https://www.bls.gov/cex/pumd.htm). It includes the diary entries, where American consumers are asked to keep diaries of the products they have purchased each month.</p> <p>These consumer goods are categorized using a six-digit classification system the UCC. This system is hierarchical, meaning that every digit represents an increasingly granular category.</p> <p>For instance, all UCC codes beginning with \u2018200\u2019 represent beverages. UCC codes beginning with \u201820011\u2019 represents beer and \u2018200111\u2019 represents \u2018beer and ale\u2019 and \u2018200112\u2019 represents \u2018nonalcoholic beer\u2019 (https://www.bls.gov/cex/pumd/ce_pumd_interview_diary_dictionary.xlsx).</p> <p>The diaries also contain a flag that indicates whether the product was purchased as a gift. The challenge is to predict that flag using other information in the diary entries.</p> <p>This can be done based on the following considerations:</p> <ol> <li><p>Some items are less likely to be purchased as gifts than others (for instance, it is unlikely that toilet paper is ever purchased as a gift).</p> </li> <li><p>Items that diverge from the usual consumption patterns are more likely to be gifts.</p> </li> </ol> <p>In total, there are three tables which we find interesting:</p> <ol> <li><p>EXPD, which contains information on the consumer expenditures, including the target variable GIFT.</p> </li> <li><p>FMLD, which contains socio-demographic information on the households.</p> </li> <li><p>MEMD, which contains socio-demographic information on each member of the households.</p> </li> </ol>","boost":0.8},{"location":"examples/enterprise-notebooks/consumer_expenditures/#analysis","title":"Analysis\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/consumer_expenditures/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/consumer_expenditures/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/consumer_expenditures/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"<p>We now have to assign roles to the columns.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/consumer_expenditures/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"<p>Enough with the data preparation. Let's get to the fun part: Extracting the features.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/consumer_expenditures/#21-defining-the-data-model","title":"2.1 Defining the data model\u00b6","text":"<p>First, we define the data model.</p> <p>What we want to do is the following:</p> <ol> <li><p>We want to compare every expenditure made to all expenditures by the same household (EXPD).</p> </li> <li><p>We want to check out whether certain kinds of items have been purchased as a gift in the past (EXPD).</p> </li> <li><p>We want to aggregate all available information on the individual members of the household (MEMD).</p> </li> </ol>","boost":0.8},{"location":"examples/enterprise-notebooks/consumer_expenditures/#22-setting-the-hyperparameters","title":"2.2 Setting the hyperparameters\u00b6","text":"<p>We use <code>XGBoost</code> as our predictor and <code>FastProp</code> (short for fast propsitionalization) to generate our features. You are free to play with the hyperparameters.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/consumer_expenditures/#23-training-the-pipeline","title":"2.3 Training the pipeline\u00b6","text":"<p>OK, let's go:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/consumer_expenditures/#24-evaluating-the-pipeline","title":"2.4 Evaluating the pipeline\u00b6","text":"<p>We want to know how well we did. We will to an in-sample and an out-of-sample evaluation:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/consumer_expenditures/#25-studying-the-features","title":"2.5 Studying the features\u00b6","text":"<p>It is very important that we get an idea about the features that the propositionalization algorithm has produced.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/consumer_expenditures/#26-productionization","title":"2.6 Productionization\u00b6","text":"<p>It is possible to productionize the pipeline by transpiling the features into production-ready SQL code. Please also refer to getML's <code>sqlite3</code> module.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/consumer_expenditures/#3-conclusion","title":"3. Conclusion\u00b6","text":"<p>In this notebook, we have shown how you can use relational learning to predict whether items were purchased as a gift. We did this to highlight the importance of relational learning. Relational learning can be used in many real-world data science applications, but unfortunately most data scientists don't even know what relation learning is.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/cora/","title":"<span class=\"ntitle\">cora.ipynb</span> <span class=\"ndesc\">Categorizing academic publications</span>","text":"<p>Let's get started with the analysis and set up your session:</p> In\u00a0[1]: Copied! <pre>import copy\nimport os\nfrom pathlib import Path\nfrom urllib import request\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline  \n\nimport getml\n\ngetml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.engine.set_project('cora')\n</pre> import copy import os from pathlib import Path from urllib import request  import numpy as np import pandas as pd from IPython.display import Image import matplotlib.pyplot as plt %matplotlib inline    import getml  getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.engine.set_project('cora') <pre>getML engine is already running.\n\nConnected to project 'cora'\n</pre> In\u00a0[2]: Copied! <pre>conn = getml.database.connect_mysql(\n    host=\"db.relational-data.org\",\n    dbname=\"CORA\",\n    port=3306,\n    user=\"guest\",\n    password=\"relational\"\n)\n\nconn\n</pre> conn = getml.database.connect_mysql(     host=\"db.relational-data.org\",     dbname=\"CORA\",     port=3306,     user=\"guest\",     password=\"relational\" )  conn Out[2]: <pre>Connection(dbname='CORA', dialect='mysql', host='db.relational-data.org', port=3306)</pre> In\u00a0[3]: Copied! <pre>def load_if_needed(name):\n    \"\"\"\n    Loads the data from the relational learning\n    repository, if the data frame has not already\n    been loaded.\n    \"\"\"\n    if not getml.data.exists(name):\n        data_frame = getml.data.DataFrame.from_db(\n            name=name,\n            table_name=name,\n            conn=conn\n        )\n        data_frame.save()\n    else:\n        data_frame = getml.data.load_data_frame(name)\n    return data_frame\n</pre> def load_if_needed(name):     \"\"\"     Loads the data from the relational learning     repository, if the data frame has not already     been loaded.     \"\"\"     if not getml.data.exists(name):         data_frame = getml.data.DataFrame.from_db(             name=name,             table_name=name,             conn=conn         )         data_frame.save()     else:         data_frame = getml.data.load_data_frame(name)     return data_frame In\u00a0[4]: Copied! <pre>paper = load_if_needed(\"paper\")\ncites = load_if_needed(\"cites\")\ncontent = load_if_needed(\"content\")\n</pre> paper = load_if_needed(\"paper\") cites = load_if_needed(\"cites\") content = load_if_needed(\"content\") In\u00a0[5]: Copied! <pre>paper\n</pre> paper Out[5]: name     paper_id class_label            role unused_float unused_string          0 35 Genetic_Algorithms 1 40 Genetic_Algorithms 2 114 Reinforcement_Learning 3 117 Reinforcement_Learning 4 128 Reinforcement_Learning ... ... 2703 1154500 Case_Based 2704 1154520 Neural_Networks 2705 1154524 Rule_Learning 2706 1154525 Rule_Learning 2707 1155073 Rule_Learning <p>     2708 rows x 2 columns     memory usage: 0.09 MB     name: paper     type: getml.DataFrame </p> In\u00a0[6]: Copied! <pre>cites\n</pre> cites Out[6]: name cited_paper_id citing_paper_id role   unused_float    unused_float 0 35 887 1 35 1033 2 35 1688 3 35 1956 4 35 8865 ... ... 5424 853116 19621 5425 853116 853155 5426 853118 1140289 5427 853155 853118 5428 954315 1155073 <p>     5429 rows x 2 columns     memory usage: 0.09 MB     name: cites     type: getml.DataFrame </p> In\u00a0[7]: Copied! <pre>content\n</pre> content Out[7]:  name     paper_id word_cited_id  role unused_float unused_string 0 35 word100 1 35 word1152 2 35 word1175 3 35 word1228 4 35 word1248 ... ... 49211 1155073 word75 49212 1155073 word759 49213 1155073 word789 49214 1155073 word815 49215 1155073 word979 <p>     49216 rows x 2 columns     memory usage: 1.20 MB     name: content     type: getml.DataFrame </p> <p>getML requires that we define roles for each of the columns.</p> In\u00a0[8]: Copied! <pre>paper.set_role(\"paper_id\", getml.data.roles.join_key)\npaper.set_role(\"class_label\", getml.data.roles.categorical)\npaper\n</pre> paper.set_role(\"paper_id\", getml.data.roles.join_key) paper.set_role(\"class_label\", getml.data.roles.categorical) paper Out[8]: name paper_id class_label            role join_key categorical            0 35 Genetic_Algorithms 1 40 Genetic_Algorithms 2 114 Reinforcement_Learning 3 117 Reinforcement_Learning 4 128 Reinforcement_Learning ... ... 2703 1154500 Case_Based 2704 1154520 Neural_Networks 2705 1154524 Rule_Learning 2706 1154525 Rule_Learning 2707 1155073 Rule_Learning <p>     2708 rows x 2 columns     memory usage: 0.02 MB     name: paper     type: getml.DataFrame </p> In\u00a0[9]: Copied! <pre>cites.set_role([\"cited_paper_id\", \"citing_paper_id\"], getml.data.roles.join_key)\ncites\n</pre> cites.set_role([\"cited_paper_id\", \"citing_paper_id\"], getml.data.roles.join_key) cites Out[9]: name cited_paper_id citing_paper_id role       join_key        join_key 0 35 887 1 35 1033 2 35 1688 3 35 1956 4 35 8865 ... ... 5424 853116 19621 5425 853116 853155 5426 853118 1140289 5427 853155 853118 5428 954315 1155073 <p>     5429 rows x 2 columns     memory usage: 0.04 MB     name: cites     type: getml.DataFrame </p> <p>We need to separate our data set into a training, testing and validation set:</p> In\u00a0[10]: Copied! <pre>content.set_role(\"paper_id\", getml.data.roles.join_key)\ncontent.set_role(\"word_cited_id\", getml.data.roles.categorical)\ncontent\n</pre> content.set_role(\"paper_id\", getml.data.roles.join_key) content.set_role(\"word_cited_id\", getml.data.roles.categorical) content Out[10]:  name paper_id word_cited_id  role join_key categorical   0 35 word100 1 35 word1152 2 35 word1175 3 35 word1228 4 35 word1248 ... ... 49211 1155073 word75 49212 1155073 word759 49213 1155073 word789 49214 1155073 word815 49215 1155073 word979 <p>     49216 rows x 2 columns     memory usage: 0.39 MB     name: content     type: getml.DataFrame </p> <p>The goal is to predict seven different labels. We generate a target column for each of those labels. We also have to separate the data set into a training and testing set.</p> In\u00a0[11]: Copied! <pre>data_full = getml.data.make_target_columns(paper, \"class_label\")\ndata_full\n</pre> data_full = getml.data.make_target_columns(paper, \"class_label\") data_full Out[11]: name paper_id class_label=Case_Based class_label=Genetic_Algorithms class_label=Neural_Networks class_label=Probabilistic_Methods class_label=Reinforcement_Learning class_label=Rule_Learning class_label=Theory role join_key                 target                         target                      target                           target                           target                    target             target 0 35 0 1 0 0 0 0 0 1 40 0 1 0 0 0 0 0 2 114 0 0 0 0 1 0 0 3 117 0 0 0 0 1 0 0 4 128 0 0 0 0 1 0 0 ... ... ... ... ... ... ... ... ... <p>     2708 rows          type: getml.data.View </p> In\u00a0[12]: Copied! <pre>split = getml.data.split.random(train=0.7, test=0.3, validation=0.0)\nsplit\n</pre> split = getml.data.split.random(train=0.7, test=0.3, validation=0.0) split Out[12]: 0 train 1 test 2 train 3 test 4 test ... <p>     infinite number of  rows          type: StringColumnView </p> In\u00a0[13]: Copied! <pre>container = getml.data.Container(population=data_full, split=split)\ncontainer.add(cites=cites, content=content, paper=paper)\ncontainer.freeze()\ncontainer\n</pre> container = getml.data.Container(population=data_full, split=split) container.add(cites=cites, content=content, paper=paper) container.freeze() container Out[13]: population subset name  rows type 0 test paper 821 View 1 train paper 1887 View peripheral name     rows type      0 cites 5429 DataFrame 1 content 49216 DataFrame 2 paper 2708 DataFrame In\u00a0[14]: Copied! <pre>dm = getml.data.DataModel(paper.to_placeholder(\"population\"))\n\n# We need two different placeholders for cites.\ndm.add(getml.data.to_placeholder(cites=[cites]*2, content=content, paper=paper))\n\ndm.population.join(\n    dm.cites[0],\n    on=('paper_id', 'cited_paper_id')\n)\n\ndm.cites[0].join(\n    dm.content,\n    on=('citing_paper_id', 'paper_id')\n)\n\ndm.cites[0].join(\n    dm.paper,\n    on=('citing_paper_id', 'paper_id'),\n    relationship=getml.data.relationship.many_to_one\n)\n\ndm.population.join(\n    dm.cites[1],\n    on=('paper_id', 'citing_paper_id')\n)\n\ndm.cites[1].join(\n    dm.content,\n    on=('cited_paper_id', 'paper_id')\n)\n\ndm.cites[1].join(\n    dm.paper,\n    on=('cited_paper_id', 'paper_id'),\n    relationship=getml.data.relationship.many_to_one\n)\n\ndm.population.join(\n    dm.content,\n    on='paper_id'\n)\n\ndm\n</pre> dm = getml.data.DataModel(paper.to_placeholder(\"population\"))  # We need two different placeholders for cites. dm.add(getml.data.to_placeholder(cites=[cites]*2, content=content, paper=paper))  dm.population.join(     dm.cites[0],     on=('paper_id', 'cited_paper_id') )  dm.cites[0].join(     dm.content,     on=('citing_paper_id', 'paper_id') )  dm.cites[0].join(     dm.paper,     on=('citing_paper_id', 'paper_id'),     relationship=getml.data.relationship.many_to_one )  dm.population.join(     dm.cites[1],     on=('paper_id', 'citing_paper_id') )  dm.cites[1].join(     dm.content,     on=('cited_paper_id', 'paper_id') )  dm.cites[1].join(     dm.paper,     on=('cited_paper_id', 'paper_id'),     relationship=getml.data.relationship.many_to_one )  dm.population.join(     dm.content,     on='paper_id' )  dm Out[14]: diagram contentpapercitescontentpapercitescontentpopulationpaper_id = citing_paper_idpaper_id = citing_paper_idRelationship: many-to-onepaper_id = cited_paper_idpaper_id = cited_paper_idRelationship: many-to-onecited_paper_id = paper_idciting_paper_id = paper_idpaper_id = paper_id staging data frames  staging table               0 population POPULATION__STAGING_TABLE_1 1 cites, paper CITES__STAGING_TABLE_2 2 cites, paper CITES__STAGING_TABLE_3 3 content CONTENT__STAGING_TABLE_4 <p>Set-up the feature learner &amp; predictor</p> <p>We use the relboost algorithms for this problem. Because of the large number of keywords, we regularize the model a bit by requiring a minimum support for the keywords (<code>min_num_samples</code>).</p> In\u00a0[15]: Copied! <pre>mapping = getml.preprocessors.Mapping()\n\nfast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    num_threads=1\n)\n\nrelboost = getml.feature_learning.Relboost(\n    num_features=10,\n    num_subfeatures=10,\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    seed=4367,\n    num_threads=1,\n    min_num_samples=30\n)\n\npredictor = getml.predictors.XGBoostClassifier()\n</pre> mapping = getml.preprocessors.Mapping()  fast_prop = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     num_threads=1 )  relboost = getml.feature_learning.Relboost(     num_features=10,     num_subfeatures=10,     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     seed=4367,     num_threads=1,     min_num_samples=30 )  predictor = getml.predictors.XGBoostClassifier() <p>Build the pipeline</p> In\u00a0[16]: Copied! <pre>pipe1 = getml.pipeline.Pipeline(\n    tags=['fast_prop'],\n    data_model=dm,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    predictors=[predictor]\n)\n\npipe1\n</pre> pipe1 = getml.pipeline.Pipeline(     tags=['fast_prop'],     data_model=dm,     preprocessors=[mapping],     feature_learners=[fast_prop],     predictors=[predictor] )  pipe1 Out[16]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['cites', 'content', 'paper'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['fast_prop'])</pre> In\u00a0[17]: Copied! <pre>pipe2 = getml.pipeline.Pipeline(\n    tags=['relboost'],\n    data_model=dm,\n    feature_learners=[relboost],\n    predictors=[predictor]\n)\n\npipe2\n</pre> pipe2 = getml.pipeline.Pipeline(     tags=['relboost'],     data_model=dm,     feature_learners=[relboost],     predictors=[predictor] )  pipe2 Out[17]: <pre>Pipeline(data_model='population',\n         feature_learners=['Relboost'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['cites', 'content', 'paper'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['relboost'])</pre> In\u00a0[18]: Copied! <pre>pipe1.check(container.train)\n</pre> pipe1.check(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 3 issues labeled INFO and 0 issues labeled WARNING.\n</pre> Out[18]: type label                  message                          0 INFO MIGHT TAKE LONG The number of unique entries in column 'word_cited_id' in CONTENT__STAGING_TABLE_4 is 1432. This might take a long time to fit. You should consider setting its role to unused_string or using it for comparison only (you can do the latter by setting a unit that contains 'comparison only'). 1 INFO FOREIGN KEYS NOT FOUND When joining POPULATION__STAGING_TABLE_1 and CITES__STAGING_TABLE_2 over 'paper_id' and 'cited_paper_id', there are no corresponding entries for 41.759406% of entries in 'paper_id' in 'POPULATION__STAGING_TABLE_1'. You might want to double-check your join keys. 2 INFO FOREIGN KEYS NOT FOUND When joining POPULATION__STAGING_TABLE_1 and CITES__STAGING_TABLE_3 over 'paper_id' and 'citing_paper_id', there are no corresponding entries for 17.700053% of entries in 'paper_id' in 'POPULATION__STAGING_TABLE_1'. You might want to double-check your join keys. In\u00a0[19]: Copied! <pre>pipe1.fit(container.train)\n</pre> pipe1.fit(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 3 issues labeled INFO and 0 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 3780 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:07, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:14.893815\n\n</pre> Out[19]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['cites', 'content', 'paper'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['fast_prop', 'container-SfPngp'])</pre> In\u00a0[20]: Copied! <pre>pipe2.check(container.train)\n</pre> pipe2.check(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 3 issues labeled INFO and 0 issues labeled WARNING.\n</pre> Out[20]: type label                  message                          0 INFO MIGHT TAKE LONG The number of unique entries in column 'word_cited_id' in CONTENT__STAGING_TABLE_4 is 1432. This might take a long time to fit. You should consider setting its role to unused_string or using it for comparison only (you can do the latter by setting a unit that contains 'comparison only'). 1 INFO FOREIGN KEYS NOT FOUND When joining POPULATION__STAGING_TABLE_1 and CITES__STAGING_TABLE_2 over 'paper_id' and 'cited_paper_id', there are no corresponding entries for 41.759406% of entries in 'paper_id' in 'POPULATION__STAGING_TABLE_1'. You might want to double-check your join keys. 2 INFO FOREIGN KEYS NOT FOUND When joining POPULATION__STAGING_TABLE_1 and CITES__STAGING_TABLE_3 over 'paper_id' and 'citing_paper_id', there are no corresponding entries for 17.700053% of entries in 'paper_id' in 'POPULATION__STAGING_TABLE_1'. You might want to double-check your join keys. <p>The training process seems a bit intimidating. That is because the relboost algorithms needs to train separate models for each class label. This is due to the nature of the generated features.</p> In\u00a0[21]: Copied! <pre>pipe2.fit(container.train)\n</pre> pipe2.fit(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 3 issues labeled INFO and 0 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:26.081776\n\n</pre> Out[21]: <pre>Pipeline(data_model='population',\n         feature_learners=['Relboost'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['cites', 'content', 'paper'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['relboost', 'container-SfPngp'])</pre> In\u00a0[22]: Copied! <pre>pipe1.score(container.test)\n</pre> pipe1.score(container.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[22]: date time           set used target                           accuracy     auc cross entropy 0 2024-02-21 14:56:28 train class_label=Case_Based 0.9979 0.9999 0.02323 1 2024-02-21 14:56:28 train class_label=Genetic_Algorithms 1.0 1. 0.004915 2 2024-02-21 14:56:28 train class_label=Neural_Networks 0.9846 0.9983 0.065852 3 2024-02-21 14:56:28 train class_label=Probabilistic_Methods 0.9958 0.9998 0.02765 4 2024-02-21 14:56:28 train class_label=Reinforcement_Learning 0.9995 1. 0.009078 ... ... ... ... ... ... 9 2024-02-21 14:56:55 test class_label=Neural_Networks 0.9513 0.9787 0.163552 10 2024-02-21 14:56:55 test class_label=Probabilistic_Methods 0.9732 0.9872 0.083174 11 2024-02-21 14:56:55 test class_label=Reinforcement_Learning 0.9805 0.9736 0.074599 12 2024-02-21 14:56:55 test class_label=Rule_Learning 0.9842 0.9937 0.052146 13 2024-02-21 14:56:55 test class_label=Theory 0.9574 0.977 0.128597 In\u00a0[23]: Copied! <pre>pipe2.score(container.test)\n</pre> pipe2.score(container.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[23]: date time           set used target                           accuracy     auc cross entropy 0 2024-02-21 14:56:54 train class_label=Case_Based 1.0 1. 0.008392 1 2024-02-21 14:56:54 train class_label=Genetic_Algorithms 1.0 1. 0.003788 2 2024-02-21 14:56:54 train class_label=Neural_Networks 0.9921 0.9995 0.038185 3 2024-02-21 14:56:54 train class_label=Probabilistic_Methods 0.9979 1. 0.014505 4 2024-02-21 14:56:54 train class_label=Reinforcement_Learning 1.0 1. 0.004389 ... ... ... ... ... ... 9 2024-02-21 14:56:57 test class_label=Neural_Networks 0.9403 0.981 0.178774 10 2024-02-21 14:56:57 test class_label=Probabilistic_Methods 0.9708 0.9877 0.092301 11 2024-02-21 14:56:57 test class_label=Reinforcement_Learning 0.9829 0.9783 0.075957 12 2024-02-21 14:56:57 test class_label=Rule_Learning 0.9817 0.9914 0.067588 13 2024-02-21 14:56:57 test class_label=Theory 0.9501 0.9663 0.162815 <p>To make things a bit easier, we just look at our test results.</p> In\u00a0[24]: Copied! <pre>pipe1.scores.filter(lambda score: score.set_used == \"test\")\n</pre> pipe1.scores.filter(lambda score: score.set_used == \"test\") Out[24]: date time           set used target                           accuracy     auc cross entropy 0 2024-02-21 14:56:55 test class_label=Case_Based 0.9708 0.9861 0.08689 1 2024-02-21 14:56:55 test class_label=Genetic_Algorithms 0.9854 0.998 0.04898 2 2024-02-21 14:56:55 test class_label=Neural_Networks 0.9513 0.9787 0.16355 3 2024-02-21 14:56:55 test class_label=Probabilistic_Methods 0.9732 0.9872 0.08317 4 2024-02-21 14:56:55 test class_label=Reinforcement_Learning 0.9805 0.9736 0.0746 5 2024-02-21 14:56:55 test class_label=Rule_Learning 0.9842 0.9937 0.05215 6 2024-02-21 14:56:55 test class_label=Theory 0.9574 0.977 0.1286 In\u00a0[25]: Copied! <pre>pipe2.scores.filter(lambda score: score.set_used == \"test\")\n</pre> pipe2.scores.filter(lambda score: score.set_used == \"test\") Out[25]: date time           set used target                           accuracy     auc cross entropy 0 2024-02-21 14:56:57 test class_label=Case_Based 0.9744 0.9787 0.10446 1 2024-02-21 14:56:57 test class_label=Genetic_Algorithms 0.9915 0.9989 0.03707 2 2024-02-21 14:56:57 test class_label=Neural_Networks 0.9403 0.981 0.17877 3 2024-02-21 14:56:57 test class_label=Probabilistic_Methods 0.9708 0.9877 0.0923 4 2024-02-21 14:56:57 test class_label=Reinforcement_Learning 0.9829 0.9783 0.07596 5 2024-02-21 14:56:57 test class_label=Rule_Learning 0.9817 0.9914 0.06759 6 2024-02-21 14:56:57 test class_label=Theory 0.9501 0.9663 0.16282 <p>We take the average of the AUC values, which is also the value that appears in the getML monitor (http://localhost:1709/#/listpipelines/cora).</p> In\u00a0[26]: Copied! <pre>fastprop_auc = np.mean(pipe1.auc)\nrelboost_auc = np.mean(pipe2.auc)\nprint(fastprop_auc)\nprint(relboost_auc)\n</pre> fastprop_auc = np.mean(pipe1.auc) relboost_auc = np.mean(pipe2.auc) print(fastprop_auc) print(relboost_auc) <pre>0.9849037843150564\n0.983174892778554\n</pre> <p>The accuracy for multiple targets can be calculated using one of two methods. The first method is to simply take the average of the pair-wise accuracy values, which is also the value that appears in the getML monitor (http://localhost:1709/#/listpipelines/cora).</p> In\u00a0[27]: Copied! <pre>print(np.mean(pipe1.accuracy))\nprint(np.mean(pipe2.accuracy))\n</pre> print(np.mean(pipe1.accuracy)) print(np.mean(pipe2.accuracy)) <pre>0.9718113798503566\n0.9702453453975987\n</pre> <p>However, the benchmarking papers actually use a different approach:</p> <ul> <li>They first generate probabilities for each of the labels:</li> </ul> In\u00a0[28]: Copied! <pre>probabilities1 = pipe1.predict(container.test)\nprobabilities2 = pipe2.predict(container.test)\n</pre> probabilities1 = pipe1.predict(container.test) probabilities2 = pipe2.predict(container.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> <ul> <li>They then find the class label with the highest probability:</li> </ul> In\u00a0[29]: Copied! <pre>class_label = paper.class_label.unique()\n\nix_max = np.argmax(probabilities1, axis=1)\npredicted_labels1 = np.asarray([class_label[ix] for ix in ix_max])\n\nix_max = np.argmax(probabilities2, axis=1)\npredicted_labels2 = np.asarray([class_label[ix] for ix in ix_max])\n</pre> class_label = paper.class_label.unique()  ix_max = np.argmax(probabilities1, axis=1) predicted_labels1 = np.asarray([class_label[ix] for ix in ix_max])  ix_max = np.argmax(probabilities2, axis=1) predicted_labels2 = np.asarray([class_label[ix] for ix in ix_max]) <ul> <li>They then compare that value to the actual class label:</li> </ul> In\u00a0[30]: Copied! <pre>actual_labels = paper[split == \"test\"].class_label.to_numpy()\nfastprop_accuracy = (actual_labels == predicted_labels1).sum() / len(actual_labels)\nrelboost_accuracy = (actual_labels == predicted_labels2).sum() / len(actual_labels)\n\nprint(\"Share of accurately predicted class labels (pipe1):\")\nprint(fastprop_accuracy)\nprint()\nprint(\"Share of accurately predicted class labels (pipe2):\")\nprint(relboost_accuracy)\nprint()\n</pre> actual_labels = paper[split == \"test\"].class_label.to_numpy() fastprop_accuracy = (actual_labels == predicted_labels1).sum() / len(actual_labels) relboost_accuracy = (actual_labels == predicted_labels2).sum() / len(actual_labels)  print(\"Share of accurately predicted class labels (pipe1):\") print(fastprop_accuracy) print() print(\"Share of accurately predicted class labels (pipe2):\") print(relboost_accuracy) print() <pre>Share of accurately predicted class labels (pipe1):\n0.8989037758830695\n\nShare of accurately predicted class labels (pipe2):\n0.8976857490864799\n\n</pre> <p>Since this is the method the benchmark papers use, this is the accuracy score we will report as well.</p> <p>Feature correlations</p> <p>We want to analyze how the features are correlated with the target variables.</p> In\u00a0[31]: Copied! <pre>TARGET_NUM = 0\n</pre> TARGET_NUM = 0 In\u00a0[32]: Copied! <pre>names, correlations = pipe2.features.correlations(target_num=TARGET_NUM)\n\nplt.subplots(figsize=(20, 10))\n\nplt.bar(names, correlations)\n\nplt.title('Feature correlations with class label ' + class_label[TARGET_NUM])\nplt.xlabel('Features')\nplt.ylabel('Correlations')\nplt.xticks(rotation='vertical')\nplt.show()\n</pre> names, correlations = pipe2.features.correlations(target_num=TARGET_NUM)  plt.subplots(figsize=(20, 10))  plt.bar(names, correlations)  plt.title('Feature correlations with class label ' + class_label[TARGET_NUM]) plt.xlabel('Features') plt.ylabel('Correlations') plt.xticks(rotation='vertical') plt.show() <p>Feature importances</p> <p>Feature importances are calculated by analyzing the improvement in predictive accuracy on each node of the trees in the XGBoost predictor. They are then normalized, so that all importances add up to 100%.</p> In\u00a0[33]: Copied! <pre>names, importances = pipe2.features.importances()\n\nplt.subplots(figsize=(20, 10))\n\nplt.bar(names, importances)\n\nplt.title('Feature importances for class label ' + class_label[TARGET_NUM])\nplt.xlabel('Features')\nplt.ylabel('Importances')\nplt.xticks(rotation='vertical')\nplt.show()\n</pre> names, importances = pipe2.features.importances()  plt.subplots(figsize=(20, 10))  plt.bar(names, importances)  plt.title('Feature importances for class label ' + class_label[TARGET_NUM]) plt.xlabel('Features') plt.ylabel('Importances') plt.xticks(rotation='vertical') plt.show() <p>Column importances</p> <p>Because getML uses relational learning, we can apply the principles we used to calculate the feature importances to individual columns as well.</p> In\u00a0[34]: Copied! <pre>names, importances = pipe2.columns.importances(target_num=TARGET_NUM)\n\nplt.subplots(figsize=(20, 10))\n\nplt.bar(names, importances)\n\nplt.title('Columns importances for class label ' + class_label[TARGET_NUM])\nplt.xlabel('Columns')\nplt.ylabel('Importances')\nplt.xticks(rotation='vertical')\nplt.show()\n</pre> names, importances = pipe2.columns.importances(target_num=TARGET_NUM)  plt.subplots(figsize=(20, 10))  plt.bar(names, importances)  plt.title('Columns importances for class label ' + class_label[TARGET_NUM]) plt.xlabel('Columns') plt.ylabel('Importances') plt.xticks(rotation='vertical') plt.show() <p>The most important features look as follows:</p> In\u00a0[35]: Copied! <pre>pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name]\n</pre> pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name] Out[35]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_64\";\n\nCREATE TABLE \"FEATURE_1_64\" AS\nSELECT AVG( t2.\"t4__class_label__mapping_2_target_6_avg\" ) AS \"feature_1_64\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"CITES__STAGING_TABLE_3\" t2\nON t1.\"paper_id\" = t2.\"citing_paper_id\"\nGROUP BY t1.rowid;\n</pre> In\u00a0[36]: Copied! <pre>pipe2.features.to_sql()[pipe2.features.sort(by=\"importances\")[0].name]\n</pre> pipe2.features.to_sql()[pipe2.features.sort(by=\"importances\")[0].name] Out[36]: <pre>DROP TABLE IF EXISTS \"FEATURE_6_1\";\n\nCREATE TABLE \"FEATURE_6_1\" AS\nSELECT AVG( \n    CASE\n        WHEN ( f_6_2.\"feature_6_2_2\" &gt; 3.142350 ) AND ( f_6_2.\"feature_6_2_11\" &gt; 5.518669 ) AND ( f_6_2.\"feature_6_2_1\" &gt; 15.824859 ) THEN 19.11484833926196\n        WHEN ( f_6_2.\"feature_6_2_2\" &gt; 3.142350 ) AND ( f_6_2.\"feature_6_2_11\" &gt; 5.518669 ) AND ( f_6_2.\"feature_6_2_1\" &lt;= 15.824859 ) THEN 16.25336464210706\n        WHEN ( f_6_2.\"feature_6_2_2\" &gt; 3.142350 ) AND ( f_6_2.\"feature_6_2_11\" &lt;= 5.518669 ) AND ( f_6_2.\"feature_6_2_16\" &gt; 0.759650 ) THEN 13.54988592990512\n        WHEN ( f_6_2.\"feature_6_2_2\" &gt; 3.142350 ) AND ( f_6_2.\"feature_6_2_11\" &lt;= 5.518669 ) AND ( f_6_2.\"feature_6_2_16\" &lt;= 0.759650 ) THEN 7.015267202459227\n        WHEN ( f_6_2.\"feature_6_2_2\" &lt;= 3.142350 ) AND ( f_6_2.\"feature_6_2_5\" &gt; 1.818403 ) THEN 5.178143055435354\n        WHEN ( f_6_2.\"feature_6_2_2\" &lt;= 3.142350 ) AND ( f_6_2.\"feature_6_2_5\" &lt;= 1.818403 ) AND ( f_6_2.\"feature_6_2_13\" &gt; 0.586650 ) THEN 3.066163439590274\n        WHEN ( f_6_2.\"feature_6_2_2\" &lt;= 3.142350 ) AND ( f_6_2.\"feature_6_2_5\" &lt;= 1.818403 ) AND ( f_6_2.\"feature_6_2_13\" &lt;= 0.586650 ) THEN -0.674856124411419\n        ELSE NULL\n    END\n) AS \"feature_6_1\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"CITES__STAGING_TABLE_3\" t2\nON t1.\"paper_id\" = t2.\"citing_paper_id\"\nLEFT JOIN \"FEATURES_6_2\" f_6_2\nON t2.rowid = f_6_2.\"rownum\"\nGROUP BY t1.rowid;\n</pre> In\u00a0[37]: Copied! <pre># Creates a folder containing the SQL code.\npipe1.features.to_sql().save(\"cora_pipeline\")\n</pre> # Creates a folder containing the SQL code. pipe1.features.to_sql().save(\"cora_pipeline\") In\u00a0[38]: Copied! <pre>pipe1.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"cora_spark\")\n</pre> pipe1.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"cora_spark\") <p>State-of-the-art approaches on this data set perform as follows:</p> Approach Study Accuracy AUC RelF Dinh et al (2012) 85.7% -- LBP Dinh et al (2012) 85.0% -- EPRN Preisach and Thieme (2006) 84.0% -- PRN Preisach and Thieme (2006) 81.0% -- ACORA Perlich and Provost (2006) -- 97.0% <p>As we can see, the performance of the relboost algorithm, as used in this notebook, compares favorably to these benchmarks.</p> In\u00a0[39]: Copied! <pre>pd.DataFrame(data={\n    'Approach': ['FastProp', 'Relboost'],\n    'Accuracy': [f'{score:.1%}' for score in [fastprop_accuracy, relboost_accuracy]],\n    'AUC': [f'{score:,.1%}' for score in [fastprop_auc, relboost_auc]]\n})\n</pre> pd.DataFrame(data={     'Approach': ['FastProp', 'Relboost'],     'Accuracy': [f'{score:.1%}' for score in [fastprop_accuracy, relboost_accuracy]],     'AUC': [f'{score:,.1%}' for score in [fastprop_auc, relboost_auc]] }) Out[39]: Approach Accuracy AUC 0 FastProp 89.9% 98.5% 1 Relboost 89.8% 98.3%","boost":0.8},{"location":"examples/enterprise-notebooks/cora/#cora-categorizing-academic-publications","title":"CORA - Categorizing academic publications\u00b6","text":"<p>In this notebook, we compare getML against extant approaches in the relational learning literature on the CORA data set, which is often used for benchmarking. We demonstrate that getML outperforms the state of the art in the relational learning literature on this data set. Beyond the benchmarking aspects, this notebooks showcases getML's excellent capabilities in dealing with categorical data.</p> <p>Summary:</p> <ul> <li>Prediction type: Classification model</li> <li>Domain: Academia</li> <li>Prediction target: The category of a paper</li> <li>Population size: 2708</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/cora/#background","title":"Background\u00b6","text":"<p>CORA is a well-known benchmarking dataset in the academic literature on relational learning. The dataset contains 2708 scientific publications on machine learning. The papers are divided into 7 categories. The challenge is to predict the category of a paper based on the papers it cites, the papers it is cited by and keywords contained in the paper.</p> <p>It has been downloaded from the CTU Prague relational learning repository (Motl and Schulte, 2015)(Now residing at relational-data.org.).</p>","boost":0.8},{"location":"examples/enterprise-notebooks/cora/#analysis","title":"Analysis\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/cora/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/cora/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>We begin by downloading the data from the source file:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/cora/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/cora/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"<p>We loaded the data and defined the roles and units. Next, we create a getML pipeline for relational learning.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/cora/#21-define-relational-model","title":"2.1 Define relational model\u00b6","text":"<p>To get started with relational learning, we need to specify the data model. Even though the data set itself is quite simple with only three tables and six columns in total, the resulting data model is actually quite complicated.</p> <p>That is because the class label can be predicting using three different pieces of information:</p> <ul> <li>The keywords used by the paper</li> <li>The keywords used by papers it cites and by papers that cite the paper</li> <li>The class label of papers it cites and by papers that cite the paper</li> </ul> <p>The main challenge here is that <code>cites</code> is used twice, once to connect the cited papers and then to connect the citing papers. To resolve this, we need two placeholders on <code>cites</code>.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/cora/#22-getml-pipeline","title":"2.2 getML pipeline\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/cora/#23-model-training","title":"2.3 Model training\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/cora/#24-model-evaluation","title":"2.4 Model evaluation\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/cora/#25-studying-features","title":"2.5 Studying features\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/cora/#26-productionization","title":"2.6 Productionization\u00b6","text":"<p>It is possible to productionize the pipeline by transpiling the features into production-ready SQL code. Please also refer to getML's <code>sqlite3</code> and <code>spark</code> modules.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/cora/#27-benchmarks","title":"2.7 Benchmarks\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/cora/#3-conclusion","title":"3. Conclusion\u00b6","text":"<p>In this notebook we have demonstrated that getML outperforms state-of-the-art relational learning algorithms on the CORA dataset.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/cora/#references","title":"References\u00b6","text":"<p>Dinh, Quang-Thang, Christel Vrain, and Matthieu Exbrayat. \"A Link-Based Method for Propositionalization.\" ILP (Late Breaking Papers). 2012.</p> <p>Motl, Jan, and Oliver Schulte. \"The CTU prague relational learning repository.\" arXiv preprint arXiv:1511.03086 (2015).</p> <p>Perlich, Claudia, and Foster Provost. \"Distribution-based aggregation for relational learning with identifier attributes.\" Machine Learning 62.1-2 (2006): 65-105.</p> <p>Preisach, Christine, and Lars Schmidt-Thieme. \"Relational ensemble classification.\" Sixth International Conference on Data Mining (ICDM'06). IEEE, 2006.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/dodgers/","title":"<span class=\"ntitle\">dodgers.ipynb</span> <span class=\"ndesc\">Traffic volume prediction</span>","text":"<p>Let's get started with the analysis and set-up your session:</p> In\u00a0[1]: Copied! <pre>from datetime import datetime\nimport gc\nimport os\nfrom pathlib import Path\nfrom urllib import request\nimport time\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import pearsonr\nimport scipy\n\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</pre> from datetime import datetime import gc import os from pathlib import Path from urllib import request import time  import numpy as np import pandas as pd from scipy.stats import pearsonr import scipy  from IPython.display import Image import matplotlib.pyplot as plt %matplotlib inline <p>For various technical reasons, we want to pre-store the features for prophet and tsfresh. However, you are very welcome to try this at home and fully reproduce our results. You can just set the two constants to \"True\".</p> In\u00a0[2]: Copied! <pre>RUN_PROPHET = False\nRUN_TSFRESH = False\n\nif RUN_PROPHET:\n    import logging\n    import cmdstanpy\n    logger = logging.getLogger('cmdstanpy')\n    logger.addHandler(logging.NullHandler())\n    logger.propagate = False\n    logger.setLevel(logging.CRITICAL)\n\n    from prophet import Prophet\n\nif RUN_TSFRESH:\n    import tsfresh\n    from tsfresh.utilities.dataframe_functions import roll_time_series\n    from tsfresh.feature_selection.relevance import calculate_relevance_table\n</pre> RUN_PROPHET = False RUN_TSFRESH = False  if RUN_PROPHET:     import logging     import cmdstanpy     logger = logging.getLogger('cmdstanpy')     logger.addHandler(logging.NullHandler())     logger.propagate = False     logger.setLevel(logging.CRITICAL)      from prophet import Prophet  if RUN_TSFRESH:     import tsfresh     from tsfresh.utilities.dataframe_functions import roll_time_series     from tsfresh.feature_selection.relevance import calculate_relevance_table In\u00a0[3]: Copied! <pre>import getml\n\ngetml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.engine.set_project('dodgers')\n</pre> import getml  getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.engine.set_project('dodgers') <pre>getML engine is already running.\n\nConnected to project 'dodgers'\n</pre> In\u00a0[4]: Copied! <pre>fname = \"Dodgers.data\"\n\nif not os.path.exists(fname):\n    fname, res = request.urlretrieve(\n        \"https://archive.ics.uci.edu/ml/machine-learning-databases/event-detection/\" + fname, \n        fname\n    )\n    \ndata_full_pandas = pd.read_csv(fname, header=None)\n</pre> fname = \"Dodgers.data\"  if not os.path.exists(fname):     fname, res = request.urlretrieve(         \"https://archive.ics.uci.edu/ml/machine-learning-databases/event-detection/\" + fname,          fname     )      data_full_pandas = pd.read_csv(fname, header=None) <p>If we use the pre-stored features, we have to download them as well:</p> In\u00a0[5]: Copied! <pre>PROPHET_FILES = [\n    \"predictions_prophet_train.csv\",\n    \"predictions_prophet_test.csv\",\n    \"combined_train_pandas.csv\",\n    \"combined_test_pandas.csv\"\n]\n\nif not RUN_PROPHET:\n    for fname in PROPHET_FILES:\n        if not os.path.exists(fname):\n            fname, res = request.urlretrieve(\n                \"https://static.getml.com/datasets/dodgers/\" + fname, \n                fname\n            )\n</pre> PROPHET_FILES = [     \"predictions_prophet_train.csv\",     \"predictions_prophet_test.csv\",     \"combined_train_pandas.csv\",     \"combined_test_pandas.csv\" ]  if not RUN_PROPHET:     for fname in PROPHET_FILES:         if not os.path.exists(fname):             fname, res = request.urlretrieve(                 \"https://static.getml.com/datasets/dodgers/\" + fname,                  fname             ) In\u00a0[6]: Copied! <pre>TSFRESH_FILES = [\n    \"tsfresh_train_pandas.csv\",\n    \"tsfresh_test_pandas.csv\"\n]\n\nif not RUN_TSFRESH:\n    for fname in TSFRESH_FILES:\n        if not os.path.exists(fname):\n            fname, res = request.urlretrieve(\n                \"https://static.getml.com/datasets/dodgers/\" + fname, \n                fname\n            )\n</pre> TSFRESH_FILES = [     \"tsfresh_train_pandas.csv\",     \"tsfresh_test_pandas.csv\" ]  if not RUN_TSFRESH:     for fname in TSFRESH_FILES:         if not os.path.exists(fname):             fname, res = request.urlretrieve(                 \"https://static.getml.com/datasets/dodgers/\" + fname,                  fname             ) <p>Prophet is pretty strict about how the columns should be named, so we adapt to these restriction:</p> In\u00a0[7]: Copied! <pre>data_full_pandas.columns = [\"ds\", \"y\"]\n\ndata_full_pandas = data_full_pandas[data_full_pandas[\"y\"] &gt;= 0]\n\ndata_full_pandas = data_full_pandas.reset_index()\n\ndel data_full_pandas[\"index\"]\n\ndata_full_pandas[\"ds\"] = [\n    datetime.strptime(dt, \"%m/%d/%Y %H:%M\") for dt in data_full_pandas[\"ds\"]\n]\n</pre> data_full_pandas.columns = [\"ds\", \"y\"]  data_full_pandas = data_full_pandas[data_full_pandas[\"y\"] &gt;= 0]  data_full_pandas = data_full_pandas.reset_index()  del data_full_pandas[\"index\"]  data_full_pandas[\"ds\"] = [     datetime.strptime(dt, \"%m/%d/%Y %H:%M\") for dt in data_full_pandas[\"ds\"] ] In\u00a0[8]: Copied! <pre>data_full_pandas\n</pre> data_full_pandas Out[8]: ds y 0 2005-04-11 07:35:00 23 1 2005-04-11 07:40:00 42 2 2005-04-11 07:45:00 37 3 2005-04-11 07:50:00 24 4 2005-04-11 07:55:00 39 ... ... ... 47492 2005-09-30 23:45:00 14 47493 2005-09-30 23:50:00 12 47494 2005-09-30 23:55:00 8 47495 2005-10-01 00:00:00 13 47496 2005-10-01 00:05:00 13 <p>47497 rows \u00d7 2 columns</p> In\u00a0[9]: Copied! <pre>data_full = getml.data.DataFrame.from_pandas(data_full_pandas, \"data_full\")\n</pre> data_full = getml.data.DataFrame.from_pandas(data_full_pandas, \"data_full\") In\u00a0[10]: Copied! <pre>data_full.set_role(\"y\", getml.data.roles.target)\ndata_full.set_role(\"ds\", getml.data.roles.time_stamp)\n</pre> data_full.set_role(\"y\", getml.data.roles.target) data_full.set_role(\"ds\", getml.data.roles.time_stamp) In\u00a0[11]: Copied! <pre>data_full\n</pre> data_full Out[11]:  name                          ds      y  role                  time_stamp target  unit time stamp, comparison only 0 2005-04-11 07:35:00 23 1 2005-04-11 07:40:00 42 2 2005-04-11 07:45:00 37 3 2005-04-11 07:50:00 24 4 2005-04-11 07:55:00 39 ... ... 47492 2005-09-30 23:45:00 14 47493 2005-09-30 23:50:00 12 47494 2005-09-30 23:55:00 8 47495 2005-10-01 13 47496 2005-10-01 00:05:00 13 <p>     47497 rows x 2 columns     memory usage: 0.76 MB     name: data_full     type: getml.DataFrame </p> In\u00a0[12]: Copied! <pre>split = getml.data.split.time(population=data_full, time_stamp=\"ds\", test=getml.data.time.datetime(2005, 8, 20))\nsplit\n</pre> split = getml.data.split.time(population=data_full, time_stamp=\"ds\", test=getml.data.time.datetime(2005, 8, 20)) split Out[12]: 0 train 1 train 2 train 3 train 4 train ... <p>     47497 rows          type: StringColumnView </p> <p>Traffic: population table</p> <p>To allow the algorithm to capture seasonal information, we include time components (such as the day of the week) as categorical variables.</p> In\u00a0[13]: Copied! <pre># 1. The horizon is 1 hour (we predict the traffic volume in one hour). \n# 2. The memory is 2 hours, so we allow the algorithm to \n#    use information from up to 2 hours ago. \n# 3. We allow lagged targets. Thus, the algorithm can \n#    identify autoregressive processes.\n\ntime_series = getml.data.TimeSeries(\n    population=data_full,\n    split=split,\n    time_stamps='ds',\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.hours(2),\n    lagged_targets=True\n)\n\ntime_series\n</pre> # 1. The horizon is 1 hour (we predict the traffic volume in one hour).  # 2. The memory is 2 hours, so we allow the algorithm to  #    use information from up to 2 hours ago.  # 3. We allow lagged targets. Thus, the algorithm can  #    identify autoregressive processes.  time_series = getml.data.TimeSeries(     population=data_full,     split=split,     time_stamps='ds',     horizon=getml.data.time.hours(1),     memory=getml.data.time.hours(2),     lagged_targets=True )  time_series Out[13]: data model diagram data_fullpopulationds &lt;= dsMemory: 2.0 hoursHorizon: 1.0 hoursLagged targets allowed staging data frames staging table               0 population POPULATION__STAGING_TABLE_1 1 data_full DATA_FULL__STAGING_TABLE_2 container population subset name       rows type 0 test data_full 11625 View 1 train data_full 35872 View peripheral name       rows type      0 data_full 47497 DataFrame <p>Set-up of feature learners, selectors &amp; predictor</p> In\u00a0[14]: Copied! <pre>mapping = getml.preprocessors.Mapping()\n\nseasonal = getml.preprocessors.Seasonal()\n\nfast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    num_threads=1,\n    aggregation=getml.feature_learning.FastProp.agg_sets.All,\n)\n\nrelboost = getml.feature_learning.Relboost(\n    num_features=10,\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    seed=4367,\n    num_threads=1\n)\n\npredictor = getml.predictors.XGBoostRegressor()\n</pre> mapping = getml.preprocessors.Mapping()  seasonal = getml.preprocessors.Seasonal()  fast_prop = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.SquareLoss,     num_threads=1,     aggregation=getml.feature_learning.FastProp.agg_sets.All, )  relboost = getml.feature_learning.Relboost(     num_features=10,     loss_function=getml.feature_learning.loss_functions.SquareLoss,     seed=4367,     num_threads=1 )  predictor = getml.predictors.XGBoostRegressor() <p>Build the pipeline</p> In\u00a0[15]: Copied! <pre>pipe = getml.pipeline.Pipeline(\n    tags=['memory: 2h', 'horizon: 1h', 'fast_prop', 'relboost'],\n    data_model=time_series.data_model,\n    preprocessors=[seasonal, mapping],\n    feature_learners=[fast_prop, relboost],\n    predictors=[predictor]\n)\n</pre> pipe = getml.pipeline.Pipeline(     tags=['memory: 2h', 'horizon: 1h', 'fast_prop', 'relboost'],     data_model=time_series.data_model,     preprocessors=[seasonal, mapping],     feature_learners=[fast_prop, relboost],     predictors=[predictor] ) In\u00a0[16]: Copied! <pre>pipe.check(time_series.train)\n</pre> pipe.check(time_series.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:10, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \n\nOK.\n</pre> In\u00a0[17]: Copied! <pre>pipe.fit(time_series.train)\n</pre> pipe.fit(time_series.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 2866 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:31, remaining: 00:00]          \nRelboost: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:09, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:03, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:13, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:58.007428\n\n</pre> Out[17]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp', 'Relboost'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=['data_full'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=['Seasonal', 'Mapping'],\n         share_selected_features=0.5,\n         tags=['memory: 2h', 'horizon: 1h', 'fast_prop', 'relboost', 'container-MwG8Qm'])</pre> In\u00a0[18]: Copied! <pre>getml_score = pipe.score(time_series.test)\ngetml_score\n</pre> getml_score = pipe.score(time_series.test) getml_score <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\n</pre> Out[18]: date time           set used target     mae    rmse rsquared 0 2024-03-11 10:23:51 train y 4.4912 6.1897 0.7782 1 2024-03-11 10:23:53 test y 4.6652 6.4173 0.7601 In\u00a0[19]: Copied! <pre>predictions_getml_test = pipe.predict(time_series.test)\n</pre> predictions_getml_test = pipe.predict(time_series.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\n</pre> <p>Prophet is a library for generating predictions on univariate time series that contain strong seasonal components. We would therefore expect it to do well on this particular time series.</p> In\u00a0[20]: Copied! <pre>data_train_pandas = time_series.train.population.to_pandas()\ndata_test_pandas = time_series.test.population[1:].to_pandas()\ndata_train_pandas\n</pre> data_train_pandas = time_series.train.population.to_pandas() data_test_pandas = time_series.test.population[1:].to_pandas() data_train_pandas Out[20]: y ds 0 23.0 2005-04-11 07:35:00 1 42.0 2005-04-11 07:40:00 2 37.0 2005-04-11 07:45:00 3 24.0 2005-04-11 07:50:00 4 39.0 2005-04-11 07:55:00 ... ... ... 35867 6.0 2005-08-19 23:35:00 35868 16.0 2005-08-19 23:40:00 35869 11.0 2005-08-19 23:45:00 35870 10.0 2005-08-19 23:50:00 35871 11.0 2005-08-19 23:55:00 <p>35872 rows \u00d7 2 columns</p> In\u00a0[21]: Copied! <pre>if RUN_PROPHET:\n    model_prophet = Prophet()\n\n    model_prophet = model_prophet.fit(data_train_pandas)\n\n    predictions_prophet_train = model_prophet.predict(data_train_pandas)[\"yhat\"]\n    predictions_prophet_test = model_prophet.predict(data_test_pandas)[\"yhat\"]\nelse:\n    predictions_prophet_train = pd.read_csv(\"predictions_prophet_train.csv\")[\"yhat\"]\n    predictions_prophet_test = pd.read_csv(\"predictions_prophet_test.csv\")[\"yhat\"]\n</pre> if RUN_PROPHET:     model_prophet = Prophet()      model_prophet = model_prophet.fit(data_train_pandas)      predictions_prophet_train = model_prophet.predict(data_train_pandas)[\"yhat\"]     predictions_prophet_test = model_prophet.predict(data_test_pandas)[\"yhat\"] else:     predictions_prophet_train = pd.read_csv(\"predictions_prophet_train.csv\")[\"yhat\"]     predictions_prophet_test = pd.read_csv(\"predictions_prophet_test.csv\")[\"yhat\"] <p>Since we are not using the getML engine for Prophet, we have to implement the metrics ourselves. Luckily, that is not very hard.</p> In\u00a0[22]: Copied! <pre>in_sample = dict()\nout_of_sample = dict()\n</pre> in_sample = dict() out_of_sample = dict() In\u00a0[23]: Copied! <pre>predictions_prophet_test\n</pre> predictions_prophet_test Out[23]: <pre>0        11.182024\n1        10.809442\n2        10.439468\n3        10.072405\n4         9.708546\n           ...    \n11619     8.201863\n11620     7.822110\n11621     7.443635\n11622     7.066790\n11623     6.691914\nName: yhat, Length: 11624, dtype: float64</pre> In\u00a0[24]: Copied! <pre>data_train_pandas\n</pre> data_train_pandas Out[24]: y ds 0 23.0 2005-04-11 07:35:00 1 42.0 2005-04-11 07:40:00 2 37.0 2005-04-11 07:45:00 3 24.0 2005-04-11 07:50:00 4 39.0 2005-04-11 07:55:00 ... ... ... 35867 6.0 2005-08-19 23:35:00 35868 16.0 2005-08-19 23:40:00 35869 11.0 2005-08-19 23:45:00 35870 10.0 2005-08-19 23:50:00 35871 11.0 2005-08-19 23:55:00 <p>35872 rows \u00d7 2 columns</p> In\u00a0[25]: Copied! <pre>def r_squared(yhat, y):\n    yhat = np.asarray(yhat)\n    y = np.asarray(y)\n    r = scipy.stats.pearsonr(yhat, y)[0]\n    return r * r\n    \nin_sample[\"rsquared\"] = r_squared(predictions_prophet_train, data_train_pandas[\"y\"])\nout_of_sample[\"rsquared\"] = r_squared(predictions_prophet_test, data_test_pandas[\"y\"])\n</pre> def r_squared(yhat, y):     yhat = np.asarray(yhat)     y = np.asarray(y)     r = scipy.stats.pearsonr(yhat, y)[0]     return r * r      in_sample[\"rsquared\"] = r_squared(predictions_prophet_train, data_train_pandas[\"y\"]) out_of_sample[\"rsquared\"] = r_squared(predictions_prophet_test, data_test_pandas[\"y\"]) In\u00a0[26]: Copied! <pre>def rmse(yhat, y):\n    yhat = np.asarray(yhat)\n    y = np.asarray(y)\n    return np.sqrt(\n        ((y - yhat)*(y - yhat)).sum()/len(y)\n    )\n\nin_sample[\"rmse\"] = rmse(predictions_prophet_train, data_train_pandas[\"y\"])\nout_of_sample[\"rmse\"] = rmse(predictions_prophet_test, data_test_pandas[\"y\"])\n</pre> def rmse(yhat, y):     yhat = np.asarray(yhat)     y = np.asarray(y)     return np.sqrt(         ((y - yhat)*(y - yhat)).sum()/len(y)     )  in_sample[\"rmse\"] = rmse(predictions_prophet_train, data_train_pandas[\"y\"]) out_of_sample[\"rmse\"] = rmse(predictions_prophet_test, data_test_pandas[\"y\"]) In\u00a0[27]: Copied! <pre>def mae(yhat, y):\n    yhat = np.asarray(yhat)\n    y = np.asarray(y)\n    return np.abs(y - yhat).sum()/len(y)\n    \nin_sample[\"mae\"] = mae(predictions_prophet_train, data_train_pandas[\"y\"])\nout_of_sample[\"mae\"] = mae(predictions_prophet_test, data_test_pandas[\"y\"])\n</pre> def mae(yhat, y):     yhat = np.asarray(yhat)     y = np.asarray(y)     return np.abs(y - yhat).sum()/len(y)      in_sample[\"mae\"] = mae(predictions_prophet_train, data_train_pandas[\"y\"]) out_of_sample[\"mae\"] = mae(predictions_prophet_test, data_test_pandas[\"y\"]) In\u00a0[28]: Copied! <pre>print(\"\"\"\nIn sample mae: {:.4f}\nIn sample rmse: {:.4f}\nIn sample rsquared: {:.4f}\\n\nOut of sample mae: {:.4f}\nOut of sample rmse: {:.4f}\nOut of sample rsquared: {:.4f}\n\"\"\".format(\n    in_sample['mae'], \n    in_sample['rmse'],\n    in_sample['rsquared'],\n    out_of_sample['mae'], \n    out_of_sample['rmse'],\n    out_of_sample['rsquared'])\n)\n</pre> print(\"\"\" In sample mae: {:.4f} In sample rmse: {:.4f} In sample rsquared: {:.4f}\\n Out of sample mae: {:.4f} Out of sample rmse: {:.4f} Out of sample rsquared: {:.4f} \"\"\".format(     in_sample['mae'],      in_sample['rmse'],     in_sample['rsquared'],     out_of_sample['mae'],      out_of_sample['rmse'],     out_of_sample['rsquared']) ) <pre>\nIn sample mae: 5.6729\nIn sample rmse: 7.5989\nIn sample rsquared: 0.6655\n\nOut of sample mae: 6.2245\nOut of sample rmse: 8.3225\nOut of sample rsquared: 0.6306\n\n</pre> <p>Let's take a closer look at the predictions to get a better understanding why getML does better than Prophet.</p> In\u00a0[29]: Copied! <pre>length = 4000\n\nplt.subplots(figsize=(20, 10))\n\nplt.plot(np.asarray(data_test_pandas[\"y\"])[:length], label=\"ground truth\")\nplt.plot(predictions_getml_test[:length], label=\"getml\")\nplt.plot(predictions_prophet_test[:length], label=\"prophet\")\nplt.legend(loc=\"upper right\")\n</pre> length = 4000  plt.subplots(figsize=(20, 10))  plt.plot(np.asarray(data_test_pandas[\"y\"])[:length], label=\"ground truth\") plt.plot(predictions_getml_test[:length], label=\"getml\") plt.plot(predictions_prophet_test[:length], label=\"prophet\") plt.legend(loc=\"upper right\") Out[29]: <pre>&lt;matplotlib.legend.Legend at 0x7f3f3876f9d0&gt;</pre> <p>As this plot indicates, getML does better than Prophet, because it can integrate autoregressive processes in addition to seasonal data.</p> <p>tsfresh is a library for generating features on time series. It uses a brute-force approach: It generates a large number of hard-coded features and then does a feature selection.</p> <p>For convenience, we have built a wrapper around tsfresh.</p> <p>As we have discussed in a different notebook, tsfresh consumes a lot of memory. To limit the memory consumption to a feasible level, we only use tsfresh's MinimalFCParameters and IndexBasedFCParameters, which are a superset of the TimeBasedFCParameters.</p> In\u00a0[30]: Copied! <pre>class TSFreshBuilder():\n    \n    def __init__(self, num_features, memory, column_id, time_stamp, target):\n        \"\"\"\n        Scikit-learn style feature builder based on TSFresh.\n        \n        Args:\n            \n            num_features: The (maximum) number of features to build.\n            \n            memory: How much back in time you want to go until the\n                    feature builder starts \"forgetting\" data.\n                    \n            column_id: The name of the column containing the ids.\n            \n            time_stamp: The name of the column containing the time stamps.\n            \n            target: The name of the target column.\n        \"\"\"\n        self.num_features = num_features\n        self.memory = memory\n        self.column_id = column_id\n        self.time_stamp = time_stamp\n        self.target = target\n        \n        self.selected_features = []\n        \n    def _add_original_columns(self, original_df, df_selected):\n        for colname in original_df.columns:\n            df_selected[colname] = np.asarray(\n                original_df[colname])\n                    \n        return df_selected\n\n    def _extract_features(self, df):\n        df_rolled = roll_time_series(\n            df, \n            column_id=self.column_id, \n            column_sort=self.time_stamp,\n            max_timeshift=self.memory\n        )\n        \n        extracted_minimal = tsfresh.extract_features(\n            df_rolled,\n            column_id=self.column_id, \n            column_sort=self.time_stamp,\n            default_fc_parameters=tsfresh.feature_extraction.MinimalFCParameters()\n        )\n        \n        extracted_index_based = tsfresh.extract_features(\n            df_rolled,\n            column_id=self.column_id, \n            column_sort=self.time_stamp,\n            default_fc_parameters=tsfresh.feature_extraction.settings.IndexBasedFCParameters()\n        )\n        \n        extracted_features = pd.concat(\n            [extracted_minimal, extracted_index_based], axis=1\n        )\n        del extracted_minimal\n        del extracted_index_based\n        \n        gc.collect()\n        \n        extracted_features[\n            extracted_features != extracted_features] = 0.0  \n        \n        extracted_features[\n            np.isinf(extracted_features)] = 0.0 \n        \n        return extracted_features\n        \n    def _print_time_taken(self, begin, end):\n\n        seconds = end - begin\n\n        hours = int(seconds / 3600)\n        seconds -= float(hours * 3600)\n\n        minutes = int(seconds / 60)\n        seconds -= float(minutes * 60)\n\n        seconds = round(seconds, 6)\n\n        print(\n            \"Time taken: \" + str(hours) + \"h:\" +\n            str(minutes) + \"m:\" + str(seconds)\n        )\n\n        print(\"\")\n        \n    def _remove_target_column(self, df):\n        colnames = np.asarray(df.columns)\n        \n        if self.target not in colnames:\n            return df\n        \n        colnames = colnames[colnames != self.target]\n        \n        return df[colnames]\n        \n    def _select_features(self, df, target):\n        df_selected = tsfresh.select_features(\n            df, \n            target\n        )\n        \n        colnames = np.asarray(df_selected.columns)\n\n        correlations = np.asarray([\n            np.abs(pearsonr(target, df_selected[col]))[0] for col in colnames\n        ])\n        \n        # [::-1] is somewhat unintuitive syntax,\n        # but it reverses the entire column.\n        self.selected_features = colnames[\n            np.argsort(correlations)\n        ][::-1][:self.num_features]\n\n        return df_selected[self.selected_features]\n        \n    def fit(self, df):\n        \"\"\"\n        Fits the features.\n        \"\"\"\n        begin = time.time()\n\n        target = np.asarray(df[self.target])\n        \n        df_without_target = self._remove_target_column(df)\n        \n        df_extracted = self._extract_features(\n            df_without_target)\n        \n        df_selected = self._select_features(\n            df_extracted, target)\n                \n        del df_extracted\n        gc.collect()\n        \n        df_selected = self._add_original_columns(df, df_selected)\n\n        end = time.time()\n        \n        self._print_time_taken(begin, end)\n        \n        return df_selected\n    \n    def transform(self, df):\n        \"\"\"\n        Transforms the raw data into a set of features.\n        \"\"\"\n        df_extracted = self._extract_features(df)\n        \n        df_selected = df_extracted[self.selected_features]\n        \n        del df_extracted\n        gc.collect()\n        \n        df_selected = self._add_original_columns(df, df_selected)\n                                         \n        return df_selected\n</pre> class TSFreshBuilder():          def __init__(self, num_features, memory, column_id, time_stamp, target):         \"\"\"         Scikit-learn style feature builder based on TSFresh.                  Args:                          num_features: The (maximum) number of features to build.                          memory: How much back in time you want to go until the                     feature builder starts \"forgetting\" data.                                  column_id: The name of the column containing the ids.                          time_stamp: The name of the column containing the time stamps.                          target: The name of the target column.         \"\"\"         self.num_features = num_features         self.memory = memory         self.column_id = column_id         self.time_stamp = time_stamp         self.target = target                  self.selected_features = []              def _add_original_columns(self, original_df, df_selected):         for colname in original_df.columns:             df_selected[colname] = np.asarray(                 original_df[colname])                              return df_selected      def _extract_features(self, df):         df_rolled = roll_time_series(             df,              column_id=self.column_id,              column_sort=self.time_stamp,             max_timeshift=self.memory         )                  extracted_minimal = tsfresh.extract_features(             df_rolled,             column_id=self.column_id,              column_sort=self.time_stamp,             default_fc_parameters=tsfresh.feature_extraction.MinimalFCParameters()         )                  extracted_index_based = tsfresh.extract_features(             df_rolled,             column_id=self.column_id,              column_sort=self.time_stamp,             default_fc_parameters=tsfresh.feature_extraction.settings.IndexBasedFCParameters()         )                  extracted_features = pd.concat(             [extracted_minimal, extracted_index_based], axis=1         )         del extracted_minimal         del extracted_index_based                  gc.collect()                  extracted_features[             extracted_features != extracted_features] = 0.0                    extracted_features[             np.isinf(extracted_features)] = 0.0                   return extracted_features              def _print_time_taken(self, begin, end):          seconds = end - begin          hours = int(seconds / 3600)         seconds -= float(hours * 3600)          minutes = int(seconds / 60)         seconds -= float(minutes * 60)          seconds = round(seconds, 6)          print(             \"Time taken: \" + str(hours) + \"h:\" +             str(minutes) + \"m:\" + str(seconds)         )          print(\"\")              def _remove_target_column(self, df):         colnames = np.asarray(df.columns)                  if self.target not in colnames:             return df                  colnames = colnames[colnames != self.target]                  return df[colnames]              def _select_features(self, df, target):         df_selected = tsfresh.select_features(             df,              target         )                  colnames = np.asarray(df_selected.columns)          correlations = np.asarray([             np.abs(pearsonr(target, df_selected[col]))[0] for col in colnames         ])                  # [::-1] is somewhat unintuitive syntax,         # but it reverses the entire column.         self.selected_features = colnames[             np.argsort(correlations)         ][::-1][:self.num_features]          return df_selected[self.selected_features]              def fit(self, df):         \"\"\"         Fits the features.         \"\"\"         begin = time.time()          target = np.asarray(df[self.target])                  df_without_target = self._remove_target_column(df)                  df_extracted = self._extract_features(             df_without_target)                  df_selected = self._select_features(             df_extracted, target)                          del df_extracted         gc.collect()                  df_selected = self._add_original_columns(df, df_selected)          end = time.time()                  self._print_time_taken(begin, end)                  return df_selected          def transform(self, df):         \"\"\"         Transforms the raw data into a set of features.         \"\"\"         df_extracted = self._extract_features(df)                  df_selected = df_extracted[self.selected_features]                  del df_extracted         gc.collect()                  df_selected = self._add_original_columns(df, df_selected)                                                   return df_selected <p>We need to lag our target variable, so we can input as a feature to tsfresh.</p> In\u00a0[31]: Copied! <pre>y_lagged = np.asarray(data_full_pandas[\"y\"][:-12])\ndata_full_pandas.loc[12:, \"y_lagged\"] = y_lagged\ndata_full_pandas.loc[12:, \"id\"] = 1\n</pre> y_lagged = np.asarray(data_full_pandas[\"y\"][:-12]) data_full_pandas.loc[12:, \"y_lagged\"] = y_lagged data_full_pandas.loc[12:, \"id\"] = 1 In\u00a0[32]: Copied! <pre>separation = datetime(2005, 8, 20, 0, 0)\ndata_full_tsfresh = data_full_pandas[12:]\ndata_train_tsfresh = data_full_tsfresh[data_full_tsfresh[\"ds\"] &lt; separation]\ndata_test_tsfresh = data_full_tsfresh[data_full_tsfresh[\"ds\"] &gt;= separation]\n</pre> separation = datetime(2005, 8, 20, 0, 0) data_full_tsfresh = data_full_pandas[12:] data_train_tsfresh = data_full_tsfresh[data_full_tsfresh[\"ds\"] &lt; separation] data_test_tsfresh = data_full_tsfresh[data_full_tsfresh[\"ds\"] &gt;= separation] In\u00a0[33]: Copied! <pre>data_train_tsfresh\n</pre> data_train_tsfresh Out[33]: ds y y_lagged id 12 2005-04-11 08:35:00 42 23.0 1.0 13 2005-04-11 08:40:00 41 42.0 1.0 14 2005-04-11 08:45:00 38 37.0 1.0 15 2005-04-11 08:50:00 38 24.0 1.0 16 2005-04-11 08:55:00 40 39.0 1.0 ... ... ... ... ... 35867 2005-08-19 23:35:00 6 18.0 1.0 35868 2005-08-19 23:40:00 16 25.0 1.0 35869 2005-08-19 23:45:00 11 25.0 1.0 35870 2005-08-19 23:50:00 10 18.0 1.0 35871 2005-08-19 23:55:00 11 16.0 1.0 <p>35860 rows \u00d7 4 columns</p> In\u00a0[34]: Copied! <pre>data_test_tsfresh\n</pre> data_test_tsfresh Out[34]: ds y y_lagged id 35872 2005-08-20 00:00:00 13 9.0 1.0 35873 2005-08-20 00:05:00 7 24.0 1.0 35874 2005-08-20 00:10:00 10 8.0 1.0 35875 2005-08-20 00:15:00 6 13.0 1.0 35876 2005-08-20 00:20:00 12 14.0 1.0 ... ... ... ... ... 47492 2005-09-30 23:45:00 14 28.0 1.0 47493 2005-09-30 23:50:00 12 18.0 1.0 47494 2005-09-30 23:55:00 8 36.0 1.0 47495 2005-10-01 00:00:00 13 21.0 1.0 47496 2005-10-01 00:05:00 13 27.0 1.0 <p>11625 rows \u00d7 4 columns</p> <p>We build 20 features, just like we have with getML.</p> In\u00a0[35]: Copied! <pre>tsfresh_builder = TSFreshBuilder(\n    num_features=20,\n    memory=24,\n    column_id=\"id\",\n    time_stamp=\"ds\",\n    target=\"y\"\n)\n</pre> tsfresh_builder = TSFreshBuilder(     num_features=20,     memory=24,     column_id=\"id\",     time_stamp=\"ds\",     target=\"y\" ) In\u00a0[36]: Copied! <pre>if RUN_TSFRESH:\n    tsfresh_train_pandas = tsfresh_builder.fit(data_train_tsfresh)\n    tsfresh_test_pandas = tsfresh_builder.transform(data_test_tsfresh)\nelse:\n    tsfresh_train_pandas = pd.read_csv(\"tsfresh_train_pandas.csv\")\n    tsfresh_test_pandas = pd.read_csv(\"tsfresh_test_pandas.csv\")\n</pre> if RUN_TSFRESH:     tsfresh_train_pandas = tsfresh_builder.fit(data_train_tsfresh)     tsfresh_test_pandas = tsfresh_builder.transform(data_test_tsfresh) else:     tsfresh_train_pandas = pd.read_csv(\"tsfresh_train_pandas.csv\")     tsfresh_test_pandas = pd.read_csv(\"tsfresh_test_pandas.csv\") <p>Because tsfresh does not come with built-in predictors, we upload the generated features into a getML pipeline.</p> In\u00a0[37]: Copied! <pre>tsfresh_train = getml.data.DataFrame.from_pandas(tsfresh_train_pandas, \"tsfresh_train\")\ntsfresh_test = getml.data.DataFrame.from_pandas(tsfresh_test_pandas, \"tsfresh_test\")\n</pre> tsfresh_train = getml.data.DataFrame.from_pandas(tsfresh_train_pandas, \"tsfresh_train\") tsfresh_test = getml.data.DataFrame.from_pandas(tsfresh_test_pandas, \"tsfresh_test\") In\u00a0[38]: Copied! <pre>for df in [tsfresh_train, tsfresh_test]:\n    df.set_role(\"y\", getml.data.roles.target)\n    df.set_role(\"ds\", getml.data.roles.time_stamp)\n    df.set_role(df.roles.unused_float, getml.data.roles.numerical)\n    df.set_role([\"y_lagged\", \"id\"], getml.data.roles.unused_float)\n</pre> for df in [tsfresh_train, tsfresh_test]:     df.set_role(\"y\", getml.data.roles.target)     df.set_role(\"ds\", getml.data.roles.time_stamp)     df.set_role(df.roles.unused_float, getml.data.roles.numerical)     df.set_role([\"y_lagged\", \"id\"], getml.data.roles.unused_float) In\u00a0[39]: Copied! <pre>tsfresh_train\n</pre> tsfresh_train Out[39]:  name                          ds      y y_lagged__mean y_lagged__sum_values y_lagged__median y_lagged__maximum y_lagged__minimum y_lagged__percentage_of_reoccurring_values_to_all_values y_lagged__standard_deviation y_lagged__skewness y_lagged__variance y_lagged__kurtosis y_lagged__length     y_lagged           id  role                  time_stamp target      numerical            numerical        numerical         numerical         numerical                        numerical                    numerical          numerical          numerical          numerical        numerical unused_float unused_float  unit time stamp, comparison only 0 2005-04-11 08:35:00 42 23 23 23 23 23 0 0 0 0 0 1 23 1 1 2005-04-11 08:40:00 41 32.5 65 32.5 42 23 0 9.5 0 90.25 0 2 42 1 2 2005-04-11 08:45:00 38 34 102 37 42 23 0 8.0416 -1.2435 64.6667 0 3 37 1 3 2005-04-11 08:50:00 38 31.5 126 30.5 42 23 0 8.2006 0.2261 67.25 -4.6053 4 24 1 4 2005-04-11 08:55:00 40 33 165 37 42 23 0 7.9246 -0.4313 62.8 -2.9949 5 39 1 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 35855 2005-08-19 23:35:00 6 21.16 529 21 37 10 0.84 5.0333 0.7576 25.3344 3.1125 25 18 1 35856 2005-08-19 23:40:00 16 21.32 533 21 37 10 0.88 5.089 0.6509 25.8976 2.7456 25 25 1 35857 2005-08-19 23:45:00 11 21.48 537 22 37 10 0.84 5.139 0.5482 26.4096 2.4358 25 25 1 35858 2005-08-19 23:50:00 10 21.32 533 21 37 10 0.8 5.1824 0.6203 26.8576 2.3339 25 18 1 35859 2005-08-19 23:55:00 11 21.56 539 21 37 14 0.84 4.7756 1.1404 22.8064 2.7922 25 16 1 <p>     35860 rows x 15 columns     memory usage: 4.30 MB     name: tsfresh_train     type: getml.DataFrame </p> <p>We use an untuned XGBoostRegressor to generate predictions from our tsfresh features, just like we have for getML.</p> In\u00a0[40]: Copied! <pre>predictor = getml.predictors.XGBoostRegressor()\n\npipe_tsfresh = getml.pipeline.Pipeline(\n    tags=['tsfresh'],\n    predictors=[predictor]\n)\n</pre> predictor = getml.predictors.XGBoostRegressor()  pipe_tsfresh = getml.pipeline.Pipeline(     tags=['tsfresh'],     predictors=[predictor] ) In\u00a0[41]: Copied! <pre>pipe_tsfresh.fit(tsfresh_train)\n</pre> pipe_tsfresh.fit(tsfresh_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:1.049053\n\n</pre> Out[41]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['tsfresh'])</pre> In\u00a0[42]: Copied! <pre>tsfresh_score = pipe_tsfresh.score(tsfresh_test)\ntsfresh_score\n</pre> tsfresh_score = pipe_tsfresh.score(tsfresh_test) tsfresh_score <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[42]: date time           set used      target     mae    rmse rsquared 0 2024-03-11 10:23:57 tsfresh_train y 6.8096 8.7569 0.5565 1 2024-03-11 10:23:57 tsfresh_test y 7.1877 9.3024 0.4943 In\u00a0[43]: Copied! <pre>predictions_tsfresh_test = pipe_tsfresh.predict(tsfresh_test)\n</pre> predictions_tsfresh_test = pipe_tsfresh.predict(tsfresh_test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> <p>Let's take a closer look at the predictions to get a better understanding why getML does better than tsfresh.</p> In\u00a0[44]: Copied! <pre>length = 4000\n\nplt.subplots(figsize=(20, 10))\n\nplt.plot(np.asarray(data_test_pandas[\"y\"])[:length], label=\"ground truth\")\nplt.plot(predictions_getml_test[:length], label=\"getml\")\nplt.plot(predictions_tsfresh_test[:length], label=\"tsfresh\")\nplt.legend(loc=\"upper right\")\n</pre> length = 4000  plt.subplots(figsize=(20, 10))  plt.plot(np.asarray(data_test_pandas[\"y\"])[:length], label=\"ground truth\") plt.plot(predictions_getml_test[:length], label=\"getml\") plt.plot(predictions_tsfresh_test[:length], label=\"tsfresh\") plt.legend(loc=\"upper right\") Out[44]: <pre>&lt;matplotlib.legend.Legend at 0x7f3f3430fe50&gt;</pre> <p>As we can see, tsfresh struggles with the strong seasonal components of this data set and therefore cannot separate signal from noise to the same extent that getML can.</p> In\u00a0[45]: Copied! <pre>def combine(dfs):\n    combined = pd.DataFrame()\n    for df in dfs:\n        df = df.copy()\n        if \"id\" in df.columns:\n            del df[\"id\"]\n        df = df.reset_index()\n        for col in df.columns:\n            combined[col] = df[col]\n    return combined\n\nif RUN_PROPHET:\n    prophet_train_pandas = model_prophet.predict(data_train_tsfresh)\n    prophet_test_pandas = model_prophet.predict(data_test_tsfresh)\n\n    combined_train_pandas = combine([tsfresh_train_pandas, prophet_train_pandas])\n    combined_test_pandas = combine([tsfresh_test_pandas, prophet_test_pandas])\nelse:\n    combined_train_pandas = pd.read_csv(\"combined_train_pandas.csv\")\n    combined_test_pandas = pd.read_csv(\"combined_test_pandas.csv\")\n</pre> def combine(dfs):     combined = pd.DataFrame()     for df in dfs:         df = df.copy()         if \"id\" in df.columns:             del df[\"id\"]         df = df.reset_index()         for col in df.columns:             combined[col] = df[col]     return combined  if RUN_PROPHET:     prophet_train_pandas = model_prophet.predict(data_train_tsfresh)     prophet_test_pandas = model_prophet.predict(data_test_tsfresh)      combined_train_pandas = combine([tsfresh_train_pandas, prophet_train_pandas])     combined_test_pandas = combine([tsfresh_test_pandas, prophet_test_pandas]) else:     combined_train_pandas = pd.read_csv(\"combined_train_pandas.csv\")     combined_test_pandas = pd.read_csv(\"combined_test_pandas.csv\") <p>We upload the data to getML:</p> In\u00a0[46]: Copied! <pre>combined_train = getml.data.DataFrame.from_pandas(combined_train_pandas, \"combined_train\")\ncombined_test = getml.data.DataFrame.from_pandas(combined_test_pandas, \"combined_test\")\n</pre> combined_train = getml.data.DataFrame.from_pandas(combined_train_pandas, \"combined_train\") combined_test = getml.data.DataFrame.from_pandas(combined_test_pandas, \"combined_test\") <p>The multiplicative terms are all zero, so we set them to unused to avoid an ugly warning message we would get from getML.</p> In\u00a0[47]: Copied! <pre>for df in [combined_train, combined_test]:\n    df.set_role(\"y\", getml.data.roles.target)\n    df.set_role(\"ds\", getml.data.roles.time_stamp)\n    df.set_role(df.roles.unused_float, getml.data.roles.numerical)\n    df.set_role([\"multiplicative_terms\", \"multiplicative_terms_lower\", \"multiplicative_terms_upper\", \"y_lagged\"], getml.data.roles.unused_float)\n</pre> for df in [combined_train, combined_test]:     df.set_role(\"y\", getml.data.roles.target)     df.set_role(\"ds\", getml.data.roles.time_stamp)     df.set_role(df.roles.unused_float, getml.data.roles.numerical)     df.set_role([\"multiplicative_terms\", \"multiplicative_terms_lower\", \"multiplicative_terms_upper\", \"y_lagged\"], getml.data.roles.unused_float) <p>Once again, we train an untuned XGBoostRegressor on top of these features.</p> In\u00a0[48]: Copied! <pre>predictor = getml.predictors.XGBoostRegressor()\n\npipe_combined = getml.pipeline.Pipeline(\n    tags=['prophet + tsfresh'],\n    predictors=[predictor]\n)\n</pre> predictor = getml.predictors.XGBoostRegressor()  pipe_combined = getml.pipeline.Pipeline(     tags=['prophet + tsfresh'],     predictors=[predictor] ) In\u00a0[49]: Copied! <pre>pipe_combined.fit(combined_train)\n</pre> pipe_combined.fit(combined_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:03, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:2.677063\n\n</pre> Out[49]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['prophet + tsfresh'])</pre> In\u00a0[50]: Copied! <pre>combined_score = pipe_combined.score(combined_test)\ncombined_score\n</pre> combined_score = pipe_combined.score(combined_test) combined_score <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[50]: date time           set used       target     mae    rmse rsquared 0 2024-03-11 10:24:01 combined_train y 4.6572 6.4011 0.7633 1 2024-03-11 10:24:01 combined_test y 6.1772 8.4081 0.6661 <p>As we can see, combining tsfresh and Prophet generates better predictions than any single one of them, but it is still considerably worse than getML.</p> In\u00a0[51]: Copied! <pre>predictions_combined_test = pipe_combined.predict(combined_test)\n</pre> predictions_combined_test = pipe_combined.predict(combined_test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> In\u00a0[52]: Copied! <pre>length = 4000\n\nplt.subplots(figsize=(20, 10))\n\nplt.plot(np.asarray(data_test_pandas[\"y\"])[:length], label=\"ground truth\")\nplt.plot(predictions_getml_test[:length], label=\"getml\")\nplt.plot(predictions_combined_test[:length], label=\"tsfresh + prophet\")\nplt.legend(loc=\"upper right\")\n</pre> length = 4000  plt.subplots(figsize=(20, 10))  plt.plot(np.asarray(data_test_pandas[\"y\"])[:length], label=\"ground truth\") plt.plot(predictions_getml_test[:length], label=\"getml\") plt.plot(predictions_combined_test[:length], label=\"tsfresh + prophet\") plt.legend(loc=\"upper right\") Out[52]: <pre>&lt;matplotlib.legend.Legend at 0x7f3f2d28afd0&gt;</pre> In\u00a0[53]: Copied! <pre>pipe.features.to_sql()[pipe.features.sort(by=\"importances\")[0].name]\n</pre> pipe.features.to_sql()[pipe.features.sort(by=\"importances\")[0].name] Out[53]: <pre>DROP TABLE IF EXISTS \"FEATURE_2_10\";\n\nCREATE TABLE \"FEATURE_2_10\" AS\nSELECT AVG( \n    CASE\n        WHEN ( t2.\"hour__strftime__h__ds__mapping_1_target_1_avg\" &gt; 16.860223 ) AND ( t2.\"weekday__strftime__w__ds__mapping_1_target_1_avg\" &gt; 20.066490 ) AND ( t2.\"hour__strftime__h__ds\" IN ( '08', '09', '10', '11', '12', '15', '16', '17', '18', '19', '20', '04' ) ) THEN -1.394368488597821\n        WHEN ( t2.\"hour__strftime__h__ds__mapping_1_target_1_avg\" &gt; 16.860223 ) AND ( t2.\"weekday__strftime__w__ds__mapping_1_target_1_avg\" &gt; 20.066490 ) AND ( t2.\"hour__strftime__h__ds\" NOT IN ( '08', '09', '10', '11', '12', '15', '16', '17', '18', '19', '20', '04' ) OR t2.\"hour__strftime__h__ds\" IS NULL ) THEN 4.672067599328447\n        WHEN ( t2.\"hour__strftime__h__ds__mapping_1_target_1_avg\" &gt; 16.860223 ) AND ( t2.\"weekday__strftime__w__ds__mapping_1_target_1_avg\" &lt;= 20.066490 OR t2.\"weekday__strftime__w__ds__mapping_1_target_1_avg\" IS NULL ) AND ( t2.\"y\" &gt; 5.860806 ) THEN -2.101703021927502\n        WHEN ( t2.\"hour__strftime__h__ds__mapping_1_target_1_avg\" &gt; 16.860223 ) AND ( t2.\"weekday__strftime__w__ds__mapping_1_target_1_avg\" &lt;= 20.066490 OR t2.\"weekday__strftime__w__ds__mapping_1_target_1_avg\" IS NULL ) AND ( t2.\"y\" &lt;= 5.860806 OR t2.\"y\" IS NULL ) THEN -14.75388432235446\n        WHEN ( t2.\"hour__strftime__h__ds__mapping_1_target_1_avg\" &lt;= 16.860223 OR t2.\"hour__strftime__h__ds__mapping_1_target_1_avg\" IS NULL ) AND ( t2.\"y\" &gt; 18.830769 ) AND ( t1.\"ds\" - t2.\"ds\" &gt; 4784.328358 ) THEN -11.30552661895596\n        WHEN ( t2.\"hour__strftime__h__ds__mapping_1_target_1_avg\" &lt;= 16.860223 OR t2.\"hour__strftime__h__ds__mapping_1_target_1_avg\" IS NULL ) AND ( t2.\"y\" &gt; 18.830769 ) AND ( t1.\"ds\" - t2.\"ds\" &lt;= 4784.328358 OR t1.\"ds\" IS NULL OR t2.\"ds\" IS NULL ) THEN 21.46969850522014\n        WHEN ( t2.\"hour__strftime__h__ds__mapping_1_target_1_avg\" &lt;= 16.860223 OR t2.\"hour__strftime__h__ds__mapping_1_target_1_avg\" IS NULL ) AND ( t2.\"y\" &lt;= 18.830769 OR t2.\"y\" IS NULL ) AND ( t1.\"hour__strftime__h__ds__mapping_target_1_avg\" &gt; 20.927806 ) THEN -26.35680613438945\n        WHEN ( t2.\"hour__strftime__h__ds__mapping_1_target_1_avg\" &lt;= 16.860223 OR t2.\"hour__strftime__h__ds__mapping_1_target_1_avg\" IS NULL ) AND ( t2.\"y\" &lt;= 18.830769 OR t2.\"y\" IS NULL ) AND ( t1.\"hour__strftime__h__ds__mapping_target_1_avg\" &lt;= 20.927806 OR t1.\"hour__strftime__h__ds__mapping_target_1_avg\" IS NULL ) THEN -10.85061566502193\n        ELSE NULL\n    END\n) AS \"feature_2_10\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"DATA_FULL__STAGING_TABLE_2\" t2\nON 1 = 1\nWHERE t2.\"ds__1_000000_hours\" &lt;= t1.\"ds\"\nAND ( t2.\"ds__3_000000_hours\" &gt; t1.\"ds\" OR t2.\"ds__3_000000_hours\" IS NULL )\nGROUP BY t1.rowid;\n</pre> In\u00a0[54]: Copied! <pre>pipe.features.to_sql().save(\"dodgers_pipeline\", remove=True)\n</pre> pipe.features.to_sql().save(\"dodgers_pipeline\", remove=True) In\u00a0[55]: Copied! <pre>pipe.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"dodgers_spark\", remove=True)\n</pre> pipe.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"dodgers_spark\", remove=True) In\u00a0[56]: Copied! <pre>from collections import namedtuple\n\nprophet_score = namedtuple('Score', ['mae', 'rmse', 'rsquared'])(out_of_sample['mae'], out_of_sample['rmse'], out_of_sample['rsquared'])\nscores = [getml_score, prophet_score, tsfresh_score, combined_score]\n\npd.DataFrame(data={\n    'Name': ['getML', 'Prophet', 'tsfresh', 'Prophet + tsfresh'],\n    'R-squared': [f'{score.rsquared:.2%}' for score in scores],\n    'RMSE': [f'{score.rmse:,.2f}' for score in scores],\n    'MAE': [f'{score.mae:,.2f}' for score in scores]\n})\n</pre> from collections import namedtuple  prophet_score = namedtuple('Score', ['mae', 'rmse', 'rsquared'])(out_of_sample['mae'], out_of_sample['rmse'], out_of_sample['rsquared']) scores = [getml_score, prophet_score, tsfresh_score, combined_score]  pd.DataFrame(data={     'Name': ['getML', 'Prophet', 'tsfresh', 'Prophet + tsfresh'],     'R-squared': [f'{score.rsquared:.2%}' for score in scores],     'RMSE': [f'{score.rmse:,.2f}' for score in scores],     'MAE': [f'{score.mae:,.2f}' for score in scores] }) Out[56]: Name R-squared RMSE MAE 0 getML 76.01% 6.42 4.67 1 Prophet 63.06% 8.32 6.22 2 tsfresh 49.43% 9.30 7.19 3 Prophet + tsfresh 66.61% 8.41 6.18 <p>As we can see, getML outperforms both Prophet and tsfresh by all three measures.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/dodgers/#dodgers-traffic-volume-prediction","title":"Dodgers - Traffic volume prediction\u00b6","text":"<p>In this tutorial, we demonstrate a univariate time series prediction application with getML.</p> <p>We benchmark our results against Facebook's Prophet and tsfresh.</p> <p>getML's relational learning algorithms outperform Prophet's classical time series approach by ~14% and tsfresh's brute force approaches to feature engineering by ~26% (measured in terms of the predictive R-squared).</p> <p>Summary:</p> <ul> <li>Prediction type: Regression model</li> <li>Domain: Transportation</li> <li>Prediction target: traffic volume</li> <li>Source data: Univariate time series</li> <li>Population size: 47497</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/dodgers/#background","title":"Background\u00b6","text":"<p>The data set features some particularly interesting characteristics common for time series, which classical models may struggle to deal with. Such characteristics are:</p> <ul> <li>High frequency (every five minutes)</li> <li>Dependence on irregular events (holidays, Dodgers games)</li> <li>Strong and overlapping cycles (daily, weekly)</li> <li>Anomalies</li> <li>Multiple seasonalities</li> </ul> <p>To quote the maintainers of the data set:</p> <p>\"This loop sensor data was collected for the Glendale on ramp for the 101 North freeway in Los Angeles. It is close enough to the stadium to see unusual traffic after a Dodgers game, but not so close and heavily used by game traffic so that the signal for the extra traffic is overly obvious.\"</p> <p>The dataset was originally collected for this paper:</p> <p>\"Adaptive event detection with time-varying Poisson processes\" A. Ihler, J. Hutchins, and P. Smyth Proceedings of the 12th ACM SIGKDD Conference (KDD-06), August 2006.</p> <p>It is maintained by the UCI Machine Learning Repository:</p> <p>Dua, D. and Graff, C. (2019). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/dodgers/#analysis","title":"Analysis\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/dodgers/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/dodgers/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>We begin by downloading the data from the UC Irvine Machine Learning repository:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/dodgers/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/dodgers/#13-define-relational-model","title":"1.3 Define relational model\u00b6","text":"<p>To start with relational learning, we need to specify the data model. We manually replicate the appropriate time series structure by setting time series related join conditions (<code>horizon</code>, <code>memory</code> and <code>allow_lagged_targets</code>). This is done abstractly using Placeholders</p> <p>The data model consists of two tables:</p> <ul> <li>Population table <code>traffic_{test/train}</code>: holds target and the contemporarily available time-based components</li> <li>Peripheral table <code>traffic</code>: same table as the population table</li> <li>Join between both placeholders specifies (<code>horizon</code>) to prevent leaks and (<code>memory</code>) that keeps the computations feasible</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/dodgers/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"<p>We loaded the data, defined the roles, units and the abstract data model. Next, we create a getML pipeline for relational learning.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/dodgers/#21-getml-pipeline","title":"2.1 getML Pipeline\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/dodgers/#22-model-training","title":"2.2 Model training\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/dodgers/#23-model-evaluation","title":"2.3 Model evaluation\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/dodgers/#24-benchmark-against-prophet","title":"2.4 Benchmark against Prophet\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/dodgers/#25-benchmark-against-tsfresh","title":"2.5 Benchmark against tsfresh\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/dodgers/#26-combining-prophet-and-tsfresh","title":"2.6 Combining Prophet and tsfresh\u00b6","text":"<p>Prophet is good at extracting seasonal features. tsfresh is good at extracting autoregressive features. So what if we tried to combine them? How well would that perform compared to getML?</p> <p>Let's give it a try. We begin by extracting all of the seasonal features from Prophet and combining them with the tsfresh features:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/dodgers/#27-features","title":"2.7 Features\u00b6","text":"<p>The most important feature is the following:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/dodgers/#28-productionization","title":"2.8 Productionization\u00b6","text":"<p>It is possible to productionize the pipeline by transpiling the features into production-ready SQL code. Please also refer to getML's <code>sqlite3</code> and <code>spark</code> modules.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/dodgers/#29-summary-of-results","title":"2.9 Summary of results\u00b6","text":"<p>For a more convenient overview, we summarize these results into a table.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/dodgers/#3-conclusion","title":"3. Conclusion\u00b6","text":"<p>We have compared getML's feature learning algorithms to Prophet and tsfresh on a data set related to traffic on LA's 101 North freeway. We found that getML significantly outperforms both Prophet and tsfresh. These results are consistent with the view that relational learning is a powerful tool for time series analysis.</p> <p>You are encouraged to reproduce these results. You will need getML to do so. You can download it for free.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/formula1/","title":"<span class=\"ntitle\">formula1.ipynb</span> <span class=\"ndesc\">Predicting the winner of a race</span>","text":"<p>Let's get started with the analysis and set up your session:</p> In\u00a0[1]: Copied! <pre>import copy\nimport os\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\nfrom pathlib import Path\n\nfrom urllib import request\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline  \n\nimport featuretools\nimport woodwork as ww\nimport getml\n\ngetml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.engine.set_project('formula1')\n</pre> import copy import os os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\" from pathlib import Path  from urllib import request  import numpy as np import pandas as pd from IPython.display import Image import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline    import featuretools import woodwork as ww import getml  getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.engine.set_project('formula1') <pre>getML engine is already running.\n\nConnected to project 'formula1'\n</pre> In\u00a0[2]: Copied! <pre>conn = getml.database.connect_mysql(\n    host=\"db.relational-data.org\",\n    dbname=\"ErgastF1\",\n    port=3306,\n    user=\"guest\",\n    password=\"relational\"\n)\n\nconn\n</pre> conn = getml.database.connect_mysql(     host=\"db.relational-data.org\",     dbname=\"ErgastF1\",     port=3306,     user=\"guest\",     password=\"relational\" )  conn Out[2]: <pre>Connection(dbname='ErgastF1',\n           dialect='mysql',\n           host='db.relational-data.org',\n           port=3306)</pre> In\u00a0[3]: Copied! <pre>def load_if_needed(name):\n    \"\"\"\n    Loads the data from the relational learning\n    repository, if the data frame has not already\n    been loaded.\n    \"\"\"\n    if not getml.data.exists(name):\n        data_frame = getml.data.DataFrame.from_db(\n            name=name,\n            table_name=name,\n            conn=conn\n        )\n        data_frame.save()\n    else:\n        data_frame = getml.data.load_data_frame(name)\n    return data_frame\n</pre> def load_if_needed(name):     \"\"\"     Loads the data from the relational learning     repository, if the data frame has not already     been loaded.     \"\"\"     if not getml.data.exists(name):         data_frame = getml.data.DataFrame.from_db(             name=name,             table_name=name,             conn=conn         )         data_frame.save()     else:         data_frame = getml.data.load_data_frame(name)     return data_frame In\u00a0[4]: Copied! <pre>driverStandings = load_if_needed(\"driverStandings\")\ndrivers = load_if_needed(\"drivers\")\nlapTimes = load_if_needed(\"lapTimes\")\npitStops = load_if_needed(\"pitStops\")\nraces = load_if_needed(\"races\")\nqualifying = load_if_needed(\"qualifying\")\n</pre> driverStandings = load_if_needed(\"driverStandings\") drivers = load_if_needed(\"drivers\") lapTimes = load_if_needed(\"lapTimes\") pitStops = load_if_needed(\"pitStops\") races = load_if_needed(\"races\") qualifying = load_if_needed(\"qualifying\") In\u00a0[5]: Copied! <pre>driverStandings\n</pre> driverStandings Out[5]:  name driverStandingsId       raceId     driverId       points     position         wins positionText   role      unused_float unused_float unused_float unused_float unused_float unused_float unused_string 0 1 18 1 10 1 1 1 1 2 18 2 8 2 0 2 2 3 18 3 6 3 0 3 3 4 18 4 5 4 0 4 4 5 18 5 4 5 0 5 ... ... ... ... ... ... ... 31573 68456 982 835 8 16 0 16 31574 68457 982 154 26 13 0 13 31575 68458 982 836 5 18 0 18 31576 68459 982 18 0 22 0 22 31577 68460 982 814 0 23 0 23 <p>     31578 rows x 7 columns     memory usage: 1.85 MB     name: driverStandings     type: getml.DataFrame </p> In\u00a0[6]: Copied! <pre>drivers\n</pre> drivers Out[6]: name     driverId       number driverRef     code          forename      surname       dob           nationality   url                              role unused_float unused_float unused_string unused_string unused_string unused_string unused_string unused_string unused_string                    0 1 44 hamilton HAM Lewis Hamilton 1985-01-07 British http://en.wikipedia.org/wiki/Lew... 1 2 nan heidfeld HEI Nick Heidfeld 1977-05-10 German http://en.wikipedia.org/wiki/Nic... 2 3 6 rosberg ROS Nico Rosberg 1985-06-27 German http://en.wikipedia.org/wiki/Nic... 3 4 14 alonso ALO Fernando Alonso 1981-07-29 Spanish http://en.wikipedia.org/wiki/Fer... 4 5 nan kovalainen KOV Heikki Kovalainen 1981-10-19 Finnish http://en.wikipedia.org/wiki/Hei... ... ... ... ... ... ... ... ... ... 835 837 88 haryanto HAR Rio Haryanto 1993-01-22 Indonesian http://en.wikipedia.org/wiki/Rio... 836 838 2 vandoorne VAN Stoffel Vandoorne 1992-03-26 Belgian http://en.wikipedia.org/wiki/Sto... 837 839 31 ocon OCO Esteban Ocon 1996-09-17 French http://en.wikipedia.org/wiki/Est... 838 840 18 stroll STR Lance Stroll 1998-10-29 Canadian http://en.wikipedia.org/wiki/Lan... 839 841 36 giovinazzi GIO Antonio Giovinazzi 1993-12-14 Italian http://en.wikipedia.org/wiki/Ant... <p>     840 rows x 9 columns     memory usage: 0.13 MB     name: drivers     type: getml.DataFrame </p> In\u00a0[7]: Copied! <pre>lapTimes\n</pre> lapTimes Out[7]:   name       raceId     driverId          lap     position milliseconds time            role unused_float unused_float unused_float unused_float unused_float unused_string 0 1 1 1 13 109088 1:49.088 1 1 1 2 12 93740 1:33.740 2 1 1 3 11 91600 1:31.600 3 1 1 4 10 91067 1:31.067 4 1 1 5 10 92129 1:32.129 ... ... ... ... ... ... 420364 982 840 54 8 107528 1:47.528 420365 982 840 55 8 107512 1:47.512 420366 982 840 56 8 108143 1:48.143 420367 982 840 57 8 107848 1:47.848 420368 982 840 58 8 108699 1:48.699 <p>     420369 rows x 6 columns     memory usage: 23.96 MB     name: lapTimes     type: getml.DataFrame </p> In\u00a0[8]: Copied! <pre>pitStops\n</pre> pitStops Out[8]: name       raceId     driverId         stop          lap milliseconds time          duration      role unused_float unused_float unused_float unused_float unused_float unused_string unused_string 0 841 1 1 16 23227 17:28:24 23.227 1 841 1 2 36 23199 17:59:29 23.199 2 841 2 1 15 22994 17:27:41 22.994 3 841 2 2 30 25098 17:51:32 25.098 4 841 3 1 16 23716 17:29:00 23.716 ... ... ... ... ... ... ... 6065 982 839 6 38 29134 21:29:07 29.134 6066 982 840 1 1 37403 20:06:43 37.403 6067 982 840 2 2 29294 20:10:07 29.294 6068 982 840 3 3 25584 20:13:16 25.584 6069 982 840 4 26 29412 21:05:07 29.412 <p>     6070 rows x 7 columns     memory usage: 0.44 MB     name: pitStops     type: getml.DataFrame </p> In\u00a0[9]: Copied! <pre>races\n</pre> races Out[9]: name       raceId         year        round    circuitId name                     date          time          url                              role unused_float unused_float unused_float unused_float unused_string            unused_string unused_string unused_string                    0 1 2009 1 1 Australian Grand Prix 2009-03-29 06:00:00 http://en.wikipedia.org/wiki/200... 1 2 2009 2 2 Malaysian Grand Prix 2009-04-05 09:00:00 http://en.wikipedia.org/wiki/200... 2 3 2009 3 17 Chinese Grand Prix 2009-04-19 07:00:00 http://en.wikipedia.org/wiki/200... 3 4 2009 4 3 Bahrain Grand Prix 2009-04-26 12:00:00 http://en.wikipedia.org/wiki/200... 4 5 2009 5 4 Spanish Grand Prix 2009-05-10 12:00:00 http://en.wikipedia.org/wiki/200... ... ... ... ... ... ... ... ... 971 984 2017 16 22 Japanese Grand Prix 2017-10-08 05:00:00 https://en.wikipedia.org/wiki/20... 972 985 2017 17 69 United States Grand Prix 2017-10-22 19:00:00 https://en.wikipedia.org/wiki/20... 973 986 2017 18 32 Mexican Grand Prix 2017-10-29 19:00:00 https://en.wikipedia.org/wiki/20... 974 987 2017 19 18 Brazilian Grand Prix 2017-11-12 16:00:00 https://en.wikipedia.org/wiki/20... 975 988 2017 20 24 Abu Dhabi Grand Prix 2017-11-26 17:00:00 https://en.wikipedia.org/wiki/20... <p>     976 rows x 8 columns     memory usage: 0.15 MB     name: races     type: getml.DataFrame </p> In\u00a0[10]: Copied! <pre>qualifying\n</pre> qualifying Out[10]: name    qualifyId       raceId     driverId constructorId       number     position q1            q2            q3            role unused_float unused_float unused_float  unused_float unused_float unused_float unused_string unused_string unused_string 0 1 18 1 1 22 1 1:26.572 1:25.187 1:26.714 1 2 18 9 2 4 2 1:26.103 1:25.315 1:26.869 2 3 18 5 1 23 3 1:25.664 1:25.452 1:27.079 3 4 18 13 6 2 4 1:25.994 1:25.691 1:27.178 4 5 18 2 2 3 5 1:25.960 1:25.518 1:27.236 ... ... ... ... ... ... ... ... ... 7392 7415 982 825 210 20 16 1:43.756 NULL NULL 7393 7416 982 13 3 19 17 1:44.014 NULL NULL 7394 7417 982 840 3 18 18 1:44.728 NULL NULL 7395 7418 982 836 15 94 19 1:45.059 NULL NULL 7396 7419 982 828 15 9 20 1:45.570 NULL NULL <p>     7397 rows x 9 columns     memory usage: 0.66 MB     name: qualifying     type: getml.DataFrame </p> In\u00a0[11]: Copied! <pre>racesPd = races.to_pandas()\nracesPd\n</pre> racesPd = races.to_pandas() racesPd Out[11]: raceId year round circuitId name date time url 0 1.0 2009.0 1.0 1.0 Australian Grand Prix 2009-03-29 06:00:00 http://en.wikipedia.org/wiki/2009_Australian_G... 1 2.0 2009.0 2.0 2.0 Malaysian Grand Prix 2009-04-05 09:00:00 http://en.wikipedia.org/wiki/2009_Malaysian_Gr... 2 3.0 2009.0 3.0 17.0 Chinese Grand Prix 2009-04-19 07:00:00 http://en.wikipedia.org/wiki/2009_Chinese_Gran... 3 4.0 2009.0 4.0 3.0 Bahrain Grand Prix 2009-04-26 12:00:00 http://en.wikipedia.org/wiki/2009_Bahrain_Gran... 4 5.0 2009.0 5.0 4.0 Spanish Grand Prix 2009-05-10 12:00:00 http://en.wikipedia.org/wiki/2009_Spanish_Gran... ... ... ... ... ... ... ... ... ... 971 984.0 2017.0 16.0 22.0 Japanese Grand Prix 2017-10-08 05:00:00 https://en.wikipedia.org/wiki/2017_Japanese_Gr... 972 985.0 2017.0 17.0 69.0 United States Grand Prix 2017-10-22 19:00:00 https://en.wikipedia.org/wiki/2017_United_Stat... 973 986.0 2017.0 18.0 32.0 Mexican Grand Prix 2017-10-29 19:00:00 https://en.wikipedia.org/wiki/2017_Mexican_Gra... 974 987.0 2017.0 19.0 18.0 Brazilian Grand Prix 2017-11-12 16:00:00 https://en.wikipedia.org/wiki/2017_Brazilian_G... 975 988.0 2017.0 20.0 24.0 Abu Dhabi Grand Prix 2017-11-26 17:00:00 https://en.wikipedia.org/wiki/2017_Abu_Dhabi_G... <p>976 rows \u00d7 8 columns</p> <p>We actually need some set-up, because the target variable is not readily available. The <code>wins</code> column in <code>driverStandings</code> is actually the accumulated number of wins over a year, but what we want is a boolean variable indicated whether someone has one a particular race or not.</p> In\u00a0[12]: Copied! <pre>driverStandingsPd = driverStandings.to_pandas()\n\ndriverStandingsPd = driverStandingsPd.merge(\n    racesPd[[\"raceId\", \"year\", \"date\", \"round\"]],\n    on=\"raceId\"\n)\n\npreviousStanding = driverStandingsPd.merge(\n    driverStandingsPd[[\"driverId\", \"year\", \"wins\", \"round\"]],\n    on=[\"driverId\", \"year\"],\n)\n\nisPreviousRound = (previousStanding[\"round_x\"] - previousStanding[\"round_y\"] == 1.0)\n\npreviousStanding = previousStanding[isPreviousRound]\n\npreviousStanding[\"win\"] = previousStanding[\"wins_x\"] - previousStanding[\"wins_y\"]\n\ndriverStandingsPd = driverStandingsPd.merge(\n    previousStanding[[\"raceId\", \"driverId\", \"win\"]],\n    on=[\"raceId\", \"driverId\"],\n    how=\"left\",\n)\n\ndriverStandingsPd[\"win\"] = [win if win == win else wins for win, wins in zip(driverStandingsPd[\"win\"], driverStandingsPd[\"wins\"])]\n\ndriver_standings = getml.data.DataFrame.from_pandas(driverStandingsPd, \"driver_standings\")\n\ndriver_standings\n</pre> driverStandingsPd = driverStandings.to_pandas()  driverStandingsPd = driverStandingsPd.merge(     racesPd[[\"raceId\", \"year\", \"date\", \"round\"]],     on=\"raceId\" )  previousStanding = driverStandingsPd.merge(     driverStandingsPd[[\"driverId\", \"year\", \"wins\", \"round\"]],     on=[\"driverId\", \"year\"], )  isPreviousRound = (previousStanding[\"round_x\"] - previousStanding[\"round_y\"] == 1.0)  previousStanding = previousStanding[isPreviousRound]  previousStanding[\"win\"] = previousStanding[\"wins_x\"] - previousStanding[\"wins_y\"]  driverStandingsPd = driverStandingsPd.merge(     previousStanding[[\"raceId\", \"driverId\", \"win\"]],     on=[\"raceId\", \"driverId\"],     how=\"left\", )  driverStandingsPd[\"win\"] = [win if win == win else wins for win, wins in zip(driverStandingsPd[\"win\"], driverStandingsPd[\"wins\"])]  driver_standings = getml.data.DataFrame.from_pandas(driverStandingsPd, \"driver_standings\")  driver_standings Out[12]:  name driverStandingsId       raceId     driverId       points     position         wins         year        round          win positionText  date           role      unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_string unused_string 0 1 18 1 10 1 1 2008 1 1 1 2008-03-16 1 2 18 2 8 2 0 2008 1 0 2 2008-03-16 2 3 18 3 6 3 0 2008 1 0 3 2008-03-16 3 4 18 4 5 4 0 2008 1 0 4 2008-03-16 4 5 18 5 4 5 0 2008 1 0 5 2008-03-16 ... ... ... ... ... ... ... ... ... ... ... 31573 68456 982 835 8 16 0 2017 14 0 16 2017-09-17 31574 68457 982 154 26 13 0 2017 14 0 13 2017-09-17 31575 68458 982 836 5 18 0 2017 14 0 18 2017-09-17 31576 68459 982 18 0 22 0 2017 14 0 22 2017-09-17 31577 68460 982 814 0 23 0 2017 14 0 23 2017-09-17 <p>     31578 rows x 11 columns     memory usage: 3.21 MB     name: driver_standings     type: getml.DataFrame </p> <p>We also need to include the date of the race to <code>lapTimes</code> and <code>pitStops</code>, because we cannot use this data for the race we would like to predict. We can only take lap times and pit stops from previous races.</p> In\u00a0[13]: Copied! <pre>lapTimesPd = lapTimes.to_pandas()\n\nlapTimesPd = lapTimesPd.merge(\n    racesPd[[\"raceId\", \"date\", \"year\"]],\n    on=\"raceId\"\n)\n\nlap_times = getml.data.DataFrame.from_pandas(lapTimesPd, \"lap_times\")\n\nlap_times\n</pre> lapTimesPd = lapTimes.to_pandas()  lapTimesPd = lapTimesPd.merge(     racesPd[[\"raceId\", \"date\", \"year\"]],     on=\"raceId\" )  lap_times = getml.data.DataFrame.from_pandas(lapTimesPd, \"lap_times\")  lap_times Out[13]:   name       raceId     driverId          lap     position milliseconds         year time          date            role unused_float unused_float unused_float unused_float unused_float unused_float unused_string unused_string 0 1 1 1 13 109088 2009 1:49.088 2009-03-29 1 1 1 2 12 93740 2009 1:33.740 2009-03-29 2 1 1 3 11 91600 2009 1:31.600 2009-03-29 3 1 1 4 10 91067 2009 1:31.067 2009-03-29 4 1 1 5 10 92129 2009 1:32.129 2009-03-29 ... ... ... ... ... ... ... ... 420364 982 840 54 8 107528 2017 1:47.528 2017-09-17 420365 982 840 55 8 107512 2017 1:47.512 2017-09-17 420366 982 840 56 8 108143 2017 1:48.143 2017-09-17 420367 982 840 57 8 107848 2017 1:47.848 2017-09-17 420368 982 840 58 8 108699 2017 1:48.699 2017-09-17 <p>     420369 rows x 8 columns     memory usage: 35.31 MB     name: lap_times     type: getml.DataFrame </p> In\u00a0[14]: Copied! <pre>pitStopsPd = pitStops.to_pandas()\n\npitStopsPd = pitStopsPd.merge(\n    racesPd[[\"raceId\", \"date\", \"year\"]],\n    on=\"raceId\"\n)\n\npit_stops = getml.data.DataFrame.from_pandas(pitStopsPd, \"pit_stops\")\n\npit_stops\n</pre> pitStopsPd = pitStops.to_pandas()  pitStopsPd = pitStopsPd.merge(     racesPd[[\"raceId\", \"date\", \"year\"]],     on=\"raceId\" )  pit_stops = getml.data.DataFrame.from_pandas(pitStopsPd, \"pit_stops\")  pit_stops Out[14]: name       raceId     driverId         stop          lap milliseconds         year time          duration      date          role unused_float unused_float unused_float unused_float unused_float unused_float unused_string unused_string unused_string 0 841 1 1 16 23227 2011 17:28:24 23.227 2011-03-27 1 841 1 2 36 23199 2011 17:59:29 23.199 2011-03-27 2 841 2 1 15 22994 2011 17:27:41 22.994 2011-03-27 3 841 2 2 30 25098 2011 17:51:32 25.098 2011-03-27 4 841 3 1 16 23716 2011 17:29:00 23.716 2011-03-27 ... ... ... ... ... ... ... ... ... 6065 982 839 6 38 29134 2017 21:29:07 29.134 2017-09-17 6066 982 840 1 1 37403 2017 20:06:43 37.403 2017-09-17 6067 982 840 2 2 29294 2017 20:10:07 29.294 2017-09-17 6068 982 840 3 3 25584 2017 20:13:16 25.584 2017-09-17 6069 982 840 4 26 29412 2017 21:05:07 29.412 2017-09-17 <p>     6070 rows x 9 columns     memory usage: 0.60 MB     name: pit_stops     type: getml.DataFrame </p> <p>getML requires that we define roles for each of the columns.</p> In\u00a0[15]: Copied! <pre>driver_standings.set_role(\"win\", getml.data.roles.target)\ndriver_standings.set_role([\"raceId\", \"driverId\", \"year\"], getml.data.roles.join_key)\ndriver_standings.set_role(\"position\", getml.data.roles.numerical)\ndriver_standings.set_role(\"date\", getml.data.roles.time_stamp)\n\ndriver_standings\n</pre> driver_standings.set_role(\"win\", getml.data.roles.target) driver_standings.set_role([\"raceId\", \"driverId\", \"year\"], getml.data.roles.join_key) driver_standings.set_role(\"position\", getml.data.roles.numerical) driver_standings.set_role(\"date\", getml.data.roles.time_stamp)  driver_standings Out[15]:  name                        date   raceId driverId     year    win  position driverStandingsId       points         wins        round positionText   role                  time_stamp join_key join_key join_key target numerical      unused_float unused_float unused_float unused_float unused_string  unit time stamp, comparison only 0 2008-03-16 18 1 2008 1 1 1 10 1 1 1 1 2008-03-16 18 2 2008 0 2 2 8 0 1 2 2 2008-03-16 18 3 2008 0 3 3 6 0 1 3 3 2008-03-16 18 4 2008 0 4 4 5 0 1 4 4 2008-03-16 18 5 2008 0 5 5 4 0 1 5 ... ... ... ... ... ... ... ... ... ... ... 31573 2017-09-17 982 835 2017 0 16 68456 8 0 14 16 31574 2017-09-17 982 154 2017 0 13 68457 26 0 14 13 31575 2017-09-17 982 836 2017 0 18 68458 5 0 14 18 31576 2017-09-17 982 18 2017 0 22 68459 0 0 14 22 31577 2017-09-17 982 814 2017 0 23 68460 0 0 14 23 <p>     31578 rows x 11 columns     memory usage: 2.49 MB     name: driver_standings     type: getml.DataFrame </p> In\u00a0[16]: Copied! <pre>drivers.set_role(\"driverId\", getml.data.roles.join_key)\ndrivers.set_role([\"nationality\", \"driverRef\"], getml.data.roles.categorical)\n\ndrivers\n</pre> drivers.set_role(\"driverId\", getml.data.roles.join_key) drivers.set_role([\"nationality\", \"driverRef\"], getml.data.roles.categorical)  drivers Out[16]: name driverId nationality driverRef         number code          forename      surname       dob           url                              role join_key categorical categorical unused_float unused_string unused_string unused_string unused_string unused_string                    0 1 British hamilton 44 HAM Lewis Hamilton 1985-01-07 http://en.wikipedia.org/wiki/Lew... 1 2 German heidfeld nan HEI Nick Heidfeld 1977-05-10 http://en.wikipedia.org/wiki/Nic... 2 3 German rosberg 6 ROS Nico Rosberg 1985-06-27 http://en.wikipedia.org/wiki/Nic... 3 4 Spanish alonso 14 ALO Fernando Alonso 1981-07-29 http://en.wikipedia.org/wiki/Fer... 4 5 Finnish kovalainen nan KOV Heikki Kovalainen 1981-10-19 http://en.wikipedia.org/wiki/Hei... ... ... ... ... ... ... ... ... ... 835 837 Indonesian haryanto 88 HAR Rio Haryanto 1993-01-22 http://en.wikipedia.org/wiki/Rio... 836 838 Belgian vandoorne 2 VAN Stoffel Vandoorne 1992-03-26 http://en.wikipedia.org/wiki/Sto... 837 839 French ocon 31 OCO Esteban Ocon 1996-09-17 http://en.wikipedia.org/wiki/Est... 838 840 Canadian stroll 18 STR Lance Stroll 1998-10-29 http://en.wikipedia.org/wiki/Lan... 839 841 Italian giovinazzi 36 GIO Antonio Giovinazzi 1993-12-14 http://en.wikipedia.org/wiki/Ant... <p>     840 rows x 9 columns     memory usage: 0.11 MB     name: drivers     type: getml.DataFrame </p> In\u00a0[17]: Copied! <pre>lap_times.set_role([\"raceId\", \"driverId\", \"year\"], getml.data.roles.join_key)\nlap_times.set_role([\"lap\", \"milliseconds\", \"position\"], getml.data.roles.numerical)\nlap_times.set_role(\"date\", getml.data.roles.time_stamp)\n\nlap_times\n</pre> lap_times.set_role([\"raceId\", \"driverId\", \"year\"], getml.data.roles.join_key) lap_times.set_role([\"lap\", \"milliseconds\", \"position\"], getml.data.roles.numerical) lap_times.set_role(\"date\", getml.data.roles.time_stamp)  lap_times Out[17]:   name                        date   raceId driverId     year       lap milliseconds  position time            role                  time_stamp join_key join_key join_key numerical    numerical numerical unused_string   unit time stamp, comparison only 0 2009-03-29 1 1 2009 1 109088 13 1:49.088 1 2009-03-29 1 1 2009 2 93740 12 1:33.740 2 2009-03-29 1 1 2009 3 91600 11 1:31.600 3 2009-03-29 1 1 2009 4 91067 10 1:31.067 4 2009-03-29 1 1 2009 5 92129 10 1:32.129 ... ... ... ... ... ... ... ... 420364 2017-09-17 982 840 2017 54 107528 8 1:47.528 420365 2017-09-17 982 840 2017 55 107512 8 1:47.512 420366 2017-09-17 982 840 2017 56 108143 8 1:48.143 420367 2017-09-17 982 840 2017 57 107848 8 1:47.848 420368 2017-09-17 982 840 2017 58 108699 8 1:48.699 <p>     420369 rows x 8 columns     memory usage: 25.64 MB     name: lap_times     type: getml.DataFrame </p> In\u00a0[18]: Copied! <pre>pit_stops.set_role([\"raceId\", \"driverId\", \"year\"], getml.data.roles.join_key)\npit_stops.set_role([\"lap\", \"milliseconds\", \"stop\"], getml.data.roles.numerical)\npit_stops.set_role(\"date\", getml.data.roles.time_stamp)\n\npit_stops\n</pre> pit_stops.set_role([\"raceId\", \"driverId\", \"year\"], getml.data.roles.join_key) pit_stops.set_role([\"lap\", \"milliseconds\", \"stop\"], getml.data.roles.numerical) pit_stops.set_role(\"date\", getml.data.roles.time_stamp)  pit_stops Out[18]: name                        date   raceId driverId     year       lap milliseconds      stop time          duration      role                  time_stamp join_key join_key join_key numerical    numerical numerical unused_string unused_string unit time stamp, comparison only 0 2011-03-27 841 1 2011 16 23227 1 17:28:24 23.227 1 2011-03-27 841 1 2011 36 23199 2 17:59:29 23.199 2 2011-03-27 841 2 2011 15 22994 1 17:27:41 22.994 3 2011-03-27 841 2 2011 30 25098 2 17:51:32 25.098 4 2011-03-27 841 3 2011 16 23716 1 17:29:00 23.716 ... ... ... ... ... ... ... ... ... 6065 2017-09-17 982 839 2017 38 29134 6 21:29:07 29.134 6066 2017-09-17 982 840 2017 1 37403 1 20:06:43 37.403 6067 2017-09-17 982 840 2017 2 29294 2 20:10:07 29.294 6068 2017-09-17 982 840 2017 3 25584 3 20:13:16 25.584 6069 2017-09-17 982 840 2017 26 29412 4 21:05:07 29.412 <p>     6070 rows x 9 columns     memory usage: 0.46 MB     name: pit_stops     type: getml.DataFrame </p> In\u00a0[19]: Copied! <pre>qualifying.set_role([\"raceId\", \"driverId\", \"qualifyId\"], getml.data.roles.join_key)\nqualifying.set_role([\"position\", \"number\"], getml.data.roles.numerical)\n\nqualifying\n</pre> qualifying.set_role([\"raceId\", \"driverId\", \"qualifyId\"], getml.data.roles.join_key) qualifying.set_role([\"position\", \"number\"], getml.data.roles.numerical)  qualifying Out[19]: name   raceId driverId qualifyId  position    number constructorId q1            q2            q3            role join_key join_key  join_key numerical numerical  unused_float unused_string unused_string unused_string 0 18 1 1 1 22 1 1:26.572 1:25.187 1:26.714 1 18 9 2 2 4 2 1:26.103 1:25.315 1:26.869 2 18 5 3 3 23 1 1:25.664 1:25.452 1:27.079 3 18 13 4 4 2 6 1:25.994 1:25.691 1:27.178 4 18 2 5 5 3 2 1:25.960 1:25.518 1:27.236 ... ... ... ... ... ... ... ... ... 7392 982 825 7415 16 20 210 1:43.756 NULL NULL 7393 982 13 7416 17 19 3 1:44.014 NULL NULL 7394 982 840 7417 18 18 3 1:44.728 NULL NULL 7395 982 836 7418 19 94 15 1:45.059 NULL NULL 7396 982 828 7419 20 9 15 1:45.570 NULL NULL <p>     7397 rows x 9 columns     memory usage: 0.57 MB     name: qualifying     type: getml.DataFrame </p> In\u00a0[20]: Copied! <pre>split = getml.data.split.random(train=0.8, test=0.2)\nsplit\n</pre> split = getml.data.split.random(train=0.8, test=0.2) split Out[20]: 0 train 1 train 2 train 3 test 4 train ... <p>     infinite number of  rows          type: StringColumnView </p> In\u00a0[21]: Copied! <pre>star_schema = getml.data.StarSchema(population=driver_standings.drop([\"position\"]), alias=\"population\", split=split)\n\nstar_schema.join(\n    driver_standings,\n    on=[\"driverId\"],\n    time_stamps=\"date\",\n    horizon=getml.data.time.days(1),\n    lagged_targets=True,\n)\n\n# We cannot use lap times for the race\n# we would like to predict, so we set\n# a non-zero horizon.\nstar_schema.join(\n    lap_times,\n    on=[\"driverId\"],\n    time_stamps=\"date\",\n    horizon=getml.data.time.days(1),\n)\n\n# We cannot use pit stops for the race\n# we would like to predict, so we set\n# a non-zero horizon.\nstar_schema.join(\n    pit_stops,\n    on=[\"driverId\"],\n    time_stamps=\"date\",\n    horizon=getml.data.time.days(1),\n)\n\nstar_schema.join(\n    qualifying,\n    on=[\"driverId\", \"raceId\"],\n    relationship=getml.data.relationship.many_to_one,\n)\n\nstar_schema.join(\n    drivers,\n    on=[\"driverId\"],\n    relationship=getml.data.relationship.many_to_one,\n)\n\nstar_schema\n</pre> star_schema = getml.data.StarSchema(population=driver_standings.drop([\"position\"]), alias=\"population\", split=split)  star_schema.join(     driver_standings,     on=[\"driverId\"],     time_stamps=\"date\",     horizon=getml.data.time.days(1),     lagged_targets=True, )  # We cannot use lap times for the race # we would like to predict, so we set # a non-zero horizon. star_schema.join(     lap_times,     on=[\"driverId\"],     time_stamps=\"date\",     horizon=getml.data.time.days(1), )  # We cannot use pit stops for the race # we would like to predict, so we set # a non-zero horizon. star_schema.join(     pit_stops,     on=[\"driverId\"],     time_stamps=\"date\",     horizon=getml.data.time.days(1), )  star_schema.join(     qualifying,     on=[\"driverId\", \"raceId\"],     relationship=getml.data.relationship.many_to_one, )  star_schema.join(     drivers,     on=[\"driverId\"],     relationship=getml.data.relationship.many_to_one, )  star_schema Out[21]: data model diagram driver_standingslap_timespit_stopsqualifyingdriverspopulationdriverId = driverIddate &lt;= dateHorizon: 1.0 daysLagged targets alloweddriverId = driverIddate &lt;= dateHorizon: 1.0 daysdriverId = driverIddate &lt;= dateHorizon: 1.0 daysdriverId = driverIdraceId = raceIdRelationship: many-to-onedriverId = driverIdRelationship: many-to-one staging data frames                     staging table                    0 population, qualifying, drivers POPULATION__STAGING_TABLE_1 1 driver_standings DRIVER_STANDINGS__STAGING_TABLE_2 2 lap_times LAP_TIMES__STAGING_TABLE_3 3 pit_stops PIT_STOPS__STAGING_TABLE_4 container population subset name              rows type 0 test driver_standings 6229 View 1 train driver_standings 25349 View peripheral name               rows type      0 driver_standings 31578 DataFrame 1 lap_times 420369 DataFrame 2 pit_stops 6070 DataFrame 3 qualifying 7397 DataFrame 4 drivers 840 DataFrame <p>Set-up the feature learner &amp; predictor</p> <p>We use the relboost algorithms for this problem. Because of the large number of keywords, we regularize the model a bit by requiring a minimum support for the keywords (<code>min_num_samples</code>).</p> In\u00a0[22]: Copied! <pre>mapping = getml.preprocessors.Mapping()\n\nfast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    aggregation=getml.feature_learning.FastProp.agg_sets.All,\n    num_threads=1,\n)\n\nrelboost = getml.feature_learning.Relboost(\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    num_threads=1,\n)\n\nrelmt = getml.feature_learning.RelMT(\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    num_threads=1,\n)\n\npredictor = getml.predictors.XGBoostClassifier(n_jobs=1)\n</pre> mapping = getml.preprocessors.Mapping()  fast_prop = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     aggregation=getml.feature_learning.FastProp.agg_sets.All,     num_threads=1, )  relboost = getml.feature_learning.Relboost(     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     num_threads=1, )  relmt = getml.feature_learning.RelMT(     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     num_threads=1, )  predictor = getml.predictors.XGBoostClassifier(n_jobs=1) <p>Build the pipeline</p> In\u00a0[23]: Copied! <pre>pipe1 = getml.pipeline.Pipeline(\n    tags=['fast_prop'],\n    data_model=star_schema.data_model,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    predictors=[predictor],\n    include_categorical=True,\n)\n\npipe1\n</pre> pipe1 = getml.pipeline.Pipeline(     tags=['fast_prop'],     data_model=star_schema.data_model,     preprocessors=[mapping],     feature_learners=[fast_prop],     predictors=[predictor],     include_categorical=True, )  pipe1 Out[23]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='CrossEntropyLoss',\n         peripheral=['driver_standings', 'drivers', 'lap_times', 'pit_stops', 'qualifying'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['fast_prop'])</pre> In\u00a0[24]: Copied! <pre>pipe1.check(star_schema.train)\n</pre> pipe1.check(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:03, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 2 issues labeled INFO and 0 issues labeled WARNING.\n</pre> Out[24]: type label                  message                          0 INFO FOREIGN KEYS NOT FOUND When joining POPULATION__STAGING_TABLE_1 and LAP_TIMES__STAGING_TABLE_3 over 'driverId' and 'driverId', there are no corresponding entries for 68.551028% of entries in 'driverId' in 'POPULATION__STAGING_TABLE_1'. You might want to double-check your join keys. 1 INFO FOREIGN KEYS NOT FOUND When joining POPULATION__STAGING_TABLE_1 and PIT_STOPS__STAGING_TABLE_4 over 'driverId' and 'driverId', there are no corresponding entries for 82.527910% of entries in 'driverId' in 'POPULATION__STAGING_TABLE_1'. You might want to double-check your join keys. In\u00a0[25]: Copied! <pre>pipe1.fit(star_schema.train)\n</pre> pipe1.fit(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 2 issues labeled INFO and 0 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 993 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 03:02, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:34, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:12, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:3m:49.21525\n\n</pre> Out[25]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='CrossEntropyLoss',\n         peripheral=['driver_standings', 'drivers', 'lap_times', 'pit_stops', 'qualifying'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['fast_prop', 'container-jIU2eX'])</pre> In\u00a0[26]: Copied! <pre>fastprop_score = pipe1.score(star_schema.test)\nfastprop_score\n</pre> fastprop_score = pipe1.score(star_schema.test) fastprop_score <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:08, remaining: 00:00]          \n\n</pre> Out[26]: date time           set used target accuracy     auc cross entropy 0 2024-02-21 15:02:56 train win 0.9737 0.9581 0.07505 1 2024-02-21 15:03:04 test win 0.9722 0.923 0.08549 <p>featuretools requires some additional data preparation before we can start engineering features.</p> In\u00a0[27]: Copied! <pre>population_train_pd = star_schema.train.population.drop([\"position\"]).to_pandas()\npopulation_test_pd = star_schema.test.population.drop([\"position\"]).to_pandas()\n</pre> population_train_pd = star_schema.train.population.drop([\"position\"]).to_pandas() population_test_pd = star_schema.test.population.drop([\"position\"]).to_pandas() In\u00a0[28]: Copied! <pre>driver_standings_pd = driver_standings.drop(driver_standings.roles.unused).to_pandas()\nlap_times_pd = lap_times.drop(lap_times.roles.unused).to_pandas()\npit_stops_pd = pit_stops.drop(pit_stops.roles.unused).to_pandas()\nqualifying_pd = qualifying.drop(qualifying.roles.unused).to_pandas()\ndrivers_pd = drivers.drop(drivers.roles.unused).to_pandas()\n</pre> driver_standings_pd = driver_standings.drop(driver_standings.roles.unused).to_pandas() lap_times_pd = lap_times.drop(lap_times.roles.unused).to_pandas() pit_stops_pd = pit_stops.drop(pit_stops.roles.unused).to_pandas() qualifying_pd = qualifying.drop(qualifying.roles.unused).to_pandas() drivers_pd = drivers.drop(drivers.roles.unused).to_pandas() <p>Because <code>qualifying</code> and <code>drivers</code> are many-to-one joins, we can directly join them onto our table.</p> In\u00a0[29]: Copied! <pre>population_train_pd[\"id\"] = population_train_pd.index\n\npopulation_train_pd = population_train_pd.merge(\n    qualifying_pd,\n    on=[\"driverId\", \"raceId\"],\n    how=\"left\",\n)\n\npopulation_train_pd = population_train_pd.merge(\n    drivers_pd,\n    on=[\"driverId\"],\n    how=\"left\",\n)\n\npopulation_train_pd\n</pre> population_train_pd[\"id\"] = population_train_pd.index  population_train_pd = population_train_pd.merge(     qualifying_pd,     on=[\"driverId\", \"raceId\"],     how=\"left\", )  population_train_pd = population_train_pd.merge(     drivers_pd,     on=[\"driverId\"],     how=\"left\", )  population_train_pd Out[29]: raceId driverId year win date id qualifyId position number nationality driverRef 0 18 1 2008 1.0 2008-03-16 0 1 1.0 22.0 British hamilton 1 18 2 2008 0.0 2008-03-16 1 5 5.0 3.0 German heidfeld 2 18 3 2008 0.0 2008-03-16 2 7 7.0 7.0 German rosberg 3 18 5 2008 0.0 2008-03-16 3 3 3.0 23.0 Finnish kovalainen 4 18 6 2008 0.0 2008-03-16 4 14 14.0 8.0 Japanese nakajima ... ... ... ... ... ... ... ... ... ... ... ... 25344 982 835 2017 0.0 2017-09-17 25344 7410 11.0 30.0 British jolyon_palmer 25345 982 154 2017 0.0 2017-09-17 25345 7414 15.0 8.0 French grosjean 25346 982 836 2017 0.0 2017-09-17 25346 7418 19.0 94.0 German wehrlein 25347 982 18 2017 0.0 2017-09-17 25347 NaN NaN NaN British button 25348 982 814 2017 0.0 2017-09-17 25348 NaN NaN NaN British resta <p>25349 rows \u00d7 11 columns</p> <p>Same for the testing set.</p> In\u00a0[30]: Copied! <pre>population_test_pd[\"id\"] = population_test_pd.index\n\npopulation_test_pd = population_test_pd.merge(\n    qualifying_pd,\n    on=[\"driverId\", \"raceId\"],\n    how=\"left\",\n)\n\npopulation_test_pd = population_test_pd.merge(\n    drivers_pd,\n    on=[\"driverId\"],\n    how=\"left\",\n)\n\npopulation_test_pd\n</pre> population_test_pd[\"id\"] = population_test_pd.index  population_test_pd = population_test_pd.merge(     qualifying_pd,     on=[\"driverId\", \"raceId\"],     how=\"left\", )  population_test_pd = population_test_pd.merge(     drivers_pd,     on=[\"driverId\"],     how=\"left\", )  population_test_pd Out[30]: raceId driverId year win date id qualifyId position number nationality driverRef 0 18 4 2008 0.0 2008-03-16 0 12 12.0 5.0 Spanish alonso 1 18 7 2008 0.0 2008-03-16 1 18 18.0 14.0 French bourdais 2 19 3 2008 0.0 2008-03-23 2 38 16.0 7.0 German rosberg 3 19 11 2008 0.0 2008-03-23 3 42 20.0 18.0 Japanese sato 4 20 7 2008 0.0 2008-04-06 4 59 15.0 14.0 French bourdais ... ... ... ... ... ... ... ... ... ... ... ... 6224 981 835 2017 0.0 2017-09-03 6224 7396 17.0 30.0 British jolyon_palmer 6225 981 814 2017 0.0 2017-09-03 6225 NaN NaN NaN British resta 6226 982 822 2017 0.0 2017-09-17 6226 7405 6.0 77.0 Finnish bottas 6227 982 832 2017 0.0 2017-09-17 6227 7409 10.0 55.0 Spanish sainz 6228 982 826 2017 0.0 2017-09-17 6228 7412 13.0 26.0 Russian kvyat <p>6229 rows \u00d7 11 columns</p> <p>featuretools requires us to expand our peripheral tables so they can be joined using the unique id from the population table. Luckily, we can write a simple helper function that works for all three remaining peripheral tables.</p> In\u00a0[31]: Copied! <pre>def prepare_peripheral(peripheral, train_or_test):\n    \"\"\"\n    Helper function that imitates the behavior of \n    the data model defined above.\n    \"\"\"\n    peripheral_new = peripheral.merge(\n        train_or_test[[\"id\", \"driverId\", \"date\"]],\n        on=[\"driverId\"],\n    )\n\n    peripheral_new = peripheral_new[\n        peripheral_new[\"date_x\"] &lt; peripheral_new[\"date_y\"]\n    ]\n    \n    del peripheral_new[\"date_y\"]\n    del peripheral_new[\"driverId\"]\n    del peripheral_new[\"raceId\"]\n\n    return peripheral_new.rename(columns={\"date_x\": \"date\"})\n</pre> def prepare_peripheral(peripheral, train_or_test):     \"\"\"     Helper function that imitates the behavior of      the data model defined above.     \"\"\"     peripheral_new = peripheral.merge(         train_or_test[[\"id\", \"driverId\", \"date\"]],         on=[\"driverId\"],     )      peripheral_new = peripheral_new[         peripheral_new[\"date_x\"] &lt; peripheral_new[\"date_y\"]     ]          del peripheral_new[\"date_y\"]     del peripheral_new[\"driverId\"]     del peripheral_new[\"raceId\"]      return peripheral_new.rename(columns={\"date_x\": \"date\"}) In\u00a0[32]: Copied! <pre>driver_standings_train_pd = prepare_peripheral(driver_standings_pd, population_train_pd)\ndriver_standings_test_pd = prepare_peripheral(driver_standings_pd, population_test_pd)\ndriver_standings_train_pd\n</pre> driver_standings_train_pd = prepare_peripheral(driver_standings_pd, population_train_pd) driver_standings_test_pd = prepare_peripheral(driver_standings_pd, population_test_pd) driver_standings_train_pd Out[32]: year position win date id 1 2008 1.0 1.0 2008-03-16 6 2 2008 1.0 1.0 2008-03-16 22 3 2008 1.0 1.0 2008-03-16 39 4 2008 1.0 1.0 2008-03-16 54 5 2008 1.0 1.0 2008-03-16 72 ... ... ... ... ... ... 2656230 2017 12.0 0.0 2017-07-30 25323 2656231 2017 12.0 0.0 2017-07-30 25341 2656242 2017 13.0 0.0 2017-08-27 25323 2656243 2017 13.0 0.0 2017-08-27 25341 2656255 2017 12.0 0.0 2017-09-03 25341 <p>1317941 rows \u00d7 5 columns</p> In\u00a0[33]: Copied! <pre>lap_times_train_pd = prepare_peripheral(lap_times_pd, population_train_pd)\nlap_times_test_pd = prepare_peripheral(lap_times_pd, population_test_pd)\nlap_times_train_pd\n</pre> lap_times_train_pd = prepare_peripheral(lap_times_pd, population_train_pd) lap_times_test_pd = prepare_peripheral(lap_times_pd, population_test_pd) lap_times_train_pd Out[33]: year lap milliseconds position date id 28 2009 1.0 109088.0 13.0 2009-03-29 5874 29 2009 1.0 109088.0 13.0 2009-03-29 5904 30 2009 1.0 109088.0 13.0 2009-03-29 5918 31 2009 1.0 109088.0 13.0 2009-03-29 5936 32 2009 1.0 109088.0 13.0 2009-03-29 5954 ... ... ... ... ... ... ... 54811898 2017 3.0 156151.0 17.0 2017-04-09 25264 54811899 2017 3.0 156151.0 17.0 2017-04-09 25281 54811900 2017 3.0 156151.0 17.0 2017-04-09 25300 54811901 2017 3.0 156151.0 17.0 2017-04-09 25320 54811902 2017 3.0 156151.0 17.0 2017-04-09 25337 <p>25544456 rows \u00d7 6 columns</p> <p><code>lap_times</code> demonstrates one of the greatest dangers of featuretools. Because it is written in pure Python, featuretools requires you expand your tables so that they can be more easily joined. But this comes at the cost of increased memory consumption: In this case a table that used to have about 420,000 rows has now been expanded to over 25 million rows.</p> In\u00a0[34]: Copied! <pre>pit_stops_train_pd = prepare_peripheral(pit_stops_pd, population_train_pd)\npit_stops_test_pd = prepare_peripheral(pit_stops_pd, population_test_pd)\npit_stops_train_pd\n</pre> pit_stops_train_pd = prepare_peripheral(pit_stops_pd, population_train_pd) pit_stops_test_pd = prepare_peripheral(pit_stops_pd, population_test_pd) pit_stops_train_pd Out[34]: year lap milliseconds stop date id 57 2011 16.0 23227.0 1.0 2011-03-27 22459 58 2011 16.0 23227.0 1.0 2011-03-27 22498 59 2011 16.0 23227.0 1.0 2011-03-27 22536 60 2011 16.0 23227.0 1.0 2011-03-27 22555 61 2011 16.0 23227.0 1.0 2011-03-27 22574 ... ... ... ... ... ... ... 688120 2017 2.0 29443.0 1.0 2017-04-09 25264 688121 2017 2.0 29443.0 1.0 2017-04-09 25281 688122 2017 2.0 29443.0 1.0 2017-04-09 25300 688123 2017 2.0 29443.0 1.0 2017-04-09 25320 688124 2017 2.0 29443.0 1.0 2017-04-09 25337 <p>213549 rows \u00d7 6 columns</p> In\u00a0[35]: Copied! <pre>del population_train_pd[\"driverId\"]\ndel population_train_pd[\"raceId\"]\ndel population_train_pd[\"year\"]\ndel population_train_pd[\"qualifyId\"]\n</pre> del population_train_pd[\"driverId\"] del population_train_pd[\"raceId\"] del population_train_pd[\"year\"] del population_train_pd[\"qualifyId\"] In\u00a0[36]: Copied! <pre>del population_test_pd[\"driverId\"]\ndel population_test_pd[\"raceId\"]\ndel population_test_pd[\"year\"]\ndel population_test_pd[\"qualifyId\"]\n</pre> del population_test_pd[\"driverId\"] del population_test_pd[\"raceId\"] del population_test_pd[\"year\"] del population_test_pd[\"qualifyId\"] In\u00a0[37]: Copied! <pre>def add_index(df):\n    df.insert(0, \"index\", range(len(df)))\n\npopulation_pd_logical_types = {\n    \"id\": ww.logical_types.Integer,\n    \"win\": ww.logical_types.Integer,\n    \"date\": ww.logical_types.Datetime,\n    \"position\": ww.logical_types.IntegerNullable,\n    \"number\": ww.logical_types.IntegerNullable,\n    \"nationality\": ww.logical_types.Categorical,\n    \"driverRef\": ww.logical_types.Categorical,\n}\npopulation_train_pd.ww.init(logical_types=population_pd_logical_types, index=\"id\", name=\"population\")\npopulation_test_pd.ww.init(logical_types=population_pd_logical_types, index=\"id\", name=\"population\")\n\nadd_index(driver_standings_train_pd)\nadd_index(driver_standings_test_pd)\ndriver_standings_pd_logical_types = {\n    \"index\": ww.logical_types.Integer,\n    \"year\": ww.logical_types.Integer,\n    \"position\": ww.logical_types.IntegerNullable,\n    \"win\": ww.logical_types.Integer,\n    \"date\": ww.logical_types.Datetime,\n    \"id\": ww.logical_types.Integer,\n}\ndriver_standings_train_pd.ww.init(logical_types=driver_standings_pd_logical_types, index=\"index\", name=\"driver_standings\")\ndriver_standings_test_pd.ww.init(logical_types=driver_standings_pd_logical_types, index=\"index\", name=\"driver_standings\")\n\nadd_index(lap_times_train_pd)\nadd_index(lap_times_test_pd)\nlap_times_pd_logical_types = {\n    \"index\": ww.logical_types.Integer,\n    \"year\": ww.logical_types.Integer,\n    \"lap\": ww.logical_types.Integer,\n    \"milliseconds\": ww.logical_types.Integer,\n    \"position\": ww.logical_types.IntegerNullable,\n    \"date\": ww.logical_types.Datetime,\n    \"id\": ww.logical_types.Integer,\n}\nlap_times_train_pd.ww.init(logical_types=lap_times_pd_logical_types, index=\"index\", name=\"lap_times\")\nlap_times_test_pd.ww.init(logical_types=lap_times_pd_logical_types, index=\"index\", name=\"lap_times\")\n\nadd_index(pit_stops_train_pd)\nadd_index(pit_stops_test_pd)\npit_stops_pd_logical_types = {\n    \"index\": ww.logical_types.Integer,\n    \"year\": ww.logical_types.Integer,\n    \"lap\": ww.logical_types.Integer,\n    \"milliseconds\": ww.logical_types.Integer,\n    \"stop\": ww.logical_types.Categorical,\n    \"date\": ww.logical_types.Datetime,\n    \"id\": ww.logical_types.Integer,\n}\npit_stops_train_pd.ww.init(logical_types=pit_stops_pd_logical_types, index=\"index\", name=\"pit_stops\")\npit_stops_test_pd.ww.init(logical_types=pit_stops_pd_logical_types, index=\"index\", name=\"pit_stops\")\n</pre> def add_index(df):     df.insert(0, \"index\", range(len(df)))  population_pd_logical_types = {     \"id\": ww.logical_types.Integer,     \"win\": ww.logical_types.Integer,     \"date\": ww.logical_types.Datetime,     \"position\": ww.logical_types.IntegerNullable,     \"number\": ww.logical_types.IntegerNullable,     \"nationality\": ww.logical_types.Categorical,     \"driverRef\": ww.logical_types.Categorical, } population_train_pd.ww.init(logical_types=population_pd_logical_types, index=\"id\", name=\"population\") population_test_pd.ww.init(logical_types=population_pd_logical_types, index=\"id\", name=\"population\")  add_index(driver_standings_train_pd) add_index(driver_standings_test_pd) driver_standings_pd_logical_types = {     \"index\": ww.logical_types.Integer,     \"year\": ww.logical_types.Integer,     \"position\": ww.logical_types.IntegerNullable,     \"win\": ww.logical_types.Integer,     \"date\": ww.logical_types.Datetime,     \"id\": ww.logical_types.Integer, } driver_standings_train_pd.ww.init(logical_types=driver_standings_pd_logical_types, index=\"index\", name=\"driver_standings\") driver_standings_test_pd.ww.init(logical_types=driver_standings_pd_logical_types, index=\"index\", name=\"driver_standings\")  add_index(lap_times_train_pd) add_index(lap_times_test_pd) lap_times_pd_logical_types = {     \"index\": ww.logical_types.Integer,     \"year\": ww.logical_types.Integer,     \"lap\": ww.logical_types.Integer,     \"milliseconds\": ww.logical_types.Integer,     \"position\": ww.logical_types.IntegerNullable,     \"date\": ww.logical_types.Datetime,     \"id\": ww.logical_types.Integer, } lap_times_train_pd.ww.init(logical_types=lap_times_pd_logical_types, index=\"index\", name=\"lap_times\") lap_times_test_pd.ww.init(logical_types=lap_times_pd_logical_types, index=\"index\", name=\"lap_times\")  add_index(pit_stops_train_pd) add_index(pit_stops_test_pd) pit_stops_pd_logical_types = {     \"index\": ww.logical_types.Integer,     \"year\": ww.logical_types.Integer,     \"lap\": ww.logical_types.Integer,     \"milliseconds\": ww.logical_types.Integer,     \"stop\": ww.logical_types.Categorical,     \"date\": ww.logical_types.Datetime,     \"id\": ww.logical_types.Integer, } pit_stops_train_pd.ww.init(logical_types=pit_stops_pd_logical_types, index=\"index\", name=\"pit_stops\") pit_stops_test_pd.ww.init(logical_types=pit_stops_pd_logical_types, index=\"index\", name=\"pit_stops\") In\u00a0[38]: Copied! <pre>dataframes_train = {\n    \"population\" : (population_train_pd, ),\n    \"driver_standings\": (driver_standings_train_pd, ),\n    \"lap_times\" : (lap_times_train_pd, ),\n    \"pit_stops\" : (pit_stops_train_pd, ),\n}\n</pre> dataframes_train = {     \"population\" : (population_train_pd, ),     \"driver_standings\": (driver_standings_train_pd, ),     \"lap_times\" : (lap_times_train_pd, ),     \"pit_stops\" : (pit_stops_train_pd, ), } In\u00a0[39]: Copied! <pre>dataframes_test = {\n    \"population\" : (population_test_pd, ),\n    \"driver_standings\": (driver_standings_test_pd, ),\n    \"lap_times\" : (lap_times_test_pd, ),\n    \"pit_stops\" : (pit_stops_test_pd, )\n}\n</pre> dataframes_test = {     \"population\" : (population_test_pd, ),     \"driver_standings\": (driver_standings_test_pd, ),     \"lap_times\" : (lap_times_test_pd, ),     \"pit_stops\" : (pit_stops_test_pd, ) } In\u00a0[40]: Copied! <pre>relationships = [\n    (\"population\", \"id\", \"driver_standings\", \"id\"),\n    (\"population\", \"id\", \"lap_times\", \"id\"),\n    (\"population\", \"id\", \"pit_stops\", \"id\")\n]\n</pre> relationships = [     (\"population\", \"id\", \"driver_standings\", \"id\"),     (\"population\", \"id\", \"lap_times\", \"id\"),     (\"population\", \"id\", \"pit_stops\", \"id\") ] In\u00a0[41]: Copied! <pre>featuretools_train_pd = featuretools.dfs(\n    dataframes=dataframes_train,\n    relationships=relationships,\n    target_dataframe_name=\"population\")[0]\n</pre> featuretools_train_pd = featuretools.dfs(     dataframes=dataframes_train,     relationships=relationships,     target_dataframe_name=\"population\")[0] In\u00a0[42]: Copied! <pre>featuretools_test_pd = featuretools.dfs(\n    dataframes=dataframes_test,\n    relationships=relationships,\n    target_dataframe_name=\"population\")[0]\n</pre> featuretools_test_pd = featuretools.dfs(     dataframes=dataframes_test,     relationships=relationships,     target_dataframe_name=\"population\")[0] In\u00a0[43]: Copied! <pre>featuretools_train = getml.data.DataFrame.from_pandas(featuretools_train_pd, \"featuretools_train\")\nfeaturetools_test = getml.data.DataFrame.from_pandas(featuretools_test_pd, \"featuretools_test\")\n</pre> featuretools_train = getml.data.DataFrame.from_pandas(featuretools_train_pd, \"featuretools_train\") featuretools_test = getml.data.DataFrame.from_pandas(featuretools_test_pd, \"featuretools_test\") In\u00a0[44]: Copied! <pre>featuretools_train.set_role(\"win\", getml.data.roles.target)\nfeaturetools_train.set_role(featuretools_train.roles.unused_float, getml.data.roles.numerical)\nfeaturetools_train.set_role(featuretools_train.roles.unused_string, getml.data.roles.categorical)\n\nfeaturetools_train\n</pre> featuretools_train.set_role(\"win\", getml.data.roles.target) featuretools_train.set_role(featuretools_train.roles.unused_float, getml.data.roles.numerical) featuretools_train.set_role(featuretools_train.roles.unused_string, getml.data.roles.categorical)  featuretools_train Out[44]:  name    win position    number      nationality driverRef     COUNT(driver_standings) COUNT(lap_times) COUNT(pit_stops) MODE(pit_stops.stop) NUM_UNIQUE(pit_stops.stop) DAY(date)   MONTH(date) WEEKDAY(date) YEAR(date)  MODE(driver_standings.DAY(date)) MODE(driver_standings.MONTH(date)) MODE(driver_standings.WEEKDAY(date)) MODE(driver_standings.YEAR(date)) NUM_UNIQUE(driver_standings.DAY(date)) NUM_UNIQUE(driver_standings.MONTH(date)) NUM_UNIQUE(driver_standings.WEEKDAY(date)) NUM_UNIQUE(driver_standings.YEAR(date)) MODE(lap_times.DAY(date)) MODE(lap_times.MONTH(date)) MODE(lap_times.WEEKDAY(date)) MODE(lap_times.YEAR(date)) NUM_UNIQUE(lap_times.DAY(date)) NUM_UNIQUE(lap_times.MONTH(date)) NUM_UNIQUE(lap_times.WEEKDAY(date)) NUM_UNIQUE(lap_times.YEAR(date)) MODE(pit_stops.DAY(date)) MODE(pit_stops.MONTH(date)) MODE(pit_stops.WEEKDAY(date)) MODE(pit_stops.YEAR(date)) NUM_UNIQUE(pit_stops.DAY(date)) NUM_UNIQUE(pit_stops.MONTH(date)) NUM_UNIQUE(pit_stops.WEEKDAY(date)) NUM_UNIQUE(pit_stops.YEAR(date)) MAX(driver_standings.position) MAX(driver_standings.win) MAX(driver_standings.year) MEAN(driver_standings.position) MEAN(driver_standings.win) MEAN(driver_standings.year) MIN(driver_standings.position) MIN(driver_standings.win) MIN(driver_standings.year) SKEW(driver_standings.position) SKEW(driver_standings.win) SKEW(driver_standings.year) STD(driver_standings.position) STD(driver_standings.win) STD(driver_standings.year) SUM(driver_standings.position) SUM(driver_standings.win) SUM(driver_standings.year) MAX(lap_times.lap) MAX(lap_times.milliseconds) MAX(lap_times.position) MAX(lap_times.year) MEAN(lap_times.lap) MEAN(lap_times.milliseconds) MEAN(lap_times.position) MEAN(lap_times.year) MIN(lap_times.lap) MIN(lap_times.milliseconds) MIN(lap_times.position) MIN(lap_times.year) SKEW(lap_times.lap) SKEW(lap_times.milliseconds) SKEW(lap_times.position) SKEW(lap_times.year) STD(lap_times.lap) STD(lap_times.milliseconds) STD(lap_times.position) STD(lap_times.year) SUM(lap_times.lap) SUM(lap_times.milliseconds) SUM(lap_times.position) SUM(lap_times.year) MAX(pit_stops.lap) MAX(pit_stops.milliseconds) MAX(pit_stops.year) MEAN(pit_stops.lap) MEAN(pit_stops.milliseconds) MEAN(pit_stops.year) MIN(pit_stops.lap) MIN(pit_stops.milliseconds) MIN(pit_stops.year) SKEW(pit_stops.lap) SKEW(pit_stops.milliseconds) SKEW(pit_stops.year) STD(pit_stops.lap) STD(pit_stops.milliseconds) STD(pit_stops.year) SUM(pit_stops.lap) SUM(pit_stops.milliseconds) SUM(pit_stops.year)  role target categorical categorical categorical categorical   categorical             categorical      categorical      categorical          categorical                categorical categorical categorical   categorical categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical               categorical                 categorical                   categorical                categorical                     categorical                      categorical                      categorical                      categorical               categorical                 categorical                   categorical                categorical                     categorical                      categorical                      categorical                                           numerical                 numerical                  numerical                       numerical                  numerical                   numerical                      numerical                 numerical                  numerical                       numerical                  numerical                   numerical                      numerical                 numerical                  numerical                      numerical                 numerical                  numerical          numerical                   numerical               numerical           numerical           numerical                    numerical                numerical            numerical          numerical                   numerical               numerical           numerical           numerical                    numerical                numerical            numerical          numerical                   numerical               numerical           numerical          numerical                   numerical               numerical           numerical          numerical                   numerical           numerical           numerical                    numerical            numerical          numerical                   numerical           numerical           numerical                    numerical            numerical          numerical                   numerical           numerical          numerical                   numerical           numerical 0 1 1 22 British hamilton 17 1037 0 NULL NULL 16 3 6 2008 8 7 6 2007 16 8 1 1 8 7 6 2007 16 8 1 1 NULL NULL NULL NULL NULL NULL NULL NULL 3 1 2007 1.4706 0.2353 2007 1 0 2007 1.3538 1.3723 0 0.7998 0.4372 0 25 4 34119 78 1453884 19 2007 32.0559 90600.8602 3.2575 2007 1 72506 1 2007 0.1882 26.093 2.4493 0 19.1156 45564.9471 3.4488 0 33242 93953092 3378 2081259 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 1 0 5 3 German heidfeld 134 6863 0 NULL NULL 16 3 6 2008 1 7 6 2005 31 8 1 8 1 7 6 2007 31 8 1 8 NULL NULL NULL NULL NULL NULL NULL NULL 21 0 2007 10.8582 0 2003.5448 4 0 2000 0.4681 0 -0.04499 5.136 0 2.3255 1455 0 268475 78 1285986 22 2007 30.4682 89072.7102 9.6799 2003.7322 1 70516 1 2000 0.2623 30.5344 0.3339 -0.05998 18.7041 20544.1746 4.199 2.258 209103 611306010 66433 13751614 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 2 0 7 7 German rosberg 35 1627 0 NULL NULL 16 3 6 2008 8 7 6 2006 25 8 1 2 8 5 6 2007 25 8 1 2 NULL NULL NULL NULL NULL NULL NULL NULL 17 0 2007 11.9714 0 2006.4857 7 0 2006 0.1328 0 0.05976 3.1761 0 0.5071 419 0 70227 77 191891 22 2007 30.4124 89820.1174 9.9809 2006.5931 1 73159 3 2006 0.2225 1.7858 0.5491 -0.3794 18.6109 14212.8848 3.733 0.4914 49481 146137331 16239 3264727 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 3 0 3 23 Finnish kovalainen 17 1023 0 NULL NULL 16 3 6 2008 8 7 6 2007 16 8 1 1 8 7 6 2007 16 8 1 1 NULL NULL NULL NULL NULL NULL NULL NULL 11 0 2007 8.3529 0 2007 7 0 2007 0.7931 0 0 1.4116 0 0 142 0 34119 76 1394884 22 2007 31.4526 92458.8299 9.4018 2007 1 73998 1 2007 0.1906 26.2493 0.6941 0 18.6943 43578.4946 4.1724 0 32176 94585383 9618 2053161 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 4 0 14 8 Japanese nakajima 1 70 0 NULL NULL 16 3 6 2008 21 10 6 2007 1 1 1 1 21 10 6 2007 1 1 1 1 NULL NULL NULL NULL NULL NULL NULL NULL 22 0 2007 22 0 2007 22 0 2007 nan nan nan nan nan nan 22 0 2007 70 104396 16 2007 35.5 76228.3571 10.9143 2007 1 73116 9 2007 0 4.4222 1.2783 0 20.3511 4785.2529 1.8552 0 2485 5335985 764 140490 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 25344 0 11 30 British jolyon_palmer 34 1517 56 1 4 17 9 6 2017 3 7 6 2016 25 9 1 2 9 7 6 2016 25 9 1 2 9 7 6 2016 21 9 1 2 20 0 2017 18.0588 0 2016.3824 11 0 2016 -1.7626 0 0.5068 1.8413 0 0.4933 614 0 68557 78 1223356 22 2017 29.4199 98411.6038 14.5175 2016.3665 1 68652 7 2016 0.2921 21.0237 0.3751 0.5546 18.5854 43393.1519 3.0483 0.482 44630 149290403 22023 3058828 50 1088294 2017 20.9286 61015.1429 2016.2857 1 16851 2016 0.3035 5.1507 0.975 13.473 191214.0556 0.4558 1172 3416848 112912 25345 0 15 8 French grosjean 118 5518 206 1 5 17 9 6 2017 27 7 6 2016 31 9 1 7 9 7 6 2016 31 9 1 7 9 7 6 2014 31 9 1 6 24 0 2017 11.5254 0 2014.0508 5 0 2009 1.3306 0 -0.5931 4.1709 0 2.0542 1360 0 237658 78 3670560 24 2017 29.4511 99489.4235 10.7356 2014.1414 1 68590 1 2009 0.2565 39.9299 0.06907 -0.6145 18.0521 63108.3058 4.8371 1.9904 162511 548982639 59239 11114032 62 1313665 2017 22.9369 46260.6214 2014.4272 1 15073 2012 0.3075 7.1215 0.04707 13.7483 154388.9496 1.6024 4725 9529688 414972 25346 0 19 94 German wehrlein 32 1636 62 1 5 17 9 6 2017 3 7 6 2016 24 9 1 2 3 7 6 2016 24 9 1 2 3 7 6 2016 24 8 1 2 22 0 2017 17.25 0 2016.3438 13 0 2016 0.3213 0 0.6908 2.2718 0 0.4826 552 0 64523 76 2118880 22 2017 30.3839 99952.8594 15.8319 2016.3594 1 69241 4 2016 0.2027 20.0905 -0.8509 0.5865 18.2369 77850.4087 3.2136 0.48 49708 163522878 25901 3298764 52 2008464 2017 21.6129 119925.5968 2016.3065 1 18064 2016 0.1765 3.9986 0.8606 12.8896 368710.9372 0.4648 1340 7435387 125011 25347 0 NULL NULL British button 309 16272 259 1 6 17 9 6 2017 24 7 6 2016 31 9 1 18 24 7 6 2013 31 9 1 18 25 7 6 2011 31 9 1 7 22 1 2017 9.6828 0.04854 2008.6181 1 0 2000 0.3055 4.2218 -0.099 5.8899 0.2153 5.0436 2992 15 620663 78 7503775 22 2017 30.2853 95662.1676 8.4123 2008.4672 1 69828 1 2000 0.2244 64.7495 0.4521 -0.1278 18.3544 79242.6165 4.859 4.8663 492803 1556614792 136885 32681778 64 2010062 2017 24.9344 45562.5985 2013.3977 1 14501 2011 0.3229 8.7373 0.09889 13.5509 179696.8608 1.785 6458 11800713 521470 25348 0 NULL NULL British resta 61 3284 127 1 4 17 9 6 2017 27 7 6 2012 27 9 1 4 27 7 6 2012 27 9 1 4 25 5 6 2011 27 9 1 4 23 0 2017 12.6066 0 2012.2459 8 0 2011 1.244 0 2.0563 3.4558 0 1.3498 769 0 122747 78 7503980 24 2017 30.3097 103049.4318 10.7728 2012.0539 1 76855 1 2011 0.1989 44.5439 0.1348 1.8215 18.0742 144158.4761 3.5107 1.0439 99537 338414334 35378 6607585 57 123124 2017 25.1496 24084.2598 2011.9528 1 14538 2011 0.3166 8.1337 1.2541 13.8456 9973.774 0.9416 3194 3058701 255518 <p>     25349 rows x 98 columns     memory usage: 16.12 MB     name: featuretools_train     type: getml.DataFrame </p> In\u00a0[45]: Copied! <pre>featuretools_test.set_role(\"win\", getml.data.roles.target)\nfeaturetools_test.set_role(featuretools_test.roles.unused_float, getml.data.roles.numerical)\nfeaturetools_test.set_role(featuretools_test.roles.unused_string, getml.data.roles.categorical)\n\nfeaturetools_test\n</pre> featuretools_test.set_role(\"win\", getml.data.roles.target) featuretools_test.set_role(featuretools_test.roles.unused_float, getml.data.roles.numerical) featuretools_test.set_role(featuretools_test.roles.unused_string, getml.data.roles.categorical)  featuretools_test Out[45]: name    win position    number      nationality driverRef     COUNT(driver_standings) COUNT(lap_times) COUNT(pit_stops) MODE(pit_stops.stop) NUM_UNIQUE(pit_stops.stop) DAY(date)   MONTH(date) WEEKDAY(date) YEAR(date)  MODE(driver_standings.DAY(date)) MODE(driver_standings.MONTH(date)) MODE(driver_standings.WEEKDAY(date)) MODE(driver_standings.YEAR(date)) NUM_UNIQUE(driver_standings.DAY(date)) NUM_UNIQUE(driver_standings.MONTH(date)) NUM_UNIQUE(driver_standings.WEEKDAY(date)) NUM_UNIQUE(driver_standings.YEAR(date)) MODE(lap_times.DAY(date)) MODE(lap_times.MONTH(date)) MODE(lap_times.WEEKDAY(date)) MODE(lap_times.YEAR(date)) NUM_UNIQUE(lap_times.DAY(date)) NUM_UNIQUE(lap_times.MONTH(date)) NUM_UNIQUE(lap_times.WEEKDAY(date)) NUM_UNIQUE(lap_times.YEAR(date)) MODE(pit_stops.DAY(date)) MODE(pit_stops.MONTH(date)) MODE(pit_stops.WEEKDAY(date)) MODE(pit_stops.YEAR(date)) NUM_UNIQUE(pit_stops.DAY(date)) NUM_UNIQUE(pit_stops.MONTH(date)) NUM_UNIQUE(pit_stops.WEEKDAY(date)) NUM_UNIQUE(pit_stops.YEAR(date)) MAX(driver_standings.position) MAX(driver_standings.win) MAX(driver_standings.year) MEAN(driver_standings.position) MEAN(driver_standings.win) MEAN(driver_standings.year) MIN(driver_standings.position) MIN(driver_standings.win) MIN(driver_standings.year) SKEW(driver_standings.position) SKEW(driver_standings.win) SKEW(driver_standings.year) STD(driver_standings.position) STD(driver_standings.win) STD(driver_standings.year) SUM(driver_standings.position) SUM(driver_standings.win) SUM(driver_standings.year) MAX(lap_times.lap) MAX(lap_times.milliseconds) MAX(lap_times.position) MAX(lap_times.year) MEAN(lap_times.lap) MEAN(lap_times.milliseconds) MEAN(lap_times.position) MEAN(lap_times.year) MIN(lap_times.lap) MIN(lap_times.milliseconds) MIN(lap_times.position) MIN(lap_times.year) SKEW(lap_times.lap) SKEW(lap_times.milliseconds) SKEW(lap_times.position) SKEW(lap_times.year) STD(lap_times.lap) STD(lap_times.milliseconds) STD(lap_times.position) STD(lap_times.year) SUM(lap_times.lap) SUM(lap_times.milliseconds) SUM(lap_times.position) SUM(lap_times.year) MAX(pit_stops.lap) MAX(pit_stops.milliseconds) MAX(pit_stops.year) MEAN(pit_stops.lap) MEAN(pit_stops.milliseconds) MEAN(pit_stops.year) MIN(pit_stops.lap) MIN(pit_stops.milliseconds) MIN(pit_stops.year) SKEW(pit_stops.lap) SKEW(pit_stops.milliseconds) SKEW(pit_stops.year) STD(pit_stops.lap) STD(pit_stops.milliseconds) STD(pit_stops.year) SUM(pit_stops.lap) SUM(pit_stops.milliseconds) SUM(pit_stops.year) role target categorical categorical categorical categorical   categorical             categorical      categorical      categorical          categorical                categorical categorical categorical   categorical categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical               categorical                 categorical                   categorical                categorical                     categorical                      categorical                      categorical                      categorical               categorical                 categorical                   categorical                categorical                     categorical                      categorical                      categorical                                           numerical                 numerical                  numerical                       numerical                  numerical                   numerical                      numerical                 numerical                  numerical                       numerical                  numerical                   numerical                      numerical                 numerical                  numerical                      numerical                 numerical                  numerical          numerical                   numerical               numerical           numerical           numerical                    numerical                numerical            numerical          numerical                   numerical               numerical           numerical           numerical                    numerical                numerical            numerical          numerical                   numerical               numerical           numerical          numerical                   numerical               numerical           numerical          numerical                   numerical           numerical           numerical                    numerical            numerical          numerical                   numerical           numerical           numerical                    numerical            numerical          numerical                   numerical           numerical          numerical                   numerical           numerical 0 0 12 5 Spanish alonso 105 5769 0 NULL NULL 16 3 6 2008 1 7 6 2005 31 8 1 6 24 7 6 2006 31 8 1 6 NULL NULL NULL NULL NULL NULL NULL NULL 23 1 2007 5.6667 0.181 2004.3714 1 0 2001 1.7319 1.6816 -0.3952 7.1598 0.3868 1.9575 595 19 210459 78 1406432 21 2007 30.436 88439.8289 5.2479 2004.5883 1 70526 1 2001 0.2301 34.9402 1.3932 -0.4971 18.3774 22633.3599 4.9145 1.8675 175585 510209373 30275 11564470 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 1 0 18 14 French bourdais 0 0 0 NULL NULL 16 3 6 2008 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 2 0 16 7 German rosberg 36 1685 0 NULL NULL 23 3 6 2008 8 7 6 2006 25 8 1 3 16 5 6 2007 25 8 1 3 NULL NULL NULL NULL NULL NULL NULL NULL 17 0 2008 11.7222 0 2006.5278 3 0 2006 -0.1958 0 0.4024 3.4691 0 0.5599 422 0 72235 77 191891 22 2008 30.381 90110.451 9.7822 2006.6415 1 73159 2 2006 0.2179 1.8323 0.5148 0.05473 18.5504 14476.8352 3.8237 0.5468 51192 151836110 16483 3381191 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 3 0 20 18 Japanese sato 84 4309 0 NULL NULL 23 3 6 2008 12 7 6 2004 31 8 1 6 25 7 6 2007 31 8 1 7 NULL NULL NULL NULL NULL NULL NULL NULL 23 0 2007 16.9405 0 2004.8095 7 0 2002 -0.493 0 -0.3829 5.5479 0 1.7319 1423 0 168404 76 1921214 22 2008 29.1091 91287.2075 12.4342 2004.8867 1 70727 1 2002 0.2832 32.4719 -0.2034 -0.3841 18.0839 42719.4804 4.861 1.7193 125431 393356577 53579 8639057 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 4 0 15 14 French bourdais 2 55 0 NULL NULL 6 4 6 2008 16 3 6 2008 2 1 1 1 16 3 6 2008 1 1 1 1 NULL NULL NULL NULL NULL NULL NULL NULL 10 0 2008 8.5 0 2008 7 0 2008 nan nan nan 2.1213 0 0 17 0 4016 55 156156 16 2008 28 98864.3818 9.7818 2008 1 89534 4 2008 0 2.2604 -0.06348 0 16.0208 17491.8075 4.3149 0 1540 5437541 538 110440 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 6224 0 17 30 British jolyon_palmer 33 1488 55 1 4 3 9 6 2017 9 7 6 2016 25 9 1 2 9 7 6 2016 25 9 1 2 9 7 6 2016 21 9 1 2 20 0 2017 18.0303 0 2016.3636 11 0 2016 -1.7159 0 0.5943 1.8622 0 0.4885 595 0 66540 78 1223356 22 2017 29.7009 98587.2056 14.5195 2016.3542 1 68652 7 2016 0.2691 20.8455 0.3776 0.6105 18.6186 43789.6099 3.0523 0.4784 44195 146697762 21605 3000335 50 1088294 2017 20.8545 61560.7273 2016.2727 1 16851 2016 0.3181 5.1018 1.0495 13.5857 192932.4401 0.4495 1147 3385840 110895 6225 0 NULL NULL British resta 60 3284 127 1 4 3 9 6 2017 27 7 6 2012 27 9 1 4 27 7 6 2012 27 9 1 4 25 5 6 2011 27 9 1 4 23 0 2017 12.4333 0 2012.1667 8 0 2011 1.155 0 2.0461 3.2068 0 1.2097 746 0 120730 78 7503980 24 2017 30.3097 103049.4318 10.7728 2012.0539 1 76855 1 2011 0.1989 44.5439 0.1348 1.8215 18.0742 144158.4761 3.5107 1.0439 99537 338414334 35378 6607585 57 123124 2017 25.1496 24084.2598 2011.9528 1 14538 2011 0.3166 8.1337 1.2541 13.8456 9973.774 0.9416 3194 3058701 255518 6226 0 6 77 Finnish bottas 91 5153 183 1 5 17 9 6 2017 9 7 6 2016 31 9 1 5 9 7 6 2016 31 9 1 5 9 7 6 2016 31 9 1 5 18 1 2017 8.1868 0.02198 2014.8901 3 0 2013 1.0124 6.6307 0.04113 4.9976 0.1474 1.3618 745 2 183355 78 3689718 22 2017 30.4306 99678.869 8.0584 2014.8894 1 67847 1 2013 0.2271 32.1149 0.5698 0.05111 18.18 74208.5105 4.885 1.3584 156809 513645212 41525 10382725 59 2008595 2017 24.1093 68069.5082 2014.8087 1 14978 2013 0.3284 5.9016 0.07557 13.4151 242520.4324 1.3349 4412 12456720 368710 6227 0 10 55 Spanish sainz 53 2690 97 1 4 17 9 6 2017 3 7 6 2016 29 9 1 3 30 5 6 2016 29 9 1 3 9 7 6 2016 29 9 1 3 16 0 2017 11.8679 0 2015.8868 7 0 2015 0.06274 0 0.2015 2.481 0 0.7761 629 0 106842 78 2122068 21 2017 29.7457 99861.4349 10.655 2015.8587 1 69150 3 2015 0.3078 24.5072 0.3527 0.2326 18.3133 61998.6099 3.5636 0.7425 80016 268627260 28662 5422660 51 2008828 2017 22.6289 84961.6907 2015.8247 1 14966 2015 0.3388 5.1188 0.2777 12.7919 297744.5008 0.7218 2195 8241284 195535 6228 0 13 26 Russian kvyat 72 3585 145 1 5 17 9 6 2017 9 7 6 2016 31 9 1 4 30 7 6 2015 31 9 1 4 9 7 6 2016 31 9 1 4 22 0 2017 12.6111 0 2015.3889 7 0 2014 -0.3133 0 0.08466 3.4214 0 1.0688 908 0 145108 78 3690660 22 2017 29.2739 100722.5623 11.2388 2015.3819 1 68061 2 2014 0.2624 33.405 -0.07759 0.126 18.0505 79720.0053 4.0792 1.0596 104947 361090386 40291 7225144 59 2011266 2017 23.6207 64171.0069 2015.3586 1 16139 2014 0.2769 6.5459 0.07908 15.0675 236423.1748 1.0651 3425 9304796 292227 <p>     6229 rows x 98 columns     memory usage: 3.96 MB     name: featuretools_test     type: getml.DataFrame </p> <p>We train an untuned XGBoostRegressor on top of featuretools' features, just like we have done for getML's features.</p> <p>Since some of featuretools features are categorical, we allow the pipeline to include these features as well. Other features contain NaN values, which is why we also apply getML's Imputation preprocessor.</p> In\u00a0[46]: Copied! <pre>imputation = getml.preprocessors.Imputation()\n\npredictor = getml.predictors.XGBoostClassifier(n_jobs=1)\n\npipe2 = getml.pipeline.Pipeline(\n    tags=['featuretools'],\n    preprocessors=[imputation],\n    predictors=[predictor],\n    include_categorical=True,\n)\n\npipe2\n</pre> imputation = getml.preprocessors.Imputation()  predictor = getml.predictors.XGBoostClassifier(n_jobs=1)  pipe2 = getml.pipeline.Pipeline(     tags=['featuretools'],     preprocessors=[imputation],     predictors=[predictor],     include_categorical=True, )  pipe2 Out[46]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Imputation'],\n         share_selected_features=0.5,\n         tags=['featuretools'])</pre> In\u00a0[47]: Copied! <pre>pipe2.fit(featuretools_train)\n</pre> pipe2.fit(featuretools_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 1 issues labeled INFO and 16 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:05, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:5.21325\n\n</pre> Out[47]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Imputation'],\n         share_selected_features=0.5,\n         tags=['featuretools'])</pre> In\u00a0[48]: Copied! <pre>featuretools_score = pipe2.score(featuretools_test)\nfeaturetools_score\n</pre> featuretools_score = pipe2.score(featuretools_test) featuretools_score <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[48]: date time           set used           target accuracy     auc cross entropy 0 2024-02-21 15:04:08 featuretools_train win 0.9717 0.9389 0.08364 1 2024-02-21 15:04:08 featuretools_test win 0.9722 0.9153 0.08797 In\u00a0[49]: Copied! <pre>names, importances = pipe1.features.importances(target_num=0)\n\nplt.subplots(figsize=(20, 10))\n\nplt.bar(names[:30], importances[:30])\n\nplt.title(\"feature importances\")\nplt.grid(True)\nplt.xlabel(\"column\")\nplt.ylabel(\"importance\")\nplt.xticks(rotation='vertical')\n\nplt.show()\n</pre> names, importances = pipe1.features.importances(target_num=0)  plt.subplots(figsize=(20, 10))  plt.bar(names[:30], importances[:30])  plt.title(\"feature importances\") plt.grid(True) plt.xlabel(\"column\") plt.ylabel(\"importance\") plt.xticks(rotation='vertical')  plt.show() <p>We take a look at the most important features, to get an idea where the predictive power comes from:</p> In\u00a0[50]: Copied! <pre>pipe1.features.to_sql().find(names[0])[0]\n</pre> pipe1.features.to_sql().find(names[0])[0] Out[50]: <pre>DROP TABLE IF EXISTS \"T4__DRIVERREF__MAPPING_TARGET_1_AVG\";\n\nCREATE TABLE \"T4__DRIVERREF__MAPPING_TARGET_1_AVG\"(\"key\" TEXT, \"value\" REAL);\n\nINSERT INTO \"T4__DRIVERREF__MAPPING_TARGET_1_AVG\" (\"key\", \"value\")\nVALUES('fangio', 0.3653846153846154),\n      ('ascari', 0.3055555555555556),\n      ('michael_schumacher', 0.3032786885245902),\n      ('hamilton', 0.2981366459627329),\n      ('stewart', 0.2948717948717949),\n      ('prost', 0.264367816091954),\n      ('vettel', 0.2467532467532468),\n      ('clark', 0.2394366197183098),\n      ('senna', 0.2323943661971831),\n      ('damon_hill', 0.2159090909090909),\n      ('moss', 0.1866666666666667),\n      ('mansell', 0.1569767441860465),\n      ('lauda', 0.1390728476821192),\n      ('alonso', 0.1282051282051282),\n      ('hakkinen', 0.1278195488721804),\n      ('rosberg', 0.10625),\n      ('hunt', 0.1058823529411765),\n      ('jack_brabham', 0.1052631578947368),\n      ('piquet', 0.09714285714285714),\n      ('farina', 0.09523809523809523),\n      ('jones', 0.09259259259259259),\n      ('brooks', 0.09090909090909091),\n      ('scheckter', 0.09),\n      ('montoya', 0.08536585365853659),\n      ('emerson_fittipaldi', 0.08403361344537816),\n      ('rindt', 0.08333333333333333),\n      ('hulme', 0.08247422680412371),\n      ('peterson', 0.08247422680412371),\n      ('reutemann', 0.07936507936507936),\n      ('mario_andretti', 0.07142857142857142),\n      ('raikkonen', 0.0684931506849315),\n      ('villeneuve', 0.06153846153846154),\n      ('surtees', 0.0576923076923077),\n      ('hill', 0.05194805194805195),\n      ('gilles_villeneuve', 0.05),\n      ('berger', 0.05),\n      ('keke_rosberg', 0.04901960784313725),\n      ('hawthorn', 0.04878048780487805),\n      ('ickx', 0.04878048780487805),\n      ('gurney', 0.04819277108433735),\n      ('collins', 0.04761904761904762),\n      ('arnoux', 0.04697986577181208),\n      ('massa', 0.04587155963302753),\n      ('button', 0.04511278195488722),\n      ('pironi', 0.04347826086956522),\n      ('coulthard', 0.0427807486631016),\n      ('revson', 0.0425531914893617),\n      ('ricciardo', 0.0425531914893617),\n      ('webber', 0.04216867469879518),\n      ('ralf_schumacher', 0.03846153846153846),\n      ('barrichello', 0.036),\n      ('phil_hill', 0.03448275862068965),\n      ('rodriguez', 0.03333333333333333),\n      ('scarfiotti', 0.03333333333333333),\n      ('laffite', 0.03311258278145696),\n      ('regazzoni', 0.03225806451612903),\n      ('jabouille', 0.03076923076923077),\n      ('baghetti', 0.02941176470588235),\n      ('flaherty', 0.02941176470588235),\n      ('watson', 0.02898550724637681),\n      ('mclaren', 0.02803738317757009),\n      ('gethin', 0.02777777777777778),\n      ('musso', 0.02702702702702703),\n      ('cevert', 0.02631578947368421),\n      ('ruttman', 0.025),\n      ('irvine', 0.025),\n      ('max_verstappen', 0.02380952380952381),\n      ('hanks', 0.02325581395348837),\n      ('alboreto', 0.02298850574712644),\n      ('boutsen', 0.02290076335877863),\n      ('herbert', 0.02222222222222222),\n      ('parsons', 0.02083333333333333),\n      ('bryan', 0.02040816326530612),\n      ('trintignant', 0.02),\n      ('kubica', 0.0196078431372549),\n      ('patrese', 0.01951219512195122),\n      ('rathmann', 0.01923076923076923),\n      ('bandini', 0.01851851851851852),\n      ('tambay', 0.01834862385321101),\n      ('ginther', 0.01639344262295082),\n      ('fisichella', 0.01621621621621622),\n      ('ward', 0.01612903225806452),\n      ('frentzen', 0.01574803149606299),\n      ('brambilla', 0.01492537313432836),\n      ('bottas', 0.01470588235294118),\n      ('pace', 0.0136986301369863),\n      ('maldonado', 0.01298701298701299),\n      ('beltoise', 0.01282051282051282),\n      ('siffert', 0.01265822784810127),\n      ('depailler', 0.0108695652173913),\n      ('angelis', 0.0108695652173913),\n      ('bonnier', 0.009009009009009009),\n      ('mass', 0.008771929824561403),\n      ('panis', 0.007692307692307693),\n      ('darter', 0),\n      ('pretorius', 0),\n      ('adamich', 0),\n      ('trevor_taylor', 0),\n      ('love', 0),\n      ('anderson', 0),\n      ('bianchi', 0),\n      ('courage', 0),\n      ('tingle', 0),\n      ('attwood', 0),\n      ('spence', 0),\n      ('resta', 0),\n      ('chiron', 0),\n      ('reece', 0),\n      ('linden', 0),\n      ('agabashian', 0),\n      ('manzon', 0),\n      ('rosier', 0),\n      ('villoresi', 0),\n      ('graffenried', 0),\n      ('hulkenberg', 0),\n      ('petrov', 0),\n      ('bruno_senna', 0),\n      ('daywalt', 0),\n      ('perez', 0),\n      ('vergne', 0),\n      ('pic', 0),\n      ('chilton', 0),\n      ('gutierrez', 0),\n      ('jules_bianchi', 0),\n      ('kevin_magnussen', 0),\n      ('kvyat', 0),\n      ('ericsson', 0),\n      ('nasr', 0),\n      ('sainz', 0),\n      ('schell', 0),\n      ('maggs', 0),\n      ('gregory', 0),\n      ('andre_pilette', 0),\n      ('beaufort', 0),\n      ('burgess', 0),\n      ('salvadori', 0),\n      ('trips', 0),\n      ('herrmann', 0),\n      ('scarlatti', 0),\n      ('menditeguy', 0),\n      ('gonzalez', 0),\n      ('ireland', 0),\n      ('thomson', 0),\n      ('johnson', 0),\n      ('ertl', 0),\n      ('hartley', 0),\n      ('stevenson', 0),\n      ('freeland', 0),\n      ('bettenhausen', 0),\n      ('boyd', 0),\n      ('gould', 0),\n      ('behra', 0),\n      ('paul_russo', 0),\n      ('martini', 0),\n      ('larini', 0),\n      ('katayama', 0),\n      ('morbidelli', 0),\n      ('lamy', 0),\n      ('brundle', 0),\n      ('montermini', 0),\n      ('blundell', 0),\n      ('suzuki', 0),\n      ('moreno', 0),\n      ('wendlinger', 0),\n      ('gachot', 0),\n      ('magnussen', 0),\n      ('tarquini', 0),\n      ('comas', 0),\n      ('bernard', 0),\n      ('fittipaldi', 0),\n      ('lehto', 0),\n      ('cesaris', 0),\n      ('alliot', 0),\n      ('dalmas', 0),\n      ('warwick', 0),\n      ('capelli', 0),\n      ('gugelmin', 0),\n      ('rosa', 0),\n      ('heidfeld', 0),\n      ('kovalainen', 0),\n      ('glock', 0),\n      ('sato', 0),\n      ('trulli', 0),\n      ('sutil', 0),\n      ('liuzzi', 0),\n      ('wurz', 0),\n      ('speed', 0),\n      ('albers', 0),\n      ('klien', 0),\n      ('grouillard', 0),\n      ('karthikeyan', 0),\n      ('zonta', 0),\n      ('gene', 0),\n      ('verstappen', 0),\n      ('alesi', 0),\n      ('salo', 0),\n      ('diniz', 0),\n      ('buemi', 0),\n      ('badoer', 0),\n      ('zanardi', 0),\n      ('hoffmann', 0),\n      ('henton', 0),\n      ('daly', 0),\n      ('villota', 0),\n      ('keegan', 0),\n      ('rebaque', 0),\n      ('merzario', 0),\n      ('stuck', 0),\n      ('lunger', 0),\n      ('stommelen', 0),\n      ('ian_scheckter', 0),\n      ('pryce', 0),\n      ('jarier', 0),\n      ('oliver', 0),\n      ('amon', 0),\n      ('pescarolo', 0),\n      ('wilson_fittipaldi', 0),\n      ('keizan', 0),\n      ('charlton', 0),\n      ('hailwood', 0),\n      ('ganley', 0),\n      ('redman', 0),\n      ('schenken', 0),\n      ('palmer', 0),\n      ('modena', 0),\n      ('caffi', 0),\n      ('lammers', 0),\n      ('satoru_nakajima', 0),\n      ('johansson', 0),\n      ('nannini', 0),\n      ('schneider', 0),\n      ('giacomelli', 0),\n      ('alguersuari', 0),\n      ('grosjean', 0),\n      ('kobayashi', 0),\n      ('wisell', 0),\n      ('danner', 0),\n      ('cheever', 0),\n      ('ghinzani', 0),\n      ('streiff', 0),\n      ('fabi', 0),\n      ('surer', 0),\n      ('manfred_winkelhock', 0),\n      ('baldi', 0),\n      ('serra', 0),\n      ('salazar', 0);\n\nALTER TABLE \"POPULATION__STAGING_TABLE_1\" ADD COLUMN \"t4__driverref__mapping_target_1_avg\" REAL;\n\nUPDATE \"POPULATION__STAGING_TABLE_1\" SET \"t4__driverref__mapping_target_1_avg\" = 0.0;\n\nUPDATE \"POPULATION__STAGING_TABLE_1\"\nSET \"t4__driverref__mapping_target_1_avg\" = \"T4__DRIVERREF__MAPPING_TARGET_1_AVG\".\"value\"\nFROM \"T4__DRIVERREF__MAPPING_TARGET_1_AVG\"\nWHERE \"POPULATION__STAGING_TABLE_1\".\"t4__driverref\" = \"T4__DRIVERREF__MAPPING_TARGET_1_AVG\".\"key\";\n\nDROP TABLE IF EXISTS \"T4__DRIVERREF__MAPPING_TARGET_1_AVG\";\n</pre> In\u00a0[51]: Copied! <pre>pipe1.features.to_sql()[names[1]]\n</pre> pipe1.features.to_sql()[names[1]] Out[51]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_78\";\n\nCREATE TABLE \"FEATURE_1_78\" AS\nSELECT EWMA_90D( t2.\"win\", t1.\"date\" - t2.\"date__1_000000_days\" ) AS \"feature_1_78\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"DRIVER_STANDINGS__STAGING_TABLE_2\" t2\nON t1.\"driverid\" = t2.\"driverid\"\nWHERE t2.\"date__1_000000_days\" &lt;= t1.\"date\"\nGROUP BY t1.rowid;\n</pre> In\u00a0[52]: Copied! <pre>pipe1.features.to_sql()[names[2]]\n</pre> pipe1.features.to_sql()[names[2]] Out[52]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_79\";\n\nCREATE TABLE \"FEATURE_1_79\" AS\nSELECT EWMA_365D( t2.\"win\", t1.\"date\" - t2.\"date__1_000000_days\" ) AS \"feature_1_79\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"DRIVER_STANDINGS__STAGING_TABLE_2\" t2\nON t1.\"driverid\" = t2.\"driverid\"\nWHERE t2.\"date__1_000000_days\" &lt;= t1.\"date\"\nGROUP BY t1.rowid;\n</pre> <p>What we can learn from these features is the following: Knowing which driver we are talking aboyt and who won the most recent races is the best predictor for whether a driver will win this race.</p> In\u00a0[53]: Copied! <pre>names, importances = pipe2.features.importances()\n\nplt.subplots(figsize=(20, 10))\n\nplt.bar(names[:30], importances[:30])\n\nplt.title(\"feature importances\")\nplt.grid(True)\nplt.xlabel(\"column\")\nplt.ylabel(\"importance\")\nplt.xticks(rotation='vertical')\n\nplt.show()\n</pre> names, importances = pipe2.features.importances()  plt.subplots(figsize=(20, 10))  plt.bar(names[:30], importances[:30])  plt.title(\"feature importances\") plt.grid(True) plt.xlabel(\"column\") plt.ylabel(\"importance\") plt.xticks(rotation='vertical')  plt.show() <p>As we can see, featuretools generally builds features that are similar to the features identified by getML's FastProp.</p> In\u00a0[54]: Copied! <pre># Creates a folder named formula1_pipeline containing\n# the SQL code.\npipe1.features.to_sql().save(\"formula1_pipeline\")\n</pre> # Creates a folder named formula1_pipeline containing # the SQL code. pipe1.features.to_sql().save(\"formula1_pipeline\") In\u00a0[55]: Copied! <pre>pipe1.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"formula1_spark\")\n</pre> pipe1.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"formula1_spark\") In\u00a0[56]: Copied! <pre>scores = [fastprop_score, featuretools_score]\n\npd.DataFrame(data={    'Name': ['getML: FastProp', 'featuretools'],\n    'Accuracy': [f'{score.accuracy:.2%}' for score in scores],\n    'AUC': [f'{score.auc:,.2%}' for score in scores],\n    'Cross entropy': [f'{score.cross_entropy:,.5f}' for score in scores]\n})\n</pre> scores = [fastprop_score, featuretools_score]  pd.DataFrame(data={    'Name': ['getML: FastProp', 'featuretools'],     'Accuracy': [f'{score.accuracy:.2%}' for score in scores],     'AUC': [f'{score.auc:,.2%}' for score in scores],     'Cross entropy': [f'{score.cross_entropy:,.5f}' for score in scores] }) Out[56]: Name Accuracy AUC Cross entropy 0 getML: FastProp 97.22% 92.30% 0.08549 1 featuretools 97.22% 91.53% 0.08797 <p>As we can see, getML's FastProp outperforms featuretools according to all three measures.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/formula1/#formula-1-predicting-the-winner-of-a-race","title":"Formula 1 - Predicting the winner of a race\u00b6","text":"<p>In this notebook we will benchmark getML against featuretools to predict the winner of a Formula 1 race.</p> <p>Summary:</p> <ul> <li>Prediction type: Classification model</li> <li>Domain: Sports</li> <li>Prediction target: Win</li> <li>Population size: 31578</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/formula1/#background","title":"Background\u00b6","text":"<p>We would like to develop a prediction model for Formula 1 races, that would allow us to predict the winner of a race before the race has started.</p> <p>We use dataset of all Formula 1 races from 1950 to 2017. The dataset includes information such as the time taken in each lap, the time taken for pit stops, the performance in the qualifying rounds etc.</p> <p>The dataset has been downloaded from the CTU Prague relational learning repository (Motl and Schulte, 2015) (Now residing at relational-data.org.).</p> <p>We will benchmark getML's feature learning algorithms against featuretools, an open-source implementation of the propositionalization algorithm, similar to getML's FastProp.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/formula1/#analysis","title":"Analysis\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/formula1/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/formula1/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>We begin by downloading the data:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/formula1/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/formula1/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"<p>We loaded the data and defined the roles and units. Next, we create a getML pipeline for relational learning.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/formula1/#21-define-relational-model","title":"2.1 Define relational model\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/formula1/#22-getml-pipeline","title":"2.2 getML pipeline\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/formula1/#23-model-training","title":"2.3 Model training\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/formula1/#24-model-evaluation","title":"2.4 Model evaluation\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/formula1/#25-featuretools","title":"2.5 featuretools\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/formula1/#26-studying-features","title":"2.6 Studying features\u00b6","text":"<p>We would like to understand why getML outperforms featuretools. In particular, getML's FastProp is based on an approach that is very similar to featuretools.</p> <p>To investigate this matter, we first take a look at the importance of the features FastProp has learned:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/formula1/#27-productionization","title":"2.7 Productionization\u00b6","text":"<p>It is possible to productionize the pipeline by transpiling the features into production-ready SQL code. Please also refer to getML's <code>sqlite3</code> and <code>spark</code> modules.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/formula1/#28-discussion","title":"2.8 Discussion\u00b6","text":"<p>For a more convenient overview, we summarize our results into a table.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/formula1/#3-conclusion","title":"3. Conclusion\u00b6","text":"<p>We have benchmarked getML against featuretools on dataset related to Formula 1 races. We have found that getML's FastProp outperforms featuretools.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/formula1/#references","title":"References\u00b6","text":"<p>Motl, Jan, and Oliver Schulte. \"The CTU prague relational learning repository.\" arXiv preprint arXiv:1511.03086 (2015).</p>","boost":0.8},{"location":"examples/enterprise-notebooks/imdb/","title":"<span class=\"ntitle\">imdb.ipynb</span> <span class=\"ndesc\">Predicting actors' gender</span>","text":"<p>Let's get started with the analysis and set up your session:</p> In\u00a0[1]: Copied! <pre>import copy\nimport os\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\nfrom pathlib import Path\n\nfrom urllib import request\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline  \n\nimport getml\nfrom pyspark.sql import SparkSession\n\ngetml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.engine.set_project('imdb')\n</pre> import copy import os os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\" from pathlib import Path  from urllib import request  import numpy as np import pandas as pd from IPython.display import Image import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline    import getml from pyspark.sql import SparkSession  getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.engine.set_project('imdb') <pre>getML engine is already running.\n\nConnected to project 'imdb'\n</pre> <p> In the following, we set some flags that affect execution of the notebook:</p> <ul> <li>We don't let the algorithms utilize the information on actors' first names (see below for an explanation).</li> </ul> In\u00a0[2]: Copied! <pre>USE_FIRST_NAMES = False\nRUN_SPARK = False\n</pre> USE_FIRST_NAMES = False RUN_SPARK = False In\u00a0[3]: Copied! <pre>conn = getml.database.connect_mysql(\n    host=\"db.relational-data.org\",\n    dbname=\"imdb_ijs\",\n    port=3306,\n    user=\"guest\",\n    password=\"relational\"\n)\n\nconn\n</pre> conn = getml.database.connect_mysql(     host=\"db.relational-data.org\",     dbname=\"imdb_ijs\",     port=3306,     user=\"guest\",     password=\"relational\" )  conn Out[3]: <pre>Connection(dbname='imdb_ijs',\n           dialect='mysql',\n           host='db.relational-data.org',\n           port=3306)</pre> In\u00a0[4]: Copied! <pre>def load_if_needed(name):\n    \"\"\"\n    Loads the data from the relational learning\n    repository, if the data frame has not already\n    been loaded.\n    \"\"\"\n    if getml.data.exists(name):\n        return getml.data.load_data_frame(name)\n    data_frame = getml.data.DataFrame.from_db(\n        name=name,\n        table_name=name,\n        conn=conn\n    )\n    data_frame.save()\n    return data_frame\n</pre> def load_if_needed(name):     \"\"\"     Loads the data from the relational learning     repository, if the data frame has not already     been loaded.     \"\"\"     if getml.data.exists(name):         return getml.data.load_data_frame(name)     data_frame = getml.data.DataFrame.from_db(         name=name,         table_name=name,         conn=conn     )     data_frame.save()     return data_frame In\u00a0[5]: Copied! <pre>actors = load_if_needed(\"actors\")\nroles = load_if_needed(\"roles\")\nmovies = load_if_needed(\"movies\")\nmovies_genres = load_if_needed(\"movies_genres\")\n</pre> actors = load_if_needed(\"actors\") roles = load_if_needed(\"roles\") movies = load_if_needed(\"movies\") movies_genres = load_if_needed(\"movies_genres\") In\u00a0[6]: Copied! <pre>actors\n</pre> actors Out[6]:   name           id first_name    last_name          gender          role unused_float unused_string unused_string      unused_string 0 2 Michael 'babeepower' Viera M 1 3 Eloy 'Chincheta' M 2 4 Dieguito 'El Cigala' M 3 5 Antonio 'El de Chipiona' M 4 6 Jos\u00e9 'El Franc\u00e9s' M ... ... ... ... 817713 845461 Herd\u00eds \u00deorvaldsd\u00f3ttir F 817714 845462 Katla Margr\u00e9t \u00deorvaldsd\u00f3ttir F 817715 845463 Lilja N\u00f3tt \u00de\u00f3rarinsd\u00f3ttir F 817716 845464 H\u00f3lmfr\u00ed\u00f0ur \u00de\u00f3rhallsd\u00f3ttir F 817717 845465 The\u00f3d\u00f3ra \u00de\u00f3r\u00f0ard\u00f3ttir F <p>     817718 rows x 4 columns     memory usage: 40.22 MB     name: actors     type: getml.DataFrame </p> In\u00a0[7]: Copied! <pre>roles\n</pre> roles Out[7]:    name     actor_id     movie_id role                role unused_float unused_float unused_string    0 2 280088 Stevie 1 2 396232 Various/lyricist 2 3 376687 Gitano 1 3 4 336265 El Cigala 4 5 135644 Himself ... ... ... 3431961 845461 137097 Kata 3431962 845462 208838 Magga 3431963 845463 870 Gunna 3431964 845464 378123 Gudrun 3431965 845465 378123 NULL <p>     3431966 rows x 3 columns     memory usage: 115.41 MB     name: roles     type: getml.DataFrame </p> In\u00a0[8]: Copied! <pre>movies\n</pre> movies Out[8]:   name           id         year         rank name                               role unused_float unused_float unused_float unused_string                    0 0 2002 nan #28 1 1 2000 nan #7 Train: An Immigrant Journey, ... 2 2 1971 6.4 $ 3 3 1913 nan $1,000 Reward 4 4 1915 nan $1,000 Reward ... ... ... ... 388264 412316 1991 nan \"zem blch krlu\" 388265 412317 1995 nan \"rgammk\" 388266 412318 2002 nan \"zgnm Leyla\" 388267 412319 1983 nan \" Istanbul\" 388268 412320 1958 nan \"sterreich\" <p>     388269 rows x 4 columns     memory usage: 19.92 MB     name: movies     type: getml.DataFrame </p> In\u00a0[9]: Copied! <pre>movies_genres\n</pre> movies_genres Out[9]:   name     movie_id genre           role unused_float unused_string 0 1 Documentary 1 1 Short 2 2 Comedy 3 2 Crime 4 5 Western ... ... 395114 378612 Adventure 395115 378612 Drama 395116 378613 Comedy 395117 378613 Drama 395118 378614 Comedy <p>     395119 rows x 2 columns     memory usage: 9.24 MB     name: movies_genres     type: getml.DataFrame </p> <p>getML requires that we define roles for each of the columns.</p> In\u00a0[10]: Copied! <pre>actors[\"target\"] = (actors.gender == 'F')\nactors.set_role(\"id\", getml.data.roles.join_key)\nactors.set_role(\"target\", getml.data.roles.target)\n</pre> actors[\"target\"] = (actors.gender == 'F') actors.set_role(\"id\", getml.data.roles.join_key) actors.set_role(\"target\", getml.data.roles.target) <p> The benchmark studies do not state clearly, whether it is fair game to use the first names of the actors. Using the first names, we can easily increase the predictive accuracy to above 90%. However, when doing so the problem basically becomes a first name identification problem rather than a relational learning problem. This would undermine the point of this notebook: Showcase relational learning. Therefore, our assumption is that using the first names is not allowed. Feel free to set this flag above to see how well getML incoporates such starightforward information into its feature logic.</p> In\u00a0[11]: Copied! <pre>if USE_FIRST_NAMES:\n    actors.set_role(\"first_name\", getml.data.roles.text)\nactors\n</pre> if USE_FIRST_NAMES:     actors.set_role(\"first_name\", getml.data.roles.text) actors Out[11]:   name       id target first_name    last_name          gender          role join_key target unused_string unused_string      unused_string 0 2 0 Michael 'babeepower' Viera M 1 3 0 Eloy 'Chincheta' M 2 4 0 Dieguito 'El Cigala' M 3 5 0 Antonio 'El de Chipiona' M 4 6 0 Jos\u00e9 'El Franc\u00e9s' M ... ... ... ... ... 817713 845461 1 Herd\u00eds \u00deorvaldsd\u00f3ttir F 817714 845462 1 Katla Margr\u00e9t \u00deorvaldsd\u00f3ttir F 817715 845463 1 Lilja N\u00f3tt \u00de\u00f3rarinsd\u00f3ttir F 817716 845464 1 H\u00f3lmfr\u00ed\u00f0ur \u00de\u00f3rhallsd\u00f3ttir F 817717 845465 1 The\u00f3d\u00f3ra \u00de\u00f3r\u00f0ard\u00f3ttir F <p>     817718 rows x 5 columns     memory usage: 43.49 MB     name: actors     type: getml.DataFrame </p> In\u00a0[12]: Copied! <pre>roles.set_role([\"actor_id\", \"movie_id\"], getml.data.roles.join_key)\nroles.set_role(\"role\", getml.data.roles.text)\nroles\n</pre> roles.set_role([\"actor_id\", \"movie_id\"], getml.data.roles.join_key) roles.set_role(\"role\", getml.data.roles.text) roles Out[12]:    name actor_id movie_id role                role join_key join_key text             0 2 280088 Stevie 1 2 396232 Various/lyricist 2 3 376687 Gitano 1 3 4 336265 El Cigala 4 5 135644 Himself ... ... ... 3431961 845461 137097 Kata 3431962 845462 208838 Magga 3431963 845463 870 Gunna 3431964 845464 378123 Gudrun 3431965 845465 378123 NULL <p>     3431966 rows x 3 columns     memory usage: 87.96 MB     name: roles     type: getml.DataFrame </p> In\u00a0[13]: Copied! <pre>movies.set_role(\"id\", getml.data.roles.join_key)\nmovies.set_role([\"year\", \"rank\"], getml.data.roles.numerical)\nmovies\n</pre> movies.set_role(\"id\", getml.data.roles.join_key) movies.set_role([\"year\", \"rank\"], getml.data.roles.numerical) movies Out[13]:   name       id      year      rank name                               role join_key numerical numerical unused_string                    0 0 2002 nan #28 1 1 2000 nan #7 Train: An Immigrant Journey, ... 2 2 1971 6.4 $ 3 3 1913 nan $1,000 Reward 4 4 1915 nan $1,000 Reward ... ... ... ... 388264 412316 1991 nan \"zem blch krlu\" 388265 412317 1995 nan \"rgammk\" 388266 412318 2002 nan \"zgnm Leyla\" 388267 412319 1983 nan \" Istanbul\" 388268 412320 1958 nan \"sterreich\" <p>     388269 rows x 4 columns     memory usage: 18.37 MB     name: movies     type: getml.DataFrame </p> In\u00a0[14]: Copied! <pre>movies_genres.set_role(\"movie_id\", getml.data.roles.join_key)\nmovies_genres.set_role(\"genre\", getml.data.roles.categorical)\nmovies_genres\n</pre> movies_genres.set_role(\"movie_id\", getml.data.roles.join_key) movies_genres.set_role(\"genre\", getml.data.roles.categorical) movies_genres Out[14]:   name movie_id genre         role join_key categorical 0 1 Documentary 1 1 Short 2 2 Comedy 3 2 Crime 4 5 Western ... ... 395114 378612 Adventure 395115 378612 Drama 395116 378613 Comedy 395117 378613 Drama 395118 378614 Comedy <p>     395119 rows x 2 columns     memory usage: 3.16 MB     name: movies_genres     type: getml.DataFrame </p> <p>We need to separate our data set into a training, testing and validation set:</p> In\u00a0[15]: Copied! <pre>split = getml.data.split.random(train=0.7, validation=0.15, test=0.15)\nsplit\n</pre> split = getml.data.split.random(train=0.7, validation=0.15, test=0.15) split Out[15]: 0 train 1 validation 2 train 3 validation 4 validation ... <p>     infinite number of  rows          type: StringColumnView </p> In\u00a0[16]: Copied! <pre>container = getml.data.Container(population=actors, split=split)\n\ncontainer.add(\n    roles=roles,\n    movies=movies,\n    movies_genres=movies_genres,\n)\n\ncontainer\n</pre> container = getml.data.Container(population=actors, split=split)  container.add(     roles=roles,     movies=movies,     movies_genres=movies_genres, )  container Out[16]: population subset     name     rows type 0 test actors 122794 View 1 train actors 571807 View 2 validation actors 123117 View peripheral name             rows type      0 roles 3431966 DataFrame 1 movies 388269 DataFrame 2 movies_genres 395119 DataFrame In\u00a0[17]: Copied! <pre>dm = getml.data.DataModel(\"actors\")\n\ndm.add(getml.data.to_placeholder(\n    roles=roles,\n    movies=movies,\n    movies_genres=movies_genres,\n))\n\ndm.population.join(\n    dm.roles,\n    on=(\"id\", \"actor_id\"),\n)\n\ndm.roles.join(\n    dm.movies,\n    on=(\"movie_id\", \"id\"),\n    relationship=getml.data.relationship.many_to_one,\n)\n\ndm.movies.join(\n    dm.movies_genres,\n    on=(\"id\", \"movie_id\"),\n)\n\ndm\n</pre> dm = getml.data.DataModel(\"actors\")  dm.add(getml.data.to_placeholder(     roles=roles,     movies=movies,     movies_genres=movies_genres, ))  dm.population.join(     dm.roles,     on=(\"id\", \"actor_id\"), )  dm.roles.join(     dm.movies,     on=(\"movie_id\", \"id\"),     relationship=getml.data.relationship.many_to_one, )  dm.movies.join(     dm.movies_genres,     on=(\"id\", \"movie_id\"), )  dm Out[17]: diagram movies_genresmoviesrolesactorsmovie_id = idid = movie_idRelationship: many-to-oneactor_id = id staging data frames   staging table                  0 actors ACTORS__STAGING_TABLE_1 1 movies_genres MOVIES_GENRES__STAGING_TABLE_2 2 roles, movies ROLES__STAGING_TABLE_3 <p>Set-up the feature learner &amp; predictor</p> <p>We can either use the relboost default parameters or some more fine-tuned parameters. Fine-tuning these parameters in this way can increase our predictive accuracy to 85%, but the training time increases to over 4 hours. We therefore assume that we want to use the default parameters.</p> In\u00a0[18]: Copied! <pre>text_field_splitter = getml.preprocessors.TextFieldSplitter()\n\nmapping = getml.preprocessors.Mapping()\n\nfast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n)\n\nfeature_selector = getml.predictors.XGBoostClassifier()\n\npredictor = getml.predictors.XGBoostClassifier()\n</pre> text_field_splitter = getml.preprocessors.TextFieldSplitter()  mapping = getml.preprocessors.Mapping()  fast_prop = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss, )  feature_selector = getml.predictors.XGBoostClassifier()  predictor = getml.predictors.XGBoostClassifier() <p>Build the pipeline</p> In\u00a0[19]: Copied! <pre>pipe = getml.pipeline.Pipeline(\n    tags=['fast_prop'],\n    data_model=dm,\n    preprocessors=[text_field_splitter, mapping],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=[predictor],\n    share_selected_features=0.1,\n)\n</pre> pipe = getml.pipeline.Pipeline(     tags=['fast_prop'],     data_model=dm,     preprocessors=[text_field_splitter, mapping],     feature_learners=[fast_prop],     feature_selectors=[feature_selector],     predictors=[predictor],     share_selected_features=0.1, ) In\u00a0[20]: Copied! <pre>pipe.check(container.train)\n</pre> pipe.check(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:22, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\nThe pipeline check generated 1 issues labeled INFO and 0 issues labeled WARNING.\n</pre> Out[20]: type label                  message                          0 INFO FOREIGN KEYS NOT FOUND When joining ROLES__STAGING_TABLE_3 and MOVIES_GENRES__STAGING_TABLE_2 over 'id' and 'movie_id', there are no corresponding entries for 26.899421% of entries in 'id' in 'ROLES__STAGING_TABLE_3'. You might want to double-check your join keys. In\u00a0[21]: Copied! <pre>pipe.fit(container.train)\n</pre> pipe.fit(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:07, remaining: 00:00]          \n\nThe pipeline check generated 1 issues labeled INFO and 0 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:06, remaining: 00:00]          \nIndexing text fields... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:05, remaining: 00:00]          \nFastProp: Trying 226 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:20, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:03, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:20, remaining: 00:00]          \nXGBoost: Training as feature selector... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 04:60, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:43, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:6m:42.850666\n\n</pre> Out[21]: <pre>Pipeline(data_model='actors',\n         feature_learners=['FastProp'],\n         feature_selectors=['XGBoostClassifier'],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['movies', 'movies_genres', 'roles'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['TextFieldSplitter', 'Mapping'],\n         share_selected_features=0.1,\n         tags=['fast_prop', 'container-kiwgkg'])</pre> In\u00a0[22]: Copied! <pre>pipe.score(container.test)\n</pre> pipe.score(container.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:07, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \nFastProp: Building features... 113% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[22]: date time           set used target accuracy     auc cross entropy 0 2024-02-21 15:07:05 train target 0.8417 0.9139 0.3213 1 2024-02-21 15:07:19 test target 0.842 0.9139 0.3225 In\u00a0[23]: Copied! <pre>pipe.features.to_sql()[pipe.features.sort(by=\"importances\")[0].name]\n</pre> pipe.features.to_sql()[pipe.features.sort(by=\"importances\")[0].name] Out[23]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_114\";\n\nCREATE TABLE \"FEATURE_1_114\" AS\nSELECT AVG( COALESCE( f_1_1_13.\"feature_1_1_13\", 0.0 ) ) AS \"feature_1_114\",\n       t1.rowid AS rownum\nFROM \"ACTORS__STAGING_TABLE_1\" t1\nINNER JOIN \"ROLES__STAGING_TABLE_3\" t2\nON t1.\"id\" = t2.\"actor_id\"\nLEFT JOIN \"FEATURE_1_1_13\" f_1_1_13\nON t2.rowid = f_1_1_13.rownum\nGROUP BY t1.rowid;\n</pre> In\u00a0[24]: Copied! <pre>pipe.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"imdb_spark\")\n</pre> pipe.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"imdb_spark\") In\u00a0[25]: Copied! <pre>if RUN_SPARK:\n    spark = SparkSession.builder.appName(\n        \"online_retail\"\n    ).config(\n        \"spark.driver.maxResultSize\",\"10g\"\n    ).config(\n        \"spark.driver.memory\", \"10g\"\n    ).config(\n        \"spark.executor.memory\", \"20g\"\n    ).config(\n        \"spark.sql.execution.arrow.pyspark.enabled\", \"true\"\n    ).config(\n        \"spark.sql.session.timeZone\", \"UTC\"\n    ).enableHiveSupport().getOrCreate()\n\n    spark.sparkContext.setLogLevel(\"ERROR\")\n</pre> if RUN_SPARK:     spark = SparkSession.builder.appName(         \"online_retail\"     ).config(         \"spark.driver.maxResultSize\",\"10g\"     ).config(         \"spark.driver.memory\", \"10g\"     ).config(         \"spark.executor.memory\", \"20g\"     ).config(         \"spark.sql.execution.arrow.pyspark.enabled\", \"true\"     ).config(         \"spark.sql.session.timeZone\", \"UTC\"     ).enableHiveSupport().getOrCreate()      spark.sparkContext.setLogLevel(\"ERROR\") In\u00a0[26]: Copied! <pre>if RUN_SPARK:\n    population_spark = container.train.population.to_pyspark(spark, name=\"actors\")\n</pre> if RUN_SPARK:     population_spark = container.train.population.to_pyspark(spark, name=\"actors\") In\u00a0[27]: Copied! <pre>if RUN_SPARK:\n    movies_genres_spark = container.movies_genres.to_pyspark(spark, name=\"movies_genres\")\n    roles_spark = container.roles.to_pyspark(spark, name=\"roles\")\n    movies_spark = container.movies.to_pyspark(spark, name=\"movies\")\n</pre> if RUN_SPARK:     movies_genres_spark = container.movies_genres.to_pyspark(spark, name=\"movies_genres\")     roles_spark = container.roles.to_pyspark(spark, name=\"roles\")     movies_spark = container.movies.to_pyspark(spark, name=\"movies\") In\u00a0[28]: Copied! <pre>if RUN_SPARK:\n    getml.spark.execute(spark, \"imdb_spark\")\n</pre> if RUN_SPARK:     getml.spark.execute(spark, \"imdb_spark\") In\u00a0[29]: Copied! <pre>if RUN_SPARK:\n    spark.sql(\"SELECT * FROM `FEATURES` LIMIT 20\").toPandas()\n</pre> if RUN_SPARK:     spark.sql(\"SELECT * FROM `FEATURES` LIMIT 20\").toPandas()","boost":0.8},{"location":"examples/enterprise-notebooks/imdb/#imdb-predicting-actors-gender","title":"IMDb - Predicting actors' gender\u00b6","text":"<p>In this tutorial, we demonstrate how getML can be applied to text fields. In relational databases, text fields are less structured and less standardized than categorical data, making it more difficult to extract useful information from them. Therefore, they are ignored in most data science projects on relational data. However, when using a relational learning tool such as getML, we can easily generate simple features from text fields and leverage the information contained therein.</p> <p>The point of this exercise is not to compete with modern deep-learning-based NLP approaches. The point is to develop an approach by which we can leverage fields in relational databases that would otherwise be ignored.</p> <p>As an example data set, we use the Internet Movie Database, which has been used by previous studies in the relational learning literature. This allows us to benchmark our approach to state-of-the-art algorithms in the relational learning literature. We demonstrate that getML outperforms these state-of-the-art algorithms.</p> <p>Summary:</p> <ul> <li>Prediction type: Classification model</li> <li>Domain: Entertainment</li> <li>Prediction target: The gender of an actor</li> <li>Population size: 817718</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/imdb/#background","title":"Background\u00b6","text":"<p>The data set contains about 800,000 actors. The goal is to predict the gender of said actors based on other information we have about them, such as the movies they have participated in and the roles they have played in these movies.</p> <p>It has been downloaded from the CTU Prague relational learning repository (Motl and Schulte, 2015) (Now residing at relational-data.org.).</p>","boost":0.8},{"location":"examples/enterprise-notebooks/imdb/#analysis","title":"Analysis\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/imdb/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/imdb/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>We begin by downloading the data from the source file:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/imdb/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/imdb/#2-predictive-modelling","title":"2. Predictive modelling\u00b6","text":"<p>We loaded the data and defined the roles and units. Next, we create a getML pipeline for relational learning.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/imdb/#21-define-relational-model","title":"2.1 Define relational model\u00b6","text":"<p>To get started with relational learning, we need to specify the data model.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/imdb/#22-getml-pipeline","title":"2.2 getML pipeline\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/imdb/#23-model-training","title":"2.3 Model training\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/imdb/#24-model-evaluation","title":"2.4 Model evaluation\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/imdb/#25-features","title":"2.5 Features\u00b6","text":"<p>The most important feature looks as follows:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/imdb/#26-productionization","title":"2.6 Productionization\u00b6","text":"<p>It is possible to productionize the pipeline by transpiling the features into production-ready SQL code. Here, we will demonstrate how the pipeline can be transpiled to Spark SQL and then executed on a Spark cluster.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/imdb/#3-conclusion","title":"3. Conclusion\u00b6","text":"<p>In this notebook we have demonstrated how getML can be applied to text fields. We have demonstrated the our  approach outperforms state-of-the-art relational learning algorithms on the IMDb dataset.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/imdb/#references","title":"References\u00b6","text":"<p>Motl, Jan, and Oliver Schulte. \"The CTU prague relational learning repository.\" arXiv preprint arXiv:1511.03086 (2015).</p> <p>Neville, Jennifer, and David Jensen. \"Relational dependency networks.\" Journal of Machine Learning Research 8.Mar (2007): 653-692.</p> <p>Neville, Jennifer, and David Jensen. \"Collective classification with relational dependency networks.\" Workshop on Multi-Relational Data Mining (MRDM-2003). 2003.</p> <p>Neville, Jennifer, et al. \"Learning relational probability trees.\" Proceedings of the Ninth ACM SIGKDD international conference on Knowledge discovery and data mining. 2003.</p> <p>Perov\u0161ek, Matic, et al. \"Wordification: Propositionalization by unfolding relational data into bags of words.\" Expert Systems with Applications 42.17-18 (2015): 6442-6456.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/interstate94/","title":"<span class=\"ntitle\">interstate94.ipynb</span> <span class=\"ndesc\">Multivariate time series prediction</span>","text":"<p>Let's get started with the analysis and set-up your session:</p> In\u00a0[1]: Copied! <pre>import os\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image\n\n%matplotlib inline\n\nimport getml\n\nprint(f\"getML API version: {getml.__version__}\\n\")\n\ngetml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.engine.set_project(\"interstate94\")\n</pre> import os os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\" from pathlib import Path  import matplotlib.pyplot as plt import seaborn as sns import numpy as np import pandas as pd from IPython.display import Image  %matplotlib inline  import getml  print(f\"getML API version: {getml.__version__}\\n\")  getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.engine.set_project(\"interstate94\") <pre>getML API version: 1.4.0\n\ngetML engine is already running.\n\nConnected to project 'interstate94'\n</pre> In\u00a0[2]: Copied! <pre>traffic = getml.datasets.load_interstate94()\n</pre> traffic = getml.datasets.load_interstate94() <pre>\nLoading traffic...\n</pre> In\u00a0[3]: Copied! <pre>traffic\n</pre> traffic Out[3]:  name                          ds traffic_volume holiday       day         month       weekday     hour        year         role                  time_stamp         target categorical   categorical categorical categorical categorical categorical  unit time stamp, comparison only day         month       weekday     hour        year        0 2016-01-01 1513 New Years Day 1 1 4 0 2016 1 2016-01-01 01:00:00 1550 New Years Day 1 1 4 1 2016 2 2016-01-01 02:00:00 993 New Years Day 1 1 4 2 2016 3 2016-01-01 03:00:00 719 New Years Day 1 1 4 3 2016 4 2016-01-01 04:00:00 533 New Years Day 1 1 4 4 2016 ... ... ... ... ... ... ... ... 24091 2018-09-30 19:00:00 3543 No holiday 30 9 6 19 2018 24092 2018-09-30 20:00:00 2781 No holiday 30 9 6 20 2018 24093 2018-09-30 21:00:00 2159 No holiday 30 9 6 21 2018 24094 2018-09-30 22:00:00 1450 No holiday 30 9 6 22 2018 24095 2018-09-30 23:00:00 954 No holiday 30 9 6 23 2018 <p>     24096 rows x 8 columns     memory usage: 0.96 MB     name: traffic     type: getml.DataFrame </p> <p>Data visualization</p> <p>The first week of the original traffic time series is plotted below.</p> In\u00a0[4]: Copied! <pre>fig, ax = plt.subplots(figsize=(20, 10))\n\n# 2016/01/01 was a friday, we'd like to start the visualizations on a monday\nstart = getml.data.time.datetime(2016, 1, 4)\nend = getml.data.time.datetime(2016, 1, 11)\n\nfig.suptitle(\n    \"Traffic volume for first full week of the training set\",\n    fontsize=14,\n    fontweight=\"bold\",\n)\ntraffic_first_week = traffic[(traffic.ds&gt;=start) &amp; (traffic.ds&lt;end)]\nax.plot(\n    traffic_first_week[\"ds\"].to_numpy(),\n    traffic_first_week[\"traffic_volume\"].to_numpy(),\n)\n</pre> fig, ax = plt.subplots(figsize=(20, 10))  # 2016/01/01 was a friday, we'd like to start the visualizations on a monday start = getml.data.time.datetime(2016, 1, 4) end = getml.data.time.datetime(2016, 1, 11)  fig.suptitle(     \"Traffic volume for first full week of the training set\",     fontsize=14,     fontweight=\"bold\", ) traffic_first_week = traffic[(traffic.ds&gt;=start) &amp; (traffic.ds Out[4]: <pre>[&lt;matplotlib.lines.Line2D at 0x7f1f95a3adc0&gt;]</pre> <p>Traffic: population table</p> <p>To allow the algorithm to capture seasonal information, we include time components (such as the day of the week) as categorical variables. Note that we could have also used getML's Seasonal preprocessor (<code>getml.prepreprocessors.Seasonal()</code>), but in this case the information was already included in the dataset.</p> <p>Train/test split</p> <p>We use getML's split functionality to retrieve a lazily evaluated split column, that we can supply to the time series api below.</p> In\u00a0[5]: Copied! <pre>split_date = pd.Timestamp(year=2018, month=3, day=15)\nsplit = getml.data.split.time(traffic, \"ds\", test=split_date.timestamp())\n</pre> split_date = pd.Timestamp(year=2018, month=3, day=15) split = getml.data.split.time(traffic, \"ds\", test=split_date.timestamp()) <p>Split columns are columns of mere strings that can be used to subset the data by forming bolean conditions over them:</p> In\u00a0[6]: Copied! <pre>traffic[split == \"test\"]\n</pre> traffic[split == \"test\"] Out[6]: name                          ds traffic_volume holiday     day         month       weekday     hour        year        role                  time_stamp         target categorical categorical categorical categorical categorical categorical unit time stamp, comparison only day         month       weekday     hour        year        0 2018-03-15 577 No holiday 15 3 3 0 2018 1 2018-03-15 01:00:00 354 No holiday 15 3 3 1 2018 2 2018-03-15 02:00:00 259 No holiday 15 3 3 2 2018 3 2018-03-15 03:00:00 360 No holiday 15 3 3 3 2018 4 2018-03-15 04:00:00 910 No holiday 15 3 3 4 2018 ... ... ... ... ... ... ... ... ... <p>     unknown number of rows          type: getml.data.View </p> In\u00a0[7]: Copied! <pre>time_series = getml.data.TimeSeries(\n    population=traffic,\n    split=split,\n    time_stamps=\"ds\",\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.days(7),\n    lagged_targets=True,\n)\n\ntime_series\n</pre> time_series = getml.data.TimeSeries(     population=traffic,     split=split,     time_stamps=\"ds\",     horizon=getml.data.time.hours(1),     memory=getml.data.time.days(7),     lagged_targets=True, )  time_series Out[7]: data model diagram trafficpopulationds &lt;= dsMemory: 7.0 daysHorizon: 1.0 hoursLagged targets allowed staging data frames staging table               0 population POPULATION__STAGING_TABLE_1 1 traffic TRAFFIC__STAGING_TABLE_2 container population subset name     rows type 0 test traffic 4800 View 1 train traffic 19296 View peripheral name     rows type      0 traffic 24096 DataFrame <p>Set-up of feature learners, selectors &amp; predictor</p> In\u00a0[8]: Copied! <pre>relmt = getml.feature_learning.RelMT(\n    num_features=20,\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    seed=4367,\n    num_threads=1,\n)\n\npredictor = getml.predictors.XGBoostRegressor()\n</pre> relmt = getml.feature_learning.RelMT(     num_features=20,     loss_function=getml.feature_learning.loss_functions.SquareLoss,     seed=4367,     num_threads=1, )  predictor = getml.predictors.XGBoostRegressor() <p>Build the pipeline</p> In\u00a0[9]: Copied! <pre>pipe = getml.pipeline.Pipeline(\n    tags=[\"memory: 7d\", \"horizon: 1h\", \"relmt\"],\n    data_model=time_series.data_model,\n    feature_learners=[relmt],\n    predictors=[predictor],\n)\n</pre> pipe = getml.pipeline.Pipeline(     tags=[\"memory: 7d\", \"horizon: 1h\", \"relmt\"],     data_model=time_series.data_model,     feature_learners=[relmt],     predictors=[predictor], ) In\u00a0[10]: Copied! <pre>pipe.fit(time_series.train)\n</pre> pipe.fit(time_series.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelMT: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 01:33, remaining: 00:00]          \nRelMT: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:21, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:1m:55.66844\n\n</pre> Out[10]: <pre>Pipeline(data_model='population',\n         feature_learners=['RelMT'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=['traffic'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['memory: 7d', 'horizon: 1h', 'relmt', 'container-5wrGv6'])</pre> In\u00a0[11]: Copied! <pre>getml_score = pipe.score(time_series.test)\ngetml_score\n</pre> getml_score = pipe.score(time_series.test) getml_score <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelMT: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:06, remaining: 00:00]          \n\n</pre> Out[11]: date time           set used target               mae      rmse rsquared 0 2024-02-21 16:41:44 train traffic_volume 201.298 295.6177 0.9774 1 2024-02-21 16:41:50 test traffic_volume 183.2422 271.2663 0.9814 <p>Feature correlations</p> <p>Correlations of the calculated features with the target</p> In\u00a0[12]: Copied! <pre>names, correlations = pipe.features.correlations()\n\nplt.subplots(figsize=(20, 10))\n\nplt.bar(names, correlations)\nplt.title(\"Feature Correlations\")\nplt.xlabel(\"Features\")\nplt.ylabel(\"Correlations\")\nplt.xticks(rotation=\"vertical\")\nplt.show()\n</pre> names, correlations = pipe.features.correlations()  plt.subplots(figsize=(20, 10))  plt.bar(names, correlations) plt.title(\"Feature Correlations\") plt.xlabel(\"Features\") plt.ylabel(\"Correlations\") plt.xticks(rotation=\"vertical\") plt.show() <p>Feature importances</p> In\u00a0[13]: Copied! <pre>names, importances = pipe.features.importances()\n\nplt.subplots(figsize=(20, 10))\n\nplt.bar(names, importances)\nplt.title(\"Feature Importances\")\nplt.xlabel(\"Features\")\nplt.ylabel(\"Importances\")\nplt.xticks(rotation=\"vertical\")\nplt.show()\n</pre> names, importances = pipe.features.importances()  plt.subplots(figsize=(20, 10))  plt.bar(names, importances) plt.title(\"Feature Importances\") plt.xlabel(\"Features\") plt.ylabel(\"Importances\") plt.xticks(rotation=\"vertical\") plt.show() <p>Visualizing the learned features</p> <p>We can also transpile the features as SQL code. Here, we show the most important feature.</p> In\u00a0[14]: Copied! <pre>by_importance = pipe.features.sort(by=\"importance\")\nby_importance[0].sql\n</pre> by_importance = pipe.features.sort(by=\"importance\") by_importance[0].sql Out[14]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_11\";\n\nCREATE TABLE \"FEATURE_1_11\" AS\nSELECT SUM( \n    CASE\n        WHEN ( t1.\"ds\" - t2.\"ds\" &gt; 6965.710287 ) AND ( t2.\"hour\" IN ( '0', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23' ) ) THEN COALESCE( t1.\"ds\" - 1486337853.612977, 0.0 ) * 3.056994616117964e-05 + COALESCE( t2.\"traffic_volume\" - 3302.204612593936, 0.0 ) * 0.0004592958687265713 + COALESCE( t2.\"ds\" - 1486335600, 0.0 ) * -1.47446253771948e-05 + COALESCE( t2.\"ds__1_000000_hours\" - 1486339200, 0.0 ) * -1.474462407374121e-05 + 5.7136772527002925e+01\n        WHEN ( t1.\"ds\" - t2.\"ds\" &gt; 6965.710287 ) AND ( t2.\"hour\" NOT IN ( '0', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23' ) OR t2.\"hour\" IS NULL ) THEN COALESCE( t1.\"ds\" - 1486337853.612977, 0.0 ) * -0.0002728527078393066 + COALESCE( t2.\"traffic_volume\" - 3302.204612593936, 0.0 ) * -0.01052789409335724 + COALESCE( t2.\"ds\" - 1486335600, 0.0 ) * 0.0001346563960090505 + COALESCE( t2.\"ds__1_000000_hours\" - 1486339200, 0.0 ) * 0.0001346563960090505 + -1.6323514509683892e+02\n        WHEN ( t1.\"ds\" - t2.\"ds\" &lt;= 6965.710287 OR t1.\"ds\" IS NULL OR t2.\"ds\" IS NULL ) AND ( t2.\"hour\" IN ( '0', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23' ) ) THEN COALESCE( t1.\"ds\" - 1486337853.612977, 0.0 ) * -6.450603601298901e-06 + COALESCE( t2.\"traffic_volume\" - 3302.204612593936, 0.0 ) * 0.4011052890935949 + COALESCE( t2.\"ds\" - 1486335600, 0.0 ) * -6.365908267767004e-06 + COALESCE( t2.\"ds__1_000000_hours\" - 1486339200, 0.0 ) * -6.368337472002008e-06 + 3.5377949203029338e+01\n        WHEN ( t1.\"ds\" - t2.\"ds\" &lt;= 6965.710287 OR t1.\"ds\" IS NULL OR t2.\"ds\" IS NULL ) AND ( t2.\"hour\" NOT IN ( '0', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23' ) OR t2.\"hour\" IS NULL ) THEN COALESCE( t1.\"ds\" - 1486337853.612977, 0.0 ) * 3.830812354148993e-06 + COALESCE( t2.\"traffic_volume\" - 3302.204612593936, 0.0 ) * 0.8224585907500582 + COALESCE( t2.\"ds\" - 1486335600, 0.0 ) * -1.352120390568317e-05 + COALESCE( t2.\"ds__1_000000_hours\" - 1486339200, 0.0 ) * -1.36062674844675e-05 + 1.0670459471423501e+03\n        ELSE NULL\n    END\n) AS \"feature_1_11\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"TRAFFIC__STAGING_TABLE_2\" t2\nON 1 = 1\nWHERE t2.\"ds__1_000000_hours\" &lt;= t1.\"ds\"\nAND ( t2.\"ds__7_041667_days\" &gt; t1.\"ds\" OR t2.\"ds__7_041667_days\" IS NULL )\nGROUP BY t1.rowid;\n</pre> <p>To showcase getML's ability to handle categorical data, we now look for features that contain information from the holiday column:</p> In\u00a0[15]: Copied! <pre>w_holiday = by_importance.filter(lambda feature: \"holiday\" in feature.sql)\nw_holiday\n</pre> w_holiday = by_importance.filter(lambda feature: \"holiday\" in feature.sql) w_holiday Out[15]: target         name         correlation importance 0 traffic_volume feature_1_12 0.9597 0.0142 1 traffic_volume feature_1_5 0.9669 0.0117 2 traffic_volume feature_1_18 0.9627 0.0087 3 traffic_volume feature_1_4 0.9634 0.0066 4 traffic_volume feature_1_14 0.9664 0.0038 ... ... ... ... 6 traffic_volume feature_1_1 0.9653 0.0029 7 traffic_volume feature_1_3 0.9661 0.0028 8 traffic_volume feature_1_10 0.9602 0.0025 9 traffic_volume feature_1_8 0.9648 0.0002 10 traffic_volume feature_1_6 0.9629 0.0 <p>As you can see, getML features which incorporate information about holidays have a rather low importance. This is not that surprising given the fact that most information about holidays is fully reproducible from the extracted calendarial information that is already present. In other words: for the algorithm, it doesn't matter if the traffic is lower on every 4th of July of a given year or if there is a corresponding holiday named 'Independence Day'. Here is the SQL transpilation of the most important feature relying on information about holdidays anyway:</p> In\u00a0[16]: Copied! <pre>w_holiday[0].sql\n</pre> w_holiday[0].sql Out[16]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_12\";\n\nCREATE TABLE \"FEATURE_1_12\" AS\nSELECT AVG( \n    CASE\n        WHEN ( t1.\"ds\" - t2.\"ds\" &gt; 604379.286214 ) AND ( t2.\"holiday\" IN ( 'No holiday' ) ) THEN COALESCE( t1.\"ds\" - 1486337853.612977, 0.0 ) * 0.0003655426559629976 + COALESCE( t2.\"traffic_volume\" - 3302.204612593936, 0.0 ) * 71.04907704233962 + COALESCE( t2.\"ds\" - 1486335600, 0.0 ) * 0.0003659565812331032 + COALESCE( t2.\"ds__1_000000_hours\" - 1486339200, 0.0 ) * 0.0003659565812330983 + -1.2102245006452505e+02\n        WHEN ( t1.\"ds\" - t2.\"ds\" &gt; 604379.286214 ) AND ( t2.\"holiday\" NOT IN ( 'No holiday' ) OR t2.\"holiday\" IS NULL ) THEN COALESCE( t1.\"ds\" - 1486337853.612977, 0.0 ) * 7.966581896562819e-05 + COALESCE( t2.\"traffic_volume\" - 3302.204612593936, 0.0 ) * 68.9956068955138 + COALESCE( t2.\"ds\" - 1486335600, 0.0 ) * 7.975106805403055e-05 + COALESCE( t2.\"ds__1_000000_hours\" - 1486339200, 0.0 ) * 7.975106805403055e-05 + 5.2980510009154398e+02\n        WHEN ( t1.\"ds\" - t2.\"ds\" &lt;= 604379.286214 OR t1.\"ds\" IS NULL OR t2.\"ds\" IS NULL ) AND ( t1.\"ds\" - t2.\"ds\" &gt; 6957.303371 ) THEN COALESCE( t1.\"ds\" - 1486337853.612977, 0.0 ) * -1.314712363235606e-06 + COALESCE( t2.\"traffic_volume\" - 3302.204612593936, 0.0 ) * 0.04717003289907078 + COALESCE( t2.\"ds\" - 1486335600, 0.0 ) * -1.317428773219315e-06 + COALESCE( t2.\"ds__1_000000_hours\" - 1486339200, 0.0 ) * -1.317428773219332e-06 + 7.8257840351760217e+00\n        WHEN ( t1.\"ds\" - t2.\"ds\" &lt;= 604379.286214 OR t1.\"ds\" IS NULL OR t2.\"ds\" IS NULL ) AND ( t1.\"ds\" - t2.\"ds\" &lt;= 6957.303371 OR t1.\"ds\" IS NULL OR t2.\"ds\" IS NULL ) THEN COALESCE( t1.\"ds\" - 1486337853.612977, 0.0 ) * -0.0001622247708287244 + COALESCE( t2.\"traffic_volume\" - 3302.204612593936, 0.0 ) * 9.219195611541375 + COALESCE( t2.\"ds\" - 1486335600, 0.0 ) * -0.0001624142455173989 + COALESCE( t2.\"ds__1_000000_hours\" - 1486339200, 0.0 ) * -0.0001624142455173987 + -9.1615088286977944e+00\n        ELSE NULL\n    END\n) AS \"feature_1_12\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"TRAFFIC__STAGING_TABLE_2\" t2\nON 1 = 1\nWHERE t2.\"ds__1_000000_hours\" &lt;= t1.\"ds\"\nAND ( t2.\"ds__7_041667_days\" &gt; t1.\"ds\" OR t2.\"ds__7_041667_days\" IS NULL )\nGROUP BY t1.rowid;\n</pre> <p>Plot predictions &amp; traffic volume vs. time</p> <p>We now plot the predictions against the observed values of the target for the first 7 days of the testing set. You can see that the predictions closely follows the original series. RelMT was able to identify certain patterns in the series, including:</p> <ul> <li>Day and night separation</li> <li>The daily commuting peeks (on weekdays)</li> <li>The decline on weekends</li> </ul> In\u00a0[17]: Copied! <pre>predictions = pipe.predict(time_series.test)\n</pre> predictions = pipe.predict(time_series.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelMT: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:05, remaining: 00:00]          \n\n</pre> In\u00a0[18]: Copied! <pre># the test set starts at 2018/03/15 \u2013 a thursday; we introduce an offset to, once again, start on a monday\ndef limit_view(view):\n    start_date = '2018-03-19'\n    end_date = '2018-03-26'\n    return view[(view.ds &gt;= start_date) &amp; (view.ds &lt; end_date)]\n</pre> # the test set starts at 2018/03/15 \u2013 a thursday; we introduce an offset to, once again, start on a monday def limit_view(view):     start_date = '2018-03-19'     end_date = '2018-03-26'     return view[(view.ds &gt;= start_date) &amp; (view.ds &lt; end_date)] In\u00a0[19]: Copied! <pre># Predict with getML\nprediction_getml = pd.DataFrame(np.array(predictions), columns=['traffic_volume'])\nprediction_getml['ds'] = time_series.test.population['ds'].to_numpy()\nprediction_getml\n</pre> # Predict with getML prediction_getml = pd.DataFrame(np.array(predictions), columns=['traffic_volume']) prediction_getml['ds'] = time_series.test.population['ds'].to_numpy() prediction_getml Out[19]: traffic_volume ds 0 585.338745 2018-03-15 00:00:00 1 359.089447 2018-03-15 01:00:00 2 303.897705 2018-03-15 02:00:00 3 356.708466 2018-03-15 03:00:00 4 856.191162 2018-03-15 04:00:00 ... ... ... 4795 3371.007812 2018-09-30 19:00:00 4796 2977.440186 2018-09-30 20:00:00 4797 2493.370850 2018-09-30 21:00:00 4798 1436.891602 2018-09-30 22:00:00 4799 905.403809 2018-09-30 23:00:00 <p>4800 rows \u00d7 2 columns</p> In\u00a0[20]: Copied! <pre>fig, ax = plt.subplots(figsize=(20, 10))\n\nactual = limit_view(time_series.test.population.to_pandas())\n\nax.plot(actual[\"ds\"], actual[\"traffic_volume\"], label=\"Actual\")\nax.plot(actual[\"ds\"], limit_view(prediction_getml)['traffic_volume'], label=\"Predicted getML\")\nfig.suptitle(\n    \"Predicted vs. actual traffic volume for first full week of testing set\",\n    fontsize=14,\n    fontweight=\"bold\",\n)\nfig.legend()\n</pre> fig, ax = plt.subplots(figsize=(20, 10))  actual = limit_view(time_series.test.population.to_pandas())  ax.plot(actual[\"ds\"], actual[\"traffic_volume\"], label=\"Actual\") ax.plot(actual[\"ds\"], limit_view(prediction_getml)['traffic_volume'], label=\"Predicted getML\") fig.suptitle(     \"Predicted vs. actual traffic volume for first full week of testing set\",     fontsize=14,     fontweight=\"bold\", ) fig.legend() Out[20]: <pre>&lt;matplotlib.legend.Legend at 0x7f1f895887f0&gt;</pre> In\u00a0[21]: Copied! <pre>pipe.features.to_sql()[pipe.features.sort(by=\"importances\")[0].name]\n</pre> pipe.features.to_sql()[pipe.features.sort(by=\"importances\")[0].name] Out[21]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_11\";\n\nCREATE TABLE \"FEATURE_1_11\" AS\nSELECT SUM( \n    CASE\n        WHEN ( t1.\"ds\" - t2.\"ds\" &gt; 6965.710287 ) AND ( t2.\"hour\" IN ( '0', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23' ) ) THEN COALESCE( t1.\"ds\" - 1486337853.612977, 0.0 ) * 3.056994616117964e-05 + COALESCE( t2.\"traffic_volume\" - 3302.204612593936, 0.0 ) * 0.0004592958687265713 + COALESCE( t2.\"ds\" - 1486335600, 0.0 ) * -1.47446253771948e-05 + COALESCE( t2.\"ds__1_000000_hours\" - 1486339200, 0.0 ) * -1.474462407374121e-05 + 5.7136772527002925e+01\n        WHEN ( t1.\"ds\" - t2.\"ds\" &gt; 6965.710287 ) AND ( t2.\"hour\" NOT IN ( '0', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23' ) OR t2.\"hour\" IS NULL ) THEN COALESCE( t1.\"ds\" - 1486337853.612977, 0.0 ) * -0.0002728527078393066 + COALESCE( t2.\"traffic_volume\" - 3302.204612593936, 0.0 ) * -0.01052789409335724 + COALESCE( t2.\"ds\" - 1486335600, 0.0 ) * 0.0001346563960090505 + COALESCE( t2.\"ds__1_000000_hours\" - 1486339200, 0.0 ) * 0.0001346563960090505 + -1.6323514509683892e+02\n        WHEN ( t1.\"ds\" - t2.\"ds\" &lt;= 6965.710287 OR t1.\"ds\" IS NULL OR t2.\"ds\" IS NULL ) AND ( t2.\"hour\" IN ( '0', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23' ) ) THEN COALESCE( t1.\"ds\" - 1486337853.612977, 0.0 ) * -6.450603601298901e-06 + COALESCE( t2.\"traffic_volume\" - 3302.204612593936, 0.0 ) * 0.4011052890935949 + COALESCE( t2.\"ds\" - 1486335600, 0.0 ) * -6.365908267767004e-06 + COALESCE( t2.\"ds__1_000000_hours\" - 1486339200, 0.0 ) * -6.368337472002008e-06 + 3.5377949203029338e+01\n        WHEN ( t1.\"ds\" - t2.\"ds\" &lt;= 6965.710287 OR t1.\"ds\" IS NULL OR t2.\"ds\" IS NULL ) AND ( t2.\"hour\" NOT IN ( '0', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23' ) OR t2.\"hour\" IS NULL ) THEN COALESCE( t1.\"ds\" - 1486337853.612977, 0.0 ) * 3.830812354148993e-06 + COALESCE( t2.\"traffic_volume\" - 3302.204612593936, 0.0 ) * 0.8224585907500582 + COALESCE( t2.\"ds\" - 1486335600, 0.0 ) * -1.352120390568317e-05 + COALESCE( t2.\"ds__1_000000_hours\" - 1486339200, 0.0 ) * -1.36062674844675e-05 + 1.0670459471423501e+03\n        ELSE NULL\n    END\n) AS \"feature_1_11\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"TRAFFIC__STAGING_TABLE_2\" t2\nON 1 = 1\nWHERE t2.\"ds__1_000000_hours\" &lt;= t1.\"ds\"\nAND ( t2.\"ds__7_041667_days\" &gt; t1.\"ds\" OR t2.\"ds__7_041667_days\" IS NULL )\nGROUP BY t1.rowid;\n</pre> In\u00a0[22]: Copied! <pre># Creates a folder named interstate94_pipeline containing\n# the SQL code.\npipe.features.to_sql().save(\"interstate94_pipeline\", remove=True)\n</pre> # Creates a folder named interstate94_pipeline containing # the SQL code. pipe.features.to_sql().save(\"interstate94_pipeline\", remove=True) <p>While the feature is less smooth, it is really close to what we got in the 1-step case. This is another indication for the prescence of strong time-related patterns in the data.</p> <p>Here we fit the model. We fit a new Prophet model for every hour. This is a computationally expensive operation even for our test subsample of 30 days. For the 720 hours (= 720 models) it takes around 6 hours. Therefore we read the predicitions from disc. If you want to reestimate the Prophet predicitions, you can rename the csv files on disc.</p> In\u00a0[23]: Copied! <pre>from scipy import stats\n\ntry:\n    prediction_prophet = pd.read_csv('pred_prophet_30d.csv')\n    prediction_prophet['ds'] = pd.to_datetime(prediction_prophet['ds'])\n\nexcept FileNotFoundError:\n    import logging\n    import cmdstanpy\n    logger = logging.getLogger('cmdstanpy')\n    logger.addHandler(logging.NullHandler())\n    logger.propagate = False\n    logger.setLevel(logging.CRITICAL)\n\n    from prophet import Prophet\n    from prophet.diagnostics import cross_validation\n\n    # Rename columns to follow Prophet convention\n    traffic_prophet = (\n        traffic\n        .to_pandas()[['ds', 'traffic_volume']]\n        .rename(\n            {'traffic_volume': 'y'}, axis='columns'\n        )\n    )\n\n    # The actual prediction. One model for every 1h ahead prediction.\n    fit_df = traffic_prophet[traffic_prophet.ds &lt; split_date + pd.Timedelta('30d')]\n    train_ds = traffic_prophet[traffic_prophet.ds &lt; split_date]['ds']\n    train_window = train_ds.max() - train_ds.min()\n\n    model_prophet = Prophet()\n    model_prophet.fit(fit_df)\n\n    # Cross validate\n    prediction_prophet = cross_validation(model_prophet,\n                                          horizon='1h',\n                                          period='1h',\n                                          initial=train_window)\n    # Save predictions\n    prediction_prophet.to_csv('pred_prophet_30d.csv',\n                              encoding='utf-8', index=False)\n</pre> from scipy import stats  try:     prediction_prophet = pd.read_csv('pred_prophet_30d.csv')     prediction_prophet['ds'] = pd.to_datetime(prediction_prophet['ds'])  except FileNotFoundError:     import logging     import cmdstanpy     logger = logging.getLogger('cmdstanpy')     logger.addHandler(logging.NullHandler())     logger.propagate = False     logger.setLevel(logging.CRITICAL)      from prophet import Prophet     from prophet.diagnostics import cross_validation      # Rename columns to follow Prophet convention     traffic_prophet = (         traffic         .to_pandas()[['ds', 'traffic_volume']]         .rename(             {'traffic_volume': 'y'}, axis='columns'         )     )      # The actual prediction. One model for every 1h ahead prediction.     fit_df = traffic_prophet[traffic_prophet.ds &lt; split_date + pd.Timedelta('30d')]     train_ds = traffic_prophet[traffic_prophet.ds &lt; split_date]['ds']     train_window = train_ds.max() - train_ds.min()      model_prophet = Prophet()     model_prophet.fit(fit_df)      # Cross validate     prediction_prophet = cross_validation(model_prophet,                                           horizon='1h',                                           period='1h',                                           initial=train_window)     # Save predictions     prediction_prophet.to_csv('pred_prophet_30d.csv',                               encoding='utf-8', index=False) <pre>  0%|          | 0/720 [00:00&lt;?, ?it/s]</pre> <p>Score the model.</p> In\u00a0[24]: Copied! <pre># Calculate score\nr2_prophet = stats.pearsonr(prediction_prophet['yhat'].values,\n                            prediction_prophet['y'].values)[0]**2\nprint('R2:', r2_prophet)\n</pre> # Calculate score r2_prophet = stats.pearsonr(prediction_prophet['yhat'].values,                             prediction_prophet['y'].values)[0]**2 print('R2:', r2_prophet) <pre>R2: 0.8332236364715048\n</pre> <p>getML is able to outperform Prophet's 1-step ahead predictions by about 14 percentage points in terms of predictive accuracy. This is a substantial margin. But we have to state that it may be an unfair comparison because we use Prophet for an application it wasn't designed for. To deal with this critique we also calculate h-step predictions below.</p> <p>Next, we visually compare the 1-step ahead Prophet with 1-step ahead getML predictions.</p> <p>Note that with getML we calculate predictions for the full sample (instead of just 30 days). This plays as another advantage for Prophet.</p> <p>We inspect the predictions of both models over the course of a week. Here, we plot the getML and Prophet predictions against actual values (data).</p> In\u00a0[25]: Copied! <pre>fig, ax = plt.subplots(figsize=(20, 10))\n# Plot all together\nax.plot(time_series.test.population.ds, time_series.test.population.traffic_volume, label='Actual')\nax.plot(prediction_getml['ds'], prediction_getml.traffic_volume, label='Predicted getML')\nax.plot(prediction_prophet.ds, prediction_prophet.yhat, label='Predicted prophet')\nplt.title('1-Step-Ahaed Predicitions')\nplt.legend()\nplt.ylabel('Traffic Volume')\nplt.xlabel('Time')\n# We shift the data by 5 days to let the plot start on a mondey\nstart_date = pd.Timestamp(year=2018, month=3, day=19)\nend_date = pd.Timestamp(year=2018, month=3, day=26)\nplt.xlim(start_date, end_date)\n</pre> fig, ax = plt.subplots(figsize=(20, 10)) # Plot all together ax.plot(time_series.test.population.ds, time_series.test.population.traffic_volume, label='Actual') ax.plot(prediction_getml['ds'], prediction_getml.traffic_volume, label='Predicted getML') ax.plot(prediction_prophet.ds, prediction_prophet.yhat, label='Predicted prophet') plt.title('1-Step-Ahaed Predicitions') plt.legend() plt.ylabel('Traffic Volume') plt.xlabel('Time') # We shift the data by 5 days to let the plot start on a mondey start_date = pd.Timestamp(year=2018, month=3, day=19) end_date = pd.Timestamp(year=2018, month=3, day=26) plt.xlim(start_date, end_date) Out[25]: <pre>(17609.0, 17616.0)</pre> In\u00a0[26]: Copied! <pre>prediction_getml[(prediction_getml.ds&gt;=start_date) &amp; (prediction_getml.ds&lt;=end_date)]\n</pre> prediction_getml[(prediction_getml.ds&gt;=start_date) &amp; (prediction_getml.ds&lt;=end_date)] Out[26]: traffic_volume ds 96 509.549133 2018-03-19 00:00:00 97 359.089447 2018-03-19 01:00:00 98 290.026855 2018-03-19 02:00:00 99 356.708466 2018-03-19 03:00:00 100 856.191162 2018-03-19 04:00:00 ... ... ... 260 2687.968994 2018-03-25 20:00:00 261 2284.754395 2018-03-25 21:00:00 262 2950.236084 2018-03-25 22:00:00 263 1135.205688 2018-03-25 23:00:00 264 561.838135 2018-03-26 00:00:00 <p>169 rows \u00d7 2 columns</p> In\u00a0[27]: Copied! <pre>prediction_prophet[(prediction_prophet.ds&gt;=start_date) &amp; (prediction_prophet.ds&lt;=end_date)]\n</pre> prediction_prophet[(prediction_prophet.ds&gt;=start_date) &amp; (prediction_prophet.ds&lt;=end_date)] Out[27]: ds yhat yhat_lower yhat_upper y cutoff 96 2018-03-19 00:00:00 855.916110 -234.163297 1960.056205 540.0 2018-03-18 23:00:00 97 2018-03-19 01:00:00 214.890270 -871.870881 1347.346909 293.0 2018-03-19 00:00:00 98 2018-03-19 02:00:00 -201.129924 -1202.677841 887.193163 254.0 2018-03-19 01:00:00 99 2018-03-19 03:00:00 -0.633600 -1047.991998 1079.592098 348.0 2018-03-19 02:00:00 100 2018-03-19 04:00:00 925.225010 -114.387569 1971.703068 819.0 2018-03-19 03:00:00 ... ... ... ... ... ... ... 260 2018-03-25 20:00:00 2488.696134 1410.526848 3577.459370 2784.0 2018-03-25 19:00:00 261 2018-03-25 21:00:00 2020.131236 943.217663 3200.053813 3967.0 2018-03-25 20:00:00 262 2018-03-25 22:00:00 1743.971268 656.480465 2721.658385 1794.0 2018-03-25 21:00:00 263 2018-03-25 23:00:00 1434.999641 375.048048 2493.040061 984.0 2018-03-25 22:00:00 264 2018-03-26 00:00:00 916.526718 -192.310771 1869.164473 518.0 2018-03-25 23:00:00 <p>169 rows \u00d7 6 columns</p> <p>What can we take from this plot? The strong time-related patterns carry on to the testing set and the xgboost is able to incorporate this information to deliver highly accurate 1-step ahead predictions. Notice that Prophets additive components model results in negative predictions for the weekends' lows at night. We can also see an anomaly on March, 24 that neither model is able to predict (feel free to play with the window to verify that its an annomaly).</p> <p>Now we benchmark the performance of h-step ahead forecasts. Remember, in the models, only deterministic features are incorporated. Here, we fit the h-step ahead forecast with Prophet. This is how Prophet is meant to be applied. We allow for multiplicative seasonalities to at least partially remedy the problem with negative predictions discussed above.</p> In\u00a0[28]: Copied! <pre># h-step ahead forecast with Prophet\nfrom IPython.display import Image\n\ntry:\n    forecast_prophet = pd.read_csv('forecast_prophet.csv')\n    forecast_prophet['ds'] = pd.to_datetime(forecast_prophet['ds'])\n    display(Image(filename='components.png'))\nexcept FileNotFoundError:\n    import logging\n    import cmdstanpy\n    logger = logging.getLogger('cmdstanpy')\n    logger.addHandler(logging.NullHandler())\n    logger.propagate = False\n    logger.setLevel(logging.CRITICAL)\n    \n    from prophet import Prophet\n    \n    traffic_prophet = (\n        traffic\n        .to_pandas()\n        .rename(\n            {'traffic_volume': 'y'}, axis='columns'\n        )\n    )\n    \n    model_forecast_prophet = Prophet(seasonality_mode='multiplicative')\n    model_forecast_prophet.fit(traffic_prophet[traffic_prophet.ds&lt;split_date])\n\n    future = pd.DataFrame(traffic_prophet[traffic_prophet.ds&gt;=split_date].ds)\n\n    forecast_prophet = model_forecast_prophet.predict(future)\n\n    forecast_prophet.to_csv('forecast_prophet.csv',\n                            encoding='utf-8', index=False)\n    model_forecast_prophet.plot_components(forecast_prophet)\n    pd.plotting.register_matplotlib_converters()\n</pre> # h-step ahead forecast with Prophet from IPython.display import Image  try:     forecast_prophet = pd.read_csv('forecast_prophet.csv')     forecast_prophet['ds'] = pd.to_datetime(forecast_prophet['ds'])     display(Image(filename='components.png')) except FileNotFoundError:     import logging     import cmdstanpy     logger = logging.getLogger('cmdstanpy')     logger.addHandler(logging.NullHandler())     logger.propagate = False     logger.setLevel(logging.CRITICAL)          from prophet import Prophet          traffic_prophet = (         traffic         .to_pandas()         .rename(             {'traffic_volume': 'y'}, axis='columns'         )     )          model_forecast_prophet = Prophet(seasonality_mode='multiplicative')     model_forecast_prophet.fit(traffic_prophet[traffic_prophet.ds=split_date].ds)      forecast_prophet = model_forecast_prophet.predict(future)      forecast_prophet.to_csv('forecast_prophet.csv',                             encoding='utf-8', index=False)     model_forecast_prophet.plot_components(forecast_prophet)     pd.plotting.register_matplotlib_converters() <p>A typical Prophet components graph.</p> <p>Seasonalities are now applied multiplicatively, but the combination of the totally bottoming out daily cycle and the negative additive trend will result in negative predictions nontheless.</p> <p>Score the Prophet model.</p> In\u00a0[29]: Copied! <pre>r2_forecast_prophet = stats.pearsonr(forecast_prophet['yhat'].values,\n                                     time_series.test.population.traffic_volume.to_numpy())[0]**2\nprint('R2:', r2_forecast_prophet)\n</pre> r2_forecast_prophet = stats.pearsonr(forecast_prophet['yhat'].values,                                      time_series.test.population.traffic_volume.to_numpy())[0]**2 print('R2:', r2_forecast_prophet) <pre>R2: 0.8244497208087814\n</pre> In\u00a0[30]: Copied! <pre>scores = [getml_score.rsquared, r2_prophet, r2_forecast_prophet]\n\npd.DataFrame(data={    \n    'Name': ['getML', 'prophet 1-step-ahead', 'prophet forecast'],\n    'R-squared': [f'{score:.2%}' for score in scores]\n})\n</pre> scores = [getml_score.rsquared, r2_prophet, r2_forecast_prophet]  pd.DataFrame(data={         'Name': ['getML', 'prophet 1-step-ahead', 'prophet forecast'],     'R-squared': [f'{score:.2%}' for score in scores] }) Out[30]: Name R-squared 0 getML 98.14% 1 prophet 1-step-ahead 83.32% 2 prophet forecast 82.44%","boost":0.8},{"location":"examples/enterprise-notebooks/interstate94/#interstate-94-multivariate-time-series-prediction","title":"Interstate 94 - Multivariate time series prediction\u00b6","text":"<p>In this tutorial, we demonstrate a time series application of getML. We predict the hourly traffic volume on I-94 westbound from Minneapolis-St Paul and benchmark our results against Facebook's Prophet. getML's relational learning algorithms outperform Prophet's classical time series approach by ~15%.</p> <p>Summary:</p> <ul> <li>Prediction type: Regression model</li> <li>Domain: Transportation</li> <li>Prediction target: Hourly traffic volume</li> <li>Source data: Multivariate time series, 5 components</li> <li>Population size: 24096</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/interstate94/#background","title":"Background\u00b6","text":"<p>The dataset features some particularly interesting characteristics common for time series, which classical models may struggle to deal with appropriately. Such characteristics are:</p> <ul> <li>High frequency (hourly)</li> <li>Dependence on irregular events (holidays)</li> <li>Strong and overlapping cycles (daily, weekly)</li> <li>Anomalies</li> <li>Multiple seasonalities</li> </ul> <p>The analysis is built on top of a dataset provided by the MN Department of Transportation, with some data preparation done by John Hogue.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/interstate94/#analysis","title":"Analysis\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/interstate94/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/interstate94/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>Downloading the raw data and convert it into a prediction ready format takes time. To get to the getML model building as fast as possible, we prepared the data for you and excluded the code from this notebook. It is made available in the example notebook featuring the full analysis. We only include data after 2016 and introduced a fixed train/test split at 80% of the available data.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/interstate94/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"<p>The <code>getml.datasets.load_interstate94</code> method took care of the entire data preparation:</p> <ul> <li>Downloads csv's from our servers into python</li> <li>Converts csv's to getML DataFrames</li> <li>Sets roles &amp; units to columns inside getML DataFrames</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/interstate94/#13-define-relational-model","title":"1.3 Define relational model\u00b6","text":"<p>To start with relational learning, we need to specify the data model. We manually replicate the appropriate time series structure by setting time series related join conditions (<code>horizon</code>, <code>memory</code> and <code>allow_lagged_targets</code>). We use the high-level time series api for this.</p> <p>Under the hood, the time series api abstracts away a self cross join of the population table (<code>traffic</code>) that allows getML's feature learning algorithms to learn patterns from past observations.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/interstate94/#2predictive-modeling","title":"2.Predictive modeling\u00b6","text":"<p>We loaded the data, defined the roles, units and the abstract data model. Next, we create a getML pipeline for relational learning.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/interstate94/#21-getml-pipeline","title":"2.1 getML Pipeline\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/interstate94/#22-model-training","title":"2.2 Model training\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/interstate94/#23-model-evaluation","title":"2.3 Model evaluation\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/interstate94/#24-studying-features","title":"2.4 Studying features\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/interstate94/#25-features","title":"2.5 Features\u00b6","text":"<p>The most important feature looks as follows:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/interstate94/#26-productionization","title":"2.6 Productionization\u00b6","text":"<p>It is possible to productionize the pipeline by transpiling the features into production-ready SQL code. Please also refer to getML's <code>sqlite3</code> module.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/interstate94/#3-benchmarks-against-prophet","title":"3. Benchmarks against Prophet\u00b6","text":"<p>By design, Prophet isn't capable of delivering the 1-step ahead predictions we did with getML. In order to retrieve a benchmark in the 1-step case nonetheless, we mimic 1-step ahead predictions through cross validating the model on a rolling origin. This clearly gives Prophet an advantage as all information up to the origin is incorporated when fitting the model and a new fit is calculated for every 1-step ahead forecast. Prophet's performance thus has to be viewed as an upper bound. Further, as noted above, we thought it would be interesting to let Multirel and Relboost figure out time based patterns by itself if we provide only deterministic components. So, in a second step, we benchmark this case against Prophet. For both tools, we use very simple models with all hyperparameters set to their default values.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/interstate94/#4-conclusion","title":"4. Conclusion\u00b6","text":"<p>Benchmarks against Prophet</p> <p>By design, Prophet isn't capable of delivering the 1-step ahead predictions we did with getML. To retrieve a benchmark in the 1-step case nonetheless, we mimic 1-step ahead predictions through cross-validating the model on a rolling origin. This gives Prophet an advantage as all information up to the origin is incorporated when fitting the model and a new fit is calculated for every 1-step ahead forecast.</p> <p>Results</p> <p>We have benchmarked getML against Facebook\u2019s Prophet library on a univariate time series with strong seasonal components. Prophet is made for exactly these sort of data sets, so you would expect this to be a home run for Prophet. The opposite is true - getML\u2019s relational learning algorithms outperform Prophet's 1-step ahead predictions by ~15 percentage points:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/loans/","title":"<span class=\"ntitle\">loans.ipynb</span> <span class=\"ndesc\">Predicting loan default risk</span>","text":"<p>Let's get started with the analysis and set-up your session:</p> In\u00a0[1]: Copied! <pre>import os\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image, Markdown\n\n%matplotlib inline\n\nimport getml\n\ngetml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.engine.set_project(\"loans\")\n</pre> import os from pathlib import Path  import matplotlib.pyplot as plt import numpy as np import pandas as pd from IPython.display import Image, Markdown  %matplotlib inline  import getml  getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.engine.set_project(\"loans\") <pre>getML engine is already running.\n\nConnected to project 'loans'\n</pre> In\u00a0[2]: Copied! <pre>population_train, population_test, order, trans, meta = getml.datasets.load_loans(roles=True, units=True)\n</pre> population_train, population_test, order, trans, meta = getml.datasets.load_loans(roles=True, units=True) <pre>\nLoading population_train...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nLoading population_test...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nLoading order...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nLoading trans...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\nLoading meta...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n</pre> <p>Data visualization</p> <p>To simplify the notebook, original data model (image below) is condensed into 4 tables, by resolving the trivial one-to-one and many-to-one joins:</p> <ul> <li>A population table <code>population_{train, test}</code>, consiting of <code>loan</code> and <code>account</code> tables</li> <li>Three peripheral tables: <code>order</code>, <code>trans</code>, and <code>meta</code>.</li> <li>Whereas <code>meta</code> is made up of <code>card</code>, <code>client</code>, <code>disp</code> and <code>district</code></li> </ul> In\u00a0[3]: Copied! <pre>Image(\"assets/loans-schema.png\", width=500)\n</pre> Image(\"assets/loans-schema.png\", width=500) Out[3]: <p>Population table</p> <ul> <li>Information on the loan itself (duration, amount, date, ...)</li> <li>Geo-information about the branch where the loans was granted (A**)</li> <li>Column <code>status</code> contains binary target. Levels [A, C] := loan paid back and [B, D] := loan default; we recoded status to our binary target: <code>default</code></li> </ul> In\u00a0[4]: Copied! <pre>population_train.set_role(\"date_loan\", \"time_stamp\")\npopulation_test.set_role(\"date_loan\", \"time_stamp\")\n</pre> population_train.set_role(\"date_loan\", \"time_stamp\") population_test.set_role(\"date_loan\", \"time_stamp\") In\u00a0[5]: Copied! <pre>population_test\n</pre> population_test Out[5]: name                   date_loan account_id default frequency           duration  payments    amount      loan_id  district_id date_account status        role                  time_stamp   join_key  target categorical        numerical numerical numerical unused_float unused_float unused_float unused_string unit time stamp, comparison only     money 0 1996-04-29 19 1 POPLATEK MESICNE 12 2523 30276 4961 21 1995 B 1 1998-10-14 37 1 POPLATEK MESICNE 60 5308 318480 4967 20 1997 D 2 1998-04-19 38 0 POPLATEK TYDNE 48 2307 110736 4968 19 1997 C 3 1997-08-10 97 0 POPLATEK MESICNE 12 8573 102876 4986 74 1996 A 4 1996-11-06 132 0 POPLATEK PO OBRATU 12 7370 88440 4996 40 1996 A ... ... ... ... ... ... ... ... ... ... ... 218 1995-12-04 11042 0 POPLATEK MESICNE 36 6032 217152 7243 72 1995 A 219 1996-08-20 11054 0 POPLATEK TYDNE 60 2482 148920 7246 59 1996 C 220 1994-01-31 11111 0 POPLATEK MESICNE 36 3004 108144 7259 1 1993 A 221 1998-11-22 11317 0 POPLATEK MESICNE 60 5291 317460 7292 50 1997 C 222 1996-12-27 11362 0 POPLATEK MESICNE 24 5392 129408 7308 67 1995 A <p>     223 rows x 11 columns     memory usage: 0.02 MB     name: population_test     type: getml.DataFrame </p> <p>Peripheral tables</p> <ul> <li><code>meta</code><ul> <li>Meta info about the client (card_type, gender, ...)</li> <li>Geo-information about the client</li> </ul> </li> <li><code>order</code><ul> <li>Permanent orders related to a loan (amount, balance, ...)</li> </ul> </li> <li><code>trans</code><ul> <li>Transactions related to a given loan (amount, ...)</li> </ul> </li> </ul> <p>While the contents of <code>meta</code> and <code>order</code> are omitted for brevity, here are contents of <code>trans</code>:</p> In\u00a0[6]: Copied! <pre>trans\n</pre> trans Out[6]:    name                        date account_id type        k_symbol    bank        operation        amount   balance     trans_id account          role                  time_stamp   join_key categorical categorical categorical categorical   numerical numerical unused_float unused_string    unit time stamp, comparison only     money 0 1995-03-24 1 PRIJEM NULL NULL VKLAD 1000 1000 1 NULL 1 1995-04-13 1 PRIJEM NULL AB PREVOD Z UCTU 3679 4679 5 41403269.0 2 1995-05-13 1 PRIJEM NULL AB PREVOD Z UCTU 3679 20977 6 41403269.0 3 1995-06-13 1 PRIJEM NULL AB PREVOD Z UCTU 3679 26835 7 41403269.0 4 1995-07-13 1 PRIJEM NULL AB PREVOD Z UCTU 3679 30415 8 41403269.0 ... ... ... ... ... ... ... ... ... ... 1056315 1998-08-31 10451 PRIJEM UROK NULL NULL 62 17300 3682983 NULL 1056316 1998-09-30 10451 PRIJEM UROK NULL NULL 49 13442 3682984 NULL 1056317 1998-10-31 10451 PRIJEM UROK NULL NULL 34 10118 3682985 NULL 1056318 1998-11-30 10451 PRIJEM UROK NULL NULL 26 8398 3682986 NULL 1056319 1998-12-31 10451 PRIJEM UROK NULL NULL 42 13695 3682987 NULL <p>     1056320 rows x 10 columns     memory usage: 67.20 MB     name: trans     type: getml.DataFrame </p> In\u00a0[7]: Copied! <pre>star_schema = getml.data.StarSchema(\n    train=population_train, test=population_test, alias=\"population\"\n)\n\nstar_schema.join(\n    trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\"),\n)\n\nstar_schema.join(\n    order,\n    on=\"account_id\",\n)\n\nstar_schema.join(\n    meta,\n    on=\"account_id\",\n)\n\nstar_schema\n</pre> star_schema = getml.data.StarSchema(     train=population_train, test=population_test, alias=\"population\" )  star_schema.join(     trans,     on=\"account_id\",     time_stamps=(\"date_loan\", \"date\"), )  star_schema.join(     order,     on=\"account_id\", )  star_schema.join(     meta,     on=\"account_id\", )  star_schema Out[7]: data model diagram transordermetapopulationaccount_id = account_iddate &lt;= date_loanaccount_id = account_idaccount_id = account_id staging data frames staging table               0 population POPULATION__STAGING_TABLE_1 1 meta META__STAGING_TABLE_2 2 order ORDER__STAGING_TABLE_3 3 trans TRANS__STAGING_TABLE_4 container population subset name             rows type      0 train population_train 459 DataFrame 1 test population_test 223 DataFrame peripheral name     rows type      0 trans 1056320 DataFrame 1 order 6471 DataFrame 2 meta 5369 DataFrame In\u00a0[8]: Copied! <pre>meta\n</pre> meta Out[8]: name account_id type_disp   type_card   gender      A3                     A4        A5        A6        A7        A8        A9       A10       A11       A12       A13       A14       A15       A16      disp_id    client_id   birth_date  district_id card_id       issued        A2              role   join_key categorical categorical categorical categorical     numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical unused_float unused_float unused_float unused_float unused_string unused_string unused_string   0 1 OWNER F south Bohemia 70699 60 13 2 1 4 65.3 8968 2.8 3.35 131 1740 1910 1 1 1970 18 NULL NULL Pisek 1 2 OWNER M Prague 1204953 0 0 0 1 1 100 12541 0.2 0.43 167 85677 99107 2 2 1945 1 NULL NULL Hl.m. Praha 2 2 DISPONENT F Prague 1204953 0 0 0 1 1 100 12541 0.2 0.43 167 85677 99107 3 3 1940 1 NULL NULL Hl.m. Praha 3 3 OWNER M central Bohemia 95616 65 30 4 1 6 51.4 9307 3.8 4.43 118 2616 3040 4 4 1956 5 NULL NULL Kolin 4 3 DISPONENT F central Bohemia 95616 65 30 4 1 6 51.4 9307 3.8 4.43 118 2616 3040 5 5 1960 5 NULL NULL Kolin ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 5364 11349 OWNER F Prague 1204953 0 0 0 1 1 100 12541 0.2 0.43 167 85677 99107 13647 13955 1945 1 NULL NULL Hl.m. Praha 5365 11349 DISPONENT M Prague 1204953 0 0 0 1 1 100 12541 0.2 0.43 167 85677 99107 13648 13956 1943 1 NULL NULL Hl.m. Praha 5366 11359 OWNER classic M south Moravia 117897 139 28 5 1 6 53.8 8814 4.7 5.74 107 2112 2059 13660 13968 1968 61 1247.0 1995-06-13 Trebic 5367 11362 OWNER F north Moravia 106054 38 25 6 2 6 63.1 8110 5.7 6.55 109 3244 3079 13663 13971 1962 67 NULL NULL Bruntal 5368 11382 OWNER F north Moravia 323870 0 0 0 1 1 100 10673 4.7 5.44 100 18782 18347 13690 13998 1953 74 NULL NULL Ostrava - mesto <p>     5369 rows x 25 columns     memory usage: 1.05 MB     name: meta     type: getml.DataFrame </p> <p>Set-up of feature learners, selectors &amp; predictor</p> In\u00a0[9]: Copied! <pre>mapping = getml.preprocessors.Mapping(min_freq=100)\n\nfast_prop = getml.feature_learning.FastProp(\n    aggregation=getml.feature_learning.FastProp.agg_sets.All,\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    num_threads=1,\n)\n\nfeature_selector = getml.predictors.XGBoostClassifier(n_jobs=1)\n\n# the population is really small, so we set gamma to mitigate overfitting\npredictor = getml.predictors.XGBoostClassifier(gamma=2, n_jobs=1,)\n</pre> mapping = getml.preprocessors.Mapping(min_freq=100)  fast_prop = getml.feature_learning.FastProp(     aggregation=getml.feature_learning.FastProp.agg_sets.All,     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     num_threads=1, )  feature_selector = getml.predictors.XGBoostClassifier(n_jobs=1)  # the population is really small, so we set gamma to mitigate overfitting predictor = getml.predictors.XGBoostClassifier(gamma=2, n_jobs=1,) <p>Build the pipeline</p> In\u00a0[10]: Copied! <pre>pipe = getml.pipeline.Pipeline(\n    data_model=star_schema.data_model,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=predictor,\n)\n</pre> pipe = getml.pipeline.Pipeline(     data_model=star_schema.data_model,     preprocessors=[mapping],     feature_learners=[fast_prop],     feature_selectors=[feature_selector],     predictors=predictor, ) In\u00a0[11]: Copied! <pre>pipe.fit(star_schema.train)\n</pre> pipe.fit(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 808 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as feature selector... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:0.970731\n\n</pre> Out[11]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=['XGBoostClassifier'],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['meta', 'order', 'trans'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['container-6Bk0Du'])</pre> In\u00a0[12]: Copied! <pre>pipe.score(star_schema.test)\n</pre> pipe.score(star_schema.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[12]: date time           set used target  accuracy     auc cross entropy 0 2024-02-21 15:01:29 train default 0.9978 0.9999 0.06221 1 2024-02-21 15:01:29 test default 0.9596 0.9314 0.1415 <p>Visualizing the learned features</p> <p>The feature with the highest importance is:</p> In\u00a0[13]: Copied! <pre>by_importances = pipe.features.sort(by=\"importances\")\nby_importances[0].sql\n</pre> by_importances = pipe.features.sort(by=\"importances\") by_importances[0].sql Out[13]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_29\";\n\nCREATE TABLE \"FEATURE_1_29\" AS\nSELECT Q1( t2.\"balance\" ) AS \"feature_1_29\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"TRANS__STAGING_TABLE_4\" t2\nON t1.\"account_id\" = t2.\"account_id\"\nWHERE t2.\"date\" &lt;= t1.\"date_loan\"\nGROUP BY t1.rowid;\n</pre> <p>Feature correlations</p> <p>We want to analyze how the features are correlated with the target variable.</p> In\u00a0[14]: Copied! <pre>names, correlations = pipe.features[:50].correlations()\n\nfig, ax = plt.subplots(figsize=(20, 10))\n\nax.bar(names, correlations)\n\nax.set_title(\"feature correlations\")\nax.set_xlabel(\"feature\")\nax.set_ylabel(\"correlation\")\nax.tick_params(axis=\"x\", rotation=90)\n</pre> names, correlations = pipe.features[:50].correlations()  fig, ax = plt.subplots(figsize=(20, 10))  ax.bar(names, correlations)  ax.set_title(\"feature correlations\") ax.set_xlabel(\"feature\") ax.set_ylabel(\"correlation\") ax.tick_params(axis=\"x\", rotation=90) <p>Feature importances</p> <p>Feature importances are calculated by analyzing the improvement in predictive accuracy on each node of the trees in the XGBoost predictor. They are then normalized, so that all importances add up to 100%.</p> In\u00a0[15]: Copied! <pre>names, importances = pipe.features[:50].importances()\n\nfig, ax = plt.subplots(figsize=(20, 10))\n\nax.bar(names, importances)\n\nax.set_title(\"feature importances\")\nax.set_xlabel(\"feature\")\nax.set_ylabel(\"importance\")\nax.tick_params(axis=\"x\", rotation=90)\n</pre> names, importances = pipe.features[:50].importances()  fig, ax = plt.subplots(figsize=(20, 10))  ax.bar(names, importances)  ax.set_title(\"feature importances\") ax.set_xlabel(\"feature\") ax.set_ylabel(\"importance\") ax.tick_params(axis=\"x\", rotation=90) <p>Column importances</p> <p>Because getML uses relational learning, we can apply the principles we used to calculate the feature importances to individual columns as well.</p> <p>As we can see, a lot of the predictive power stems from the account balance. This is unsurprising: People with less money on their bank accounts are more likely to default on their loans.</p> In\u00a0[16]: Copied! <pre>names, importances = pipe.columns.importances()\n\nfig, ax = plt.subplots(figsize=(20, 10))\n\nax.bar(names, importances)\n\nax.set_title(\"column importances\")\nax.set_xlabel(\"column\")\nax.set_ylabel(\"importance\")\nax.tick_params(axis=\"x\", rotation=90)\n</pre> names, importances = pipe.columns.importances()  fig, ax = plt.subplots(figsize=(20, 10))  ax.bar(names, importances)  ax.set_title(\"column importances\") ax.set_xlabel(\"column\") ax.set_ylabel(\"importance\") ax.tick_params(axis=\"x\", rotation=90) <p>The most important feature looks as follows:</p> In\u00a0[17]: Copied! <pre>pipe.features.to_sql()[pipe.features.sort(by=\"importances\")[0].name]\n</pre> pipe.features.to_sql()[pipe.features.sort(by=\"importances\")[0].name] Out[17]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_29\";\n\nCREATE TABLE \"FEATURE_1_29\" AS\nSELECT Q1( t2.\"balance\" ) AS \"feature_1_29\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"TRANS__STAGING_TABLE_4\" t2\nON t1.\"account_id\" = t2.\"account_id\"\nWHERE t2.\"date\" &lt;= t1.\"date_loan\"\nGROUP BY t1.rowid;\n</pre> In\u00a0[18]: Copied! <pre># Creates a folder named loans_pipeline containing\n# the SQL code.\npipe.features.to_sql().save(\"loans_pipeline\", remove=True)\n</pre> # Creates a folder named loans_pipeline containing # the SQL code. pipe.features.to_sql().save(\"loans_pipeline\", remove=True) In\u00a0[19]: Copied! <pre># Creates a folder named baseball_pipeline_spark containing\n# the SQL code.\npipe.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"loans_pipeline_spark\", remove=True)\n</pre> # Creates a folder named baseball_pipeline_spark containing # the SQL code. pipe.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"loans_pipeline_spark\", remove=True) <p>By applying getML to the PKDD'99 Financial dataset, we were able to show the power and relevance of Relational Learning on a real-world data set. Within a training time below 1 minute, we outperformed almost all approaches based on manually generated features. This makes getML the prime choice when dealing with complex relational data schemes. This result holds independent of the problem domain since no expertise in the financial sector was used in this analysis.</p> <p>The present analysis could be improved in two directions. By performing an extensive hyperparameter optimization, the out of sample AUC could be further improved. On the other hand, the hyperparameters could be tuned to produce less complex features that result in worse performance (in terms of AUC) but are better interpretable by humans.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/loans/#loans-predicting-loan-default-risk","title":"Loans - Predicting loan default risk\u00b6","text":"<p>This notebook demonstrates the application of our relational learning algorithm to predict if a customer of a bank will default on his loan. We train the predictor on customer metadata, transaction history, as well as other successful and unsuccessful loans.</p> <p>Summary:</p> <ul> <li>Prediction type: Binary classification</li> <li>Domain: Finance</li> <li>Prediction target: Loan default</li> <li>Source data: 8 tables, 78.8 MB</li> <li>Population size: 682</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/loans/#background","title":"Background\u00b6","text":"<p>This notebook features a textbook example of predictive analytics applied to the financial sector. A loan is the lending of money to companies or individuals. Banks grant loans in exchange for the promise of repayment. Loan default is defined as the failure to meet this legal obligation, for example, when a home buyer fails to make a mortgage payment. A bank needs to estimate the risk it carries when granting loans to potentially non-performing customers.</p> <p>The analysis is based on the financial dataset from the the CTU Prague Relational Learning Repository (Motl and Schulte, 2015) (Now residing at relational-data.org.) .</p>","boost":0.8},{"location":"examples/enterprise-notebooks/loans/#analysis","title":"Analysis\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/loans/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/loans/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>Downloading the raw data from the CTU Prague Relational Learning Repository into a prediction ready format takes time. To get to the getML model building as fast as possible, we prepared the data for you and excluded the code from this notebook. It will be made available in a future version.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/loans/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"<p>The <code>getml.datasets.load_loans</code> method took care of the entire data lifting:</p> <ul> <li>Downloads csv's from our servers in python</li> <li>Converts csv's to getML DataFrames</li> <li>Sets roles to columns inside getML DataFrames</li> </ul> <p>The only thing left is to set units to columns that the relational learning algorithm is allowed to compare to each other.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/loans/#13-define-relational-model","title":"1.3 Define relational model\u00b6","text":"<p>To start with relational learning, we need to specify an abstract data model. Here, we use the high-level star schema API that allows us to define the abstract data model and construct a container with the concrete data at one-go. While a simple <code>StarSchema</code> indeed works in many cases, it is not sufficient for more complex data models like schoflake schemas, where you would have to define the data model and construct the container in separate steps, by utilzing getML's full-fledged data model and container APIs respectively.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/loans/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"<p>We loaded the data, defined the roles, units and the abstract data model. Next, we create a getML pipeline for relational learning.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/loans/#21-getml-pipeline","title":"2.1 getML Pipeline\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/loans/#22-model-training","title":"2.2 Model training\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/loans/#23-model-evaluation","title":"2.3 Model evaluation\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/loans/#24-studying-features","title":"2.4 Studying features\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/loans/#25-productionization","title":"2.5 Productionization\u00b6","text":"<p>It is possible to productionize the pipeline by transpiling the features into production-ready SQL code. Please also refer to getML's <code>sqlite3</code> and <code>spark</code> modules.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/loans/#3-conclusion","title":"3. Conclusion\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/loans/#references","title":"References\u00b6","text":"<p>Schulte, Oliver, et al. \"A hierarchy of independence assumptions for multi-relational Bayes net classifiers.\" 2013 IEEE Symposium on Computational Intelligence and Data Mining (CIDM). IEEE, 2013.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/movie_lens/","title":"<span class=\"ntitle\">movie_lens.ipynb</span> <span class=\"ndesc\">Predicting a user's gender based on the movies they have watched</span>","text":"<p>Let's get started with the analysis and set up your session:</p> In\u00a0[1]: Copied! <pre>import copy\nimport os\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\nfrom pathlib import Path\n\nfrom urllib import request\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline  \n\nimport getml\n\ngetml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.engine.set_project('MovieLens')\n</pre> import copy import os os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\" from pathlib import Path  from urllib import request  import numpy as np import pandas as pd from IPython.display import Image import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline    import getml  getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.engine.set_project('MovieLens') <pre>getML engine is already running.\n\nConnected to project 'MovieLens'\n</pre> In\u00a0[2]: Copied! <pre>conn = getml.database.connect_mysql(\n    host=\"db.relational-data.org\",\n    dbname=\"imdb_MovieLens\",\n    port=3306,\n    user=\"guest\",\n    password=\"relational\"\n)\n\nconn\n</pre> conn = getml.database.connect_mysql(     host=\"db.relational-data.org\",     dbname=\"imdb_MovieLens\",     port=3306,     user=\"guest\",     password=\"relational\" )  conn Out[2]: <pre>Connection(dbname='imdb_MovieLens',\n           dialect='mysql',\n           host='db.relational-data.org',\n           port=3306)</pre> In\u00a0[3]: Copied! <pre>def load_if_needed(name):\n    \"\"\"\n    Loads the data from the relational learning\n    repository, if the data frame has not already\n    been loaded.\n    \"\"\"\n    if not getml.data.exists(name):\n        data_frame = getml.data.DataFrame.from_db(\n            name=name,\n            table_name=name,\n            conn=conn\n        )\n        data_frame.save()\n    else:\n        data_frame = getml.data.load_data_frame(name)\n    return data_frame\n</pre> def load_if_needed(name):     \"\"\"     Loads the data from the relational learning     repository, if the data frame has not already     been loaded.     \"\"\"     if not getml.data.exists(name):         data_frame = getml.data.DataFrame.from_db(             name=name,             table_name=name,             conn=conn         )         data_frame.save()     else:         data_frame = getml.data.load_data_frame(name)     return data_frame In\u00a0[4]: Copied! <pre>users = load_if_needed(\"users\")\nu2base = load_if_needed(\"u2base\")\nmovies = load_if_needed(\"movies\")\nmovies2directors = load_if_needed(\"movies2directors\")\ndirectors = load_if_needed(\"directors\")\nmovies2actors = load_if_needed(\"movies2actors\")\nactors = load_if_needed(\"actors\")\n</pre> users = load_if_needed(\"users\") u2base = load_if_needed(\"u2base\") movies = load_if_needed(\"movies\") movies2directors = load_if_needed(\"movies2directors\") directors = load_if_needed(\"directors\") movies2actors = load_if_needed(\"movies2actors\") actors = load_if_needed(\"actors\") <p>getML requires that we define roles for each of the columns.</p> In\u00a0[5]: Copied! <pre>users[\"target\"] = (users.u_gender == 'F')\n</pre> users[\"target\"] = (users.u_gender == 'F') In\u00a0[6]: Copied! <pre>users.set_role(\"userid\", getml.data.roles.join_key)\nusers.set_role(\"age\", getml.data.roles.numerical)\nusers.set_role(\"occupation\", getml.data.roles.categorical)\nusers.set_role(\"target\", getml.data.roles.target)\n\nusers.save()\n</pre> users.set_role(\"userid\", getml.data.roles.join_key) users.set_role(\"age\", getml.data.roles.numerical) users.set_role(\"occupation\", getml.data.roles.categorical) users.set_role(\"target\", getml.data.roles.target)  users.save() Out[6]: name   userid target occupation        age u_gender      role join_key target categorical numerical unused_string 0 1 1 2 1 F 1 51 1 2 1 F 2 75 1 2 1 F 3 86 1 2 1 F 4 99 1 2 1 F ... ... ... ... ... 6034 5658 0 5 56 M 6035 5669 0 5 56 M 6036 5703 0 5 56 M 6037 5948 0 5 56 M 6038 5980 0 5 56 M <p>     6039 rows x 5 columns     memory usage: 0.21 MB     name: users     type: getml.DataFrame </p> In\u00a0[7]: Copied! <pre>u2base.set_role([\"userid\", \"movieid\"], getml.data.roles.join_key)\nu2base.set_role(\"rating\", getml.data.roles.numerical)\n\nu2base.save()\n</pre> u2base.set_role([\"userid\", \"movieid\"], getml.data.roles.join_key) u2base.set_role(\"rating\", getml.data.roles.numerical)  u2base.save() Out[7]:   name   userid  movieid    rating   role join_key join_key numerical 0 2 1964242 1 1 2 2219779 1 2 3 1856939 1 3 4 2273044 1 4 5 1681655 1 ... ... ... 996154 6040 2560616 5 996155 6040 2564194 5 996156 6040 2581228 5 996157 6040 2581428 5 996158 6040 2593112 5 <p>     996159 rows x 3 columns     memory usage: 15.94 MB     name: u2base     type: getml.DataFrame </p> In\u00a0[8]: Copied! <pre>movies.set_role(\"movieid\", getml.data.roles.join_key)\nmovies.set_role([\"year\", \"runningtime\"], getml.data.roles.numerical)\nmovies.set_role([\"isEnglish\", \"country\"], getml.data.roles.categorical)\n\nmovies.save()\n</pre> movies.set_role(\"movieid\", getml.data.roles.join_key) movies.set_role([\"year\", \"runningtime\"], getml.data.roles.numerical) movies.set_role([\"isEnglish\", \"country\"], getml.data.roles.categorical)  movies.save() Out[8]: name  movieid isEnglish   country          year runningtime role join_key categorical categorical numerical   numerical 0 1672052 T other 3 2 1 1672111 T other 4 2 2 1672580 T USA 4 3 3 1672716 T USA 4 2 4 1672946 T USA 4 0 ... ... ... ... ... 3827 2591814 T other 4 2 3828 2592334 T USA 4 2 3829 2592963 F France 2 2 3830 2593112 T USA 4 1 3831 2593313 F other 4 3 <p>     3832 rows x 5 columns     memory usage: 0.11 MB     name: movies     type: getml.DataFrame </p> In\u00a0[9]: Copied! <pre>movies2directors.set_role([\"movieid\", \"directorid\"], getml.data.roles.join_key)\nmovies2directors.set_role( \"genre\", getml.data.roles.categorical)\n\nmovies2directors.save()\n</pre> movies2directors.set_role([\"movieid\", \"directorid\"], getml.data.roles.join_key) movies2directors.set_role( \"genre\", getml.data.roles.categorical)  movies2directors.save() Out[9]: name  movieid directorid genre       role join_key   join_key categorical 0 1672111 54934 Action 1 1672946 188940 Action 2 1679461 179783 Action 3 1691387 291700 Action 4 1693305 14663 Action ... ... ... 4136 2570825 265215 Other 4137 2572478 149311 Other 4138 2577062 304827 Other 4139 2590181 270707 Other 4140 2591814 57348 Other <p>     4141 rows x 3 columns     memory usage: 0.05 MB     name: movies2directors     type: getml.DataFrame </p> In\u00a0[10]: Copied! <pre>directors.set_role(\"directorid\", getml.data.roles.join_key)\ndirectors.set_role([\"d_quality\", \"avg_revenue\"], getml.data.roles.numerical)\n\ndirectors.save()\n</pre> directors.set_role(\"directorid\", getml.data.roles.join_key) directors.set_role([\"d_quality\", \"avg_revenue\"], getml.data.roles.numerical)  directors.save() Out[10]: name directorid d_quality avg_revenue role   join_key numerical   numerical 0 67 4 1 1 92 2 3 2 284 4 0 3 708 4 1 4 746 4 4 ... ... ... 2196 305962 4 4 2197 305978 4 2 2198 306168 3 2 2199 306343 4 1 2200 306351 4 1 <p>     2201 rows x 3 columns     memory usage: 0.04 MB     name: directors     type: getml.DataFrame </p> In\u00a0[11]: Copied! <pre>movies2actors.set_role([\"movieid\", \"actorid\"], getml.data.roles.join_key)\nmovies2actors.set_role( \"cast_num\", getml.data.roles.numerical)\n\nmovies2actors.save()\n</pre> movies2actors.set_role([\"movieid\", \"actorid\"], getml.data.roles.join_key) movies2actors.set_role( \"cast_num\", getml.data.roles.numerical)  movies2actors.save() Out[11]:   name  movieid  actorid  cast_num   role join_key join_key numerical 0 1672580 981535 0 1 1672946 1094968 0 2 1673647 149985 0 3 1673647 261595 0 4 1673647 781357 0 ... ... ... 138344 2593313 947005 3 138345 2593313 1090590 3 138346 2593313 1347419 3 138347 2593313 2099917 3 138348 2593313 2633550 3 <p>     138349 rows x 3 columns     memory usage: 2.21 MB     name: movies2actors     type: getml.DataFrame </p> <p>We need to separate our data set into a training, testing and validation set:</p> In\u00a0[12]: Copied! <pre>actors.set_role(\"actorid\", getml.data.roles.join_key)\nactors.set_role(\"a_quality\", getml.data.roles.numerical)\nactors.set_role(\"a_gender\", getml.data.roles.categorical)\n\nactors.save()\n</pre> actors.set_role(\"actorid\", getml.data.roles.join_key) actors.set_role(\"a_quality\", getml.data.roles.numerical) actors.set_role(\"a_gender\", getml.data.roles.categorical)  actors.save() Out[12]:  name  actorid a_gender    a_quality  role join_key categorical numerical 0 4 M 4 1 16 M 0 2 28 M 4 3 566 M 4 4 580 M 4 ... ... ... 98685 2749162 F 3 98686 2749168 F 3 98687 2749204 F 3 98688 2749377 F 4 98689 2749386 F 4 <p>     98690 rows x 3 columns     memory usage: 1.58 MB     name: actors     type: getml.DataFrame </p> In\u00a0[13]: Copied! <pre>split = getml.data.split.random(train=0.75, test=0.25)\nsplit\n</pre> split = getml.data.split.random(train=0.75, test=0.25) split Out[13]: 0 train 1 train 2 train 3 test 4 test ... <p>     infinite number of  rows          type: StringColumnView </p> In\u00a0[14]: Copied! <pre>container = getml.data.Container(population=users, split=split)\n\ncontainer.add(\n    u2base=u2base,\n    movies=movies,\n    movies2directors=movies2directors,\n    directors=directors,\n    movies2actors=movies2actors,\n    actors=actors,\n)\n\ncontainer\n</pre> container = getml.data.Container(population=users, split=split)  container.add(     u2base=u2base,     movies=movies,     movies2directors=movies2directors,     directors=directors,     movies2actors=movies2actors,     actors=actors, )  container Out[14]: population subset name  rows type 0 test users 1511 View 1 train users 4528 View peripheral name               rows type      0 u2base 996159 DataFrame 1 movies 3832 DataFrame 2 movies2directors 4141 DataFrame 3 directors 2201 DataFrame 4 movies2actors 138349 DataFrame 5 actors 98690 DataFrame In\u00a0[15]: Copied! <pre>dm = getml.data.DataModel(users.to_placeholder())\n\ndm.add(getml.data.to_placeholder(\n    u2base=u2base,\n    movies=movies,\n    movies2directors=movies2directors,\n    directors=directors,\n    movies2actors=movies2actors,\n    actors=actors,\n))\n\ndm.population.join(\n    dm.u2base,\n    on='userid'\n)\n\ndm.u2base.join(\n    dm.movies,\n    on='movieid',\n    relationship=getml.data.relationship.many_to_one\n)\n\ndm.movies.join(\n    dm.movies2directors,\n    on='movieid',\n    relationship=getml.data.relationship.propositionalization\n)\n\ndm.movies2directors.join(\n    dm.directors,\n    on='directorid',\n    relationship=getml.data.relationship.many_to_one\n)\n\ndm.movies.join(\n    dm.movies2actors,\n    on='movieid',\n    relationship=getml.data.relationship.propositionalization\n)\n\ndm.movies2actors.join(\n    dm.actors,\n    on='actorid',\n    relationship=getml.data.relationship.many_to_one\n)\n\ndm\n</pre> dm = getml.data.DataModel(users.to_placeholder())  dm.add(getml.data.to_placeholder(     u2base=u2base,     movies=movies,     movies2directors=movies2directors,     directors=directors,     movies2actors=movies2actors,     actors=actors, ))  dm.population.join(     dm.u2base,     on='userid' )  dm.u2base.join(     dm.movies,     on='movieid',     relationship=getml.data.relationship.many_to_one )  dm.movies.join(     dm.movies2directors,     on='movieid',     relationship=getml.data.relationship.propositionalization )  dm.movies2directors.join(     dm.directors,     on='directorid',     relationship=getml.data.relationship.many_to_one )  dm.movies.join(     dm.movies2actors,     on='movieid',     relationship=getml.data.relationship.propositionalization )  dm.movies2actors.join(     dm.actors,     on='actorid',     relationship=getml.data.relationship.many_to_one )  dm Out[15]: diagram directorsmovies2directorsactorsmovies2actorsmoviesu2baseusersdirectorid = directoridRelationship: many-to-oneactorid = actoridRelationship: many-to-onemovieid = movieidRelationship: propositionalizationmovieid = movieidRelationship: propositionalizationmovieid = movieidRelationship: many-to-oneuserid = userid staging data frames                 staging table                    0 users USERS__STAGING_TABLE_1 1 movies2actors, actors MOVIES2ACTORS__STAGING_TABLE_2 2 movies2directors, directors MOVIES2DIRECTORS__STAGING_TABLE_3 3 u2base, movies U2BASE__STAGING_TABLE_4 <p>Set-up the feature learner &amp; predictor</p> <p>We will set up two pipelines. One of them uses <code>FastProp</code>, the other one uses <code>Relboost</code>. Note that we have marked some of the joins in the data model with the <code>propositionalization</code> tag. This means that <code>FastProp</code> will be used for these relationships, even for the second pipeline. This can significantly speed up the training process.</p> In\u00a0[16]: Copied! <pre>mapping = getml.preprocessors.Mapping()\n\nfast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    num_threads=1,\n)\n\nrelboost = getml.feature_learning.Relboost(\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    num_subfeatures=50,\n    num_threads=1\n)\n\npredictor = getml.predictors.XGBoostClassifier(\n    max_depth=5,\n    n_jobs=1,\n)\n</pre> mapping = getml.preprocessors.Mapping()  fast_prop = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     num_threads=1, )  relboost = getml.feature_learning.Relboost(     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     num_subfeatures=50,     num_threads=1 )  predictor = getml.predictors.XGBoostClassifier(     max_depth=5,     n_jobs=1, ) <p>Build the pipeline</p> In\u00a0[17]: Copied! <pre>pipe1 = getml.pipeline.Pipeline(\n    tags=['fast_prop'],\n    data_model=dm,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    predictors=[predictor]\n)\n\npipe1\n</pre> pipe1 = getml.pipeline.Pipeline(     tags=['fast_prop'],     data_model=dm,     preprocessors=[mapping],     feature_learners=[fast_prop],     predictors=[predictor] )  pipe1 Out[17]: <pre>Pipeline(data_model='users',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['actors', 'directors', 'movies', 'movies2actors', 'movies2directors',\n                     'u2base'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['fast_prop'])</pre> In\u00a0[18]: Copied! <pre>pipe2 = getml.pipeline.Pipeline(\n    tags=['relboost'],\n    data_model=dm,\n    preprocessors=[mapping],\n    feature_learners=[relboost],\n    predictors=[predictor]\n)\n\npipe2\n</pre> pipe2 = getml.pipeline.Pipeline(     tags=['relboost'],     data_model=dm,     preprocessors=[mapping],     feature_learners=[relboost],     predictors=[predictor] )  pipe2 Out[18]: <pre>Pipeline(data_model='users',\n         feature_learners=['Relboost'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['actors', 'directors', 'movies', 'movies2actors', 'movies2directors',\n                     'u2base'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['relboost'])</pre> In\u00a0[19]: Copied! <pre>pipe1.check(container.train)\n</pre> pipe1.check(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:08, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 2 issues labeled INFO and 0 issues labeled WARNING.\n</pre> Out[19]: type label                  message                          0 INFO FOREIGN KEYS NOT FOUND When joining U2BASE__STAGING_TABLE_4 and MOVIES2DIRECTORS__STAGING_TABLE_3 over 'movieid' and 'movieid', there are no corresponding entries for 0.159513% of entries in 'movieid' in 'U2BASE__STAGING_TABLE_4'. You might want to double-check your join keys. 1 INFO FOREIGN KEYS NOT FOUND When joining U2BASE__STAGING_TABLE_4 and MOVIES2ACTORS__STAGING_TABLE_2 over 'movieid' and 'movieid', there are no corresponding entries for 0.340408% of entries in 'movieid' in 'U2BASE__STAGING_TABLE_4'. You might want to double-check your join keys. In\u00a0[20]: Copied! <pre>pipe1.fit(container.train)\n</pre> pipe1.fit(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 2 issues labeled INFO and 0 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 941 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:43, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:23, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:03, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:05, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:1m:13.926562\n\n</pre> Out[20]: <pre>Pipeline(data_model='users',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['actors', 'directors', 'movies', 'movies2actors', 'movies2directors',\n                     'u2base'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['fast_prop', 'container-fr4Ui8'])</pre> In\u00a0[21]: Copied! <pre>pipe2.check(container.train)\n</pre> pipe2.check(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 2 issues labeled INFO and 0 issues labeled WARNING.\n</pre> Out[21]: type label                  message                          0 INFO FOREIGN KEYS NOT FOUND When joining U2BASE__STAGING_TABLE_4 and MOVIES2DIRECTORS__STAGING_TABLE_3 over 'movieid' and 'movieid', there are no corresponding entries for 0.159513% of entries in 'movieid' in 'U2BASE__STAGING_TABLE_4'. You might want to double-check your join keys. 1 INFO FOREIGN KEYS NOT FOUND When joining U2BASE__STAGING_TABLE_4 and MOVIES2ACTORS__STAGING_TABLE_2 over 'movieid' and 'movieid', there are no corresponding entries for 0.340408% of entries in 'movieid' in 'U2BASE__STAGING_TABLE_4'. You might want to double-check your join keys. In\u00a0[22]: Copied! <pre>pipe2.fit(container.train)\n</pre> pipe2.fit(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 2 issues labeled INFO and 0 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:05, remaining: 00:00]          \nRelboost: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 08:55, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:05, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 01:49, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:04, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:10m:58.115767\n\n</pre> Out[22]: <pre>Pipeline(data_model='users',\n         feature_learners=['Relboost'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['actors', 'directors', 'movies', 'movies2actors', 'movies2directors',\n                     'u2base'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['relboost', 'container-fr4Ui8'])</pre> In\u00a0[23]: Copied! <pre>fastprop_score = pipe1.score(container.test)\nfastprop_score\n</pre> fastprop_score = pipe1.score(container.test) fastprop_score <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:21, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\n</pre> Out[23]: date time           set used target accuracy     auc cross entropy 0 2024-02-21 15:03:15 train target 0.9114 0.9658 0.2847 1 2024-02-21 15:14:36 test target 0.7776 0.7896 0.4757 In\u00a0[24]: Copied! <pre>relboost_score = pipe2.score(container.test)\nrelboost_score\n</pre> relboost_score = pipe2.score(container.test) relboost_score <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:05, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:37, remaining: 00:00]          \n\n</pre> Out[24]: date time           set used target accuracy     auc cross entropy 0 2024-02-21 15:14:14 train target 0.966 0.9943 0.1609 1 2024-02-21 15:15:18 test target 0.8101 0.8409 0.4384 <p>Column importances</p> <p>Because getML uses relational learning, we can apply the principles we used to calculate the feature importances to individual columns as well.</p> <p>As we can see, most of the predictive accuracy is drawn from the roles played by the actors. This suggests that the text fields contained in this relational database have a higher impact on predictive accuracy than for most other data sets.</p> In\u00a0[25]: Copied! <pre>names, importances = pipe1.columns.importances()\n\nplt.subplots(figsize=(20, 10))\n\nplt.bar(names, importances)\n\nplt.title('Columns importances')\nplt.xlabel('Columns')\nplt.ylabel('Importances')\nplt.xticks(rotation='vertical')\nplt.show()\n</pre> names, importances = pipe1.columns.importances()  plt.subplots(figsize=(20, 10))  plt.bar(names, importances)  plt.title('Columns importances') plt.xlabel('Columns') plt.ylabel('Importances') plt.xticks(rotation='vertical') plt.show() In\u00a0[26]: Copied! <pre>names, importances = pipe2.columns.importances()\n\nplt.subplots(figsize=(20, 10))\n\nplt.bar(names, importances)\n\nplt.title('Columns importances')\nplt.xlabel('Columns')\nplt.ylabel('Importances')\nplt.xticks(rotation='vertical')\nplt.show()\n</pre> names, importances = pipe2.columns.importances()  plt.subplots(figsize=(20, 10))  plt.bar(names, importances)  plt.title('Columns importances') plt.xlabel('Columns') plt.ylabel('Importances') plt.xticks(rotation='vertical') plt.show() In\u00a0[27]: Copied! <pre>pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name]\n</pre> pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name] Out[27]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_138\";\n\nCREATE TABLE \"FEATURE_1_138\" AS\nSELECT MEDIAN( COALESCE( f_1_1_69.\"feature_1_1_69\", 0.0 ) ) AS \"feature_1_138\",\n       t1.rowid AS rownum\nFROM \"USERS__STAGING_TABLE_1\" t1\nINNER JOIN \"U2BASE__STAGING_TABLE_4\" t2\nON t1.\"userid\" = t2.\"userid\"\nLEFT JOIN \"FEATURE_1_1_69\" f_1_1_69\nON t2.rowid = f_1_1_69.rownum\nGROUP BY t1.rowid;\n</pre> In\u00a0[28]: Copied! <pre>pipe2.features.to_sql()[pipe2.features.sort(by=\"importances\")[0].name]\n</pre> pipe2.features.to_sql()[pipe2.features.sort(by=\"importances\")[0].name] Out[28]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_1\";\n\nCREATE TABLE \"FEATURE_1_1\" AS\nSELECT AVG( \n    CASE\n        WHEN ( p_1_1.\"feature_1_1_69\" &gt; 0.242159 ) AND ( p_1_1.\"feature_1_1_21\" &gt; 0.232813 ) AND ( t2.\"t3__year__mapping_1_target_1_avg\" &gt; 0.282119 ) THEN 20.46317569156853\n        WHEN ( p_1_1.\"feature_1_1_69\" &gt; 0.242159 ) AND ( p_1_1.\"feature_1_1_21\" &gt; 0.232813 ) AND ( t2.\"t3__year__mapping_1_target_1_avg\" &lt;= 0.282119 OR t2.\"t3__year__mapping_1_target_1_avg\" IS NULL ) THEN 7.321538279840953\n        WHEN ( p_1_1.\"feature_1_1_69\" &gt; 0.242159 ) AND ( p_1_1.\"feature_1_1_21\" &lt;= 0.232813 OR p_1_1.\"feature_1_1_21\" IS NULL ) AND ( p_1_1.\"feature_1_1_69\" &gt; 0.243429 ) THEN 5.046599618766721\n        WHEN ( p_1_1.\"feature_1_1_69\" &gt; 0.242159 ) AND ( p_1_1.\"feature_1_1_21\" &lt;= 0.232813 OR p_1_1.\"feature_1_1_21\" IS NULL ) AND ( p_1_1.\"feature_1_1_69\" &lt;= 0.243429 OR p_1_1.\"feature_1_1_69\" IS NULL ) THEN -8.250725468943104\n        WHEN ( p_1_1.\"feature_1_1_69\" &lt;= 0.242159 OR p_1_1.\"feature_1_1_69\" IS NULL ) AND ( p_1_1.\"feature_1_1_21\" &gt; 0.273123 ) AND ( p_1_1.\"feature_1_1_76\" &gt; 0.008673 ) THEN -3.885674068832839\n        WHEN ( p_1_1.\"feature_1_1_69\" &lt;= 0.242159 OR p_1_1.\"feature_1_1_69\" IS NULL ) AND ( p_1_1.\"feature_1_1_21\" &gt; 0.273123 ) AND ( p_1_1.\"feature_1_1_76\" &lt;= 0.008673 OR p_1_1.\"feature_1_1_76\" IS NULL ) THEN -12.86974979841147\n        WHEN ( p_1_1.\"feature_1_1_69\" &lt;= 0.242159 OR p_1_1.\"feature_1_1_69\" IS NULL ) AND ( p_1_1.\"feature_1_1_21\" &lt;= 0.273123 OR p_1_1.\"feature_1_1_21\" IS NULL ) AND ( p_1_1.\"feature_1_1_85\" &gt; 0.003477 ) THEN 26.50336909269918\n        WHEN ( p_1_1.\"feature_1_1_69\" &lt;= 0.242159 OR p_1_1.\"feature_1_1_69\" IS NULL ) AND ( p_1_1.\"feature_1_1_21\" &lt;= 0.273123 OR p_1_1.\"feature_1_1_21\" IS NULL ) AND ( p_1_1.\"feature_1_1_85\" &lt;= 0.003477 OR p_1_1.\"feature_1_1_85\" IS NULL ) THEN -2.663699179011978\n        ELSE NULL\n    END\n) AS \"feature_1_1\",\n       t1.rowid AS rownum\nFROM \"USERS__STAGING_TABLE_1\" t1\nINNER JOIN \"U2BASE__STAGING_TABLE_4\" t2\nON t1.\"userid\" = t2.\"userid\"\nLEFT JOIN \"FEATURES_1_1_PROPOSITIONALIZATION\" p_1_1\nON t2.rowid = p_1_1.\"rownum\"\nGROUP BY t1.rowid;\n</pre> In\u00a0[29]: Copied! <pre># Creates a folder named movie_lens_pipeline containing\n# the SQL code.\npipe2.features.to_sql().save(\"movie_lens_pipeline\")\n</pre> # Creates a folder named movie_lens_pipeline containing # the SQL code. pipe2.features.to_sql().save(\"movie_lens_pipeline\") In\u00a0[30]: Copied! <pre>pipe2.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"movie_lens_spark\")\n</pre> pipe2.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"movie_lens_spark\") <p>State-of-the-art approaches on this dataset perform as follows:</p> Approach Study Accuracy AUC Probabalistic Relational Model Ghanem (2009) -- 69.2% Multi-Relational Bayesian Network Schulte and Khosravi (2012) 69% -- Multi-Relational Bayesian Network Schulte et al (2013) 66% -- <p>By contrast, getML's algorithms, as used in this notebook, perform as follows:</p> In\u00a0[31]: Copied! <pre>scores = [fastprop_score, relboost_score]\npd.DataFrame(data={\n    'Approach': ['FastProp', 'Relboost'],\n    'Accuracy': [f'{score.accuracy:.1%}' for score in scores],\n    'AUC': [f'{score.auc:,.1%}' for score in scores]\n})\n</pre> scores = [fastprop_score, relboost_score] pd.DataFrame(data={     'Approach': ['FastProp', 'Relboost'],     'Accuracy': [f'{score.accuracy:.1%}' for score in scores],     'AUC': [f'{score.auc:,.1%}' for score in scores] }) Out[31]: Approach Accuracy AUC 0 FastProp 77.8% 79.0% 1 Relboost 81.0% 84.1%","boost":0.8},{"location":"examples/enterprise-notebooks/movie_lens/#movielens-predicting-a-users-gender-based-on-the-movies-they-have-watched","title":"MovieLens - Predicting a user's gender based on the movies they have watched\u00b6","text":"<p>In this notebook, we will apply getML to a dataset that is often used for benchmarking in the relational learning literature: The MovieLens dataset.</p> <p>Summary:</p> <ul> <li>Prediction type: Classification model</li> <li>Domain: Entertainment</li> <li>Prediction target: The gender of a user</li> <li>Population size: 6039</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/movie_lens/#background","title":"Background\u00b6","text":"<p>The MovieLens dataset is often used in the relational learning literature has a benchmark for newly developed algorithms. Following the tradition, we benchmark getML's own algorithms on this dataset as well. The task is to predict a user's gender based on the movies they have watched.</p> <p>It has been downloaded from the CTU Prague relational learning repository (Motl and Schulte, 2015) (Now residing at relational-data.org.).</p>","boost":0.8},{"location":"examples/enterprise-notebooks/movie_lens/#analysis","title":"Analysis\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/movie_lens/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/movie_lens/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>We begin by downloading the data from the source file:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/movie_lens/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/movie_lens/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"<p>We loaded the data and defined the roles and units. Next, we create a getML pipeline for relational learning.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/movie_lens/#21-define-relational-model","title":"2.1 Define relational model\u00b6","text":"<p>To get started with relational learning, we need to specify the data model.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/movie_lens/#22-getml-pipeline","title":"2.2 getML pipeline\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/movie_lens/#23-model-training","title":"2.3 Model training\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/movie_lens/#24-model-evaluation","title":"2.4 Model evaluation\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/movie_lens/#26-studying-features","title":"2.6 Studying features\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/movie_lens/#27-features","title":"2.7 Features\u00b6","text":"<p>The most important features look as follows:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/movie_lens/#28-productionization","title":"2.8 Productionization\u00b6","text":"<p>It is possible to productionize the pipeline by transpiling the features into production-ready SQL code. Please also refer to getML's <code>sqlite3</code> and <code>spark</code> modules.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/movie_lens/#29-benchmarks","title":"2.9 Benchmarks\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/movie_lens/#3-conclusion","title":"3. Conclusion\u00b6","text":"<p>In this notebook we have demonstrated how getML can be applied to the MovieLens dataset. We have demonstrated the our  approach outperforms state-of-the-art relational learning algorithms.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/movie_lens/#references","title":"References\u00b6","text":"<p>Motl, Jan, and Oliver Schulte. \"The CTU prague relational learning repository.\" arXiv preprint arXiv:1511.03086 (2015).</p> <p>Ghanem, Amal S. \"Probabilistic models for mining imbalanced relational data.\" Doctoral dissertation, Curtin University (2009).</p> <p>Schulte, Oliver, and Hassan Khosravi. \"Learning graphical models for relational data via lattice search.\" Machine Learning 88.3 (2012): 331-368.</p> <p>Schulte, Oliver, et al. \"A hierarchy of independence assumptions for multi-relational Bayes net classifiers.\" 2013 IEEE Symposium on Computational Intelligence and Data Mining (CIDM). IEEE, 2013.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/occupancy/","title":"<span class=\"ntitle\">occupancy.ipynb</span> <span class=\"ndesc\">A multivariate time series example</span>","text":"<p>Our use case is a public domain data set for predicting room occupancy from sensor data. The results achieved using getML outperform all published results on this data set. Note that this is not only a neat use case for machine learning algorithms, but a real-world application with tangible consequences: If room occupancy is known with sufficient certainty, it can be applied to the control systems of a building. Such as system can reduce the energy consumption by up to 50 %.</p> <p>Let's get started with the analysis and set-up your session:</p> In\u00a0[1]: Copied! <pre>import os\nfrom pathlib import Path\nfrom urllib import request\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image, Markdown\n\n%matplotlib inline\n\nimport getml\n\ngetml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.engine.set_project(\"occupancy\")\n</pre> import os from pathlib import Path from urllib import request  import matplotlib.pyplot as plt import numpy as np import pandas as pd from IPython.display import Image, Markdown  %matplotlib inline  import getml  getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.engine.set_project(\"occupancy\") <pre>getML engine is already running.\n\nConnected to project 'occupancy'\n</pre> <p>The data set can be downloaded directly from GitHub. It is conveniently separated into a training, a validation and a testing set. This allows us to directly benchmark our results against the results of the original paper later.</p> In\u00a0[2]: Copied! <pre>(\n    population_train,\n    population_test,\n    population_validation,\n) = getml.datasets.load_occupancy()\n</pre> (     population_train,     population_test,     population_validation, ) = getml.datasets.load_occupancy() <pre>\nLoading population_train...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nLoading population_test...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nLoading population_validation...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n</pre> In\u00a0[3]: Copied! <pre>population, split = getml.data.split.concat(\n    \"population\",\n    train=population_train,\n    validation=population_validation,\n    test=population_test,\n)\n</pre> population, split = getml.data.split.concat(     \"population\",     train=population_train,     validation=population_validation,     test=population_test, ) <p>The training set looks like this:</p> In\u00a0[4]: hide_input Copied! <pre>population\n</pre> population Out[4]:  name                date Occupancy Temperature  Humidity     Light       CO2 HumidityRatio  role          time_stamp    target   numerical numerical numerical numerical     numerical  unit          time stamp 0 2015-02-04 17:51:00 1 23.18 27.272 426 721.25 0.004793 1 2015-02-04 17:51:59 1 23.15 27.2675 429.5 714 0.004783 2 2015-02-04 17:53:00 1 23.15 27.245 426 713.5 0.004779 3 2015-02-04 17:54:00 1 23.15 27.2 426 708.25 0.004772 4 2015-02-04 17:55:00 1 23.1 27.2 426 704.5 0.004757 ... ... ... ... ... ... ... 20555 2015-02-18 09:15:00 1 20.815 27.7175 429.75 1505.25 0.004213 20556 2015-02-18 09:16:00 1 20.865 27.745 423.5 1514.5 0.00423 20557 2015-02-18 09:16:59 1 20.89 27.745 423.5 1521.5 0.004237 20558 2015-02-18 09:17:59 1 20.89 28.0225 418.75 1632 0.004279 20559 2015-02-18 09:19:00 1 21 28.1 409 1864 0.004321 <p>     20560 rows x 7 columns     memory usage: 1.15 MB     name: population     type: getml.DataFrame </p> In\u00a0[5]: Copied! <pre>split\n</pre> split Out[5]: 0 train 1 train 2 train 3 train 4 train ... <p>     20560 rows          type: StringColumnView </p> <p>We also assign roles to each column. To learn more about what roles do and why we need them, check out the official documentation.</p> In\u00a0[6]: Copied! <pre>population.set_role([\"Occupancy\"], getml.data.roles.target)\npopulation.set_role(\n    [\"Temperature\", \"Humidity\", \"Light\", \"CO2\", \"HumidityRatio\"],\n    getml.data.roles.numerical,\n)\npopulation.set_role([\"date\"], getml.data.roles.time_stamp)\n\npopulation\n</pre> population.set_role([\"Occupancy\"], getml.data.roles.target) population.set_role(     [\"Temperature\", \"Humidity\", \"Light\", \"CO2\", \"HumidityRatio\"],     getml.data.roles.numerical, ) population.set_role([\"date\"], getml.data.roles.time_stamp)  population Out[6]:  name                date Occupancy Temperature  Humidity     Light       CO2 HumidityRatio  role          time_stamp    target   numerical numerical numerical numerical     numerical  unit          time stamp 0 2015-02-04 17:51:00 1 23.18 27.272 426 721.25 0.004793 1 2015-02-04 17:51:59 1 23.15 27.2675 429.5 714 0.004783 2 2015-02-04 17:53:00 1 23.15 27.245 426 713.5 0.004779 3 2015-02-04 17:54:00 1 23.15 27.2 426 708.25 0.004772 4 2015-02-04 17:55:00 1 23.1 27.2 426 704.5 0.004757 ... ... ... ... ... ... ... 20555 2015-02-18 09:15:00 1 20.815 27.7175 429.75 1505.25 0.004213 20556 2015-02-18 09:16:00 1 20.865 27.745 423.5 1514.5 0.00423 20557 2015-02-18 09:16:59 1 20.89 27.745 423.5 1521.5 0.004237 20558 2015-02-18 09:17:59 1 20.89 28.0225 418.75 1632 0.004279 20559 2015-02-18 09:19:00 1 21 28.1 409 1864 0.004321 <p>     20560 rows x 7 columns     memory usage: 1.15 MB     name: population     type: getml.DataFrame </p> In\u00a0[7]: Copied! <pre># Our forecast horizon is 0.\n# We do not predict the future, instead we infer\n# the present state from current and past sensor data.\nhorizon = 0.0\n\n# We do not allow the time series features\n# to use target values from the past.\n# (Otherwise, we would need the horizon to\n# be greater than 0.0).\nallow_lagged_targets = False\n\n# We want our time series features to only use\n# data from the last 15 minutes\nmemory = getml.data.time.minutes(15)\n\ntime_series = getml.data.TimeSeries(\n    population=population,\n    split=split,\n    time_stamps=\"date\",\n    horizon=horizon,\n    memory=memory,\n    lagged_targets=allow_lagged_targets,\n)\n\ntime_series\n</pre> # Our forecast horizon is 0. # We do not predict the future, instead we infer # the present state from current and past sensor data. horizon = 0.0  # We do not allow the time series features # to use target values from the past. # (Otherwise, we would need the horizon to # be greater than 0.0). allow_lagged_targets = False  # We want our time series features to only use # data from the last 15 minutes memory = getml.data.time.minutes(15)  time_series = getml.data.TimeSeries(     population=population,     split=split,     time_stamps=\"date\",     horizon=horizon,     memory=memory,     lagged_targets=allow_lagged_targets, )  time_series Out[7]: data model diagram populationpopulationdate &lt;= dateMemory: 15.0 minutes staging data frames staging table               0 population POPULATION__STAGING_TABLE_1 1 population POPULATION__STAGING_TABLE_2 container population subset     name       rows type 0 test population 9751 View 1 train population 8144 View 2 validation population 2665 View peripheral name        rows type      0 population 20560 DataFrame In\u00a0[8]: Copied! <pre>feature_learner = getml.feature_learning.Multirel(\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n)\n\npredictor = getml.predictors.LogisticRegression()\n\npipe = getml.pipeline.Pipeline(\n    data_model=time_series.data_model,\n    tags=[\"memory=15\", \"logistic regression\"],\n    feature_learners=[feature_learner],\n    predictors=[predictor],\n)\n\npipe\n</pre> feature_learner = getml.feature_learning.Multirel(     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss, )  predictor = getml.predictors.LogisticRegression()  pipe = getml.pipeline.Pipeline(     data_model=time_series.data_model,     tags=[\"memory=15\", \"logistic regression\"],     feature_learners=[feature_learner],     predictors=[predictor], )  pipe Out[8]: <pre>Pipeline(data_model='population',\n         feature_learners=['Multirel'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['population'],\n         predictors=['LogisticRegression'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['memory=15', 'logistic regression'])</pre> <p><code>.check(...)</code> will be automatically called by <code>.fit(...)</code>. But it is always a good idea to call <code>.check(...)</code> separately before fitting, so we still have time for some last-minute fixes.</p> In\u00a0[9]: Copied! <pre>pipe.check(time_series.train)\n</pre> pipe.check(time_series.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\n</pre> In\u00a0[10]: Copied! <pre>pipe = getml.hyperopt.tune_predictors(pipe, time_series)\n</pre> pipe = getml.hyperopt.tune_predictors(pipe, time_series) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nTuning logistic regression... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 01:56, remaining: 00:00]          \n\nTime taken: 0h:1m:55.861831\n\nBuilding final pipeline...\n\nChecking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nMultirel: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:06, remaining: 00:00]          \nMultirel: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nMultirel: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nLogisticRegression: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:9.045621\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nMultirel: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\n</pre> <p>The logistic regression has one hyperparameter that can be optimized, namely its L2 regularization parameter. We fine-tune this parameter using the validation set. Note that the validation set is different from the testing set on which the final outcome will be evaluated.</p> <p>Let's see how well we did by scoring the model on the testing set.</p> In\u00a0[11]: Copied! <pre>pipe.score(time_series.test)\n</pre> pipe.score(time_series.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nMultirel: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \n\n</pre> Out[11]: date time           set used   target    accuracy     auc cross entropy 0 2024-02-21 15:06:09 train Occupancy 0.9887 0.9948 0.5173 1 2024-02-21 15:06:09 validation Occupancy 0.9786 0.9912 0.4873 2 2024-02-21 15:06:11 test Occupancy 0.9934 0.9976 0.517 <p>In the original paper, the authors tried several approaches. The best out-of-sample values of all the approaches they tried are the following:</p> <ul> <li>Accuracy (testing): 0.99061</li> <li>AUC (testing): 0.99574</li> </ul> <p>Note that our results outperform the best approach from the original paper, both in terms of accuracy as well as AUC. This demonstrates how powerful getML's relational learning approach is also a powerful tool for time series.</p> In\u00a0[12]: Copied! <pre># Refers to the data from the last time\n# we called .score(...).\nfpr, tpr = pipe.plots.roc_curve()\n\nplt.subplots(figsize=(20, 10))\n\nplt.plot(fpr, tpr)\n\nplt.title(\"receiver operating characteristic (ROC)\")\nplt.grid(True)\nplt.xlabel(\"false positive rate\")\nplt.ylabel(\"true positive rate\")\n\nplt.show()\n</pre> # Refers to the data from the last time # we called .score(...). fpr, tpr = pipe.plots.roc_curve()  plt.subplots(figsize=(20, 10))  plt.plot(fpr, tpr)  plt.title(\"receiver operating characteristic (ROC)\") plt.grid(True) plt.xlabel(\"false positive rate\") plt.ylabel(\"true positive rate\")  plt.show() In\u00a0[13]: Copied! <pre># Refers to the data from the last time\n# we called .score(...).\nrecall, precision = pipe.plots.precision_recall_curve()\n\nplt.subplots(figsize=(20, 10))\n\nplt.plot(recall, precision)\n\nplt.title(\"precision-recall curve\")\nplt.grid(True)\nplt.xlabel(\"recall (true positive rate)\")\nplt.ylabel(\"precision\")\n\nplt.show()\n</pre> # Refers to the data from the last time # we called .score(...). recall, precision = pipe.plots.precision_recall_curve()  plt.subplots(figsize=(20, 10))  plt.plot(recall, precision)  plt.title(\"precision-recall curve\") plt.grid(True) plt.xlabel(\"recall (true positive rate)\") plt.ylabel(\"precision\")  plt.show() In\u00a0[14]: Copied! <pre># Refers to the data from the last time\n# we called .score(...).\nproportion, lift = pipe.plots.lift_curve()\n\nplt.subplots(figsize=(20, 10))\n\nplt.plot(proportion, lift)\n\nplt.title(\"lift curve\")\nplt.grid(True)\nplt.xlabel(\"proportion\")\nplt.ylabel(\"lift\")\n\nplt.show()\n</pre> # Refers to the data from the last time # we called .score(...). proportion, lift = pipe.plots.lift_curve()  plt.subplots(figsize=(20, 10))  plt.plot(proportion, lift)  plt.title(\"lift curve\") plt.grid(True) plt.xlabel(\"proportion\") plt.ylabel(\"lift\")  plt.show() In\u00a0[15]: Copied! <pre>pipe.features\n</pre> pipe.features Out[15]: target    name          correlation importance 0 Occupancy feature_1_1 0.979 0.009919 1 Occupancy feature_1_2 0.9785 0.009914 2 Occupancy feature_1_3 0.9726 0.009587 3 Occupancy feature_1_4 -0.9776 0.009918 4 Occupancy feature_1_5 0.9795 0.009939 ... ... ... ... 100 Occupancy temperature 0.5217 0.00547 101 Occupancy humidity -0.0878 0.001223 102 Occupancy light 0.9145 0.009329 103 Occupancy co2 0.2618 0.0072 104 Occupancy humidityratio 0.19 0.002917 In\u00a0[16]: Copied! <pre>names, correlations = pipe.features.correlations()\n\nplt.subplots(figsize=(20, 10))\n\nplt.bar(names, correlations)\n\nplt.title(\"feature correlations\")\nplt.grid(True)\nplt.xlabel(\"features\")\nplt.ylabel(\"correlations\")\nplt.xticks(rotation=\"vertical\")\n\nplt.show()\n</pre> names, correlations = pipe.features.correlations()  plt.subplots(figsize=(20, 10))  plt.bar(names, correlations)  plt.title(\"feature correlations\") plt.grid(True) plt.xlabel(\"features\") plt.ylabel(\"correlations\") plt.xticks(rotation=\"vertical\")  plt.show() In\u00a0[17]: hide_input Copied! <pre>names, importances = pipe.features.importances()\n\nplt.subplots(figsize=(20, 10))\n\nplt.bar(names, importances)\n\nplt.title(\"feature importances\")\nplt.grid(True)\nplt.xlabel(\"features\")\nplt.ylabel(\"importance\")\nplt.xticks(rotation=\"vertical\")\n\nplt.show()\n</pre> names, importances = pipe.features.importances()  plt.subplots(figsize=(20, 10))  plt.bar(names, importances)  plt.title(\"feature importances\") plt.grid(True) plt.xlabel(\"features\") plt.ylabel(\"importance\") plt.xticks(rotation=\"vertical\")  plt.show() <p>The feature importance is calculated by XGBoost based on the improvement of the optimizing criterium at each split in the decision tree and is normalized to 100%.</p> <p>We first look at the most important feature. The names returned by feature importances are already sorted, so we can just index them, like this:</p> In\u00a0[18]: Copied! <pre>pipe.features.to_sql()[names[0]]\n</pre> pipe.features.to_sql()[names[0]] Out[18]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_23\";\n\nCREATE TABLE \"FEATURE_1_23\" AS\nSELECT MAX( t1.\"date\" - t2.\"date\" ) AS \"feature_1_23\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"POPULATION__STAGING_TABLE_2\" t2\nON 1 = 1\nWHERE ( t2.\"date\" &lt;= t1.\"date\"\nAND ( t2.\"date__15_000000_minutes\" &gt; t1.\"date\" OR t2.\"date__15_000000_minutes\" IS NULL )\n) AND (\n   ( ( t1.\"light\" &gt; 386.583333 OR t1.\"light\" IS NULL  ) AND ( t2.\"light\" &gt; 609.161616 ) )\nOR ( ( t1.\"light\" &lt;= 386.583333 ) )\n)\nGROUP BY t1.rowid;\n</pre> <p>Let's check out the second most important feature.</p> In\u00a0[19]: Copied! <pre>pipe.features.to_sql()[names[1]]\n</pre> pipe.features.to_sql()[names[1]] Out[19]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_19\";\n\nCREATE TABLE \"FEATURE_1_19\" AS\nSELECT MIN( t1.\"date\" - t2.\"date\" ) AS \"feature_1_19\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"POPULATION__STAGING_TABLE_2\" t2\nON 1 = 1\nWHERE ( t2.\"date\" &lt;= t1.\"date\"\nAND ( t2.\"date__15_000000_minutes\" &gt; t1.\"date\" OR t2.\"date__15_000000_minutes\" IS NULL )\n) AND (\n   ( ( t2.\"light\" &gt; 465.562724 OR t2.\"light\" IS NULL  ) AND ( t2.\"light\" &lt;= 560.225000 ) )\nOR ( ( t2.\"light\" &lt;= 465.562724 ) AND ( t1.\"date\" - t2.\"date\" &gt; 781.593284 ) )\nOR ( ( t2.\"light\" &lt;= 465.562724 ) AND ( t1.\"date\" - t2.\"date\" &lt;= 781.593284 ) AND ( t2.\"light\" &gt; 365.490234 ) AND ( t1.\"light\" &gt; 5.338384 ) )\n)\nGROUP BY t1.rowid;\n</pre> <p>These two features demonstrate the power of the getML feature learning algorithms. It is very unlikely to find these features using manual, trial-and-error based methods. The general structure of features found using such methods might be similar, but you would have had to put in much more effort while getting worse results.</p> <p>When browsing through the remaining features, you will notice that some are columns directly taken from the original table, such as Light and CO2. But these columns are less correlated and less important than the features generated with the relational model based on self-join and upper time stamps.</p> <p>Because getML uses a feature learning approach, the concept of feature importances can also be carried over to the individual columns.</p> In\u00a0[20]: Copied! <pre>names, importances = pipe.columns.importances()\n\nplt.subplots(figsize=(20, 10))\n\nplt.bar(names, importances)\n\nplt.title(\"column importances\")\nplt.grid(True)\nplt.xlabel(\"columns\")\nplt.ylabel(\"importance\")\nplt.xticks(rotation=\"vertical\")\n\nplt.show()\n</pre> names, importances = pipe.columns.importances()  plt.subplots(figsize=(20, 10))  plt.bar(names, importances)  plt.title(\"column importances\") plt.grid(True) plt.xlabel(\"columns\") plt.ylabel(\"importance\") plt.xticks(rotation=\"vertical\")  plt.show() In\u00a0[21]: Copied! <pre>pipe.columns\n</pre> pipe.columns Out[21]: name          marker       table      importance target    0 CO2 [PERIPHERAL] population 0.078077 Occupancy 1 Humidity [PERIPHERAL] population 0.001753 Occupancy 2 HumidityRatio [PERIPHERAL] population 0.013003 Occupancy 3 Light [PERIPHERAL] population 0.13181 Occupancy 4 Temperature [PERIPHERAL] population 0.032809 Occupancy ... ... ... ... ... 7 Humidity [POPULATION] population 0.001223 Occupancy 8 HumidityRatio [POPULATION] population 0.002917 Occupancy 9 Light [POPULATION] population 0.716998 Occupancy 10 Temperature [POPULATION] population 0.00613 Occupancy 11 date [POPULATION] population 0.003508 Occupancy <p>As we can see, about 80% of the predictive power comes from the Light column. The second most important column is the Temperature column, we contributes about 7% of the predictive power.</p> In\u00a0[22]: Copied! <pre># Creates a folder named occupancy_pipeline containing\n# the SQL code.\npipe.features.to_sql().save(\"occupancy_pipeline\")\n</pre> # Creates a folder named occupancy_pipeline containing # the SQL code. pipe.features.to_sql().save(\"occupancy_pipeline\") In\u00a0[23]: Copied! <pre>pipe.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"occupancy_spark\")\n</pre> pipe.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"occupancy_spark\")","boost":0.8},{"location":"examples/enterprise-notebooks/occupancy/#occupancy-a-multivariate-time-series-example","title":"Occupancy - A multivariate time series example\u00b6","text":"<p>In this tutorial, you will learn how to apply getML to multivariate time series. It also demonstrates how to use getML's high-level interface for hyperparameter tuning.</p> <p>Summary:</p> <ul> <li>Prediction type: Binary classification</li> <li>Domain: Energy</li> <li>Prediction target: Room occupancy</li> <li>Source data: 1 table, 32k rows</li> <li>Population size: 32k</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/occupancy/#background","title":"Background\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/occupancy/#introduction-to-occupancy-prediction","title":"Introduction to occupancy prediction\u00b6","text":"<p>Usually, getML is considered to be a tool for feature engineering and machine learning on relational data sets. How can we apply it to (multivariate) time series?</p> <p>The key is a self-join. Instead of creating features by merging and aggregating peripheral tables in a relational data model, for a time-series, we perform the same operations on the population table itself. This results in features like these:</p> <ul> <li><p>Aggregations over time, such as the average value of some column for the last 3 days.</p> </li> <li><p>Seasonal effects, such as today is a Wednesday, so let's get the average value for the last four Wednesdays.</p> </li> <li><p>Lag variables, such as get the value of some column from two hours ago.</p> </li> </ul> <p>Using getML's algorithms for relational learning, we can extract all of these features automatically. Having created a flat table of such features, we can then apply state-of-the-art machine learning algorithms, like xgboost. As you will see in this example, this performs better than traditional time series analysis.</p> <p>The present analysis is based on a public domain time series dataset. It is available in the UC Irvine Machine Learning Repository. The challenge is straightforward: We want to predict whether an office room is occupied at a given moment in time using sensor data. The data is measured about once a minute. Ground-truth occupancy was obtained from time-stamped pictures. The available columns are</p> <ul> <li>Date, year-month-day hour:minute:second</li> <li>Temperature, in Celsius</li> <li>Relative Humidity, %</li> <li>Light, in Lux</li> <li>CO2, in ppm</li> <li>Humidity Ratio, Derived quantity from temperature and relative humidity, in kgwater-vapor/kg-air</li> <li>Occupancy, 0 or 1, 0 for not occupied, 1 for occupied status</li> </ul> <p>As a reference and benchmark, we use this paper:</p> <p>Accurate occupancy detection of an office room from light, temperature, humidity and CO2 measurements using statistical learning models. Luis M. Candanedo, Veronique Feldheim. Energy and Buildings. Volume 112, 15 January 2016, Pages 28-39.</p> <p>The authors apply various artifical neural networks algorithms to the data set at hand and achieved accuracies between 80.324% (batch back algorithm) and 99.061% (limited memory quasi-Newton algorithm).</p>","boost":0.8},{"location":"examples/enterprise-notebooks/occupancy/#analysis","title":"Analysis\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/occupancy/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/occupancy/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"<p>We loaded the data, defined the roles, units and the abstract data model. Next, we create a getML pipeline for relational learning.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/occupancy/#21-getml-pipeline","title":"2.1 getML Pipeline\u00b6","text":"<p>We use a Multirel for generating the features and a simple logistic regression for prediction.</p> <p>We do not spend much effort on the hyperparameters and largely go with the default values. The only exception is that we add some regularization to the XGBoostClassifiers.</p> <p>We choose to consider data within the last 15 minutes for creating our features.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/occupancy/#22-model-training","title":"2.2 Model training\u00b6","text":"<p>We use a routine for automatic hyperparameter optimization to find the best parameters for the predictor:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/occupancy/#23-model-evaluation","title":"2.3 Model evaluation\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/occupancy/#24-studying-the-features","title":"2.4 Studying the features\u00b6","text":"<p>It is always a good idea to study the features the relational learning algorithm has extracted. We can do so in the feature view of the getML monitor. Open the monitor and select the models tab in the sidebar. You will see an overview over the trained pipelines. Select a pipeline to see the most essential summary plots.</p> <p>If you want to document them inside your notebook, here is how you can do that:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/occupancy/#25-productionization","title":"2.5 Productionization\u00b6","text":"<p>It is possible to productionize the pipeline by transpiling the features into production-ready SQL code. Please also refer to getML's <code>sqlite3</code> and <code>spark</code> modules.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/occupancy/#3-conclusion","title":"3. Conclusion\u00b6","text":"<p>This tutorial demonstrates that relational learning is a powerful tool for time series. We able to outperform the benchmarks for a scientific paper on a simple public domain time series data set using relatively little effort.</p> <p>If you want to learn more about getML, check out the official documentation.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/online_retail/","title":"<span class=\"ntitle\">online_retail.ipynb</span> <span class=\"ndesc\">Predicting order cancellations</span>","text":"<p>Let's get started with the analysis and set up your session:</p> In\u00a0[1]: Copied! <pre>import copy\nimport os\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\nfrom pathlib import Path\n\nfrom urllib import request\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline  \n\nfrom pyspark.sql import SparkSession\nimport getml\n\ngetml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.engine.set_project('online_retail')\n</pre> import copy import os os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\" from pathlib import Path  from urllib import request  import numpy as np import pandas as pd from IPython.display import Image import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline    from pyspark.sql import SparkSession import getml  getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.engine.set_project('online_retail') <pre>getML engine is already running.\n\nConnected to project 'online_retail'\n</pre> In\u00a0[2]: Copied! <pre>RUN_SPARK = False\n</pre> RUN_SPARK = False In\u00a0[3]: Copied! <pre>fname = \"online_retail.csv\"\n\nif not os.path.exists(fname):\n    fname, res = request.urlretrieve(\n        \"https://static.getml.com/datasets/online_retail/\" + fname, \n        fname\n    )\n    \nfull_data_pandas = pd.read_csv(fname, sep=\"|\")\n</pre> fname = \"online_retail.csv\"  if not os.path.exists(fname):     fname, res = request.urlretrieve(         \"https://static.getml.com/datasets/online_retail/\" + fname,          fname     )      full_data_pandas = pd.read_csv(fname, sep=\"|\") In\u00a0[4]: Copied! <pre>def add_zero(string):\n    if len(string) == 1:\n        return \"0\" + string\n    return string\n</pre> def add_zero(string):     if len(string) == 1:         return \"0\" + string     return string In\u00a0[5]: Copied! <pre>def format_date(string):\n    datetime = string.split(\" \")\n    assert len(datetime) == 2, \"Expected date and time\"\n    \n    date_components = datetime[0].split(\"/\")\n    assert len(date_components) == 3, \"Expected three date components\"\n    \n    date_components = [add_zero(x) for x in date_components]\n    \n    return \"-\".join(date_components) + \" \" + datetime[1]\n</pre> def format_date(string):     datetime = string.split(\" \")     assert len(datetime) == 2, \"Expected date and time\"          date_components = datetime[0].split(\"/\")     assert len(date_components) == 3, \"Expected three date components\"          date_components = [add_zero(x) for x in date_components]          return \"-\".join(date_components) + \" \" + datetime[1]  In\u00a0[6]: Copied! <pre>full_data_pandas[\"InvoiceDate\"] = [\n    format_date(string) for string in np.asarray(full_data_pandas[\"InvoiceDate\"])\n]\n</pre> full_data_pandas[\"InvoiceDate\"] = [     format_date(string) for string in np.asarray(full_data_pandas[\"InvoiceDate\"]) ] In\u00a0[7]: Copied! <pre>full_data_pandas\n</pre> full_data_pandas Out[7]: Invoice StockCode Description Quantity InvoiceDate Price Customer ID Country 0 536365 85123A WHITE HANGING HEART T-LIGHT HOLDER 6 2010-12-01 08:26 2.55 17850.0 United Kingdom 1 536365 71053 WHITE METAL LANTERN 6 2010-12-01 08:26 3.39 17850.0 United Kingdom 2 536365 84406B CREAM CUPID HEARTS COAT HANGER 8 2010-12-01 08:26 2.75 17850.0 United Kingdom 3 536365 84029G KNITTED UNION FLAG HOT WATER BOTTLE 6 2010-12-01 08:26 3.39 17850.0 United Kingdom 4 536365 84029E RED WOOLLY HOTTIE WHITE HEART. 6 2010-12-01 08:26 3.39 17850.0 United Kingdom ... ... ... ... ... ... ... ... ... 541905 581587 22899 CHILDREN'S APRON DOLLY GIRL 6 2011-12-09 12:50 2.10 12680.0 France 541906 581587 23254 CHILDRENS CUTLERY DOLLY GIRL 4 2011-12-09 12:50 4.15 12680.0 France 541907 581587 23255 CHILDRENS CUTLERY CIRCUS PARADE 4 2011-12-09 12:50 4.15 12680.0 France 541908 581587 22138 BAKING SET 9 PIECE RETROSPOT 3 2011-12-09 12:50 4.95 12680.0 France 541909 581587 POST POSTAGE 1 2011-12-09 12:50 18.00 12680.0 France <p>541910 rows \u00d7 8 columns</p> <p>In this data set, the targets aren't as clearly defined as we would like to, so we have do define them ourselves.</p> In\u00a0[8]: Copied! <pre>def add_target(df):\n    df = df.sort_values(by=[\"Customer ID\", \"InvoiceDate\"])\n    \n    cancelled = np.zeros(df.shape[0])\n\n    invoice = np.asarray(df[\"Invoice\"])\n    stock_code = np.asarray(df[\"StockCode\"])\n    customer_id = np.asarray(df[\"Customer ID\"])\n\n    for i in range(len(invoice)):\n        if (invoice[i][0] == 'C') or (i == len(invoice) - 1):\n            continue\n\n        j = i + 1\n\n        while customer_id[j] == customer_id[i]:\n            if (invoice[j][0] == 'C') and (stock_code[i] == stock_code[j]):\n                cancelled[i] = 1.0\n                break\n\n            if stock_code[i] == stock_code[j]:\n                break\n\n            j += 1\n    \n    df[\"cancelled\"] = cancelled\n    \n    return df\n</pre> def add_target(df):     df = df.sort_values(by=[\"Customer ID\", \"InvoiceDate\"])          cancelled = np.zeros(df.shape[0])      invoice = np.asarray(df[\"Invoice\"])     stock_code = np.asarray(df[\"StockCode\"])     customer_id = np.asarray(df[\"Customer ID\"])      for i in range(len(invoice)):         if (invoice[i][0] == 'C') or (i == len(invoice) - 1):             continue          j = i + 1          while customer_id[j] == customer_id[i]:             if (invoice[j][0] == 'C') and (stock_code[i] == stock_code[j]):                 cancelled[i] = 1.0                 break              if stock_code[i] == stock_code[j]:                 break              j += 1          df[\"cancelled\"] = cancelled          return df <p>Also, we want to remove any orders in the data set that are actually cancellations.</p> In\u00a0[9]: Copied! <pre>def remove_cancellations(df):\n    invoice = np.asarray(df[\"Invoice\"])\n\n    is_order = [inv[0] != 'C' for inv in invoice]\n    \n    df = df[is_order]\n    \n    return df\n</pre> def remove_cancellations(df):     invoice = np.asarray(df[\"Invoice\"])      is_order = [inv[0] != 'C' for inv in invoice]          df = df[is_order]          return df In\u00a0[10]: Copied! <pre>full_data_pandas = add_target(full_data_pandas)\nfull_data_pandas = remove_cancellations(full_data_pandas)\n</pre> full_data_pandas = add_target(full_data_pandas) full_data_pandas = remove_cancellations(full_data_pandas) <p>Finally, there are some order for which we do not have a customer ID. We want to remove those.</p> In\u00a0[11]: Copied! <pre>full_data_pandas = full_data_pandas[~np.isnan(full_data_pandas[\"Customer ID\"])]\n</pre> full_data_pandas = full_data_pandas[~np.isnan(full_data_pandas[\"Customer ID\"])] <p>Now we can upload the data to getML.</p> In\u00a0[12]: Copied! <pre>full_data = getml.data.DataFrame.from_pandas(full_data_pandas, \"full_data\")\n\nfull_data\n</pre> full_data = getml.data.DataFrame.from_pandas(full_data_pandas, \"full_data\")  full_data Out[12]:   name     Quantity        Price  Customer ID    cancelled Invoice       StockCode     Description                      InvoiceDate      Country          role unused_float unused_float unused_float unused_float unused_string unused_string unused_string                    unused_string    unused_string  0 74215 1.04 12346 1 541431 23166 MEDIUM CERAMIC TOP STORAGE JAR 2011-01-18 10:01 United Kingdom 1 12 2.1 12347 0 537626 85116 BLACK CANDELABRA T-LIGHT HOLDER 2010-12-07 14:57 Iceland 2 4 4.25 12347 0 537626 22375 AIRLINE BAG VINTAGE JET SET BROW... 2010-12-07 14:57 Iceland 3 12 3.25 12347 0 537626 71477 COLOUR GLASS. STAR T-LIGHT HOLDE... 2010-12-07 14:57 Iceland 4 36 0.65 12347 0 537626 22492 MINI PAINT SET VINTAGE 2010-12-07 14:57 Iceland ... ... ... ... ... ... ... ... ... 397920 12 0.42 18287 0 570715 22419 LIPSTICK PEN RED 2011-10-12 10:23 United Kingdom 397921 12 2.1 18287 0 570715 22866 HAND WARMER SCOTTY DOG DESIGN 2011-10-12 10:23 United Kingdom 397922 36 1.25 18287 0 573167 23264 SET OF 3 WOODEN SLEIGH DECORATIO... 2011-10-28 09:29 United Kingdom 397923 48 0.39 18287 0 573167 21824 PAINTED METAL STAR WITH HOLLY BE... 2011-10-28 09:29 United Kingdom 397924 24 0.29 18287 0 573167 21014 SWISS CHALET TREE DECORATION 2011-10-28 09:29 United Kingdom <p>     397925 rows x 9 columns     memory usage: 57.28 MB     name: full_data     type: getml.DataFrame </p> <p>getML requires that we define roles for each of the columns.</p> In\u00a0[13]: Copied! <pre>full_data.set_role(\"InvoiceDate\", getml.data.roles.time_stamp, time_formats=['%Y-%m-%d %H:%M'])\nfull_data.set_role([\"Customer ID\", \"Invoice\"], getml.data.roles.join_key)\nfull_data.set_role([\"cancelled\"], getml.data.roles.target)\nfull_data.set_role([\"Quantity\", \"Price\"], getml.data.roles.numerical)\nfull_data.set_role(\"Country\", getml.data.roles.categorical)\nfull_data.set_role(\"Description\", getml.data.roles.text)\n</pre> full_data.set_role(\"InvoiceDate\", getml.data.roles.time_stamp, time_formats=['%Y-%m-%d %H:%M']) full_data.set_role([\"Customer ID\", \"Invoice\"], getml.data.roles.join_key) full_data.set_role([\"cancelled\"], getml.data.roles.target) full_data.set_role([\"Quantity\", \"Price\"], getml.data.roles.numerical) full_data.set_role(\"Country\", getml.data.roles.categorical) full_data.set_role(\"Description\", getml.data.roles.text) <p>The StockCode is a 5-digit code that uniquely defines a product. It is hierarchical, meaning that every digit has a meaning. We want to make use of that, so we assign a unit to the stock code, which we can reference in our preprocessors.</p> In\u00a0[14]: Copied! <pre>full_data.set_unit(\"StockCode\", \"code\")\n</pre> full_data.set_unit(\"StockCode\", \"code\") In\u00a0[15]: Copied! <pre>split = getml.data.split.random(train=0.7, validation=0.15, test=0.15)\nsplit\n</pre> split = getml.data.split.random(train=0.7, validation=0.15, test=0.15) split Out[15]: 0 train 1 validation 2 train 3 validation 4 validation ... <p>     infinite number of  rows          type: StringColumnView </p> In\u00a0[16]: Copied! <pre>star_schema = getml.data.StarSchema(\n    population=full_data, \n    alias=\"population\",\n    split=split,\n)\n\nstar_schema.join(\n    full_data.drop(\"Description\"),\n    alias=\"full_data\",\n    on='Invoice',\n)\n\nstar_schema.join(\n    full_data.drop(\"Description\"),\n    alias=\"full_data\",\n    on='Customer ID',\n    time_stamps='InvoiceDate',\n    horizon=getml.data.time.days(1),\n    memory=getml.data.time.days(90),\n    lagged_targets=True,\n)\n\nstar_schema\n</pre> star_schema = getml.data.StarSchema(     population=full_data,      alias=\"population\",     split=split, )  star_schema.join(     full_data.drop(\"Description\"),     alias=\"full_data\",     on='Invoice', )  star_schema.join(     full_data.drop(\"Description\"),     alias=\"full_data\",     on='Customer ID',     time_stamps='InvoiceDate',     horizon=getml.data.time.days(1),     memory=getml.data.time.days(90),     lagged_targets=True, )  star_schema Out[16]: data model diagram full_datafull_datapopulationInvoice = InvoiceCustomer ID = Customer IDInvoiceDate &lt;= InvoiceDateMemory: 90.0 daysHorizon: 1.0 daysLagged targets allowed staging data frames staging table               0 population POPULATION__STAGING_TABLE_1 1 full_data FULL_DATA__STAGING_TABLE_2 container population subset     name        rows type 0 test full_data 60013 View 1 train full_data 278171 View 2 validation full_data 59741 View peripheral name        rows type 0 full_data 397925 View <p>Set-up the feature learner &amp; predictor</p> <p>We have mentioned that the StockCode is a hierarchical code. To make use of that fact, we use getML's substring preprocessor, extracting the first digit, the first two digits etc. Since we have assigned the unit code to the StockCode, the preprocessors know which column they should be applied to.</p> In\u00a0[17]: Copied! <pre>substr1 = getml.preprocessors.Substring(0, 1, \"code\")\nsubstr2 = getml.preprocessors.Substring(0, 2, \"code\")\nsubstr3 = getml.preprocessors.Substring(0, 3, \"code\")\n\nmapping = getml.preprocessors.Mapping()\n\ntext_field_splitter = getml.preprocessors.TextFieldSplitter()\n\nfast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    num_threads=1,\n    sampling_factor=0.1,\n)\n\nfeature_selector = getml.predictors.XGBoostClassifier()\n\npredictor = getml.predictors.XGBoostClassifier()\n</pre> substr1 = getml.preprocessors.Substring(0, 1, \"code\") substr2 = getml.preprocessors.Substring(0, 2, \"code\") substr3 = getml.preprocessors.Substring(0, 3, \"code\")  mapping = getml.preprocessors.Mapping()  text_field_splitter = getml.preprocessors.TextFieldSplitter()  fast_prop = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     num_threads=1,     sampling_factor=0.1, )  feature_selector = getml.predictors.XGBoostClassifier()  predictor = getml.predictors.XGBoostClassifier() <p>Build the pipeline</p> In\u00a0[18]: Copied! <pre>pipe = getml.pipeline.Pipeline(\n    tags=['fast_prop'],\n    data_model=star_schema.data_model,\n    preprocessors=[substr1, substr2, substr3, mapping, text_field_splitter],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=[predictor],\n    share_selected_features=0.2,\n)\n\npipe\n</pre> pipe = getml.pipeline.Pipeline(     tags=['fast_prop'],     data_model=star_schema.data_model,     preprocessors=[substr1, substr2, substr3, mapping, text_field_splitter],     feature_learners=[fast_prop],     feature_selectors=[feature_selector],     predictors=[predictor],     share_selected_features=0.2, )  pipe Out[18]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=['XGBoostClassifier'],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['full_data'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Substring', 'Substring', 'Substring', 'Mapping', 'TextFieldSplitter'],\n         share_selected_features=0.2,\n         tags=['fast_prop'])</pre> In\u00a0[19]: Copied! <pre>pipe.check(star_schema.train)\n</pre> pipe.check(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:10, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\n</pre> In\u00a0[20]: Copied! <pre>pipe.fit(star_schema.train)\n</pre> pipe.fit(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nIndexing text fields... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nFastProp: Trying 206 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:04, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:35, remaining: 00:00]          \nXGBoost: Training as feature selector... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 01:42, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:24, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:2m:47.703503\n\n</pre> Out[20]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=['XGBoostClassifier'],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['full_data'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Substring', 'Substring', 'Substring', 'Mapping', 'TextFieldSplitter'],\n         share_selected_features=0.2,\n         tags=['fast_prop', 'container-jIOQxX'])</pre> In\u00a0[21]: Copied! <pre>pipe.score(star_schema.test)\n</pre> pipe.score(star_schema.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\n</pre> Out[21]: date time           set used target    accuracy     auc cross entropy 0 2024-02-21 15:08:20 train cancelled 0.9825 0.8446 0.0736 1 2024-02-21 15:08:22 test cancelled 0.9825 0.8119 0.07529 In\u00a0[22]: Copied! <pre>pipe.features.to_sql()[pipe.features.sort(by=\"importances\")[0].name]\n</pre> pipe.features.to_sql()[pipe.features.sort(by=\"importances\")[0].name] Out[22]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_190\";\n\nCREATE TABLE \"FEATURE_1_190\" AS\nSELECT AVG( t2.\"description__mapping_3_target_1_avg\" ) AS \"feature_1_190\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"POPULATION__STAGING_TABLE_1__DESCRIPTION\" t2\nON t1.\"rowid\" = t2.\"rownum\"\nGROUP BY t1.rowid;\n</pre> In\u00a0[23]: Copied! <pre>pipe.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"online_retail_spark\")\n</pre> pipe.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"online_retail_spark\") In\u00a0[24]: Copied! <pre>if RUN_SPARK:\n    spark = SparkSession.builder.appName(\n        \"online_retail\"\n    ).config(\n        \"spark.driver.maxResultSize\",\"5g\"\n    ).config(\n        \"spark.driver.memory\", \"5g\"\n    ).config(\n        \"spark.executor.memory\", \"5g\"\n    ).config(\n        \"spark.sql.execution.arrow.pyspark.enabled\", \"true\"\n    ).config(\n        \"spark.sql.session.timeZone\", \"UTC\"\n    ).enableHiveSupport().getOrCreate()\n\n    spark.sparkContext.setLogLevel(\"ERROR\")\n</pre> if RUN_SPARK:     spark = SparkSession.builder.appName(         \"online_retail\"     ).config(         \"spark.driver.maxResultSize\",\"5g\"     ).config(         \"spark.driver.memory\", \"5g\"     ).config(         \"spark.executor.memory\", \"5g\"     ).config(         \"spark.sql.execution.arrow.pyspark.enabled\", \"true\"     ).config(         \"spark.sql.session.timeZone\", \"UTC\"     ).enableHiveSupport().getOrCreate()      spark.sparkContext.setLogLevel(\"ERROR\") In\u00a0[25]: Copied! <pre>if RUN_SPARK:\n    population_spark = star_schema.train.population.to_pyspark(spark, name=\"population\")\n    peripheral_spark = star_schema.full_data.to_pyspark(spark, name=\"full_data\")\n</pre> if RUN_SPARK:     population_spark = star_schema.train.population.to_pyspark(spark, name=\"population\")     peripheral_spark = star_schema.full_data.to_pyspark(spark, name=\"full_data\") In\u00a0[26]: Copied! <pre>if RUN_SPARK:\n    getml.spark.execute(spark, \"online_retail_spark\")\n</pre> if RUN_SPARK:     getml.spark.execute(spark, \"online_retail_spark\") <p>The resulting features are in a table called features. Here is how you can retrieve them:</p> In\u00a0[27]: Copied! <pre>if RUN_SPARK:\n    spark.sql(\"SELECT * FROM `FEATURES` LIMIT 20\").toPandas()\n</pre> if RUN_SPARK:     spark.sql(\"SELECT * FROM `FEATURES` LIMIT 20\").toPandas()","boost":0.8},{"location":"examples/enterprise-notebooks/online_retail/#online-retail-predicting-order-cancellations","title":"Online Retail - Predicting order cancellations\u00b6","text":"<p>In this tutorial, we demonstrate how getML can be applied in an e-commerce context. Using a dataset of about 400,000 orders, our goal is to predict whether an order will be cancelled.</p> <p>We also show that we can significantly improve our results by using getML's built-in hyperparameter tuning routines.</p> <p>Summary:</p> <ul> <li>Prediction type: Classification model</li> <li>Domain: E-commerce</li> <li>Prediction target: Whether an order will be cancelled</li> <li>Population size: 397925</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/online_retail/#background","title":"Background\u00b6","text":"<p>The data set contains about 400,000 orders from a British online retailer. Each order consists of a product that has been ordered and a corresponding quantity. Several orders can be summarized onto a single invoice. The goal is to predict whether an order will be cancelled.</p> <p>Because the company mainly sells to other businesses, the cancellation rate is relatively low, namely 1.83%.</p> <p>The data set has been originally collected for this study:</p> <p>Daqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining, Journal of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197-208, 2012 (Published online before print: 27 August 2012. doi: 10.1057/dbm.2012.17).</p> <p>It has been downloaded from the UCI Machine Learning Repository:</p> <p>Dua, D. and Graff, C. (2019). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/online_retail/#analysis","title":"Analysis\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/online_retail/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/online_retail/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>We begin by downloading the data from the source file:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/online_retail/#12-data-preparation","title":"1.2 Data preparation\u00b6","text":"<p>The invoice dates are in a somewhat unusual format, fo we need to rectify that.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/online_retail/#13-prepare-data-for-getml","title":"1.3 Prepare data for getML\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/online_retail/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"<p>We loaded the data and defined the roles and units. Next, we create a getML pipeline for relational learning.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/online_retail/#21-define-relational-model","title":"2.1 Define relational model\u00b6","text":"<p>To get started with relational learning, we need to specify the data model.</p> <p>In our case, there are two joins we are interested in:</p> <ol> <li><p>We want to take a look at all of the other orders on the same invoice.</p> </li> <li><p>We want to check out how often a certain customer has cancelled orders in the past. Here, we limit ourselves to the last 90 days. To avoid data leaks, we set a horizon of one day.</p> </li> </ol>","boost":0.8},{"location":"examples/enterprise-notebooks/online_retail/#22-getml-pipeline","title":"2.2 getML pipeline\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/online_retail/#23-model-training","title":"2.3 Model training\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/online_retail/#24-model-evaluation","title":"2.4 Model evaluation\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/online_retail/#25-features","title":"2.5 Features\u00b6","text":"<p>The most important feature looks as follows:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/online_retail/#26-productionization","title":"2.6 Productionization\u00b6","text":"<p>It is possible to productionize the pipeline by transpiling the features into production-ready SQL code.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/online_retail/#3-conclusion","title":"3. Conclusion\u00b6","text":"<p>In this notebook we have demonstrated how getML can be applied to an e-commerce setting. In particular, we have seen how results can be improved using the built-in hyperparamater tuning routines.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/robot/","title":"<span class=\"ntitle\">robot.ipynb</span> <span class=\"ndesc\">Feature engineering on sensor data</span>","text":"In\u00a0[1]: Copied! <pre>import datetime\nimport os\nfrom pathlib import Path\nfrom urllib import request\n\nimport pandas as pd\nimport numpy as np\nimport getml\nimport getml.data as data\nimport getml.database as database\nimport getml.engine as engine\nimport getml.feature_learning.aggregations as agg\nimport getml.data.roles as roles\n\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline  \n \ngetml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.engine.set_project('robot')\n</pre> import datetime import os from pathlib import Path from urllib import request  import pandas as pd import numpy as np import getml import getml.data as data import getml.database as database import getml.engine as engine import getml.feature_learning.aggregations as agg import getml.data.roles as roles  from IPython.display import Image import matplotlib.pyplot as plt %matplotlib inline     getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.engine.set_project('robot') <pre>getML engine is already running.\n\nConnected to project 'robot'\n</pre> In\u00a0[2]: Copied! <pre>data_all = getml.data.DataFrame.from_csv(\n    \"https://static.getml.com/datasets/robotarm/robot-demo.csv\", \n    \"data_all\"\n)\n</pre> data_all = getml.data.DataFrame.from_csv(     \"https://static.getml.com/datasets/robotarm/robot-demo.csv\",      \"data_all\" ) <pre>Downloading robot-demo.csv...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n</pre> In\u00a0[3]: Copied! <pre>data_all\n</pre> data_all Out[3]:  name            3            4            5            6            7            8            9           10           11           12           13           14           15           16           17           18           19           20           21           22           23           24           25           26           27           28           29           30           31           32           33           34           35           36           37           38           39           40           41           42           43           44           45           46           47           48           49           50           51           52           53           54           55           56           57           58           59           60           61           62           63           64           65           66           67           68           69           70           71           72           73           74           75           76           77           78           79           80           81           82           83           84           85           86           98           99          100          101          102          103          104          105          106          f_x          f_y          f_z  role unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float 0 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3274 0.9605 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1233 -6.5483 -2.8045 -0.8296 0.07625 -0.1906 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1983 0.7699 0.41 0.08279 -1.4094 0.786 -0.3682 0 0 0 0 0 0 -22.654 -11.503 -18.673 -3.5155 5.8354 -2.05 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.069 48.009 0.9668 47.834 47.925 47.818 47.834 47.955 47.971 -11.03 6.9 -7.33 1 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3274 0.9604 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1188 -6.5506 -2.8404 -0.8281 0.06405 -0.1998 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1983 0.7699 0.41 0.0828 -1.4094 0.7859 -0.3682 0 0 0 0 0 0 -21.627 -11.046 -18.66 -3.5395 5.7577 -1.9805 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.009 48.009 0.8594 47.834 47.925 47.818 47.834 47.955 47.971 -10.848 6.7218 -7.4427 2 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3274 0.9605 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1099 -6.5438 -2.8 -0.8205 0.07473 -0.183 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1922 0.7699 0.41 0.08279 -1.4094 0.7859 -0.3682 0 0 0 0 0 0 -23.843 -12.127 -18.393 -3.6453 5.978 -1.9978 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.009 48.069 0.931 47.879 47.925 47.818 47.834 47.955 47.971 -10.666 6.5436 -7.5555 3 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3273 0.9604 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1233 -6.5483 -2.8224 -0.8266 0.07168 -0.1998 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1967 0.7699 0.41 0.08275 -1.4094 0.786 -0.3681 0 0 0 0 0 0 -21.772 -10.872 -18.691 -3.5512 5.6648 -1.9976 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.069 48.069 0.931 47.879 47.925 47.818 47.834 47.955 47.971 -10.507 6.4533 -7.65 4 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3274 0.9604 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1255 -6.5394 -2.8 -0.8327 0.07473 -0.1952 0.1211 -6.5483 -2.8157 -0.8327 0.07015 -0.1922 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 -22.823 -11.645 -18.524 -3.5305 5.8712 -2.0096 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.069 48.069 0.8952 47.879 47.925 47.818 47.834 47.955 47.971 -10.413 6.6267 -7.69 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 14996 3.0837 -0.8836 1.4501 -2.2102 -1.559 -5.3265 -0.03151 -0.05375 0.04732 0.1482 -0.05218 0.06706 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3694 -4.1879 -1.1847 -0.09441 -0.1568 0.1898 1.1605 -42.951 -19.023 -2.6343 0.1551 -0.1338 3.0836 -0.8836 1.4503 -2.2101 -1.5591 -5.3263 -0.03347 -0.05585 0.04805 0.151 -0.05513 0.07114 -0.3564 -6.0394 -2.3001 -0.2181 -0.1159 0.09608 -0.3632 -6.0394 -2.3023 -0.212 -0.125 0.1113 0.7116 0.06957 0.06036 -0.8506 2.9515 -0.03352 -0.03558 -0.03029 0.002444 -0.04208 0.1458 -0.1098 -0.8784 -0.07291 -37.584 0.0001132 -2.1031 0.03318 0.7117 0.0697 0.06044 -0.8511 2.951 -0.03356 -0.03508 -0.02849 0.001571 -0.03951 0.1442 -0.1036 48.069 48.009 0.8952 47.818 47.834 47.818 47.803 47.94 47.94 10.84 -1.41 16.14 14997 3.0835 -0.884 1.4505 -2.2091 -1.5594 -5.326 -0.02913 -0.0497 0.04376 0.137 -0.04825 0.062 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3677 -4.1837 -1.1874 -0.09682 -0.1562 0.189 1.1592 -42.937 -19.023 -2.6331 0.1545 -0.1338 3.0833 -0.8841 1.4507 -2.209 -1.5596 -5.3258 -0.02909 -0.04989 0.04198 0.1481 -0.05465 0.06249 -0.3161 -6.1179 -2.253 -0.3752 -0.03965 0.08693 -0.3273 -6.1022 -2.2597 -0.366 -0.05033 0.0915 0.7114 0.06932 0.06039 -0.8497 2.953 -0.03359 -0.0335 -0.02723 0.001208 -0.04242 0.1428 -0.0967 -2.7137 0.8552 -38.514 -0.6088 -3.2383 -0.9666 0.7114 0.06948 0.06045 -0.8503 2.9525 -0.03359 -0.03246 -0.02633 0.001469 -0.03657 0.1333 -0.09571 48.009 48.009 0.8594 47.818 47.834 47.818 47.803 47.94 47.94 10.857 -1.52 15.943 14998 3.0833 -0.8844 1.4508 -2.208 -1.5598 -5.3256 -0.02676 -0.04565 0.04019 0.1258 -0.04431 0.05695 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3659 -4.1797 -1.1901 -0.09922 -0.1555 0.1881 1.1579 -42.924 -19.023 -2.6321 0.154 -0.1338 3.0831 -0.8844 1.451 -2.2078 -1.56 -5.3253 -0.02776 -0.04382 0.03652 0.1295 -0.05064 0.04818 -0.343 -6.2569 -2.1566 -0.3035 0.00305 0.1434 -0.3385 -6.2322 -2.1589 -0.302 -0.00915 0.1571 0.7111 0.06912 0.06039 -0.849 2.9544 -0.0337 -0.02911 -0.02589 0.001292 -0.04046 0.1246 -0.08058 4.2749 1.0128 -36.412 -1.2811 -0.4296 -1.1013 0.7112 0.06928 0.06046 -0.8495 2.9538 -0.03362 -0.02984 -0.02417 0.001364 -0.03362 0.1224 -0.08786 48.009 48.009 0.931 47.818 47.834 47.818 47.803 47.94 47.94 10.89 -1.74 15.55 14999 3.0831 -0.8847 1.4511 -2.2071 -1.5602 -5.3251 -0.02438 -0.0416 0.03662 0.1147 -0.04038 0.0519 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3642 -4.1758 -1.1928 -0.1016 -0.1548 0.1873 1.1568 -42.912 -19.023 -2.6311 0.1535 -0.1338 3.0829 -0.8848 1.4513 -2.2068 -1.5604 -5.3249 -0.02149 -0.04059 0.03417 0.1202 -0.0395 0.04178 -0.4237 -6.2703 -2.0939 -0.302 -0.01372 0.1739 -0.4125 -6.2569 -2.0916 -0.2943 -0.02898 0.1891 0.7109 0.06894 0.06039 -0.8484 2.9557 -0.03384 -0.02738 -0.01982 0.001031 -0.03028 0.1157 -0.06702 11.518 1.5002 -39.314 -1.8671 -0.3734 -0.5733 0.7109 0.06909 0.06047 -0.8488 2.955 -0.03364 -0.02721 -0.02201 0.001255 -0.03067 0.1115 -0.08003 48.009 48.009 0.931 47.818 47.834 47.818 47.803 47.94 47.94 11.29 -1.4601 15.743 15000 3.0829 -0.885 1.4514 -2.2062 -1.5605 -5.3247 -0.02201 -0.03755 0.03305 0.1035 -0.03645 0.04684 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3624 -4.172 -1.1955 -0.1041 -0.1542 0.1864 1.1558 -42.901 -19.023 -2.6302 0.1531 -0.1338 3.0827 -0.8851 1.4516 -2.2059 -1.5607 -5.3246 -0.02096 -0.03808 0.02958 0.1171 -0.03289 0.03883 -0.417 -6.2434 -2.058 -0.4102 -0.04728 0.1967 -0.4237 -6.2367 -2.0714 -0.4163 -0.0671 0.2059 0.7107 0.06878 0.06041 -0.8478 2.9567 -0.03382 -0.02535 -0.01854 0.001614 -0.02421 0.11 -0.06304 15.099 2.936 -39.068 -1.9402 0.139 -0.2674 0.7107 0.06893 0.06048 -0.8482 2.9561 -0.03367 -0.02458 -0.01986 0.001142 -0.0277 0.1007 -0.07221 48.009 48.069 0.8952 47.818 47.834 47.818 47.803 47.94 47.955 11.69 -1.1801 15.937 <p>     15001 rows x 96 columns     memory usage: 11.52 MB     name: data_all     type: getml.DataFrame </p> In\u00a0[4]: Copied! <pre>data_all.set_role([\"f_x\", \"f_y\", \"f_z\"], getml.data.roles.target)\ndata_all.set_role(data_all.roles.unused, getml.data.roles.numerical)\n</pre> data_all.set_role([\"f_x\", \"f_y\", \"f_z\"], getml.data.roles.target) data_all.set_role(data_all.roles.unused, getml.data.roles.numerical) <p>This is what the data set looks like:</p> In\u00a0[5]: Copied! <pre>data_all\n</pre> data_all Out[5]:  name     f_x     f_y     f_z         3         4         5         6         7         8         9        10        11        12        13        14        15        16        17        18        19        20        21        22        23        24        25        26        27        28        29        30        31        32        33        34        35        36        37        38        39        40        41        42        43        44        45        46        47        48        49        50        51        52        53        54        55        56        57        58        59        60        61        62        63        64        65        66        67        68        69        70        71        72        73        74        75        76        77        78        79        80        81        82        83        84        85        86        98        99       100       101       102       103       104       105       106  role  target  target  target numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical 0 -11.03 6.9 -7.33 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3274 0.9605 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1233 -6.5483 -2.8045 -0.8296 0.07625 -0.1906 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1983 0.7699 0.41 0.08279 -1.4094 0.786 -0.3682 0 0 0 0 0 0 -22.654 -11.503 -18.673 -3.5155 5.8354 -2.05 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.069 48.009 0.9668 47.834 47.925 47.818 47.834 47.955 47.971 1 -10.848 6.7218 -7.4427 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3274 0.9604 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1188 -6.5506 -2.8404 -0.8281 0.06405 -0.1998 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1983 0.7699 0.41 0.0828 -1.4094 0.7859 -0.3682 0 0 0 0 0 0 -21.627 -11.046 -18.66 -3.5395 5.7577 -1.9805 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.009 48.009 0.8594 47.834 47.925 47.818 47.834 47.955 47.971 2 -10.666 6.5436 -7.5555 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3274 0.9605 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1099 -6.5438 -2.8 -0.8205 0.07473 -0.183 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1922 0.7699 0.41 0.08279 -1.4094 0.7859 -0.3682 0 0 0 0 0 0 -23.843 -12.127 -18.393 -3.6453 5.978 -1.9978 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.009 48.069 0.931 47.879 47.925 47.818 47.834 47.955 47.971 3 -10.507 6.4533 -7.65 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3273 0.9604 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1233 -6.5483 -2.8224 -0.8266 0.07168 -0.1998 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1967 0.7699 0.41 0.08275 -1.4094 0.786 -0.3681 0 0 0 0 0 0 -21.772 -10.872 -18.691 -3.5512 5.6648 -1.9976 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.069 48.069 0.931 47.879 47.925 47.818 47.834 47.955 47.971 4 -10.413 6.6267 -7.69 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3274 0.9604 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1255 -6.5394 -2.8 -0.8327 0.07473 -0.1952 0.1211 -6.5483 -2.8157 -0.8327 0.07015 -0.1922 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 -22.823 -11.645 -18.524 -3.5305 5.8712 -2.0096 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.069 48.069 0.8952 47.879 47.925 47.818 47.834 47.955 47.971 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 14996 10.84 -1.41 16.14 3.0837 -0.8836 1.4501 -2.2102 -1.559 -5.3265 -0.03151 -0.05375 0.04732 0.1482 -0.05218 0.06706 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3694 -4.1879 -1.1847 -0.09441 -0.1568 0.1898 1.1605 -42.951 -19.023 -2.6343 0.1551 -0.1338 3.0836 -0.8836 1.4503 -2.2101 -1.5591 -5.3263 -0.03347 -0.05585 0.04805 0.151 -0.05513 0.07114 -0.3564 -6.0394 -2.3001 -0.2181 -0.1159 0.09608 -0.3632 -6.0394 -2.3023 -0.212 -0.125 0.1113 0.7116 0.06957 0.06036 -0.8506 2.9515 -0.03352 -0.03558 -0.03029 0.002444 -0.04208 0.1458 -0.1098 -0.8784 -0.07291 -37.584 0.0001132 -2.1031 0.03318 0.7117 0.0697 0.06044 -0.8511 2.951 -0.03356 -0.03508 -0.02849 0.001571 -0.03951 0.1442 -0.1036 48.069 48.009 0.8952 47.818 47.834 47.818 47.803 47.94 47.94 14997 10.857 -1.52 15.943 3.0835 -0.884 1.4505 -2.2091 -1.5594 -5.326 -0.02913 -0.0497 0.04376 0.137 -0.04825 0.062 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3677 -4.1837 -1.1874 -0.09682 -0.1562 0.189 1.1592 -42.937 -19.023 -2.6331 0.1545 -0.1338 3.0833 -0.8841 1.4507 -2.209 -1.5596 -5.3258 -0.02909 -0.04989 0.04198 0.1481 -0.05465 0.06249 -0.3161 -6.1179 -2.253 -0.3752 -0.03965 0.08693 -0.3273 -6.1022 -2.2597 -0.366 -0.05033 0.0915 0.7114 0.06932 0.06039 -0.8497 2.953 -0.03359 -0.0335 -0.02723 0.001208 -0.04242 0.1428 -0.0967 -2.7137 0.8552 -38.514 -0.6088 -3.2383 -0.9666 0.7114 0.06948 0.06045 -0.8503 2.9525 -0.03359 -0.03246 -0.02633 0.001469 -0.03657 0.1333 -0.09571 48.009 48.009 0.8594 47.818 47.834 47.818 47.803 47.94 47.94 14998 10.89 -1.74 15.55 3.0833 -0.8844 1.4508 -2.208 -1.5598 -5.3256 -0.02676 -0.04565 0.04019 0.1258 -0.04431 0.05695 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3659 -4.1797 -1.1901 -0.09922 -0.1555 0.1881 1.1579 -42.924 -19.023 -2.6321 0.154 -0.1338 3.0831 -0.8844 1.451 -2.2078 -1.56 -5.3253 -0.02776 -0.04382 0.03652 0.1295 -0.05064 0.04818 -0.343 -6.2569 -2.1566 -0.3035 0.00305 0.1434 -0.3385 -6.2322 -2.1589 -0.302 -0.00915 0.1571 0.7111 0.06912 0.06039 -0.849 2.9544 -0.0337 -0.02911 -0.02589 0.001292 -0.04046 0.1246 -0.08058 4.2749 1.0128 -36.412 -1.2811 -0.4296 -1.1013 0.7112 0.06928 0.06046 -0.8495 2.9538 -0.03362 -0.02984 -0.02417 0.001364 -0.03362 0.1224 -0.08786 48.009 48.009 0.931 47.818 47.834 47.818 47.803 47.94 47.94 14999 11.29 -1.4601 15.743 3.0831 -0.8847 1.4511 -2.2071 -1.5602 -5.3251 -0.02438 -0.0416 0.03662 0.1147 -0.04038 0.0519 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3642 -4.1758 -1.1928 -0.1016 -0.1548 0.1873 1.1568 -42.912 -19.023 -2.6311 0.1535 -0.1338 3.0829 -0.8848 1.4513 -2.2068 -1.5604 -5.3249 -0.02149 -0.04059 0.03417 0.1202 -0.0395 0.04178 -0.4237 -6.2703 -2.0939 -0.302 -0.01372 0.1739 -0.4125 -6.2569 -2.0916 -0.2943 -0.02898 0.1891 0.7109 0.06894 0.06039 -0.8484 2.9557 -0.03384 -0.02738 -0.01982 0.001031 -0.03028 0.1157 -0.06702 11.518 1.5002 -39.314 -1.8671 -0.3734 -0.5733 0.7109 0.06909 0.06047 -0.8488 2.955 -0.03364 -0.02721 -0.02201 0.001255 -0.03067 0.1115 -0.08003 48.009 48.009 0.931 47.818 47.834 47.818 47.803 47.94 47.94 15000 11.69 -1.1801 15.937 3.0829 -0.885 1.4514 -2.2062 -1.5605 -5.3247 -0.02201 -0.03755 0.03305 0.1035 -0.03645 0.04684 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3624 -4.172 -1.1955 -0.1041 -0.1542 0.1864 1.1558 -42.901 -19.023 -2.6302 0.1531 -0.1338 3.0827 -0.8851 1.4516 -2.2059 -1.5607 -5.3246 -0.02096 -0.03808 0.02958 0.1171 -0.03289 0.03883 -0.417 -6.2434 -2.058 -0.4102 -0.04728 0.1967 -0.4237 -6.2367 -2.0714 -0.4163 -0.0671 0.2059 0.7107 0.06878 0.06041 -0.8478 2.9567 -0.03382 -0.02535 -0.01854 0.001614 -0.02421 0.11 -0.06304 15.099 2.936 -39.068 -1.9402 0.139 -0.2674 0.7107 0.06893 0.06048 -0.8482 2.9561 -0.03367 -0.02458 -0.01986 0.001142 -0.0277 0.1007 -0.07221 48.009 48.069 0.8952 47.818 47.834 47.818 47.803 47.94 47.955 <p>     15001 rows x 96 columns     memory usage: 11.52 MB     name: data_all     type: getml.DataFrame </p> In\u00a0[6]: Copied! <pre>split = getml.data.split.time(data_all, \"rowid\", test=10500)\nsplit\n</pre> split = getml.data.split.time(data_all, \"rowid\", test=10500) split Out[6]: 0 train 1 train 2 train 3 train 4 train ... <p>     15001 rows          type: StringColumnView </p> In\u00a0[7]: Copied! <pre>time_series = getml.data.TimeSeries(\n    population=data_all,\n    split=split,\n    time_stamps=\"rowid\",\n    lagged_targets=False,\n    memory=30,\n)\n\ntime_series\n</pre> time_series = getml.data.TimeSeries(     population=data_all,     split=split,     time_stamps=\"rowid\",     lagged_targets=False,     memory=30, )  time_series Out[7]: data model diagram data_allpopulationrowid &lt;= rowidMemory: 30 time steps staging data frames staging table               0 population POPULATION__STAGING_TABLE_1 1 data_all DATA_ALL__STAGING_TABLE_2 container population subset name      rows type 0 test data_all 4501 View 1 train data_all 10500 View peripheral name      rows type 0 data_all 15001 View In\u00a0[8]: Copied! <pre>relboost = getml.feature_learning.Relboost(\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    num_features=10,\n)\n\nxgboost = getml.predictors.XGBoostRegressor()\n\npipe1 = getml.pipeline.Pipeline(\n    data_model=time_series.data_model,\n    feature_learners=[relboost],\n    predictors=xgboost\n)\n</pre> relboost = getml.feature_learning.Relboost(     loss_function=getml.feature_learning.loss_functions.SquareLoss,     num_features=10, )  xgboost = getml.predictors.XGBoostRegressor()  pipe1 = getml.pipeline.Pipeline(     data_model=time_series.data_model,     feature_learners=[relboost],     predictors=xgboost ) <p>It is always a good idea to check the pipeline for any potential issues.</p> In\u00a0[9]: Copied! <pre>pipe1.check(time_series.train)\n</pre> pipe1.check(time_series.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\n</pre> In\u00a0[10]: Copied! <pre>pipe1.fit(time_series.train)\n</pre> pipe1.fit(time_series.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:28, remaining: 00:00]          \nRelboost: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:28, remaining: 00:00]          \nRelboost: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:27, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:05, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:05, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:05, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:03, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:03, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:03, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:1m:46.779085\n\n</pre> Out[10]: <pre>Pipeline(data_model='population',\n         feature_learners=['Relboost'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=['data_all'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['container-t4e4hB'])</pre> In\u00a0[11]: Copied! <pre>pipe1.score(time_series.test)\n</pre> pipe1.score(time_series.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \n\n</pre> Out[11]: date time           set used target     mae    rmse rsquared 0 2024-02-21 15:09:11 train f_x 0.4449 0.5852 0.9962 1 2024-02-21 15:09:11 train f_y 0.5159 0.6813 0.9893 2 2024-02-21 15:09:11 train f_z 0.275 0.3576 0.9988 3 2024-02-21 15:09:17 test f_x 0.5681 0.7374 0.9949 4 2024-02-21 15:09:17 test f_y 0.5654 0.7516 0.9871 5 2024-02-21 15:09:17 test f_z 0.2957 0.3845 0.9986 In\u00a0[12]: Copied! <pre>plt.subplots(figsize=(20, 10))\n\nnames, importances = pipe1.features.importances(target_num=0)\n\nplt.bar(names[0:30], importances[0:30])\n\nplt.title(\"feature importances for the x-component\", size=20)\nplt.grid(True)\nplt.xlabel(\"features\")\nplt.ylabel(\"importance\")\nplt.xticks(rotation='vertical')\n\nplt.show()\n</pre> plt.subplots(figsize=(20, 10))  names, importances = pipe1.features.importances(target_num=0)  plt.bar(names[0:30], importances[0:30])  plt.title(\"feature importances for the x-component\", size=20) plt.grid(True) plt.xlabel(\"features\") plt.ylabel(\"importance\") plt.xticks(rotation='vertical')  plt.show() In\u00a0[13]: Copied! <pre>plt.subplots(figsize=(20, 10))\n\nnames, importances = pipe1.features.importances(target_num=1)\n\nplt.bar(names[0:30], importances[0:30])\n\nplt.title(\"feature importances for the y-component\", size=20)\nplt.grid(True)\nplt.xlabel(\"features\")\nplt.ylabel(\"importance\")\nplt.xticks(rotation='vertical')\n\nplt.show()\n</pre> plt.subplots(figsize=(20, 10))  names, importances = pipe1.features.importances(target_num=1)  plt.bar(names[0:30], importances[0:30])  plt.title(\"feature importances for the y-component\", size=20) plt.grid(True) plt.xlabel(\"features\") plt.ylabel(\"importance\") plt.xticks(rotation='vertical')  plt.show() In\u00a0[14]: Copied! <pre>plt.subplots(figsize=(20, 10))\n\nnames, importances = pipe1.features.importances(target_num=2)\n\nplt.bar(names[0:30], importances[0:30])\n\nplt.title(\"feature importances for the z-component\", size=20)\nplt.grid(True)\nplt.xlabel(\"features\")\nplt.ylabel(\"importance\")\nplt.xticks(rotation='vertical')\n\nplt.show()\n</pre> plt.subplots(figsize=(20, 10))  names, importances = pipe1.features.importances(target_num=2)  plt.bar(names[0:30], importances[0:30])  plt.title(\"feature importances for the z-component\", size=20) plt.grid(True) plt.xlabel(\"features\") plt.ylabel(\"importance\") plt.xticks(rotation='vertical')  plt.show() In\u00a0[15]: Copied! <pre>plt.subplots(figsize=(20, 10))\n\nnames, importances = pipe1.columns.importances(target_num=0)\n\nplt.bar(names[0:30], importances[0:30])\n\nplt.title(\"column importances for the x-component\", size=20)\nplt.grid(True)\nplt.xlabel(\"column\")\nplt.ylabel(\"importance\")\nplt.xticks(rotation='vertical')\n\nplt.show()\n</pre> plt.subplots(figsize=(20, 10))  names, importances = pipe1.columns.importances(target_num=0)  plt.bar(names[0:30], importances[0:30])  plt.title(\"column importances for the x-component\", size=20) plt.grid(True) plt.xlabel(\"column\") plt.ylabel(\"importance\") plt.xticks(rotation='vertical')  plt.show() In\u00a0[16]: Copied! <pre>plt.subplots(figsize=(20, 10))\n\nnames, importances = pipe1.columns.importances(target_num=1)\n\nplt.bar(names[0:30], importances[0:30])\n\nplt.title(\"column importances for the y-component\", size=20)\nplt.grid(True)\nplt.xlabel(\"column\")\nplt.ylabel(\"importance\")\nplt.xticks(rotation='vertical')\n\nplt.show()\n</pre> plt.subplots(figsize=(20, 10))  names, importances = pipe1.columns.importances(target_num=1)  plt.bar(names[0:30], importances[0:30])  plt.title(\"column importances for the y-component\", size=20) plt.grid(True) plt.xlabel(\"column\") plt.ylabel(\"importance\") plt.xticks(rotation='vertical')  plt.show() In\u00a0[17]: Copied! <pre>plt.subplots(figsize=(20, 10))\n\nnames, importances = pipe1.columns.importances(target_num=2)\n\nplt.bar(names[0:30], importances[0:30])\n\nplt.title(\"column importances for the z-component\", size=20)\nplt.grid(True)\nplt.xlabel(\"column\")\nplt.ylabel(\"importance\")\nplt.xticks(rotation='vertical')\n\nplt.show()\n</pre> plt.subplots(figsize=(20, 10))  names, importances = pipe1.columns.importances(target_num=2)  plt.bar(names[0:30], importances[0:30])  plt.title(\"column importances for the z-component\", size=20) plt.grid(True) plt.xlabel(\"column\") plt.ylabel(\"importance\") plt.xticks(rotation='vertical')  plt.show() <p>The <code>.select(...)</code> returns a new column, in which the unimportant columns have been dropped:</p> In\u00a0[18]: Copied! <pre>time_series2 = pipe1.columns.select(time_series, share_selected_columns=0.35)\n</pre> time_series2 = pipe1.columns.select(time_series, share_selected_columns=0.35) In\u00a0[19]: Copied! <pre>time_series2\n</pre> time_series2 Out[19]: data model diagram data_allpopulationrowid &lt;= rowidMemory: 30 time steps staging data frames staging table               0 population POPULATION__STAGING_TABLE_1 1 data_all DATA_ALL__STAGING_TABLE_2 container population subset name      rows type 0 train data_all 10500 View 1 test data_all 4501 View peripheral name      rows type 0 data_all 15001 View In\u00a0[20]: Copied! <pre>multirel = getml.feature_learning.Multirel(\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    num_features=10,\n)\n\nrelboost = getml.feature_learning.Relboost(\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    num_features=10,\n)\n\nxgboost = getml.predictors.XGBoostRegressor(n_jobs=7)\n\npipe2 = getml.pipeline.Pipeline(\n    data_model=time_series2.data_model,\n    feature_learners=[multirel, relboost],\n    predictors=xgboost\n)\n</pre> multirel = getml.feature_learning.Multirel(     loss_function=getml.feature_learning.loss_functions.SquareLoss,     num_features=10, )  relboost = getml.feature_learning.Relboost(     loss_function=getml.feature_learning.loss_functions.SquareLoss,     num_features=10, )  xgboost = getml.predictors.XGBoostRegressor(n_jobs=7)  pipe2 = getml.pipeline.Pipeline(     data_model=time_series2.data_model,     feature_learners=[multirel, relboost],     predictors=xgboost ) In\u00a0[21]: Copied! <pre>pipe2.check(time_series2.train)\n</pre> pipe2.check(time_series2.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 1 issues labeled INFO and 0 issues labeled WARNING.\n</pre> Out[21]: type label           message                          0 INFO MIGHT TAKE LONG DATA_ALL__STAGING_TABLE_2 contains 27 categorical and numerical columns. Please note that columns created by the preprocessors are also part of this count. The multirel algorithm does not scale very well to data frames with many columns. This pipeline might take a very long time to fit. You should consider removing some columns or preprocessors. You could use a column selection to pick the right columns. You could also replace Multirel with Relboost or Fastboost. Both algorithms have been designed to scale well to data frames with many columns. In\u00a0[22]: Copied! <pre>pipe2.fit(time_series2.train)\n</pre> pipe2.fit(time_series2.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 1 issues labeled INFO and 0 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nMultirel: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:09, remaining: 00:00]          \nRelboost: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:08, remaining: 00:00]          \nRelboost: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:09, remaining: 00:00]          \nRelboost: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:09, remaining: 00:00]          \nMultirel: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:03, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:03, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:44.950599\n\n</pre> Out[22]: <pre>Pipeline(data_model='population',\n         feature_learners=['Multirel', 'Relboost'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=['data_all'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['container-kCGrYY'])</pre> In\u00a0[23]: Copied! <pre>pipe2.score(time_series2.test)\n</pre> pipe2.score(time_series2.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nMultirel: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\n</pre> Out[23]: date time           set used target     mae    rmse rsquared 0 2024-02-21 15:10:04 train f_x 0.4488 0.5888 0.9961 1 2024-02-21 15:10:04 train f_y 0.5262 0.6887 0.989 2 2024-02-21 15:10:04 train f_z 0.2722 0.3572 0.9988 3 2024-02-21 15:10:09 test f_x 0.5642 0.7379 0.9949 4 2024-02-21 15:10:09 test f_y 0.5689 0.7563 0.987 5 2024-02-21 15:10:09 test f_z 0.2955 0.3838 0.9986 In\u00a0[24]: Copied! <pre>f_x = time_series2.test.population[\"f_x\"].to_numpy()\nf_y = time_series2.test.population[\"f_y\"].to_numpy()\nf_z = time_series2.test.population[\"f_z\"].to_numpy()\n</pre> f_x = time_series2.test.population[\"f_x\"].to_numpy() f_y = time_series2.test.population[\"f_y\"].to_numpy() f_z = time_series2.test.population[\"f_z\"].to_numpy() In\u00a0[25]: Copied! <pre>predictions = pipe2.predict(time_series2.test)\n</pre> predictions = pipe2.predict(time_series2.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nMultirel: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\n</pre> In\u00a0[26]: Copied! <pre>plt.subplots(figsize=(20, 10))\n\nplt.title(\"x-component of the force vector\", size=20)\n\nplt.plot(f_x, label=\"ground truth\")\nplt.plot(predictions[:,0], label=\"prediction\")\n\nplt.legend(loc=\"upper right\", fontsize=16)\n</pre> plt.subplots(figsize=(20, 10))  plt.title(\"x-component of the force vector\", size=20)  plt.plot(f_x, label=\"ground truth\") plt.plot(predictions[:,0], label=\"prediction\")  plt.legend(loc=\"upper right\", fontsize=16) Out[26]: <pre>&lt;matplotlib.legend.Legend at 0x7f3dd916bee0&gt;</pre> In\u00a0[27]: Copied! <pre>plt.subplots(figsize=(20, 10))\n\nplt.title(\"y-component of the force vector\", size=20)\n\nplt.plot(f_y, label=\"ground truth\")\nplt.plot(predictions[:,1], label=\"prediction\")\n\nplt.legend(loc=\"upper right\", fontsize=16)\n</pre> plt.subplots(figsize=(20, 10))  plt.title(\"y-component of the force vector\", size=20)  plt.plot(f_y, label=\"ground truth\") plt.plot(predictions[:,1], label=\"prediction\")  plt.legend(loc=\"upper right\", fontsize=16) Out[27]: <pre>&lt;matplotlib.legend.Legend at 0x7f3dd905a6a0&gt;</pre> In\u00a0[28]: Copied! <pre>plt.subplots(figsize=(20, 10))\n\nplt.title(\"z-component of the force vector\", size=20)\n\nplt.plot(f_z, label=\"ground truth\")\nplt.plot(predictions[:,2], label=\"prediction\")\n\nplt.legend(loc=\"upper right\", fontsize=16)\n</pre> plt.subplots(figsize=(20, 10))  plt.title(\"z-component of the force vector\", size=20)  plt.plot(f_z, label=\"ground truth\") plt.plot(predictions[:,2], label=\"prediction\")  plt.legend(loc=\"upper right\", fontsize=16) Out[28]: <pre>&lt;matplotlib.legend.Legend at 0x7f3dd8108f10&gt;</pre> In\u00a0[29]: Copied! <pre>pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name]\n</pre> pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name] Out[29]: <pre>DROP TABLE IF EXISTS \"FEATURE_3_1\";\n\nCREATE TABLE \"FEATURE_3_1\" AS\nSELECT SUM( \n    CASE\n        WHEN ( t2.\"7\" &gt; -1.207173 ) AND ( t2.\"61\" &gt; 1.002216 ) AND ( t2.\"7\" &gt; -1.204047 ) THEN 0.3135786678708609\n        WHEN ( t2.\"7\" &gt; -1.207173 ) AND ( t2.\"61\" &gt; 1.002216 ) AND ( t2.\"7\" &lt;= -1.204047 OR t2.\"7\" IS NULL ) THEN -2.576753557042288\n        WHEN ( t2.\"7\" &gt; -1.207173 ) AND ( t2.\"61\" &lt;= 1.002216 OR t2.\"61\" IS NULL ) AND ( t2.\"36\" &gt; -3.727979 ) THEN 0.08004451151017475\n        WHEN ( t2.\"7\" &gt; -1.207173 ) AND ( t2.\"61\" &lt;= 1.002216 OR t2.\"61\" IS NULL ) AND ( t2.\"36\" &lt;= -3.727979 OR t2.\"36\" IS NULL ) THEN 0.04537379635771868\n        WHEN ( t2.\"7\" &lt;= -1.207173 OR t2.\"7\" IS NULL ) AND ( t2.\"37\" &gt; -1.379178 ) AND ( t2.\"7\" &gt; -1.213190 ) THEN 1.867145078325692\n        WHEN ( t2.\"7\" &lt;= -1.207173 OR t2.\"7\" IS NULL ) AND ( t2.\"37\" &gt; -1.379178 ) AND ( t2.\"7\" &lt;= -1.213190 OR t2.\"7\" IS NULL ) THEN 0.616084077488769\n        WHEN ( t2.\"7\" &lt;= -1.207173 OR t2.\"7\" IS NULL ) AND ( t2.\"37\" &lt;= -1.379178 OR t2.\"37\" IS NULL ) AND ( t2.\"18\" &gt; 1.370758 ) THEN 0.9066167932530155\n        WHEN ( t2.\"7\" &lt;= -1.207173 OR t2.\"7\" IS NULL ) AND ( t2.\"37\" &lt;= -1.379178 OR t2.\"37\" IS NULL ) AND ( t2.\"18\" &lt;= 1.370758 OR t2.\"18\" IS NULL ) THEN 0.8253826886802063\n        ELSE NULL\n    END\n) AS \"feature_3_1\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"DATA_ALL__STAGING_TABLE_2\" t2\nON 1 = 1\nWHERE t2.\"rowid\" &lt;= t1.\"rowid\"\nAND ( t2.\"rowid__30_000000\" &gt; t1.\"rowid\" OR t2.\"rowid__30_000000\" IS NULL )\nGROUP BY t1.rowid;\n</pre> <p>As we can see, the predictions are very accurate. This suggests that it is very feasible to predict the force vector based on other sensor data.</p> In\u00a0[30]: Copied! <pre># Creates a folder named robot_pipeline containing\n# the SQL code.\npipe1.features.to_sql().save(\"robot_pipeline\", remove=True)\n</pre> # Creates a folder named robot_pipeline containing # the SQL code. pipe1.features.to_sql().save(\"robot_pipeline\", remove=True) In\u00a0[31]: Copied! <pre># Creates a folder named containing the SQL code for Apache Spark.\npipe1.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"robot_pipeline_spark\", remove=True)\n</pre> # Creates a folder named containing the SQL code for Apache Spark. pipe1.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"robot_pipeline_spark\", remove=True)","boost":0.8},{"location":"examples/enterprise-notebooks/robot/#robot-feature-engineering-on-sensor-data","title":"Robot - Feature engineering on sensor data\u00b6","text":"<p>The purpose of this notebook is to illustrate how we can overcome the feature explosion problem based on an example dataset involving sensor data.</p> <p>Summary:</p> <ul> <li>Prediction type: Regression</li> <li>Domain: Robotics</li> <li>Prediction target: The force vector on the robot's arm</li> <li>Population size: 15001</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/robot/#feature-explosion","title":"Feature explosion\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/robot/#the-problem","title":"The problem\u00b6","text":"<p>The feature explosion problem is one of the most important issues in automated feature engineering. In fact, it is probably the main reason why automated feature engineering is not already the norm in data science projects involving business data.</p> <p>To illustrate the problem, consider how data scientists write features for a simple time series problem:</p> <pre>SELECT SOME_AGGREGATION(t2.some_column)\nFROM some_table t1\nLEFT JOIN some_table t2\nON t1.join_key = t2.join_key\nWHERE t2.some_other_column &gt;= some_value\nAND t2.rowid &lt;= t1.rowid\nAND t2.rowid + some_other_value &gt; t1.rowid\nGROUP BY t1.rowid;\n</pre> <p>Think about that for a second.</p> <p>Every column that we have can either be aggregated (some_column) or it can be used for our conditions (some_other_column). That means if we have n columns to aggregate, we can potentially build conditions for $n$ other columns. In other words, the computational complexity is $n^2$ in the number of columns.</p> <p>Note that this problem occurs regardless of whether you automate feature engineering or you do it by hand. The size of the search space is $n^2$ in the number of columns in either case, unless you can rule something out a-priori.</p> <p>This problem is known as feature explosion.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/robot/#the-solution","title":"The solution\u00b6","text":"<p>So when we have relational data or time series with many columns, what do we do? The answer is to write different features. Specifically, suppose we had features like this:</p> <pre>SELECT SOME_AGGREGATION(\n    CASE \n         WHEN t2.some_column &gt; some_value THEN weight1\n         WHEN t2.some_column &lt;= some_value THEN weight2\n    END\n)\nFROM some_table t1\nLEFT JOIN some_table t2\nON t1.join_key = t2.join_key\nWHERE t2.rowid &lt;= t1.rowid\nAND t2.rowid + some_other_value &gt; t1.rowid\nGROUP BY t1.rowid;\n</pre> <p>weight1 and weight2 are learnable weights. An algorithm that generates features like this can only use columns for conditions, it is not allowed to aggregate columns \u2013 and it doesn't need to do so.</p> <p>That means the computational complexity is linear instead of quadratic. For data sets with a large number of columns this can make all the difference in the world. For instance, if you have 100 columns the size of the search space of the second approach is only 1% of the size of the search space of the first one.</p> <p>getML features an algorithm called relboost, which generates features according to this principle and is therefore very suitable for data sets with many columns.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/robot/#the-data-set","title":"The data set\u00b6","text":"<p>To illustrate the problem, we use a data set related to robotics. When robots interact with humans, the most important think is that they don't hurt people. In order to prevent such accidents, the force vector on the robot's arm is measured. However, measuring the force vector is expensive.</p> <p>Therefore, we want consider an alternative approach. We would like to predict the force vector based on other sensor data that are less costly to measure. To do so, we use machine learning.</p> <p>However, the data set contains measurements from almost 100 different sensors and we do not know which and how many sensors are relevant for predicting the force vector.</p> <p>The data set has been generously provided by Erik Berger who originally collected it for his dissertation:</p> <p>Berger, E. (2018). Behavior-Specific Proprioception Models for Robotic Force Estimation: A Machine Learning Approach. Freiberg, Germany: Technische Universitaet Bergakademie Freiberg.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/robot/#1-loading-data","title":"1. Loading data\u00b6","text":"<p>We begin by importing the libraries and setting the project.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/robot/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/robot/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"<p>The force vector consists of three component (f_x, f_y and f_z), meaning that we have three targets.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/robot/#13-separate-data-into-a-training-and-testing-set","title":"1.3 Separate data into a training and testing set\u00b6","text":"<p>We also want to separate the data set into a training and testing set. We do so by using the first 10,500 measurements for training and then using the remainder for testing.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/robot/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/robot/#21-building-the-pipeline","title":"2.1 Building the pipeline\u00b6","text":"<p>We then build a pipeline based on the relboost algorithm with xgboost as our predictor.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/robot/#22-fitting-the-pipeline","title":"2.2 Fitting the pipeline\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/robot/#23-evaluating-the-pipeline","title":"2.3 Evaluating the pipeline\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/robot/#24-feature-importances","title":"2.4 Feature importances\u00b6","text":"<p>It is always a good idea to study the features the relational learning algorithm has extracted.</p> <p>The feature importance is calculated by xgboost based on the improvement of the optimizing criterium at each split in the decision tree and is normalized to 100%.</p> <p>Also note that we have three different target (f_x, f_y and f_z) and that different features are relevant for different targets.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/robot/#25-column-importances","title":"2.5 Column importances\u00b6","text":"<p>Because getML is a tool for relational learning, we can also calculate the importances for the original columns, using similar methods we have used for the feature importances.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/robot/#26-column-selection","title":"2.6 Column selection\u00b6","text":"<p>When we study the plots for the column importances we find that there are some good news. We actually don't need that many columns. About 80% of the columns contain very little predictive value.</p> <p>This means that we can also apply other algorithms that are not as scalable as relboost. All we have to do is to select the most relevant columns:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/robot/#27-fitting-a-second-pipeline","title":"2.7 Fitting a second pipeline\u00b6","text":"<p>The multirel algorithm does scale well do data sets with many columns. As we have discussed in the introduction, its computational complexity is $n^2$ in the number of columns. But now, we only use 35% of the original columns, meaning that it is fine to use multirel.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/robot/#28-visualizing-the-predictions","title":"2.8 Visualizing the predictions\u00b6","text":"<p>Sometimes a picture says more than a 1000 words. We therefore want to visualize our predictions on the testing set.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/robot/#29-features","title":"2.9 Features\u00b6","text":"<p>The most important feature looks as follows:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/robot/#29-productionization","title":"2.9 Productionization\u00b6","text":"<p>It is possible to productionize the pipeline by transpiling the features into production-ready SQL code. Please also refer to getML's <code>sqlite3</code> and <code>spark</code> modules.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/robot/#3-conclusion","title":"3. Conclusion\u00b6","text":"<p>The purpose of this notebook has been to illustrate the problem of the curse of dimensionality when engineering features from datasets with many columns.</p> <p>The most important thing to remember is that this problem exists regardless of whether you engineer your features manually or using algorithms. Whether you like it or not: If you write your features in the traditional way, your search space grows quadratically with the number of columns.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/seznam/","title":"<span class=\"ntitle\">seznam.ipynb</span> <span class=\"ndesc\">Predicting transaction volume</span>","text":"<p>Let's get started with the analysis and set up your session:</p> In\u00a0[1]: Copied! <pre>import copy\nimport os\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\nfrom pathlib import Path\n\nfrom urllib import request\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline  \n\nimport featuretools\nimport woodwork as ww\nimport getml\n\ngetml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.set_project('seznam')\n</pre> import copy import os os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\" from pathlib import Path  from urllib import request  import numpy as np import pandas as pd from IPython.display import Image import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline    import featuretools import woodwork as ww import getml  getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.set_project('seznam') <pre>getML engine is already running.\nLoading pipelines... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\nConnected to project 'seznam'\n</pre> In\u00a0[2]: Copied! <pre>conn = getml.database.connect_mysql(\n    host=\"db.relational-data.org\",\n    dbname=\"Seznam\",\n    port=3306,\n    user=\"guest\",\n    password=\"relational\"\n)\n\nconn\n</pre> conn = getml.database.connect_mysql(     host=\"db.relational-data.org\",     dbname=\"Seznam\",     port=3306,     user=\"guest\",     password=\"relational\" )  conn Out[2]: <pre>Connection(dbname='Seznam', dialect='mysql', host='db.relational-data.org', port=3306)</pre> In\u00a0[3]: Copied! <pre>def load_if_needed(name):\n    \"\"\"\n    Loads the data from the relational learning\n    repository, if the data frame has not already\n    been loaded.\n    \"\"\"\n    if not getml.data.exists(name):\n        data_frame = getml.DataFrame.from_db(\n            name=name,\n            table_name=name,\n            conn=conn\n        )\n        data_frame.save()\n    else:\n        data_frame = getml.data.load_data_frame(name)\n    return data_frame\n</pre> def load_if_needed(name):     \"\"\"     Loads the data from the relational learning     repository, if the data frame has not already     been loaded.     \"\"\"     if not getml.data.exists(name):         data_frame = getml.DataFrame.from_db(             name=name,             table_name=name,             conn=conn         )         data_frame.save()     else:         data_frame = getml.data.load_data_frame(name)     return data_frame In\u00a0[4]: Copied! <pre>dobito = load_if_needed(\"dobito\")\nprobehnuto = load_if_needed(\"probehnuto\")\nprobehnuto_mimo_penezenku = load_if_needed(\"probehnuto_mimo_penezenku\")\n</pre> dobito = load_if_needed(\"dobito\") probehnuto = load_if_needed(\"probehnuto\") probehnuto_mimo_penezenku = load_if_needed(\"probehnuto_mimo_penezenku\") In\u00a0[5]: Copied! <pre>dobito\n</pre> dobito Out[5]:   name    client_id month_year_datum_transakce sluzba        kc_dobito       role unused_float unused_string              unused_string unused_string 0 7157857 2012-10-01 c 1045.62 1 109700 2015-10-01 c 5187.28 2 51508 2015-08-01 c 408.20 3 9573550 2012-10-01 c 521.24 4 9774621 2014-11-01 c 386.22 ... ... ... ... 554341 65283 2012-09-01 g 7850.00 554342 6091446 2012-08-01 g 31400.00 554343 1264806 2013-08-01 g -8220.52 554344 101103 2012-08-01 g 3140.00 554345 8674551 2012-08-01 g 6280.00 <p>     554346 rows x 4 columns     memory usage: 29.59 MB     name: dobito     type: getml.DataFrame </p> In\u00a0[6]: Copied! <pre>probehnuto\n</pre> probehnuto Out[6]:    name    client_id month_year_datum_transakce sluzba        kc_proklikano    role unused_float unused_string              unused_string unused_string 0 109145 2013-06-01 c -31.40 1 9804394 2015-10-01 h 37.68 2 9803353 2015-10-01 h 725.34 3 9801753 2015-10-01 h 194.68 4 9800425 2015-10-01 h 1042.48 ... ... ... ... 1462073 98857 2015-08-01 NULL 153.86 1462074 95776 2015-09-01 NULL 153.86 1462075 98857 2015-09-01 NULL 153.86 1462076 90001 2015-10-01 NULL 310.86 1462077 946957 2015-10-01 NULL 153.86 <p>     1462078 rows x 4 columns     memory usage: 77.07 MB     name: probehnuto     type: getml.DataFrame </p> In\u00a0[7]: Copied! <pre>probehnuto_mimo_penezenku\n</pre> probehnuto_mimo_penezenku Out[7]:   name    client_id Month/Year    probehla_inzerce_mimo_penezenku   role unused_float unused_string unused_string                   0 3901 2012-08-01 ANO 1 3901 2012-09-01 ANO 2 3901 2012-10-01 ANO 3 3901 2012-11-01 ANO 4 3901 2012-12-01 ANO ... ... ... 599381 9804086 2015-10-01 ANO 599382 9804238 2015-10-01 ANO 599383 9804782 2015-10-01 ANO 599384 9804810 2015-10-01 ANO 599385 9805032 2015-10-01 ANO <p>     599386 rows x 3 columns     memory usage: 23.38 MB     name: probehnuto_mimo_penezenku     type: getml.DataFrame </p> <p>getML requires that we define roles for each of the columns.</p> In\u00a0[8]: Copied! <pre>dobito.set_role(\"client_id\", getml.data.roles.join_key)\ndobito.set_role(\"month_year_datum_transakce\", getml.data.roles.time_stamp)\ndobito.set_role(\"sluzba\", getml.data.roles.categorical)\ndobito.set_role(\"kc_dobito\", getml.data.roles.numerical)\n\ndobito.set_unit(\"sluzba\", \"service\")\n\ndobito\n</pre> dobito.set_role(\"client_id\", getml.data.roles.join_key) dobito.set_role(\"month_year_datum_transakce\", getml.data.roles.time_stamp) dobito.set_role(\"sluzba\", getml.data.roles.categorical) dobito.set_role(\"kc_dobito\", getml.data.roles.numerical)  dobito.set_unit(\"sluzba\", \"service\")  dobito Out[8]:   name  month_year_datum_transakce client_id sluzba      kc_dobito   role                  time_stamp  join_key categorical numerical   unit time stamp, comparison only service     0 2012-10-01 7157857 c 1045.62 1 2015-10-01 109700 c 5187.28 2 2015-08-01 51508 c 408.2 3 2012-10-01 9573550 c 521.24 4 2014-11-01 9774621 c 386.22 ... ... ... ... 554341 2012-09-01 65283 g 7850 554342 2012-08-01 6091446 g 31400 554343 2013-08-01 1264806 g -8220.52 554344 2012-08-01 101103 g 3140 554345 2012-08-01 8674551 g 6280 <p>     554346 rows x 4 columns     memory usage: 13.30 MB     name: dobito     type: getml.DataFrame </p> In\u00a0[9]: Copied! <pre>probehnuto.set_role(\"client_id\", getml.data.roles.join_key)\nprobehnuto.set_role(\"month_year_datum_transakce\", getml.data.roles.time_stamp)\nprobehnuto.set_role(\"sluzba\", getml.data.roles.categorical)\nprobehnuto.set_role(\"kc_proklikano\", getml.data.roles.target)\n\nprobehnuto.set_unit(\"sluzba\", \"service\")\n\nprobehnuto\n</pre> probehnuto.set_role(\"client_id\", getml.data.roles.join_key) probehnuto.set_role(\"month_year_datum_transakce\", getml.data.roles.time_stamp) probehnuto.set_role(\"sluzba\", getml.data.roles.categorical) probehnuto.set_role(\"kc_proklikano\", getml.data.roles.target)  probehnuto.set_unit(\"sluzba\", \"service\")  probehnuto Out[9]:    name  month_year_datum_transakce client_id kc_proklikano sluzba         role                  time_stamp  join_key        target categorical    unit time stamp, comparison only service     0 2013-06-01 109145 -31.4 c 1 2015-10-01 9804394 37.68 h 2 2015-10-01 9803353 725.34 h 3 2015-10-01 9801753 194.68 h 4 2015-10-01 9800425 1042.48 h ... ... ... ... 1462073 2015-08-01 98857 153.86 NULL 1462074 2015-09-01 95776 153.86 NULL 1462075 2015-09-01 98857 153.86 NULL 1462076 2015-10-01 90001 310.86 NULL 1462077 2015-10-01 946957 153.86 NULL <p>     1462078 rows x 4 columns     memory usage: 35.09 MB     name: probehnuto     type: getml.DataFrame </p> In\u00a0[10]: Copied! <pre>probehnuto_mimo_penezenku.set_role(\"client_id\", getml.data.roles.join_key)\nprobehnuto_mimo_penezenku.set_role(\"Month/Year\", getml.data.roles.time_stamp)\n\nprobehnuto_mimo_penezenku\n</pre> probehnuto_mimo_penezenku.set_role(\"client_id\", getml.data.roles.join_key) probehnuto_mimo_penezenku.set_role(\"Month/Year\", getml.data.roles.time_stamp)  probehnuto_mimo_penezenku Out[10]:   name                  Month/Year client_id probehla_inzerce_mimo_penezenku   role                  time_stamp  join_key unused_string                     unit time stamp, comparison only 0 2012-08-01 3901 ANO 1 2012-09-01 3901 ANO 2 2012-10-01 3901 ANO 3 2012-11-01 3901 ANO 4 2012-12-01 3901 ANO ... ... ... 599381 2015-10-01 9804086 ANO 599382 2015-10-01 9804238 ANO 599383 2015-10-01 9804782 ANO 599384 2015-10-01 9804810 ANO 599385 2015-10-01 9805032 ANO <p>     599386 rows x 3 columns     memory usage: 14.39 MB     name: probehnuto_mimo_penezenku     type: getml.DataFrame </p> In\u00a0[11]: Copied! <pre>split = getml.data.split.random(train=0.8, test=0.2)\nsplit\n</pre> split = getml.data.split.random(train=0.8, test=0.2) split Out[11]: 0 train 1 train 2 train 3 test 4 train ... <p>     infinite number of  rows          type: StringColumnView </p> In\u00a0[12]: Copied! <pre>star_schema = getml.data.StarSchema(population=probehnuto, alias=\"population\", split=split)\n\nstar_schema.join(\n    probehnuto,\n    on=\"client_id\",\n    time_stamps=\"month_year_datum_transakce\",\n    lagged_targets=True,\n    horizon=getml.data.time.days(1),\n)\n\nstar_schema.join(\n    dobito,\n    on=\"client_id\",\n    time_stamps=\"month_year_datum_transakce\",\n)\n\nstar_schema.join(\n    probehnuto_mimo_penezenku,\n    on=\"client_id\", \n    time_stamps=(\"month_year_datum_transakce\",  \"Month/Year\"),\n)\n\nstar_schema\n</pre> star_schema = getml.data.StarSchema(population=probehnuto, alias=\"population\", split=split)  star_schema.join(     probehnuto,     on=\"client_id\",     time_stamps=\"month_year_datum_transakce\",     lagged_targets=True,     horizon=getml.data.time.days(1), )  star_schema.join(     dobito,     on=\"client_id\",     time_stamps=\"month_year_datum_transakce\", )  star_schema.join(     probehnuto_mimo_penezenku,     on=\"client_id\",      time_stamps=(\"month_year_datum_transakce\",  \"Month/Year\"), )  star_schema Out[12]: data model diagram probehnutodobitoprobehnuto_mimo_penezenkupopulationclient_id = client_idmonth_year_datum_transakce &lt;= month_year_datum_transakceHorizon: 1.0 daysLagged targets allowedclient_id = client_idmonth_year_datum_transakce &lt;= month_year_datum_transakceclient_id = client_idMonth/Year &lt;= month_year_datum_transakce staging data frames               staging table                    0 population POPULATION__STAGING_TABLE_1 1 dobito DOBITO__STAGING_TABLE_2 2 probehnuto PROBEHNUTO__STAGING_TABLE_3 3 probehnuto_mimo_penezenku PROBEHNUTO_MIMO_PENEZENKU__STAGING_TABLE_4 container population subset name          rows type 0 test probehnuto 292833 View 1 train probehnuto 1169245 View peripheral name                         rows type      0 probehnuto 1462078 DataFrame 1 dobito 554346 DataFrame 2 probehnuto_mimo_penezenku 599386 DataFrame <p>Set-up the feature learner &amp; predictor</p> In\u00a0[13]: Copied! <pre>mapping = getml.preprocessors.Mapping()\n\nfast_prop = getml.feature_learning.FastProp(\n    aggregation=getml.feature_learning.FastProp.agg_sets.All,\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    num_threads=1,    \n    sampling_factor=0.1,\n)\n\nfeature_selector = getml.predictors.XGBoostRegressor(n_jobs=1, external_memory=True)\n\npredictor = getml.predictors.XGBoostRegressor(n_jobs=1)\n</pre> mapping = getml.preprocessors.Mapping()  fast_prop = getml.feature_learning.FastProp(     aggregation=getml.feature_learning.FastProp.agg_sets.All,     loss_function=getml.feature_learning.loss_functions.SquareLoss,     num_threads=1,         sampling_factor=0.1, )  feature_selector = getml.predictors.XGBoostRegressor(n_jobs=1, external_memory=True)  predictor = getml.predictors.XGBoostRegressor(n_jobs=1) <p>Build the pipeline</p> In\u00a0[14]: Copied! <pre>pipe1 = getml.Pipeline(\n    tags=['fast_prop'],\n    data_model=star_schema.data_model,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=[predictor],\n    include_categorical=True,\n)\n\npipe1\n</pre> pipe1 = getml.Pipeline(     tags=['fast_prop'],     data_model=star_schema.data_model,     preprocessors=[mapping],     feature_learners=[fast_prop],     feature_selectors=[feature_selector],     predictors=[predictor],     include_categorical=True, )  pipe1 Out[14]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=['XGBoostRegressor'],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=['dobito', 'probehnuto', 'probehnuto_mimo_penezenku'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['fast_prop'])</pre> In\u00a0[15]: Copied! <pre>pipe1.check(star_schema.train)\n</pre> pipe1.check(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:11, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \n\nThe pipeline check generated 2 issues labeled INFO and 0 issues labeled WARNING.\n</pre> Out[15]: type label                  message                          0 INFO FOREIGN KEYS NOT FOUND When joining POPULATION__STAGING_TABLE_1 and DOBITO__STAGING_TABLE_2 over 'client_id' and 'client_id', there are no corresponding entries for 2.228789% of entries in 'client_id' in 'POPULATION__STAGING_TABLE_1'. You might want to double-check your join keys. 1 INFO FOREIGN KEYS NOT FOUND When joining POPULATION__STAGING_TABLE_1 and PROBEHNUTO_MIMO_PENEZENKU__STAGING_TABLE_4 over 'client_id' and 'client_id', there are no corresponding entries for 26.543966% of entries in 'client_id' in 'POPULATION__STAGING_TABLE_1'. You might want to double-check your join keys. In\u00a0[16]: Copied! <pre>pipe1.fit(star_schema.train)\n</pre> pipe1.fit(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 2 issues labeled INFO and 0 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 909 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:49, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 01:38, remaining: 00:00]          \nXGBoost: Training as feature selector... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 14:11, remaining: 00:00]            \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 07:27, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:24m:7.549216\n\n</pre> Out[16]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=['XGBoostRegressor'],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=['dobito', 'probehnuto', 'probehnuto_mimo_penezenku'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['fast_prop', 'container-SCRdLN'])</pre> In\u00a0[17]: Copied! <pre>fastprop_score = pipe1.score(star_schema.test)\nfastprop_score\n</pre> fastprop_score = pipe1.score(star_schema.test) fastprop_score <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:11, remaining: 00:00]          \n\n</pre> Out[17]: date time           set used target               mae        rmse rsquared 0 2024-02-21 16:38:46 train kc_proklikano 2957.3749 14390.1003 0.9422 1 2024-02-21 16:39:00 test kc_proklikano 3019.858 18800.7625 0.874 In\u00a0[18]: Copied! <pre>include = (getml.data.random() &lt; 0.25)\ninclude\n</pre> include = (getml.data.random() &lt; 0.25) include Out[18]: 0 true 1 false 2 true 3 false 4 false ... <p>     infinite number of  rows          type: BooleanColumnView </p> In\u00a0[19]: Copied! <pre>population_train_pd = star_schema.train.population[include].to_pandas()\npopulation_test_pd = star_schema.test.population.to_pandas()\n</pre> population_train_pd = star_schema.train.population[include].to_pandas() population_test_pd = star_schema.test.population.to_pandas() In\u00a0[20]: Copied! <pre>population_train_pd[\"id\"] = population_train_pd.index\npopulation_test_pd[\"id\"] = population_test_pd.index\n</pre> population_train_pd[\"id\"] = population_train_pd.index population_test_pd[\"id\"] = population_test_pd.index In\u00a0[21]: Copied! <pre>probehnuto_pd = probehnuto.drop(probehnuto.roles.unused).to_pandas()\ndobito_pd = dobito.drop(dobito.roles.unused).to_pandas()\nprobehnuto_mimo_penezenku_pd = probehnuto_mimo_penezenku.drop(probehnuto_mimo_penezenku.roles.unused).to_pandas()\n</pre> probehnuto_pd = probehnuto.drop(probehnuto.roles.unused).to_pandas() dobito_pd = dobito.drop(dobito.roles.unused).to_pandas() probehnuto_mimo_penezenku_pd = probehnuto_mimo_penezenku.drop(probehnuto_mimo_penezenku.roles.unused).to_pandas() In\u00a0[22]: Copied! <pre>def prepare_peripheral(peripheral_pd, train_or_test):\n    \"\"\"\n    Helper function that imitates the behavior of \n    the data model defined above.\n    \"\"\"\n    peripheral_new = peripheral_pd.merge(\n        train_or_test[[\"id\", \"client_id\", \"month_year_datum_transakce\"]],\n        on=\"client_id\"\n    )\n\n    peripheral_new = peripheral_new[\n        peripheral_new[\"month_year_datum_transakce_x\"] &lt; peripheral_new[\"month_year_datum_transakce_y\"]\n    ]\n\n    del peripheral_new[\"month_year_datum_transakce_y\"]\n    del peripheral_new[\"client_id\"]\n\n    return peripheral_new.rename({\"month_year_datum_transakce_y\": \"month_year_datum_transakce\"})\n</pre> def prepare_peripheral(peripheral_pd, train_or_test):     \"\"\"     Helper function that imitates the behavior of      the data model defined above.     \"\"\"     peripheral_new = peripheral_pd.merge(         train_or_test[[\"id\", \"client_id\", \"month_year_datum_transakce\"]],         on=\"client_id\"     )      peripheral_new = peripheral_new[         peripheral_new[\"month_year_datum_transakce_x\"] &lt; peripheral_new[\"month_year_datum_transakce_y\"]     ]      del peripheral_new[\"month_year_datum_transakce_y\"]     del peripheral_new[\"client_id\"]      return peripheral_new.rename({\"month_year_datum_transakce_y\": \"month_year_datum_transakce\"}) In\u00a0[23]: Copied! <pre>def prepare_probehnuto_mimo_penezenku(peripheral_pd, train_or_test):\n    \"\"\"\n    Helper function that imitates the behavior of \n    the data model defined above.\n    \"\"\"\n    peripheral_new = peripheral_pd.merge(\n        train_or_test[[\"id\", \"client_id\", \"month_year_datum_transakce\"]],\n        on=\"client_id\"\n    )\n\n    peripheral_new = peripheral_new[\n        peripheral_new[\"Month/Year\"] &lt; peripheral_new[\"month_year_datum_transakce\"]\n    ]\n\n    del peripheral_new[\"month_year_datum_transakce\"]\n    del peripheral_new[\"client_id\"]\n\n    return peripheral_new\n</pre> def prepare_probehnuto_mimo_penezenku(peripheral_pd, train_or_test):     \"\"\"     Helper function that imitates the behavior of      the data model defined above.     \"\"\"     peripheral_new = peripheral_pd.merge(         train_or_test[[\"id\", \"client_id\", \"month_year_datum_transakce\"]],         on=\"client_id\"     )      peripheral_new = peripheral_new[         peripheral_new[\"Month/Year\"] &lt; peripheral_new[\"month_year_datum_transakce\"]     ]      del peripheral_new[\"month_year_datum_transakce\"]     del peripheral_new[\"client_id\"]      return peripheral_new In\u00a0[24]: Copied! <pre>dobito_train_pd = prepare_peripheral(dobito_pd, population_train_pd)\ndobito_test_pd = prepare_peripheral(dobito_pd, population_test_pd)\ndobito_train_pd\n</pre> dobito_train_pd = prepare_peripheral(dobito_pd, population_train_pd) dobito_test_pd = prepare_peripheral(dobito_pd, population_test_pd) dobito_train_pd Out[24]: sluzba kc_dobito month_year_datum_transakce_x id 0 c 1045.62 2012-10-01 2127 1 c 1045.62 2012-10-01 17709 2 c 1045.62 2012-10-01 50363 3 a 25214.20 2015-03-01 2127 4 a 25214.20 2015-03-01 17709 ... ... ... ... ... 4462025 g 37680.00 2012-08-01 217660 4462026 g 37680.00 2012-08-01 235321 4462027 g 37680.00 2012-08-01 253661 4462028 g 37680.00 2012-08-01 264765 4462031 g 31400.00 2012-08-01 288037 <p>2240543 rows \u00d7 4 columns</p> In\u00a0[25]: Copied! <pre>probehnuto_train_pd = prepare_peripheral(probehnuto_pd, population_train_pd)\nprobehnuto_test_pd = prepare_peripheral(probehnuto_pd, population_test_pd)\nprobehnuto_train_pd\n</pre> probehnuto_train_pd = prepare_peripheral(probehnuto_pd, population_train_pd) probehnuto_test_pd = prepare_peripheral(probehnuto_pd, population_test_pd) probehnuto_train_pd Out[25]: sluzba kc_proklikano month_year_datum_transakce_x id 1 c -31.40 2013-06-01 281262 4 c -31.40 2013-06-01 288356 6 c -31.40 2013-06-01 289265 7 c -31.40 2013-06-01 289267 10 c -31.40 2013-06-01 290759 ... ... ... ... ... 11186670 None 948.28 2012-08-01 291919 11186671 e 979.68 2012-09-01 291919 11186673 None 948.28 2012-08-01 291848 11186674 e 87.92 2012-08-01 291848 11186686 e 1130.40 2013-05-01 290547 <p>5388870 rows \u00d7 4 columns</p> In\u00a0[26]: Copied! <pre>probehnuto_mimo_penezenku_train_pd = prepare_probehnuto_mimo_penezenku(probehnuto_mimo_penezenku_pd, population_train_pd)\nprobehnuto_mimo_penezenku_test_pd = prepare_probehnuto_mimo_penezenku(probehnuto_mimo_penezenku_pd, population_test_pd)\nprobehnuto_mimo_penezenku_train_pd\n</pre> probehnuto_mimo_penezenku_train_pd = prepare_probehnuto_mimo_penezenku(probehnuto_mimo_penezenku_pd, population_train_pd) probehnuto_mimo_penezenku_test_pd = prepare_probehnuto_mimo_penezenku(probehnuto_mimo_penezenku_pd, population_test_pd) probehnuto_mimo_penezenku_train_pd Out[26]: Month/Year id 0 2012-08-01 269301 8 2012-08-01 9204 9 2012-08-01 23838 10 2012-08-01 24471 11 2012-08-01 45868 ... ... ... 3568048 2015-09-01 160015 3568050 2015-09-01 19 3568051 2015-09-01 1565 3568053 2015-09-01 151283 3568060 2015-09-01 158546 <p>2832768 rows \u00d7 2 columns</p> In\u00a0[27]: Copied! <pre>del population_train_pd[\"client_id\"]\ndel population_test_pd[\"client_id\"]\n</pre> del population_train_pd[\"client_id\"] del population_test_pd[\"client_id\"] In\u00a0[28]: Copied! <pre>population_train_pd\n</pre> population_train_pd Out[28]: sluzba kc_proklikano month_year_datum_transakce id 0 c -31.40 2013-06-01 0 1 h 725.34 2015-10-01 1 2 h 8550.22 2015-10-01 2 3 h 2408.38 2015-10-01 3 4 h 1893.42 2015-10-01 4 ... ... ... ... ... 292153 None 153.86 2015-03-01 292153 292154 None 153.86 2015-05-01 292154 292155 None 13545.96 2015-06-01 292155 292156 None 153.86 2015-06-01 292156 292157 None 153.86 2015-08-01 292157 <p>292158 rows \u00d7 4 columns</p> In\u00a0[29]: Copied! <pre>def add_index(df):\n    df.insert(0, \"index\", range(len(df)))\n\npopulation_pd_logical_types = {\n    'id': ww.logical_types.Integer,\n    'sluzba': ww.logical_types.Categorical,\n    'kc_proklikano': ww.logical_types.Double,\n    'month_year_datum_transakce': ww.logical_types.Datetime\n}\npopulation_train_pd.ww.init(logical_types=population_pd_logical_types, index='id', name='population')\npopulation_test_pd.ww.init(logical_types=population_pd_logical_types, index='id', name='population')\n\nadd_index(dobito_train_pd)\nadd_index(dobito_test_pd)\ndobito_pd_logical_types = {\n    'index': ww.logical_types.Integer,\n    'sluzba': ww.logical_types.Categorical,\n    'kc_dobito': ww.logical_types.Double,\n    'month_year_datum_transakce_x': ww.logical_types.Datetime,\n    'id': ww.logical_types.Integer\n}\ndobito_train_pd.ww.init(logical_types=dobito_pd_logical_types, index='index', name='dobito')\ndobito_test_pd.ww.init(logical_types=dobito_pd_logical_types, index='index', name='dobito')\n\nadd_index(probehnuto_train_pd)\nadd_index(probehnuto_test_pd)\nprobehnuto_pd_logical_types = {\n    'index': ww.logical_types.Integer,\n    'sluzba': ww.logical_types.Categorical,\n    'kc_proklikano': ww.logical_types.Double,\n    'month_year_datum_transakce_x': ww.logical_types.Datetime,\n    'id': ww.logical_types.Integer\n}\nprobehnuto_train_pd.ww.init(logical_types=probehnuto_pd_logical_types, index='index', name='probehnuto')\nprobehnuto_test_pd.ww.init(logical_types=probehnuto_pd_logical_types, index='index', name='probehnuto')\n\nadd_index(probehnuto_mimo_penezenku_train_pd)\nadd_index(probehnuto_mimo_penezenku_test_pd)\nprobehnuto_mimo_penezenku_pd_logical_types = {\n    'index': ww.logical_types.Integer,\n    'Month/Year': ww.logical_types.Datetime,\n    'id': ww.logical_types.Integer\n}\nprobehnuto_mimo_penezenku_train_pd.ww.init(logical_types=probehnuto_mimo_penezenku_pd_logical_types, index='index', name='probehnuto_mimo_penezenku')\nprobehnuto_mimo_penezenku_test_pd.ww.init(logical_types=probehnuto_mimo_penezenku_pd_logical_types, index='index', name='probehnuto_mimo_penezenku')\n</pre> def add_index(df):     df.insert(0, \"index\", range(len(df)))  population_pd_logical_types = {     'id': ww.logical_types.Integer,     'sluzba': ww.logical_types.Categorical,     'kc_proklikano': ww.logical_types.Double,     'month_year_datum_transakce': ww.logical_types.Datetime } population_train_pd.ww.init(logical_types=population_pd_logical_types, index='id', name='population') population_test_pd.ww.init(logical_types=population_pd_logical_types, index='id', name='population')  add_index(dobito_train_pd) add_index(dobito_test_pd) dobito_pd_logical_types = {     'index': ww.logical_types.Integer,     'sluzba': ww.logical_types.Categorical,     'kc_dobito': ww.logical_types.Double,     'month_year_datum_transakce_x': ww.logical_types.Datetime,     'id': ww.logical_types.Integer } dobito_train_pd.ww.init(logical_types=dobito_pd_logical_types, index='index', name='dobito') dobito_test_pd.ww.init(logical_types=dobito_pd_logical_types, index='index', name='dobito')  add_index(probehnuto_train_pd) add_index(probehnuto_test_pd) probehnuto_pd_logical_types = {     'index': ww.logical_types.Integer,     'sluzba': ww.logical_types.Categorical,     'kc_proklikano': ww.logical_types.Double,     'month_year_datum_transakce_x': ww.logical_types.Datetime,     'id': ww.logical_types.Integer } probehnuto_train_pd.ww.init(logical_types=probehnuto_pd_logical_types, index='index', name='probehnuto') probehnuto_test_pd.ww.init(logical_types=probehnuto_pd_logical_types, index='index', name='probehnuto')  add_index(probehnuto_mimo_penezenku_train_pd) add_index(probehnuto_mimo_penezenku_test_pd) probehnuto_mimo_penezenku_pd_logical_types = {     'index': ww.logical_types.Integer,     'Month/Year': ww.logical_types.Datetime,     'id': ww.logical_types.Integer } probehnuto_mimo_penezenku_train_pd.ww.init(logical_types=probehnuto_mimo_penezenku_pd_logical_types, index='index', name='probehnuto_mimo_penezenku') probehnuto_mimo_penezenku_test_pd.ww.init(logical_types=probehnuto_mimo_penezenku_pd_logical_types, index='index', name='probehnuto_mimo_penezenku') In\u00a0[30]: Copied! <pre>dataframes_train = {\n    \"population\" : (population_train_pd, ),\n    \"dobito\": (dobito_train_pd, ),\n    \"probehnuto\": (probehnuto_train_pd, ),\n    \"probehnuto_mimo_penezenku\": (probehnuto_mimo_penezenku_train_pd, ),\n}\n</pre> dataframes_train = {     \"population\" : (population_train_pd, ),     \"dobito\": (dobito_train_pd, ),     \"probehnuto\": (probehnuto_train_pd, ),     \"probehnuto_mimo_penezenku\": (probehnuto_mimo_penezenku_train_pd, ), } In\u00a0[31]: Copied! <pre>dataframes_test = {\n    \"population\" : (population_test_pd, ),\n    \"dobito\": (dobito_test_pd, ),\n    \"probehnuto\": (probehnuto_test_pd, ),\n    \"probehnuto_mimo_penezenku\": (probehnuto_mimo_penezenku_test_pd, ),\n}\n</pre> dataframes_test = {     \"population\" : (population_test_pd, ),     \"dobito\": (dobito_test_pd, ),     \"probehnuto\": (probehnuto_test_pd, ),     \"probehnuto_mimo_penezenku\": (probehnuto_mimo_penezenku_test_pd, ), } In\u00a0[32]: Copied! <pre>relationships = [\n    (\"population\", \"id\", \"dobito\", \"id\"),\n    (\"population\", \"id\", \"probehnuto\", \"id\"),\n    (\"population\", \"id\", \"probehnuto_mimo_penezenku\", \"id\"),\n]\n</pre> relationships = [     (\"population\", \"id\", \"dobito\", \"id\"),     (\"population\", \"id\", \"probehnuto\", \"id\"),     (\"population\", \"id\", \"probehnuto_mimo_penezenku\", \"id\"), ] In\u00a0[33]: Copied! <pre>featuretools_train_pd = featuretools.dfs(\n    dataframes=dataframes_train,\n    relationships=relationships,\n    target_dataframe_name=\"population\")[0]\n</pre> featuretools_train_pd = featuretools.dfs(     dataframes=dataframes_train,     relationships=relationships,     target_dataframe_name=\"population\")[0] In\u00a0[34]: Copied! <pre>featuretools_test_pd = featuretools.dfs(\n    dataframes=dataframes_test,\n    relationships=relationships,\n    target_dataframe_name=\"population\")[0]\n</pre> featuretools_test_pd = featuretools.dfs(     dataframes=dataframes_test,     relationships=relationships,     target_dataframe_name=\"population\")[0] In\u00a0[35]: Copied! <pre>featuretools_train = getml.data.DataFrame.from_pandas(featuretools_train_pd, \"featuretools_train\")\nfeaturetools_test = getml.data.DataFrame.from_pandas(featuretools_test_pd, \"featuretools_test\")\n</pre> featuretools_train = getml.data.DataFrame.from_pandas(featuretools_train_pd, \"featuretools_train\") featuretools_test = getml.data.DataFrame.from_pandas(featuretools_test_pd, \"featuretools_test\") In\u00a0[36]: Copied! <pre>featuretools_train.set_role(\"kc_proklikano\", getml.data.roles.target)\nfeaturetools_train.set_role(featuretools_train.roles.unused_float, getml.data.roles.numerical)\nfeaturetools_train.set_role(featuretools_train.roles.unused_string, getml.data.roles.categorical)\n\nfeaturetools_train\n</pre> featuretools_train.set_role(\"kc_proklikano\", getml.data.roles.target) featuretools_train.set_role(featuretools_train.roles.unused_float, getml.data.roles.numerical) featuretools_train.set_role(featuretools_train.roles.unused_string, getml.data.roles.categorical)  featuretools_train Out[36]:   name kc_proklikano sluzba      COUNT(dobito) MODE(dobito.sluzba) NUM_UNIQUE(dobito.sluzba) COUNT(probehnuto) MODE(probehnuto.sluzba) NUM_UNIQUE(probehnuto.sluzba) COUNT(probehnuto_mimo_penezenku) DAY(month_year_datum_transakce) MONTH(month_year_datum_transakce) WEEKDAY(month_year_datum_transakce) YEAR(month_year_datum_transakce) MODE(dobito.DAY(month_year_datum_transakce_x)) MODE(dobito.MONTH(month_year_datum_transakce_x)) MODE(dobito.WEEKDAY(month_year_datum_transakce_x)) MODE(dobito.YEAR(month_year_datum_transakce_x)) NUM_UNIQUE(dobito.DAY(month_year_datum_transakce_x)) NUM_UNIQUE(dobito.MONTH(month_year_datum_transakce_x)) NUM_UNIQUE(dobito.WEEKDAY(month_year_datum_transakce_x)) NUM_UNIQUE(dobito.YEAR(month_year_datum_transakce_x)) MODE(probehnuto.DAY(month_year_datum_transakce_x)) MODE(probehnuto.MONTH(month_year_datum_transakce_x)) MODE(probehnuto.WEEKDAY(month_year_datum_transakce_x)) MODE(probehnuto.YEAR(month_year_datum_transakce_x)) NUM_UNIQUE(probehnuto.DAY(month_year_datum_transakce_x)) NUM_UNIQUE(probehnuto.MONTH(month_year_datum_transakce_x)) NUM_UNIQUE(probehnuto.WEEKDAY(month_year_datum_transakce_x)) NUM_UNIQUE(probehnuto.YEAR(month_year_datum_transakce_x)) MODE(probehnuto_mimo_penezenku.DAY(Month/Year)) MODE(probehnuto_mimo_penezenku.MONTH(Month/Year)) MODE(probehnuto_mimo_penezenku.WEEKDAY(Month/Year)) MODE(probehnuto_mimo_penezenku.YEAR(Month/Year)) NUM_UNIQUE(probehnuto_mimo_penezenku.DAY(Month/Year)) NUM_UNIQUE(probehnuto_mimo_penezenku.MONTH(Month/Year)) NUM_UNIQUE(probehnuto_mimo_penezenku.WEEKDAY(Month/Year)) NUM_UNIQUE(probehnuto_mimo_penezenku.YEAR(Month/Year)) MAX(dobito.kc_dobito) MEAN(dobito.kc_dobito) MIN(dobito.kc_dobito) SKEW(dobito.kc_dobito) STD(dobito.kc_dobito) SUM(dobito.kc_dobito) MAX(probehnuto.kc_proklikano) MEAN(probehnuto.kc_proklikano) MIN(probehnuto.kc_proklikano) SKEW(probehnuto.kc_proklikano) STD(probehnuto.kc_proklikano) SUM(probehnuto.kc_proklikano)   role        target categorical categorical   categorical         categorical               categorical       categorical             categorical                   categorical                      categorical                     categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                                  numerical              numerical             numerical              numerical             numerical             numerical                     numerical                      numerical                     numerical                      numerical                     numerical                     numerical 0 -31.4 c 1 c 1 13 d 1 0 1 6 5 2013 1 12 5 2012 1 1 1 1 1 8 0 2012 1 10 6 2 NULL NULL NULL NULL NULL NULL NULL NULL 1306.24 1306.24 1306.24 nan nan 1306.24 351.68 155.7923 9.42 0.5817 79.3799 2025.3 1 725.34 h 4 h 1 5 h 1 0 1 10 3 2015 1 5 0 2015 1 4 4 1 1 5 0 2015 1 5 5 1 NULL NULL NULL NULL NULL NULL NULL NULL 1036.2 614.655 257.48 0.5563 324.3624 2458.62 634.28 388.732 131.88 0.09478 205.8605 1943.66 2 8550.22 h 7 h 2 11 h 1 0 1 10 3 2015 1 8 2 2015 1 6 5 2 1 1 2 2015 1 11 7 2 NULL NULL NULL NULL NULL NULL NULL NULL 62800 20907.9143 0 1.214 22375.0788 146355.4 39752.4 13280.7727 3215.36 1.4845 12240.1205 146088.5 3 2408.38 h 4 h 1 5 h 1 0 1 10 3 2015 1 5 1 2015 1 4 4 1 1 5 0 2015 1 5 5 1 NULL NULL NULL NULL NULL NULL NULL NULL 1554.3 1361.19 1296.82 2 128.74 5444.76 1635.94 1092.72 15.7 -1.0846 711.3296 5463.6 4 1893.42 h 12 h 4 22 d 3 0 1 10 3 2015 1 2 4 2015 1 5 6 2 1 2 2 2015 1 9 6 2 NULL NULL NULL NULL NULL NULL NULL NULL 2615.62 1078.3283 310.86 0.8429 835.3628 12939.94 1510.34 471 0 0.8615 531.1884 10362 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 292153 153.86 NULL 12 c 1 34 d 1 0 1 3 6 2015 1 1 0 2013 1 8 6 3 1 1 5 2013 1 12 7 4 NULL NULL NULL NULL NULL NULL NULL NULL 518.1 337.8117 153.86 0.02976 99.6511 4053.74 188.4 141.9465 0 -2.9319 44.2498 4826.18 292154 153.86 NULL 6 f 1 34 f 1 0 1 5 4 2015 1 2 1 2013 1 6 4 3 1 1 5 2013 1 12 7 4 NULL NULL NULL NULL NULL NULL NULL NULL 1296.82 913.74 31.4 -2.1286 444.4448 5482.44 188.4 150.8124 -31.4 -5.2155 33.226 5127.62 292155 13545.96 NULL 214 c 8 283 d 8 0 1 6 0 2015 1 1 5 2014 1 12 7 4 1 1 5 2014 1 12 7 4 NULL NULL NULL NULL NULL NULL NULL NULL 7131326 794524.2239 -11422331 -2.0115 1432748.8604 170028183.91 6965622.14 640600.4428 -866.64 2.2138 1006527.2489 181289925.3 292156 153.86 NULL 14 c 1 0 NULL NULL 0 1 6 0 2015 1 4 6 2014 1 12 7 2 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 518.1 179.8771 153.86 3.7417 97.3472 2518.28 nan nan nan nan nan 0 292157 153.86 NULL 8 c 1 36 NULL 0 0 1 8 5 2015 1 8 1 2013 1 7 5 4 1 1 0 2013 1 12 7 4 NULL NULL NULL NULL NULL NULL NULL NULL 942 764.1975 518.1 -1.1545 115.2358 6113.58 188.4 155.1683 153.86 5.7312 5.7838 5586.06 <p>     292158 rows x 49 columns     memory usage: 72.46 MB     name: featuretools_train     type: getml.DataFrame </p> In\u00a0[37]: Copied! <pre>featuretools_test.set_role(\"kc_proklikano\", getml.data.roles.target)\nfeaturetools_test.set_role(featuretools_test.roles.unused_float, getml.data.roles.numerical)\nfeaturetools_test.set_role(featuretools_test.roles.unused_string, getml.data.roles.categorical)\n\nfeaturetools_test\n</pre> featuretools_test.set_role(\"kc_proklikano\", getml.data.roles.target) featuretools_test.set_role(featuretools_test.roles.unused_float, getml.data.roles.numerical) featuretools_test.set_role(featuretools_test.roles.unused_string, getml.data.roles.categorical)  featuretools_test Out[37]:   name kc_proklikano sluzba      COUNT(dobito) MODE(dobito.sluzba) NUM_UNIQUE(dobito.sluzba) COUNT(probehnuto) MODE(probehnuto.sluzba) NUM_UNIQUE(probehnuto.sluzba) COUNT(probehnuto_mimo_penezenku) DAY(month_year_datum_transakce) MONTH(month_year_datum_transakce) WEEKDAY(month_year_datum_transakce) YEAR(month_year_datum_transakce) MODE(dobito.DAY(month_year_datum_transakce_x)) MODE(dobito.MONTH(month_year_datum_transakce_x)) MODE(dobito.WEEKDAY(month_year_datum_transakce_x)) MODE(dobito.YEAR(month_year_datum_transakce_x)) NUM_UNIQUE(dobito.DAY(month_year_datum_transakce_x)) NUM_UNIQUE(dobito.MONTH(month_year_datum_transakce_x)) NUM_UNIQUE(dobito.WEEKDAY(month_year_datum_transakce_x)) NUM_UNIQUE(dobito.YEAR(month_year_datum_transakce_x)) MODE(probehnuto.DAY(month_year_datum_transakce_x)) MODE(probehnuto.MONTH(month_year_datum_transakce_x)) MODE(probehnuto.WEEKDAY(month_year_datum_transakce_x)) MODE(probehnuto.YEAR(month_year_datum_transakce_x)) NUM_UNIQUE(probehnuto.DAY(month_year_datum_transakce_x)) NUM_UNIQUE(probehnuto.MONTH(month_year_datum_transakce_x)) NUM_UNIQUE(probehnuto.WEEKDAY(month_year_datum_transakce_x)) NUM_UNIQUE(probehnuto.YEAR(month_year_datum_transakce_x)) MODE(probehnuto_mimo_penezenku.DAY(Month/Year)) MODE(probehnuto_mimo_penezenku.MONTH(Month/Year)) MODE(probehnuto_mimo_penezenku.WEEKDAY(Month/Year)) MODE(probehnuto_mimo_penezenku.YEAR(Month/Year)) NUM_UNIQUE(probehnuto_mimo_penezenku.DAY(Month/Year)) NUM_UNIQUE(probehnuto_mimo_penezenku.MONTH(Month/Year)) NUM_UNIQUE(probehnuto_mimo_penezenku.WEEKDAY(Month/Year)) NUM_UNIQUE(probehnuto_mimo_penezenku.YEAR(Month/Year)) MAX(dobito.kc_dobito) MEAN(dobito.kc_dobito) MIN(dobito.kc_dobito) SKEW(dobito.kc_dobito) STD(dobito.kc_dobito) SUM(dobito.kc_dobito) MAX(probehnuto.kc_proklikano) MEAN(probehnuto.kc_proklikano) MIN(probehnuto.kc_proklikano) SKEW(probehnuto.kc_proklikano) STD(probehnuto.kc_proklikano) SUM(probehnuto.kc_proklikano)   role        target categorical categorical   categorical         categorical               categorical       categorical             categorical                   categorical                      categorical                     categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                      categorical                                  numerical              numerical             numerical              numerical             numerical             numerical                     numerical                      numerical                     numerical                      numerical                     numerical                     numerical 0 194.68 h 2 d 2 2 d 2 0 1 10 3 2015 1 9 1 2015 1 1 1 1 1 9 1 2015 1 1 1 1 NULL NULL NULL NULL NULL NULL NULL NULL 1296.82 777.15 257.48 nan 734.9244 1554.3 763.02 401.92 40.82 nan 510.6725 803.84 1 405.06 h 1 h 1 2 h 1 0 1 10 3 2015 1 8 5 2015 1 1 1 1 1 8 1 2015 1 2 2 1 NULL NULL NULL NULL NULL NULL NULL NULL 1296.82 1296.82 1296.82 nan nan 1296.82 565.2 452.16 339.12 nan 159.8627 904.32 2 580.9 h 4 d 2 5 d 2 0 1 10 3 2015 1 9 1 2015 1 3 3 1 1 9 1 2015 1 4 4 1 NULL NULL NULL NULL NULL NULL NULL NULL 1296.82 1231.665 1036.2 -2. 130.31 4926.66 913.74 454.044 34.54 0.2893 328.7162 2270.22 3 106.76 h 0 NULL NULL 0 NULL NULL 0 1 10 3 2015 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL nan nan nan nan nan 0 nan nan nan nan nan 0 4 1927.96 h 15 d 2 21 d 2 0 1 10 3 2015 1 9 0 2015 1 10 6 2 1 9 0 2015 1 12 7 2 NULL NULL NULL NULL NULL NULL NULL NULL 7784.06 1850.088 257.48 2.4789 1898.9207 27751.32 5199.84 1148.1933 25.12 1.8651 1342.4638 24112.06 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 292828 153.86 NULL 5 c 1 36 d 2 0 1 4 2 2015 1 12 5 2013 1 4 4 3 1 8 5 2013 1 12 7 4 NULL NULL NULL NULL NULL NULL NULL NULL 1306.24 1045.62 31.4 -2.2358 566.9809 5228.1 351.68 150.3711 -31.4 -0.2998 56.2491 5413.36 292829 153.86 NULL 3 c 1 35 c 1 0 1 6 0 2015 1 4 1 2012 1 3 3 3 1 1 5 2013 1 12 7 4 NULL NULL NULL NULL NULL NULL NULL NULL 2615.62 1757.3533 62.8 -1.7316 1467.5674 5272.06 188.4 150.0023 -62.8 -5.4539 37.9032 5250.08 292830 153.86 NULL 6 f 1 35 NULL 0 0 1 7 2 2015 1 3 5 2014 1 4 3 4 1 1 0 2013 1 12 7 4 NULL NULL NULL NULL NULL NULL NULL NULL 1884 831.5767 518.1 2.1524 530.786 4989.46 188.4 155.2057 153.86 5.6511 5.8638 5432.2 292831 310.86 NULL 3 c 2 38 NULL 0 0 1 10 3 2015 1 8 2 2012 1 2 3 3 1 8 5 2013 1 12 7 4 NULL NULL NULL NULL NULL NULL NULL NULL 5024 4442.0533 4151.08 1.7321 503.9806 13326.16 376.8 312.9258 310.86 6.0854 10.6864 11891.18 292832 153.86 NULL 4 c 1 35 NULL 0 0 1 10 3 2015 1 1 1 2013 1 4 3 3 1 1 5 2013 1 12 7 4 NULL NULL NULL NULL NULL NULL NULL NULL 1868.3 1415.355 628 -0.9865 589.9991 5661.42 157 154.0394 153.86 3.9889 0.7395 5391.38 <p>     292833 rows x 49 columns     memory usage: 72.62 MB     name: featuretools_test     type: getml.DataFrame </p> <p>We train an untuned XGBoostRegressor on top of featuretools' features, just like we have done for getML's features.</p> <p>Since some of featuretools features are categorical, we allow the pipeline to include these features as well. Other features contain NaN values, which is why we also apply getML's Imputation preprocessor.</p> In\u00a0[38]: Copied! <pre>data_model = getml.data.DataModel(\"population\")\n</pre> data_model = getml.data.DataModel(\"population\") In\u00a0[39]: Copied! <pre>imputation = getml.preprocessors.Imputation()\n\npredictor = getml.predictors.XGBoostRegressor(n_jobs=1)\n\npipe2 = getml.Pipeline(\n    tags=['featuretools'],\n    data_model=data_model,\n    preprocessors=[imputation],\n    predictors=[predictor],\n    include_categorical=True,\n)\n\npipe2\n</pre> imputation = getml.preprocessors.Imputation()  predictor = getml.predictors.XGBoostRegressor(n_jobs=1)  pipe2 = getml.Pipeline(     tags=['featuretools'],     data_model=data_model,     preprocessors=[imputation],     predictors=[predictor],     include_categorical=True, )  pipe2 Out[39]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=['Imputation'],\n         share_selected_features=0.5,\n         tags=['featuretools'])</pre> In\u00a0[40]: Copied! <pre>pipe2.fit(featuretools_train)\n</pre> pipe2.fit(featuretools_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 0 issues labeled INFO and 7 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:31, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:32.17949\n\n</pre> Out[40]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=['Imputation'],\n         share_selected_features=0.5,\n         tags=['featuretools'])</pre> In\u00a0[41]: Copied! <pre>featuretools_score = pipe2.score(featuretools_test)\nfeaturetools_score\n</pre> featuretools_score = pipe2.score(featuretools_test) featuretools_score <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[41]: date time           set used           target               mae        rmse rsquared 0 2024-02-21 16:49:16 featuretools_train kc_proklikano 5024.2643 23362.8008 0.8394 1 2024-02-21 16:49:17 featuretools_test kc_proklikano 5183.7763 34050.186 0.5751 In\u00a0[42]: Copied! <pre>pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name]\n</pre> pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name] Out[42]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_50\";\n\nCREATE TABLE \"FEATURE_1_50\" AS\nSELECT EWMA_1H( t2.\"kc_proklikano\", t1.\"month_year_datum_transakce\" - t2.\"month_year_datum_transakce__1_000000_days\" ) AS \"feature_1_50\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"PROBEHNUTO__STAGING_TABLE_3\" t2\nON t1.\"client_id\" = t2.\"client_id\"\nWHERE t2.\"month_year_datum_transakce__1_000000_days\" &lt;= t1.\"month_year_datum_transakce\"\nAND t1.\"sluzba\" = t2.\"sluzba\"\nGROUP BY t1.rowid;\n</pre> In\u00a0[43]: Copied! <pre># Creates a folder named seznam_pipeline containing\n# the SQL code.\npipe1.features.to_sql().save(\"seznam_pipeline\")\n</pre> # Creates a folder named seznam_pipeline containing # the SQL code. pipe1.features.to_sql().save(\"seznam_pipeline\") In\u00a0[44]: Copied! <pre>pipe1.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"seznam_spark\")\n</pre> pipe1.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"seznam_spark\") In\u00a0[45]: Copied! <pre>scores = [fastprop_score, featuretools_score]\npd.DataFrame(data={\n    'Name': ['getML: FastProp', 'featuretools'],\n    'R-squared': [f'{score.rsquared:.2%}' for score in scores],\n    'RMSE': [f'{score.rmse:,.0f}' for score in scores],\n    'MAE': [f'{score.mae:,.0f}' for score in scores]\n})\n</pre> scores = [fastprop_score, featuretools_score] pd.DataFrame(data={     'Name': ['getML: FastProp', 'featuretools'],     'R-squared': [f'{score.rsquared:.2%}' for score in scores],     'RMSE': [f'{score.rmse:,.0f}' for score in scores],     'MAE': [f'{score.mae:,.0f}' for score in scores] }) Out[45]: Name R-squared RMSE MAE 0 getML: FastProp 87.40% 18,801 3,020 1 featuretools 57.51% 34,050 5,184","boost":0.8},{"location":"examples/enterprise-notebooks/seznam/#seznam-predicting-transaction-volume","title":"Seznam - Predicting transaction volume\u00b6","text":"<p>Seznam is a Czech company with a scope similar to Google. The purpose of this notebook is to analyze data from Seznam's wallet, predicting the transaction volume.</p> <p>Summary:</p> <ul> <li>Prediction type: Regression model</li> <li>Domain: E-commerce</li> <li>Prediction target: Transaction volume</li> <li>Population size: 1,462,078</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/seznam/#background","title":"Background\u00b6","text":"<p>Seznam is a Czech company with a scope similar to Google. The purpose of this notebook is to analyze data from Seznam's wallet, predicting the transaction volume.</p> <p>Since the dataset is in Czech, we will quickly translate the meaning of the main tables:</p> <ul> <li>dobito: contains data on prepayments into a wallet</li> <li>probehnuto: contains data on charges from a wallet</li> <li>probehnuto_mimo_penezenku: contains data on charges, from sources other than a wallet</li> </ul> <p>The dataset has been downloaded from the CTU Prague relational learning repository (Motl and Schulte, 2015)(Now residing at relational-data.org.).</p> <p>We will benchmark getML's feature learning algorithms against featuretools, an open-source implementation of the propositionalization algorithm, similar to getML's FastProp.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/seznam/#analysis","title":"Analysis\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/seznam/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/seznam/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>We begin by downloading the data:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/seznam/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/seznam/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"<p>We loaded the data and defined the roles and units. Next, we create a getML pipeline for relational learning.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/seznam/#21-define-relational-model","title":"2.1 Define relational model\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/seznam/#22-getml-pipeline","title":"2.2 getML pipeline\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/seznam/#23-model-training","title":"2.3 Model training\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/seznam/#24-model-evaluation","title":"2.4 Model evaluation\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/seznam/#25-featuretools","title":"2.5 featuretools\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/seznam/#26-features","title":"2.6 Features\u00b6","text":"<p>The most important feature looks as follows:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/seznam/#27-productionization","title":"2.7 Productionization\u00b6","text":"<p>It is possible to productionize the pipeline by transpiling the features into production-ready SQL code. Please also refer to getML's <code>sqlite3</code> and <code>spark</code> modules.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/seznam/#28-discussion","title":"2.8 Discussion\u00b6","text":"<p>For a more convenient overview, we summarize our results into a table.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/seznam/#3-conclusion","title":"3. Conclusion\u00b6","text":"<p>We have benchmarked getML against featuretools on a dataset related to online transactions. We have found that getML outperforms featuretools by a wide margin.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/seznam/#references","title":"References\u00b6","text":"<p>Motl, Jan, and Oliver Schulte. \"The CTU prague relational learning repository.\" arXiv preprint arXiv:1511.03086 (2015).</p>","boost":0.8},{"location":"examples/enterprise-notebooks/sfscores/","title":"<span class=\"ntitle\">sfscores.ipynb</span> <span class=\"ndesc\">Predicting health inspection scores of restaurants</span>","text":"<p>Let's get started with the analysis and set up your session:</p> In\u00a0[1]: Copied! <pre>import copy\nimport os\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\nfrom pathlib import Path\n\nfrom urllib import request\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline  \n\nimport featuretools\nimport woodwork as ww\nimport getml\n\ngetml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.engine.set_project('sfscores')\n</pre> import copy import os os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\" from pathlib import Path  from urllib import request  import numpy as np import pandas as pd from IPython.display import Image import matplotlib.pyplot as plt %matplotlib inline    import featuretools import woodwork as ww import getml  getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.engine.set_project('sfscores') <pre>getML engine is already running.\n\nConnected to project 'sfscores'\n</pre> In\u00a0[2]: Copied! <pre>conn = getml.database.connect_mysql(\n    host=\"db.relational-data.org\",\n    dbname=\"SFScores\",\n    port=3306,\n    user=\"guest\",\n    password=\"relational\"\n)\n\nconn\n</pre> conn = getml.database.connect_mysql(     host=\"db.relational-data.org\",     dbname=\"SFScores\",     port=3306,     user=\"guest\",     password=\"relational\" )  conn Out[2]: <pre>Connection(dbname='SFScores',\n           dialect='mysql',\n           host='db.relational-data.org',\n           port=3306)</pre> In\u00a0[3]: Copied! <pre>def load_if_needed(name):\n    \"\"\"\n    Loads the data from the relational learning\n    repository, if the data frame has not already\n    been loaded.\n    \"\"\"\n    if not getml.data.exists(name):\n        data_frame = getml.data.DataFrame.from_db(\n            name=name,\n            table_name=name,\n            conn=conn\n        )\n        data_frame.save()\n    else:\n        data_frame = getml.data.load_data_frame(name)\n    return data_frame\n</pre> def load_if_needed(name):     \"\"\"     Loads the data from the relational learning     repository, if the data frame has not already     been loaded.     \"\"\"     if not getml.data.exists(name):         data_frame = getml.data.DataFrame.from_db(             name=name,             table_name=name,             conn=conn         )         data_frame.save()     else:         data_frame = getml.data.load_data_frame(name)     return data_frame In\u00a0[4]: Copied! <pre>businesses = load_if_needed(\"businesses\")\ninspections = load_if_needed(\"inspections\")\nviolations = load_if_needed(\"violations\")\n</pre> businesses = load_if_needed(\"businesses\") inspections = load_if_needed(\"inspections\") violations = load_if_needed(\"violations\") In\u00a0[5]: Copied! <pre>businesses\n</pre> businesses Out[5]: name  business_id     latitude    longitude phone_number business_certificate name                             address                       city          postal_code   tax_code      application_date owner_name                       owner_address                owner_city    owner_state   owner_zip     role unused_float unused_float unused_float unused_float         unused_float unused_string                    unused_string                 unused_string unused_string unused_string unused_string    unused_string                    unused_string                unused_string unused_string unused_string 0 10 37.7911 -122.404 nan 779059 Tiramisu Kitchen 033 Belden Pl San Francisco 94104 H24 NULL Tiramisu LLC 33 Belden St San Francisco CA 94104 1 24 37.7929 -122.403 nan 352312 OMNI S.F. Hotel - 2nd Floor Pant... 500 California St, 2nd  Floor San Francisco 94104 H24 NULL OMNI San Francisco Hotel Corp 500 California St, 2nd Floor San Francisco CA 94104 2 31 37.8072 -122.419 nan 346882 Norman's Ice Cream and Freezes 2801 Leavenworth St San Francisco 94133 H24 NULL Norman Antiforda 2801 Leavenworth St San Francisco CA 94133 3 45 37.7471 -122.414 nan 340024 CHARLIE'S DELI CAFE 3202 FOLSOM St S.F. 94110 H24 2001-10-10 HARB, CHARLES AND KRISTIN 1150 SANCHEZ S.F. CA 94114 4 48 37.764 -122.466 nan 318022 ART'S CAFE 747 IRVING St SAN FRANCISCO 94122 H24 NULL YOON HAE RYONG 1567 FUNSTON AVE SAN FRANCISCO CA 94122 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 6353 89335 nan nan nan 1057025 Breaking Bad Sandwiches 154 McAllister St NULL 94102 H25 2016-09-23 JPMD, LLC 662 Bellhurst Lane Castro Valley CA 94102 6354 89336 nan nan nan 1057746 Miller's Rest 1085 Sutter St NULL 94109 H26 2016-09-23 Miller's Rest, LLC 2906 Bush Street San Francisco CA 94109 6355 89393 nan nan nan 1042408 Panuchos 620 Broadway St NULL 94133 H24 2016-09-28 Los Aluxes, LLC 1032 Irving Street, #421 San Francisco CA 94122 6356 89416 nan nan nan 1051081 Nobhill Pizza &amp; Shawerma 1534 California St NULL 94109 H24 2016-09-29 BBA Foods, Inc. 840 Post Street, #218 San Francisco CA 94109 6357 89453 nan nan nan 459309 Burger King #4668 1690 Valencia St San Francisco 94110 H29 2016-10-03 Golden Gate Restaurant Group, In... P.O Box 21 Lafeyette CA 94549 <p>     6358 rows x 16 columns     memory usage: 1.57 MB     name: businesses     type: getml.DataFrame </p> In\u00a0[6]: Copied! <pre>inspections\n</pre> inspections Out[6]:  name  business_id        score date          type                   role unused_float unused_float unused_string unused_string         0 10 92 2014-01-14 Routine - Unscheduled 1 10 nan 2014-01-24 Reinspection/Followup 2 10 94 2014-07-29 Routine - Unscheduled 3 10 nan 2014-08-07 Reinspection/Followup 4 10 82 2016-05-03 Routine - Unscheduled ... ... ... ... 23759 89199 100 2016-09-12 Routine - Unscheduled 23760 89200 100 2016-09-12 Routine - Unscheduled 23761 89201 nan 2016-09-12 New Ownership 23762 89204 100 2016-09-12 Routine - Unscheduled 23763 89296 nan 2016-09-30 New Ownership <p>     23764 rows x 4 columns     memory usage: 1.51 MB     name: inspections     type: getml.DataFrame </p> In\u00a0[7]: Copied! <pre>violations\n</pre> violations Out[7]:  name  business_id date          violation_type_id risk_category description                       role unused_float unused_string unused_string     unused_string unused_string                    0 10 2014-07-29 103129 Moderate Risk Insufficient hot water or runnin... 1 10 2014-07-29 103144 Low Risk Unapproved or unmaintained equip... 2 10 2014-01-14 103119 Moderate Risk Inadequate and inaccessible hand... 3 10 2014-01-14 103145 Low Risk Improper storage of equipment ut... 4 10 2014-01-14 103154 Low Risk Unclean or degraded floors walls... ... ... ... ... ... 36045 88878 2016-08-19 103144 Low Risk Unapproved or unmaintained equip... 36046 88878 2016-08-19 103124 Moderate Risk Inadequately cleaned or sanitize... 36047 89072 2016-09-22 103120 Moderate Risk Moderate risk food holding tempe... 36048 89072 2016-09-22 103131 Moderate Risk Moderate risk vermin infestation 36049 89072 2016-09-22 103149 Low Risk Wiping cloths not clean or prope... <p>     36050 rows x 5 columns     memory usage: 4.06 MB     name: violations     type: getml.DataFrame </p> <p>getML requires that we define roles for each of the columns.</p> In\u00a0[8]: Copied! <pre>businesses.set_role(\"business_id\", getml.data.roles.join_key)\nbusinesses.set_role(\"name\", getml.data.roles.text)\nbusinesses.set_role([\"postal_code\", \"tax_code\", \"owner_zip\"], getml.data.roles.categorical)\n\nbusinesses\n</pre> businesses.set_role(\"business_id\", getml.data.roles.join_key) businesses.set_role(\"name\", getml.data.roles.text) businesses.set_role([\"postal_code\", \"tax_code\", \"owner_zip\"], getml.data.roles.categorical)  businesses Out[8]: name business_id postal_code tax_code    owner_zip   name                                 latitude    longitude phone_number business_certificate address                       city          application_date owner_name                       owner_address                owner_city    owner_state   role    join_key categorical categorical categorical text                             unused_float unused_float unused_float         unused_float unused_string                 unused_string unused_string    unused_string                    unused_string                unused_string unused_string 0 10 94104 H24 94104 Tiramisu Kitchen 37.7911 -122.404 nan 779059 033 Belden Pl San Francisco NULL Tiramisu LLC 33 Belden St San Francisco CA 1 24 94104 H24 94104 OMNI S.F. Hotel - 2nd Floor Pant... 37.7929 -122.403 nan 352312 500 California St, 2nd  Floor San Francisco NULL OMNI San Francisco Hotel Corp 500 California St, 2nd Floor San Francisco CA 2 31 94133 H24 94133 Norman's Ice Cream and Freezes 37.8072 -122.419 nan 346882 2801 Leavenworth St San Francisco NULL Norman Antiforda 2801 Leavenworth St San Francisco CA 3 45 94110 H24 94114 CHARLIE'S DELI CAFE 37.7471 -122.414 nan 340024 3202 FOLSOM St S.F. 2001-10-10 HARB, CHARLES AND KRISTIN 1150 SANCHEZ S.F. CA 4 48 94122 H24 94122 ART'S CAFE 37.764 -122.466 nan 318022 747 IRVING St SAN FRANCISCO NULL YOON HAE RYONG 1567 FUNSTON AVE SAN FRANCISCO CA ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 6353 89335 94102 H25 94102 Breaking Bad Sandwiches nan nan nan 1057025 154 McAllister St NULL 2016-09-23 JPMD, LLC 662 Bellhurst Lane Castro Valley CA 6354 89336 94109 H26 94109 Miller's Rest nan nan nan 1057746 1085 Sutter St NULL 2016-09-23 Miller's Rest, LLC 2906 Bush Street San Francisco CA 6355 89393 94133 H24 94122 Panuchos nan nan nan 1042408 620 Broadway St NULL 2016-09-28 Los Aluxes, LLC 1032 Irving Street, #421 San Francisco CA 6356 89416 94109 H24 94109 Nobhill Pizza &amp; Shawerma nan nan nan 1051081 1534 California St NULL 2016-09-29 BBA Foods, Inc. 840 Post Street, #218 San Francisco CA 6357 89453 94110 H29 94549 Burger King #4668 nan nan nan 459309 1690 Valencia St San Francisco 2016-10-03 Golden Gate Restaurant Group, In... P.O Box 21 Lafeyette CA <p>     6358 rows x 16 columns     memory usage: 1.36 MB     name: businesses     type: getml.DataFrame </p> In\u00a0[9]: Copied! <pre>inspections = inspections[~inspections.score.is_nan()].to_df(\"inspections\")\n\ninspections.set_role(\"business_id\", getml.data.roles.join_key)\ninspections.set_role(\"score\", getml.data.roles.target)\ninspections.set_role(\"date\", getml.data.roles.time_stamp)\n\ninspections\n</pre> inspections = inspections[~inspections.score.is_nan()].to_df(\"inspections\")  inspections.set_role(\"business_id\", getml.data.roles.join_key) inspections.set_role(\"score\", getml.data.roles.target) inspections.set_role(\"date\", getml.data.roles.time_stamp)  inspections Out[9]:  name                        date business_id  score type                   role                  time_stamp    join_key target unused_string          unit time stamp, comparison only 0 2014-01-14 10 92 Routine - Unscheduled 1 2014-07-29 10 94 Routine - Unscheduled 2 2016-05-03 10 82 Routine - Unscheduled 3 2013-11-18 24 100 Routine - Unscheduled 4 2014-06-12 24 96 Routine - Unscheduled ... ... ... ... 12882 2016-09-22 89072 90 Routine - Unscheduled 12883 2016-09-12 89198 100 Routine - Unscheduled 12884 2016-09-12 89199 100 Routine - Unscheduled 12885 2016-09-12 89200 100 Routine - Unscheduled 12886 2016-09-12 89204 100 Routine - Unscheduled <p>     12887 rows x 4 columns     memory usage: 0.64 MB     name: inspections     type: getml.DataFrame </p> In\u00a0[10]: Copied! <pre>violations.set_role(\"business_id\", getml.data.roles.join_key)\nviolations.set_role(\"date\", getml.data.roles.time_stamp)\nviolations.set_role([\"violation_type_id\", \"risk_category\"], getml.data.roles.categorical)\nviolations.set_role(\"description\", getml.data.roles.text)\n\nviolations\n</pre> violations.set_role(\"business_id\", getml.data.roles.join_key) violations.set_role(\"date\", getml.data.roles.time_stamp) violations.set_role([\"violation_type_id\", \"risk_category\"], getml.data.roles.categorical) violations.set_role(\"description\", getml.data.roles.text)  violations Out[10]:  name                        date business_id violation_type_id risk_category description                       role                  time_stamp    join_key categorical       categorical   text                              unit time stamp, comparison only 0 2014-07-29 10 103129 Moderate Risk Insufficient hot water or runnin... 1 2014-07-29 10 103144 Low Risk Unapproved or unmaintained equip... 2 2014-01-14 10 103119 Moderate Risk Inadequate and inaccessible hand... 3 2014-01-14 10 103145 Low Risk Improper storage of equipment ut... 4 2014-01-14 10 103154 Low Risk Unclean or degraded floors walls... ... ... ... ... ... 36045 2016-08-19 88878 103144 Low Risk Unapproved or unmaintained equip... 36046 2016-08-19 88878 103124 Moderate Risk Inadequately cleaned or sanitize... 36047 2016-09-22 89072 103120 Moderate Risk Moderate risk food holding tempe... 36048 2016-09-22 89072 103131 Moderate Risk Moderate risk vermin infestation 36049 2016-09-22 89072 103149 Low Risk Wiping cloths not clean or prope... <p>     36050 rows x 5 columns     memory usage: 2.59 MB     name: violations     type: getml.DataFrame </p> In\u00a0[11]: Copied! <pre>split = getml.data.split.random(train=0.8, test=0.2)\n</pre> split = getml.data.split.random(train=0.8, test=0.2) In\u00a0[12]: Copied! <pre>star_schema = getml.data.StarSchema(population=inspections, alias=\"population\", split=split)\n\nstar_schema.join(\n    businesses,\n    on=\"business_id\",\n    relationship=getml.data.relationship.many_to_one,\n)\n\nstar_schema.join(\n    violations,\n    on=\"business_id\",\n    time_stamps=\"date\",\n    horizon=getml.data.time.days(1),\n)\n\nstar_schema.join(\n    inspections,\n    on=\"business_id\",\n    time_stamps=\"date\",\n    lagged_targets=True,\n    horizon=getml.data.time.days(1),\n)\n\nstar_schema\n</pre> star_schema = getml.data.StarSchema(population=inspections, alias=\"population\", split=split)  star_schema.join(     businesses,     on=\"business_id\",     relationship=getml.data.relationship.many_to_one, )  star_schema.join(     violations,     on=\"business_id\",     time_stamps=\"date\",     horizon=getml.data.time.days(1), )  star_schema.join(     inspections,     on=\"business_id\",     time_stamps=\"date\",     lagged_targets=True,     horizon=getml.data.time.days(1), )  star_schema Out[12]: data model diagram businessesviolationsinspectionspopulationbusiness_id = business_idRelationship: many-to-onebusiness_id = business_iddate &lt;= dateHorizon: 1.0 daysbusiness_id = business_iddate &lt;= dateHorizon: 1.0 daysLagged targets allowed staging data frames            staging table                0 population, businesses POPULATION__STAGING_TABLE_1 1 inspections INSPECTIONS__STAGING_TABLE_2 2 violations VIOLATIONS__STAGING_TABLE_3 container population subset name         rows type 0 test inspections 2492 View 1 train inspections 10395 View peripheral name         rows type      0 businesses 6358 DataFrame 1 violations 36050 DataFrame 2 inspections 12887 DataFrame <p>Set-up the feature learner &amp; predictor</p> <p>We use the relboost algorithms for this problem. Because of the large number of keywords, we regularize the model a bit by requiring a minimum support for the keywords (<code>min_num_samples</code>).</p> In\u00a0[13]: Copied! <pre>mapping = getml.preprocessors.Mapping()\n\nfast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    num_threads=1,\n)\n\npredictor = getml.predictors.XGBoostRegressor(n_jobs=1)\n</pre> mapping = getml.preprocessors.Mapping()  fast_prop = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.SquareLoss,     num_threads=1, )  predictor = getml.predictors.XGBoostRegressor(n_jobs=1) <p>Build the pipeline</p> In\u00a0[14]: Copied! <pre>pipe1 = getml.pipeline.Pipeline(\n    tags=['fast_prop'],\n    data_model=star_schema.data_model,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    predictors=[predictor]\n)\n\npipe1\n</pre> pipe1 = getml.pipeline.Pipeline(     tags=['fast_prop'],     data_model=star_schema.data_model,     preprocessors=[mapping],     feature_learners=[fast_prop],     predictors=[predictor] )  pipe1 Out[14]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=['businesses', 'inspections', 'violations'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['fast_prop'])</pre> In\u00a0[15]: Copied! <pre>pipe1.check(star_schema.train)\n</pre> pipe1.check(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 1 issues labeled INFO and 0 issues labeled WARNING.\n</pre> Out[15]: type label                  message                          0 INFO FOREIGN KEYS NOT FOUND When joining POPULATION__STAGING_TABLE_1 and VIOLATIONS__STAGING_TABLE_3 over 'business_id' and 'business_id', there are no corresponding entries for 5.685426% of entries in 'business_id' in 'POPULATION__STAGING_TABLE_1'. You might want to double-check your join keys. In\u00a0[16]: Copied! <pre>pipe1.fit(star_schema.train)\n</pre> pipe1.fit(star_schema.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 1 issues labeled INFO and 0 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nIndexing text fields... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 104 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:2.358353\n\n</pre> Out[16]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=['businesses', 'inspections', 'violations'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['fast_prop', 'container-Z9ANmd'])</pre> In\u00a0[17]: Copied! <pre>fastprop_score = pipe1.score(star_schema.test)\nfastprop_score\n</pre> fastprop_score = pipe1.score(star_schema.test) fastprop_score <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[17]: date time           set used target     mae    rmse rsquared 0 2024-02-21 15:09:45 train score 4.8865 6.5247 0.3608 1 2024-02-21 15:09:45 test score 5.3218 7.0532 0.2889 In\u00a0[18]: Copied! <pre>population_train_pd = star_schema.train.population.to_pandas()\npopulation_test_pd = star_schema.test.population.to_pandas()\n</pre> population_train_pd = star_schema.train.population.to_pandas() population_test_pd = star_schema.test.population.to_pandas() In\u00a0[19]: Copied! <pre>inspections_pd = inspections.drop(inspections.roles.unused).to_pandas()\nviolations_pd = violations.drop(violations.roles.unused).to_pandas()\nbusinesses_pd = businesses.drop(businesses.roles.unused).to_pandas()\n</pre> inspections_pd = inspections.drop(inspections.roles.unused).to_pandas() violations_pd = violations.drop(violations.roles.unused).to_pandas() businesses_pd = businesses.drop(businesses.roles.unused).to_pandas() In\u00a0[20]: Copied! <pre>population_train_pd[\"id\"] = population_train_pd.index\n\npopulation_train_pd = population_train_pd.merge(\n    businesses_pd,\n    on=\"business_id\"\n)\n\npopulation_train_pd\n</pre> population_train_pd[\"id\"] = population_train_pd.index  population_train_pd = population_train_pd.merge(     businesses_pd,     on=\"business_id\" )  population_train_pd Out[20]: business_id score date id postal_code tax_code owner_zip name 0 10 92.0 2014-01-14 0 94104 H24 94104 Tiramisu Kitchen 1 10 94.0 2014-07-29 1 94104 H24 94104 Tiramisu Kitchen 2 10 82.0 2016-05-03 2 94104 H24 94104 Tiramisu Kitchen 3 24 96.0 2014-06-12 3 94104 H24 94104 OMNI S.F. Hotel - 2nd Floor Pantry 4 24 96.0 2014-11-24 4 94104 H24 94104 OMNI S.F. Hotel - 2nd Floor Pantry ... ... ... ... ... ... ... ... ... 10390 88878 94.0 2016-08-19 10390 94102 H24 94566 Jamba Juice 10391 89072 90.0 2016-09-22 10391 94109 H91 94109 Epicurean at Sacred Heart Catholic Prep School 10392 89198 100.0 2016-09-12 10392 94107 H36 29615 AT&amp;T Park - Beer Cart/View Level, Sec. 333 10393 89199 100.0 2016-09-12 10393 94107 H36 29615 AT&amp;T Park - Beer Cart/Lower CF, Sec. 140 10394 89200 100.0 2016-09-12 10394 94107 H36 29615 AT&amp;T Park - Beer Cart/Lower CF, Sec. 142 <p>10395 rows \u00d7 8 columns</p> In\u00a0[21]: Copied! <pre>population_test_pd[\"id\"] = population_test_pd.index\n\npopulation_test_pd = population_test_pd.merge(\n    businesses_pd,\n    on=\"business_id\"\n)\n\npopulation_test_pd\n</pre> population_test_pd[\"id\"] = population_test_pd.index  population_test_pd = population_test_pd.merge(     businesses_pd,     on=\"business_id\" )  population_test_pd Out[21]: business_id score date id postal_code tax_code owner_zip name 0 24 100.0 2013-11-18 0 94104 H24 94104 OMNI S.F. Hotel - 2nd Floor Pantry 1 24 96.0 2016-03-11 1 94104 H24 94104 OMNI S.F. Hotel - 2nd Floor Pantry 2 45 94.0 2013-12-09 2 94110 H24 94114 CHARLIE'S DELI CAFE 3 58 78.0 2014-07-25 3 94111 H24 94111 Oasis Grill 4 66 91.0 2014-05-19 4 94122 H24 94122 STARBUCKS ... ... ... ... ... ... ... ... ... 2487 87802 91.0 2016-06-07 2487 94110 H25 94110 Bernal Heights Pizzeria 2488 88082 84.0 2016-08-30 2488 94133 H24 94133 Chongqing Xiaomian 2489 88447 96.0 2016-08-17 2489 None H91 94107 Fare Resources 2490 88702 96.0 2016-08-15 2490 94118 H25 94118 Dancing Bull 2491 89204 100.0 2016-09-12 2491 94107 H36 94107 AT&amp;T - Hol n Jam Cart/Upper CF, Sec. 142 <p>2492 rows \u00d7 8 columns</p> In\u00a0[22]: Copied! <pre>def prepare_peripheral(violations_pd, train_or_test):\n    \"\"\"\n    Helper function that imitates the behavior of \n    the data model defined above.\n    \"\"\"\n    violations_new = violations_pd.merge(\n        train_or_test[[\"id\", \"business_id\", \"date\"]],\n        on=\"business_id\"\n    )\n\n    violations_new = violations_new[\n        violations_new[\"date_x\"] &lt; violations_new[\"date_y\"]\n    ]\n\n    del violations_new[\"date_y\"]\n    del violations_new[\"business_id\"]\n\n    return violations_new.rename(columns={\"date_x\": \"date\"})\n</pre> def prepare_peripheral(violations_pd, train_or_test):     \"\"\"     Helper function that imitates the behavior of      the data model defined above.     \"\"\"     violations_new = violations_pd.merge(         train_or_test[[\"id\", \"business_id\", \"date\"]],         on=\"business_id\"     )      violations_new = violations_new[         violations_new[\"date_x\"] &lt; violations_new[\"date_y\"]     ]      del violations_new[\"date_y\"]     del violations_new[\"business_id\"]      return violations_new.rename(columns={\"date_x\": \"date\"}) In\u00a0[23]: Copied! <pre>violations_train_pd = prepare_peripheral(violations_pd, population_train_pd)\nviolations_test_pd = prepare_peripheral(violations_pd, population_test_pd)\nviolations_train_pd\n</pre> violations_train_pd = prepare_peripheral(violations_pd, population_train_pd) violations_test_pd = prepare_peripheral(violations_pd, population_test_pd) violations_train_pd Out[23]: violation_type_id risk_category description date id 2 103129 Moderate Risk Insufficient hot water or running water 2014-07-29 2 5 103144 Low Risk Unapproved or unmaintained equipment or utensils 2014-07-29 2 7 103119 Moderate Risk Inadequate and inaccessible handwashing facili... 2014-01-14 1 8 103119 Moderate Risk Inadequate and inaccessible handwashing facili... 2014-01-14 2 10 103145 Low Risk Improper storage of equipment utensils or linens 2014-01-14 1 ... ... ... ... ... ... 89220 103119 Moderate Risk Inadequate and inaccessible handwashing facili... 2016-02-16 10290 89256 103131 Moderate Risk Moderate risk vermin infestation 2016-04-04 10308 89336 103154 Low Risk Unclean or degraded floors walls or ceilings 2016-04-11 10331 89338 103148 Low Risk No thermometers or uncalibrated thermometers 2016-04-11 10331 89340 103144 Low Risk Unapproved or unmaintained equipment or utensils 2016-04-11 10331 <p>29004 rows \u00d7 5 columns</p> In\u00a0[24]: Copied! <pre>inspections_train_pd = prepare_peripheral(inspections_pd, population_train_pd)\ninspections_test_pd = prepare_peripheral(inspections_pd, population_test_pd)\ninspections_train_pd\n</pre> inspections_train_pd = prepare_peripheral(inspections_pd, population_train_pd) inspections_test_pd = prepare_peripheral(inspections_pd, population_test_pd) inspections_train_pd Out[24]: score date id 1 92.0 2014-01-14 1 2 92.0 2014-01-14 2 5 94.0 2014-07-29 2 9 100.0 2013-11-18 3 10 100.0 2013-11-18 4 ... ... ... ... 32628 92.0 2016-02-16 10290 32648 96.0 2016-04-04 10308 32673 94.0 2016-04-11 10331 32707 100.0 2016-05-23 10360 32738 100.0 2016-08-17 10389 <p>11190 rows \u00d7 3 columns</p> In\u00a0[25]: Copied! <pre>del population_train_pd[\"business_id\"]\ndel population_test_pd[\"business_id\"]\n</pre> del population_train_pd[\"business_id\"] del population_test_pd[\"business_id\"] In\u00a0[26]: Copied! <pre>population_train_pd\n</pre> population_train_pd Out[26]: score date id postal_code tax_code owner_zip name 0 92.0 2014-01-14 0 94104 H24 94104 Tiramisu Kitchen 1 94.0 2014-07-29 1 94104 H24 94104 Tiramisu Kitchen 2 82.0 2016-05-03 2 94104 H24 94104 Tiramisu Kitchen 3 96.0 2014-06-12 3 94104 H24 94104 OMNI S.F. Hotel - 2nd Floor Pantry 4 96.0 2014-11-24 4 94104 H24 94104 OMNI S.F. Hotel - 2nd Floor Pantry ... ... ... ... ... ... ... ... 10390 94.0 2016-08-19 10390 94102 H24 94566 Jamba Juice 10391 90.0 2016-09-22 10391 94109 H91 94109 Epicurean at Sacred Heart Catholic Prep School 10392 100.0 2016-09-12 10392 94107 H36 29615 AT&amp;T Park - Beer Cart/View Level, Sec. 333 10393 100.0 2016-09-12 10393 94107 H36 29615 AT&amp;T Park - Beer Cart/Lower CF, Sec. 140 10394 100.0 2016-09-12 10394 94107 H36 29615 AT&amp;T Park - Beer Cart/Lower CF, Sec. 142 <p>10395 rows \u00d7 7 columns</p> In\u00a0[27]: Copied! <pre>def add_index(df):\n    df.insert(0, \"index\", range(len(df)))\n\npopulation_pd_logical_types = {\n    \"id\": ww.logical_types.Integer,\n    \"score\": ww.logical_types.Integer,\n    \"date\": ww.logical_types.Datetime,\n    \"postal_code\": ww.logical_types.Categorical,\n    \"tax_code\": ww.logical_types.Categorical,\n    \"owner_zip\": ww.logical_types.Categorical,\n    \"name\": ww.logical_types.Categorical\n}\npopulation_train_pd.ww.init(logical_types=population_pd_logical_types, index=\"id\", name=\"population\")\npopulation_test_pd.ww.init(logical_types=population_pd_logical_types, index=\"id\", name=\"population\")\n\nadd_index(inspections_train_pd)\nadd_index(inspections_test_pd)\ninspections_pd_logical_types = {\n    \"index\": ww.logical_types.Integer,\n    \"score\": ww.logical_types.Integer,\n    \"date\": ww.logical_types.Datetime,\n    \"id\": ww.logical_types.Integer\n}\ninspections_train_pd.ww.init(logical_types=inspections_pd_logical_types, index=\"index\", name=\"inspections\")\ninspections_test_pd.ww.init(logical_types=inspections_pd_logical_types, index=\"index\", name=\"inspections\")\n\nadd_index(violations_train_pd)\nadd_index(violations_test_pd)\nviolations_pd_logical_types = {\n    \"index\": ww.logical_types.Integer,\n    \"violation_type_id\": ww.logical_types.Categorical,\n    \"risk_category\": ww.logical_types.Categorical,\n    \"description\": ww.logical_types.Categorical,\n    \"date\": ww.logical_types.Datetime,\n    \"id\": ww.logical_types.Integer\n}\nviolations_train_pd.ww.init(logical_types=violations_pd_logical_types, index=\"index\", name=\"violations\")\nviolations_test_pd.ww.init(logical_types=violations_pd_logical_types, index=\"index\", name=\"violations\")\n</pre> def add_index(df):     df.insert(0, \"index\", range(len(df)))  population_pd_logical_types = {     \"id\": ww.logical_types.Integer,     \"score\": ww.logical_types.Integer,     \"date\": ww.logical_types.Datetime,     \"postal_code\": ww.logical_types.Categorical,     \"tax_code\": ww.logical_types.Categorical,     \"owner_zip\": ww.logical_types.Categorical,     \"name\": ww.logical_types.Categorical } population_train_pd.ww.init(logical_types=population_pd_logical_types, index=\"id\", name=\"population\") population_test_pd.ww.init(logical_types=population_pd_logical_types, index=\"id\", name=\"population\")  add_index(inspections_train_pd) add_index(inspections_test_pd) inspections_pd_logical_types = {     \"index\": ww.logical_types.Integer,     \"score\": ww.logical_types.Integer,     \"date\": ww.logical_types.Datetime,     \"id\": ww.logical_types.Integer } inspections_train_pd.ww.init(logical_types=inspections_pd_logical_types, index=\"index\", name=\"inspections\") inspections_test_pd.ww.init(logical_types=inspections_pd_logical_types, index=\"index\", name=\"inspections\")  add_index(violations_train_pd) add_index(violations_test_pd) violations_pd_logical_types = {     \"index\": ww.logical_types.Integer,     \"violation_type_id\": ww.logical_types.Categorical,     \"risk_category\": ww.logical_types.Categorical,     \"description\": ww.logical_types.Categorical,     \"date\": ww.logical_types.Datetime,     \"id\": ww.logical_types.Integer } violations_train_pd.ww.init(logical_types=violations_pd_logical_types, index=\"index\", name=\"violations\") violations_test_pd.ww.init(logical_types=violations_pd_logical_types, index=\"index\", name=\"violations\") In\u00a0[28]: Copied! <pre>dataframes_train = {\n    \"population\" : (population_train_pd, ),\n    \"inspections\" : (inspections_train_pd, ),\n    \"violations\" : (violations_train_pd, )\n}\n</pre> dataframes_train = {     \"population\" : (population_train_pd, ),     \"inspections\" : (inspections_train_pd, ),     \"violations\" : (violations_train_pd, ) } In\u00a0[29]: Copied! <pre>dataframes_test = {\n    \"population\" : (population_test_pd, ),\n    \"inspections\" : (inspections_test_pd, ),\n    \"violations\" : (violations_test_pd, )\n}\n</pre> dataframes_test = {     \"population\" : (population_test_pd, ),     \"inspections\" : (inspections_test_pd, ),     \"violations\" : (violations_test_pd, ) } In\u00a0[30]: Copied! <pre>relationships = [\n    (\"population\", \"id\", \"inspections\", \"id\"),\n    (\"population\", \"id\", \"violations\", \"id\")\n]\n</pre> relationships = [     (\"population\", \"id\", \"inspections\", \"id\"),     (\"population\", \"id\", \"violations\", \"id\") ] In\u00a0[31]: Copied! <pre>featuretools_train_pd = featuretools.dfs(\n    dataframes=dataframes_train,\n    relationships=relationships,\n    target_dataframe_name=\"population\")[0]\n</pre> featuretools_train_pd = featuretools.dfs(     dataframes=dataframes_train,     relationships=relationships,     target_dataframe_name=\"population\")[0] In\u00a0[32]: Copied! <pre>featuretools_test_pd = featuretools.dfs(\n    dataframes=dataframes_test,\n    relationships=relationships,\n    target_dataframe_name=\"population\")[0]\n</pre> featuretools_test_pd = featuretools.dfs(     dataframes=dataframes_test,     relationships=relationships,     target_dataframe_name=\"population\")[0] In\u00a0[33]: Copied! <pre>featuretools_train = getml.data.DataFrame.from_pandas(featuretools_train_pd, \"featuretools_train\")\nfeaturetools_test = getml.data.DataFrame.from_pandas(featuretools_test_pd, \"featuretools_test\")\n</pre> featuretools_train = getml.data.DataFrame.from_pandas(featuretools_train_pd, \"featuretools_train\") featuretools_test = getml.data.DataFrame.from_pandas(featuretools_test_pd, \"featuretools_test\") In\u00a0[34]: Copied! <pre>featuretools_train.set_role(\"score\", getml.data.roles.target)\nfeaturetools_train.set_role(featuretools_train.roles.unused_float, getml.data.roles.numerical)\nfeaturetools_train.set_role(featuretools_train.roles.unused_string, getml.data.roles.categorical)\n\nfeaturetools_train\n</pre> featuretools_train.set_role(\"score\", getml.data.roles.target) featuretools_train.set_role(featuretools_train.roles.unused_float, getml.data.roles.numerical) featuretools_train.set_role(featuretools_train.roles.unused_string, getml.data.roles.categorical)  featuretools_train Out[34]:  name  score postal_code tax_code    owner_zip   name                             COUNT(inspections) COUNT(violations) MODE(violations.description)     MODE(violations.risk_category) MODE(violations.violation_type_id) NUM_UNIQUE(violations.description) NUM_UNIQUE(violations.risk_category) NUM_UNIQUE(violations.violation_type_id) DAY(date)   MONTH(date) WEEKDAY(date) YEAR(date)  MODE(inspections.DAY(date)) MODE(inspections.MONTH(date)) MODE(inspections.WEEKDAY(date)) MODE(inspections.YEAR(date)) NUM_UNIQUE(inspections.DAY(date)) NUM_UNIQUE(inspections.MONTH(date)) NUM_UNIQUE(inspections.WEEKDAY(date)) NUM_UNIQUE(inspections.YEAR(date)) MODE(violations.DAY(date)) MODE(violations.MONTH(date)) MODE(violations.WEEKDAY(date)) MODE(violations.YEAR(date)) NUM_UNIQUE(violations.DAY(date)) NUM_UNIQUE(violations.MONTH(date)) NUM_UNIQUE(violations.WEEKDAY(date)) NUM_UNIQUE(violations.YEAR(date)) MAX(inspections.score) MEAN(inspections.score) MIN(inspections.score) SKEW(inspections.score) STD(inspections.score) SUM(inspections.score)  role target categorical categorical categorical categorical                      categorical        categorical       categorical                      categorical                    categorical                      categorical                      categorical                      categorical                      categorical categorical categorical   categorical categorical                 categorical                   categorical                     categorical                  categorical                      categorical                      categorical                      categorical                      categorical                categorical                  categorical                    categorical                 categorical                      categorical                      categorical                      categorical                                   numerical               numerical              numerical               numerical              numerical              numerical 0 92 94104 H24 94104 Tiramisu Kitchen 0 0 NULL NULL NULL NULL NULL NULL 14 1 1 2014 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL nan nan nan nan nan 0 1 94 94104 H24 94104 Tiramisu Kitchen 1 3 Improper storage of equipment ut... Low Risk 103119 3 2 3 29 7 1 2014 14 1 1 2014 1 1 1 1 14 1 1 2014 1 1 1 1 92 92 92 nan nan 92 2 82 94104 H24 94104 Tiramisu Kitchen 2 5 Improper storage of equipment ut... Low Risk 103119 5 2 5 3 5 1 2016 14 1 1 2014 2 2 1 1 14 1 1 2014 2 2 1 1 94 93 92 nan 1.4142 186 3 96 94104 H24 94104 OMNI S.F. Hotel - 2nd Floor Pant... 1 0 NULL NULL NULL NULL NULL NULL 12 6 3 2014 18 11 0 2013 1 1 1 1 NULL NULL NULL NULL NULL NULL NULL NULL 100 100 100 nan nan 100 4 96 94104 H24 94104 OMNI S.F. Hotel - 2nd Floor Pant... 2 2 Improper storage of equipment ut... Low Risk 103145 2 1 2 24 11 0 2014 12 6 0 2013 2 2 2 2 12 6 3 2014 1 1 1 1 100 98 96 nan 2.8284 196 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 10390 94 94102 H24 94566 Jamba Juice 0 0 NULL NULL NULL NULL NULL NULL 19 8 4 2016 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL nan nan nan nan nan 0 10391 90 94109 H91 94109 Epicurean at Sacred Heart Cathol... 0 0 NULL NULL NULL NULL NULL NULL 22 9 3 2016 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL nan nan nan nan nan 0 10392 100 94107 H36 29615 AT&amp;T Park - Beer Cart/View Level... 0 0 NULL NULL NULL NULL NULL NULL 12 9 0 2016 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL nan nan nan nan nan 0 10393 100 94107 H36 29615 AT&amp;T Park - Beer Cart/Lower CF, ... 0 0 NULL NULL NULL NULL NULL NULL 12 9 0 2016 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL nan nan nan nan nan 0 10394 100 94107 H36 29615 AT&amp;T Park - Beer Cart/Lower CF, ... 0 0 NULL NULL NULL NULL NULL NULL 12 9 0 2016 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL nan nan nan nan nan 0 <p>     10395 rows x 39 columns     memory usage: 1.91 MB     name: featuretools_train     type: getml.DataFrame </p> In\u00a0[35]: Copied! <pre>featuretools_test.set_role(\"score\", getml.data.roles.target)\nfeaturetools_test.set_role(featuretools_test.roles.unused_float, getml.data.roles.numerical)\nfeaturetools_test.set_role(featuretools_test.roles.unused_string, getml.data.roles.categorical)\n\nfeaturetools_test\n</pre> featuretools_test.set_role(\"score\", getml.data.roles.target) featuretools_test.set_role(featuretools_test.roles.unused_float, getml.data.roles.numerical) featuretools_test.set_role(featuretools_test.roles.unused_string, getml.data.roles.categorical)  featuretools_test Out[35]: name  score postal_code tax_code    owner_zip   name                             COUNT(inspections) COUNT(violations) MODE(violations.description)     MODE(violations.risk_category) MODE(violations.violation_type_id) NUM_UNIQUE(violations.description) NUM_UNIQUE(violations.risk_category) NUM_UNIQUE(violations.violation_type_id) DAY(date)   MONTH(date) WEEKDAY(date) YEAR(date)  MODE(inspections.DAY(date)) MODE(inspections.MONTH(date)) MODE(inspections.WEEKDAY(date)) MODE(inspections.YEAR(date)) NUM_UNIQUE(inspections.DAY(date)) NUM_UNIQUE(inspections.MONTH(date)) NUM_UNIQUE(inspections.WEEKDAY(date)) NUM_UNIQUE(inspections.YEAR(date)) MODE(violations.DAY(date)) MODE(violations.MONTH(date)) MODE(violations.WEEKDAY(date)) MODE(violations.YEAR(date)) NUM_UNIQUE(violations.DAY(date)) NUM_UNIQUE(violations.MONTH(date)) NUM_UNIQUE(violations.WEEKDAY(date)) NUM_UNIQUE(violations.YEAR(date)) MAX(inspections.score) MEAN(inspections.score) MIN(inspections.score) SKEW(inspections.score) STD(inspections.score) SUM(inspections.score) role target categorical categorical categorical categorical                      categorical        categorical       categorical                      categorical                    categorical                      categorical                      categorical                      categorical                      categorical categorical categorical   categorical categorical                 categorical                   categorical                     categorical                  categorical                      categorical                      categorical                      categorical                      categorical                categorical                  categorical                    categorical                 categorical                      categorical                      categorical                      categorical                                   numerical               numerical              numerical               numerical              numerical              numerical 0 100 94104 H24 94104 OMNI S.F. Hotel - 2nd Floor Pant... 0 0 NULL NULL NULL NULL NULL NULL 18 11 0 2013 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL nan nan nan nan nan 0 1 96 94104 H24 94104 OMNI S.F. Hotel - 2nd Floor Pant... 3 3 Improper storage of equipment ut... Low Risk 103119 3 2 3 11 3 4 2016 12 11 0 2014 3 2 2 2 12 6 3 2014 2 2 2 1 100 97.3333 96 1.7321 2.3094 292 2 94 94110 H24 94114 CHARLIE'S DELI CAFE 0 0 NULL NULL NULL NULL NULL NULL 9 12 0 2013 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL nan nan nan nan nan 0 3 78 94111 H24 94111 Oasis Grill 0 0 NULL NULL NULL NULL NULL NULL 25 7 4 2014 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL nan nan nan nan nan 0 4 91 94122 H24 94122 STARBUCKS 1 1 Wiping cloths not clean or prope... Low Risk 103149 1 1 1 19 5 0 2014 10 2 0 2014 1 1 1 1 10 2 0 2014 1 1 1 1 98 98 98 nan nan 98 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2487 91 94110 H25 94110 Bernal Heights Pizzeria 0 0 NULL NULL NULL NULL NULL NULL 7 6 1 2016 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL nan nan nan nan nan 0 2488 84 94133 H24 94133 Chongqing Xiaomian 0 0 NULL NULL NULL NULL NULL NULL 30 8 1 2016 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL nan nan nan nan nan 0 2489 96 NULL H91 94107 Fare Resources 0 0 NULL NULL NULL NULL NULL NULL 17 8 2 2016 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL nan nan nan nan nan 0 2490 96 94118 H25 94118 Dancing Bull 0 0 NULL NULL NULL NULL NULL NULL 15 8 0 2016 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL nan nan nan nan nan 0 2491 100 94107 H36 94107 AT&amp;T - Hol n Jam Cart/Upper CF, ... 0 0 NULL NULL NULL NULL NULL NULL 12 9 0 2016 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL nan nan nan nan nan 0 <p>     2492 rows x 39 columns     memory usage: 0.46 MB     name: featuretools_test     type: getml.DataFrame </p> <p>We train an untuned XGBoostRegressor on top of featuretools' features, just like we have done for getML's features.</p> <p>Since some of featuretools features are categorical, we allow the pipeline to include these features as well. Other features contain NaN values, which is why we also apply getML's Imputation preprocessor.</p> In\u00a0[36]: Copied! <pre>imputation = getml.preprocessors.Imputation()\n\npredictor = getml.predictors.XGBoostRegressor(n_jobs=1)\n\npipe2 = getml.pipeline.Pipeline(\n    tags=['featuretools'],\n    preprocessors=[imputation],\n    predictors=[predictor],\n    include_categorical=True,\n)\n\npipe2\n</pre> imputation = getml.preprocessors.Imputation()  predictor = getml.predictors.XGBoostRegressor(n_jobs=1)  pipe2 = getml.pipeline.Pipeline(     tags=['featuretools'],     preprocessors=[imputation],     predictors=[predictor],     include_categorical=True, )  pipe2 Out[36]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=['Imputation'],\n         share_selected_features=0.5,\n         tags=['featuretools'])</pre> In\u00a0[37]: Copied! <pre>pipe2.fit(featuretools_train)\n</pre> pipe2.fit(featuretools_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 1 issues labeled INFO and 1 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:2.007516\n\n</pre> Out[37]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=['Imputation'],\n         share_selected_features=0.5,\n         tags=['featuretools'])</pre> In\u00a0[38]: Copied! <pre>featuretools_score = pipe2.score(featuretools_test)\nfeaturetools_score\n</pre> featuretools_score = pipe2.score(featuretools_test) featuretools_score <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[38]: date time           set used           target     mae    rmse rsquared 0 2024-02-21 15:09:56 featuretools_train score 5.1474 6.7615 0.3184 1 2024-02-21 15:09:56 featuretools_test score 5.46 7.2008 0.2612 In\u00a0[39]: Copied! <pre>pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name]\n</pre> pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name] Out[39]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_43\";\n\nCREATE TABLE \"FEATURE_1_43\" AS\nSELECT COUNT( t1.\"date\" - t2.\"date\"  ) - COUNT( DISTINCT t1.\"date\" - t2.\"date\" ) AS \"feature_1_43\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"VIOLATIONS__STAGING_TABLE_3\" t2\nON t1.\"business_id\" = t2.\"business_id\"\nWHERE t2.\"date__1_000000_days\" &lt;= t1.\"date\"\nGROUP BY t1.rowid;\n</pre> In\u00a0[40]: Copied! <pre># Creates a folder named sfscores_pipeline containing\n# the SQL code.\npipe1.features.to_sql().save(\"sfscores_pipeline\")\n</pre> # Creates a folder named sfscores_pipeline containing # the SQL code. pipe1.features.to_sql().save(\"sfscores_pipeline\") In\u00a0[41]: Copied! <pre>pipe1.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"sfscores_spark\")\n</pre> pipe1.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"sfscores_spark\") In\u00a0[42]: Copied! <pre>scores = [fastprop_score, featuretools_score]\npd.DataFrame(data={\n    'Name': ['getML: FastProp', 'featuretools'],\n    'R-squared': [f'{score.rsquared:.1%}' for score in scores],\n    'RMSE': [f'{score.rmse:,.2f}' for score in scores],\n    'MAE': [f'{score.mae:,.2f}' for score in scores]\n})\n</pre> scores = [fastprop_score, featuretools_score] pd.DataFrame(data={     'Name': ['getML: FastProp', 'featuretools'],     'R-squared': [f'{score.rsquared:.1%}' for score in scores],     'RMSE': [f'{score.rmse:,.2f}' for score in scores],     'MAE': [f'{score.mae:,.2f}' for score in scores] }) Out[42]: Name R-squared RMSE MAE 0 getML: FastProp 28.9% 7.05 5.32 1 featuretools 26.1% 7.20 5.46 <p>As we can see, getML's FastProp outperforms featuretools according to all three measures.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/sfscores/#sfscores-predicting-health-inspection-scores-of-restaurants","title":"SFScores - Predicting health inspection scores of restaurants\u00b6","text":"<p>In this notebook, we will benchmark several of getML's feature learning algorithms against featuretools using a dataset of eateries in San Francisco.</p> <p>Summary:</p> <ul> <li>Prediction type: Regression model</li> <li>Domain: Health</li> <li>Prediction target: Sales</li> <li>Population size: 12887</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/sfscores/#background","title":"Background\u00b6","text":"<p>This notebook is based on the San Francisco Dept. of Public Health's database of eateries in San Francisco. These eateries are regularly inspected. The inspections often result in a score.</p> <p>The challenge is to predict the score resulting from an inspection.</p> <p>The dataset has been downloaded from the CTU Prague relational learning repository (Motl and Schulte, 2015)(Now residing at relational-data.org.).</p> <p>We will benchmark getML's feature learning algorithms against featuretools, an open-source implementation of the propositionalization algorithm, similar to getML's FastProp.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/sfscores/#analysis","title":"Analysis\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/sfscores/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/sfscores/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>We begin by downloading the data:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/sfscores/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/sfscores/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"<p>We loaded the data and defined the roles and units. Next, we create a getML pipeline for relational learning.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/sfscores/#21-define-relational-model","title":"2.1 Define relational model\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/sfscores/#22-getml-pipeline","title":"2.2 getML pipeline\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/sfscores/#23-model-training","title":"2.3 Model training\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/sfscores/#24-model-evaluation","title":"2.4 Model evaluation\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/sfscores/#25-featuretools","title":"2.5 featuretools\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/sfscores/#26-features","title":"2.6 Features\u00b6","text":"<p>The most important feature looks as follows:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/sfscores/#27-productionization","title":"2.7 Productionization\u00b6","text":"<p>It is possible to productionize the pipeline by transpiling the features into production-ready SQL code. Please also refer to getML's <code>sqlite3</code> and <code>spark</code> modules.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/sfscores/#28-discussion","title":"2.8 Discussion\u00b6","text":"<p>For a more convenient overview, we summarize our results into a table.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/sfscores/#3-conclusion","title":"3. Conclusion\u00b6","text":"<p>We have benchmarked getML against featuretools on dataset related to health inspections of eateries in San Francisco. We have found that getML outperforms featuretools.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/sfscores/#references","title":"References\u00b6","text":"<p>Motl, Jan, and Oliver Schulte. \"The CTU prague relational learning repository.\" arXiv preprint arXiv:1511.03086 (2015).</p>","boost":0.8},{"location":"examples/enterprise-notebooks/stats/","title":"<span class=\"ntitle\">stats.ipynb</span> <span class=\"ndesc\">Predicting users' reputations</span>","text":"<p>Let's get started with the analysis and set up your session:</p> In\u00a0[1]: Copied! <pre>import copy\nimport os\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\nfrom pathlib import Path\n\nfrom urllib import request\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline  \n\nimport featuretools\nimport woodwork as ww\nimport getml\n\ngetml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.engine.set_project('stats')\n</pre> import copy import os os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\" from pathlib import Path  from urllib import request  import numpy as np import pandas as pd from IPython.display import Image import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline    import featuretools import woodwork as ww import getml  getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.engine.set_project('stats') <pre>getML engine is already running.\n\nConnected to project 'stats'\n</pre> In\u00a0[2]: Copied! <pre>conn = getml.database.connect_mysql(\n    host=\"db.relational-data.org\",\n    dbname=\"stats\",\n    port=3306,\n    user=\"guest\",\n    password=\"relational\"\n)\n\nconn\n</pre> conn = getml.database.connect_mysql(     host=\"db.relational-data.org\",     dbname=\"stats\",     port=3306,     user=\"guest\",     password=\"relational\" )  conn Out[2]: <pre>Connection(dbname='stats', dialect='mysql', host='db.relational-data.org', port=3306)</pre> In\u00a0[3]: Copied! <pre>def load_if_needed(name):\n    \"\"\"\n    Loads the data from the relational learning\n    repository, if the data frame has not already\n    been loaded.\n    \"\"\"\n    if not getml.data.exists(name):\n        data_frame = getml.data.DataFrame.from_db(\n            name=name,\n            table_name=name,\n            conn=conn\n        )\n        data_frame.save()\n    else:\n        data_frame = getml.data.load_data_frame(name)\n    return data_frame\n</pre> def load_if_needed(name):     \"\"\"     Loads the data from the relational learning     repository, if the data frame has not already     been loaded.     \"\"\"     if not getml.data.exists(name):         data_frame = getml.data.DataFrame.from_db(             name=name,             table_name=name,             conn=conn         )         data_frame.save()     else:         data_frame = getml.data.load_data_frame(name)     return data_frame In\u00a0[4]: Copied! <pre>badges = load_if_needed(\"badges\")\nposts = load_if_needed(\"posts\")\nusers = load_if_needed(\"users\")\nvotes = load_if_needed(\"votes\")\n</pre> badges = load_if_needed(\"badges\") posts = load_if_needed(\"posts\") users = load_if_needed(\"users\") votes = load_if_needed(\"votes\") In\u00a0[5]: Copied! <pre>badges\n</pre> badges Out[5]:  name           Id       UserId Name           Date                 role unused_float unused_float unused_string  unused_string       0 1 5 Teacher 2010-07-19 19:39:07 1 2 6 Teacher 2010-07-19 19:39:07 2 3 8 Teacher 2010-07-19 19:39:07 3 4 23 Teacher 2010-07-19 19:39:07 4 5 36 Teacher 2010-07-19 19:39:07 ... ... ... ... 79846 92236 55744 Student 2014-09-13 23:25:21 79847 92237 1118 Nice Answer 2014-09-14 00:09:35 79848 92238 1118 Enlightened 2014-09-14 01:18:29 79849 92239 55746 Student 2014-09-14 01:41:18 79850 92240 12597 Autobiographer 2014-09-14 02:31:28 <p>     79851 rows x 4 columns     memory usage: 4.96 MB     name: badges     type: getml.DataFrame </p> In\u00a0[6]: Copied! <pre>posts\n</pre> posts Out[6]:  name           Id   PostTypeId AcceptedAnswerId        Score    ViewCount  OwnerUserId  AnswerCount CommentCount FavoriteCount LastEditorUserId     ParentId CreaionDate         Body                             LasActivityDate     Title                            Tags                             LastEditDate        CommunityOwnedDate  ClosedDate    OwnerDisplayName LastEditorDisplayName  role unused_float unused_float     unused_float unused_float unused_float unused_float unused_float unused_float  unused_float     unused_float unused_float unused_string       unused_string                    unused_string       unused_string                    unused_string                    unused_string       unused_string       unused_string unused_string    unused_string         0 1 1 15 23 1278 8 5 1 14 nan nan 2010-07-19 19:12:12 &lt;p&gt;How should I elicit prior dis... 2010-09-15 21:08:26 Eliciting priors from experts &lt;bayesian&gt;&lt;prior&gt;&lt;elicitation&gt; NULL NULL NULL NULL NULL 1 2 1 59 22 8198 24 7 1 8 88 nan 2010-07-19 19:12:57 &lt;p&gt;In many different statistical... 2012-11-12 09:21:54 What is normality? &lt;distributions&gt;&lt;normality&gt; 2010-08-07 17:56:44 NULL NULL NULL NULL 2 3 1 5 54 3613 18 19 4 36 183 nan 2010-07-19 19:13:28 &lt;p&gt;What are some valuable Statis... 2013-05-27 14:48:36 What are some valuable Statistic... &lt;software&gt;&lt;open-source&gt; 2011-02-12 05:50:03 2010-07-19 19:13:28 NULL NULL NULL 3 4 1 135 13 5224 23 5 2 2 nan nan 2010-07-19 19:13:31 &lt;p&gt;I have two groups of data.  E... 2010-09-08 03:00:19 Assessing the significance of di... &lt;distributions&gt;&lt;statistical-sign... NULL NULL NULL NULL NULL 4 5 2 nan 81 nan 23 nan 3 nan 23 3 2010-07-19 19:14:43 &lt;p&gt;The R-project&lt;/p&gt;  &lt;p&gt;&lt;a href... 2010-07-19 19:21:15 NULL NULL 2010-07-19 19:21:15 2010-07-19 19:14:43 NULL NULL NULL ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 91971 115374 2 nan 2 nan 805 nan 2 nan 805 115367 2014-09-13 23:45:39 &lt;p&gt;This grew too long for a comm... 2014-09-14 02:05:41 NULL NULL 2014-09-14 02:05:41 NULL NULL NULL NULL 91972 115375 1 nan 0 9 49365 1 0 nan nan nan 2014-09-13 23:46:05 &lt;p&gt;Assume a classification probl... 2014-09-14 02:09:23 Detecting a consistent pattern i... &lt;classification&gt;&lt;cross-validatio... NULL NULL NULL NULL NULL 91973 115376 1 nan 1 5 55746 0 2 nan 7290 nan 2014-09-14 01:27:54 &lt;p&gt;My goal is to create a formul... 2014-09-14 01:40:55 How to project video viewcount b... &lt;summary-statistics&gt;&lt;median&gt;&lt;evi... 2014-09-14 01:40:55 NULL NULL NULL NULL 91974 115377 2 nan 0 nan 805 nan 0 nan 805 115358 2014-09-14 02:03:28 &lt;p&gt;As a practical answer to the ... 2014-09-14 02:54:13 NULL NULL 2014-09-14 02:54:13 NULL NULL NULL NULL 91975 115378 2 nan 0 nan 7250 nan 0 nan nan 115375 2014-09-14 02:09:23 &lt;p&gt;Decision trees are notoriousl... 2014-09-14 02:09:23 NULL NULL NULL NULL NULL NULL NULL <p>     91976 rows x 21 columns     memory usage: 128.41 MB     name: posts     type: getml.DataFrame </p> In\u00a0[7]: Copied! <pre>users\n</pre> users Out[7]:  name           Id   Reputation        Views      UpVotes    DownVotes    AccountId          Age CreationDate        DisplayName    LastAccessDate      WebsiteUrl                     Location           AboutMe                          ProfileImageUrl                   role unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_string       unused_string  unused_string       unused_string                  unused_string      unused_string                    unused_string                    0 -1 1 0 5007 1920 -1 nan 2010-07-19 06:55:26 Community 2010-07-19 06:55:26 http://meta.stackexchange.com/ on the server farm &lt;p&gt;Hi, I'm not really a person.&lt;... NULL 1 2 101 25 3 0 2 37 2010-07-19 14:01:36 Geoff Dalgas 2013-11-12 22:07:23 http://stackoverflow.com Corvallis, OR &lt;p&gt;Developer on the StackOverflo... NULL 2 3 101 22 19 0 3 35 2010-07-19 15:34:50 Jarrod Dixon 2014-08-08 06:42:58 http://stackoverflow.com New York, NY &lt;p&gt;&lt;a href=\"http://blog.stackove... NULL 3 4 101 11 0 0 1998 28 2010-07-19 19:03:27 Emmett 2014-01-02 09:31:02 http://minesweeperonline.com San Francisco, CA &lt;p&gt;currently at a startup in SF&lt;... http://i.stack.imgur.com/d1oHX.j... 4 5 6792 1145 662 5 54503 35 2010-07-19 19:03:57 Shane 2014-08-13 00:23:47 http://www.statalgo.com New York, NY &lt;p&gt;Quantitative researcher focus... NULL ... ... ... ... ... ... ... ... ... ... ... ... ... ... 40320 55743 1 0 0 0 5026902 nan 2014-09-13 21:03:50 AussieMeg 2014-09-13 21:18:52 NULL NULL NULL http://graph.facebook.com/665821... 40321 55744 6 1 0 0 5026998 nan 2014-09-13 21:39:30 Mia Maria 2014-09-13 21:39:30 NULL NULL NULL NULL 40322 55745 101 0 0 0 481766 nan 2014-09-13 23:45:27 tronbabylove 2014-09-13 23:45:27 NULL United States NULL https://www.gravatar.com/avatar/... 40323 55746 106 1 0 0 976289 nan 2014-09-14 00:29:41 GPP 2014-09-14 02:05:17 NULL NULL &lt;p&gt;Stats noobie, product, market... https://www.gravatar.com/avatar/... 40324 55747 1 0 0 0 5027354 nan 2014-09-14 01:01:44 Shivam Agrawal 2014-09-14 01:19:04 NULL India &lt;p&gt;Maths Enthusiast &lt;/p&gt; https://lh4.googleusercontent.co... <p>     40325 rows x 14 columns     memory usage: 10.32 MB     name: users     type: getml.DataFrame </p> In\u00a0[8]: Copied! <pre>votes\n</pre> votes Out[8]:   name           Id       PostId   VoteTypeId       UserId BountyAmount CreationDate    role unused_float unused_float unused_float unused_float unused_float unused_string 0 1 3 2 nan nan 2010-07-19 1 2 2 2 nan nan 2010-07-19 2 3 5 2 nan nan 2010-07-19 3 4 5 2 nan nan 2010-07-19 4 5 3 2 nan nan 2010-07-19 ... ... ... ... ... ... 328059 386254 26088 2 nan nan 2014-09-14 328060 386255 26088 5 31466 nan 2014-09-14 328061 386256 115374 2 nan nan 2014-09-14 328062 386257 115368 2 nan nan 2014-09-14 328063 386258 115369 2 nan nan 2014-09-14 <p>     328064 rows x 6 columns     memory usage: 19.36 MB     name: votes     type: getml.DataFrame </p> <p>getML requires that we define roles for each of the columns.</p> In\u00a0[9]: Copied! <pre>badges.set_role([\"Id\", \"UserId\"], getml.data.roles.join_key)\nbadges.set_role(\"Date\", getml.data.roles.time_stamp)\nbadges.set_role(\"Name\", getml.data.roles.categorical)\n\nbadges\n</pre> badges.set_role([\"Id\", \"UserId\"], getml.data.roles.join_key) badges.set_role(\"Date\", getml.data.roles.time_stamp) badges.set_role(\"Name\", getml.data.roles.categorical)  badges Out[9]:  name                        Date       Id   UserId Name            role                  time_stamp join_key join_key categorical     unit time stamp, comparison only 0 2010-07-19 19:39:07 1 5 Teacher 1 2010-07-19 19:39:07 2 6 Teacher 2 2010-07-19 19:39:07 3 8 Teacher 3 2010-07-19 19:39:07 4 23 Teacher 4 2010-07-19 19:39:07 5 36 Teacher ... ... ... ... 79846 2014-09-13 23:25:21 92236 55744 Student 79847 2014-09-14 00:09:35 92237 1118 Nice Answer 79848 2014-09-14 01:18:29 92238 1118 Enlightened 79849 2014-09-14 01:41:18 92239 55746 Student 79850 2014-09-14 02:31:28 92240 12597 Autobiographer <p>     79851 rows x 4 columns     memory usage: 1.60 MB     name: badges     type: getml.DataFrame </p> In\u00a0[10]: Copied! <pre>posts.set_role([\"Id\", \"AcceptedAnswerId\", \"OwnerUserId\"], getml.data.roles.join_key)\nposts.set_role([\"Score\", \"ViewCount\", \"AnswerCount\", \"CommentCount\", \"FavoriteCount\", \"LastEditorUserId\"], getml.data.roles.numerical)\nposts.set_role([\"PostTypeId\"], getml.data.roles.categorical)\n\nposts.drop(posts.roles.unused)\n</pre> posts.set_role([\"Id\", \"AcceptedAnswerId\", \"OwnerUserId\"], getml.data.roles.join_key) posts.set_role([\"Score\", \"ViewCount\", \"AnswerCount\", \"CommentCount\", \"FavoriteCount\", \"LastEditorUserId\"], getml.data.roles.numerical) posts.set_role([\"PostTypeId\"], getml.data.roles.categorical)  posts.drop(posts.roles.unused) Out[10]: name       Id AcceptedAnswerId OwnerUserId PostTypeId      Score ViewCount AnswerCount CommentCount FavoriteCount LastEditorUserId role join_key         join_key    join_key categorical numerical numerical   numerical    numerical     numerical        numerical 0 1 15 8 1 23 1278 5 1 14 nan 1 2 59 24 1 22 8198 7 1 8 88 2 3 5 18 1 54 3613 19 4 36 183 3 4 135 23 1 13 5224 5 2 2 nan 4 5 nan 23 2 81 nan nan 3 nan 23 ... ... ... ... ... ... ... ... ... ... ... <p>     91976 rows          type: getml.data.View </p> In\u00a0[11]: Copied! <pre>users.set_role(\"Reputation\", getml.data.roles.target)\nusers.set_role([\"CreationDate\", \"LastAccessDate\"], getml.data.roles.time_stamp)\nusers.set_role([\"Id\"], getml.data.roles.join_key)\nusers.set_role([\"Views\", \"UpVotes\", \"DownVotes\"], getml.data.roles.numerical)\n\nusers.drop(users.roles.unused)\n</pre> users.set_role(\"Reputation\", getml.data.roles.target) users.set_role([\"CreationDate\", \"LastAccessDate\"], getml.data.roles.time_stamp) users.set_role([\"Id\"], getml.data.roles.join_key) users.set_role([\"Views\", \"UpVotes\", \"DownVotes\"], getml.data.roles.numerical)  users.drop(users.roles.unused) Out[11]: name                CreationDate              LastAccessDate       Id Reputation     Views   UpVotes DownVotes role                  time_stamp                  time_stamp join_key     target numerical numerical numerical unit time stamp, comparison only time stamp, comparison only 0 2010-07-19 06:55:26 2010-07-19 06:55:26 -1 1 0 5007 1920 1 2010-07-19 14:01:36 2013-11-12 22:07:23 2 101 25 3 0 2 2010-07-19 15:34:50 2014-08-08 06:42:58 3 101 22 19 0 3 2010-07-19 19:03:27 2014-01-02 09:31:02 4 101 11 0 0 4 2010-07-19 19:03:57 2014-08-13 00:23:47 5 6792 1145 662 5 ... ... ... ... ... ... ... ... <p>     40325 rows          type: getml.data.View </p> In\u00a0[12]: Copied! <pre>votes.set_role([\"CreationDate\"], getml.data.roles.time_stamp)\nvotes.set_role([\"Id\", \"PostId\"], getml.data.roles.join_key)\nvotes.set_role([\"VoteTypeId\"], getml.data.roles.categorical)\nvotes.set_role([\"BountyAmount\"], getml.data.roles.numerical)\n\nvotes.drop(users.roles.unused)\n</pre> votes.set_role([\"CreationDate\"], getml.data.roles.time_stamp) votes.set_role([\"Id\", \"PostId\"], getml.data.roles.join_key) votes.set_role([\"VoteTypeId\"], getml.data.roles.categorical) votes.set_role([\"BountyAmount\"], getml.data.roles.numerical)  votes.drop(users.roles.unused) Out[12]: name                CreationDate       Id   PostId VoteTypeId  BountyAmount       UserId role                  time_stamp join_key join_key categorical    numerical unused_float unit time stamp, comparison only 0 2010-07-19 1 3 2 nan nan 1 2010-07-19 2 2 2 nan nan 2 2010-07-19 3 5 2 nan nan 3 2010-07-19 4 5 2 nan nan 4 2010-07-19 5 3 2 nan nan ... ... ... ... ... ... ... <p>     328064 rows          type: getml.data.View </p> In\u00a0[13]: Copied! <pre>split = getml.data.split.random(train=0.8, test=0.2)\n</pre> split = getml.data.split.random(train=0.8, test=0.2) In\u00a0[14]: Copied! <pre>container = getml.data.Container(population=users, split=split)\n\ncontainer.add(badges=badges, posts=posts, votes=votes)\n\ncontainer\n</pre> container = getml.data.Container(population=users, split=split)  container.add(badges=badges, posts=posts, votes=votes)  container Out[14]: population subset name   rows type 0 test users 7992 View 1 train users 32333 View peripheral name     rows type      0 badges 79851 DataFrame 1 posts 91976 DataFrame 2 votes 328064 DataFrame In\u00a0[15]: Copied! <pre>dm = getml.data.DataModel(population=users.to_placeholder())\n\ndm.add(getml.data.to_placeholder(badges=badges, posts=[posts]*2, votes=votes))\n\ndm.population.join(\n    dm.badges,\n    on=(\"Id\", \"UserId\"),\n)\n\ndm.population.join(\n    dm.posts[0],\n    on=(\"Id\", \"OwnerUserId\"),\n)\n\ndm.posts[0].join(\n    dm.posts[1],\n    on=(\"Id\", \"AcceptedAnswerId\"),\n    relationship=getml.data.relationship.one_to_one,\n)\n\ndm.posts[0].join(\n    dm.votes,\n    on=(\"Id\", \"PostId\"),\n)\n\ndm\n</pre> dm = getml.data.DataModel(population=users.to_placeholder())  dm.add(getml.data.to_placeholder(badges=badges, posts=[posts]*2, votes=votes))  dm.population.join(     dm.badges,     on=(\"Id\", \"UserId\"), )  dm.population.join(     dm.posts[0],     on=(\"Id\", \"OwnerUserId\"), )  dm.posts[0].join(     dm.posts[1],     on=(\"Id\", \"AcceptedAnswerId\"),     relationship=getml.data.relationship.one_to_one, )  dm.posts[0].join(     dm.votes,     on=(\"Id\", \"PostId\"), )  dm Out[15]: diagram badgespostsvotespostsusersAcceptedAnswerId = IdRelationship: one-to-onePostId = IdUserId = IdOwnerUserId = Id staging data frames  staging table           0 users USERS__STAGING_TABLE_1 1 badges BADGES__STAGING_TABLE_2 2 posts, posts POSTS__STAGING_TABLE_3 3 votes VOTES__STAGING_TABLE_4 <p>Set-up the feature learner &amp; predictor</p> <p>We use the relboost algorithms for this problem. Because of the large number of keywords, we regularize the model a bit by requiring a minimum support for the keywords (<code>min_num_samples</code>).</p> In\u00a0[16]: Copied! <pre>mapping = getml.preprocessors.Mapping()\n\nfast_prop = getml.feature_learning.FastProp(\n    aggregation=[\"AVG\", \"SUM\"],\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    num_threads=1,\n    n_most_frequent=5,\n    \n)\n\nrelboost = getml.feature_learning.Relboost(\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    num_threads=1,\n    num_subfeatures=10,\n)\n\npredictor = getml.predictors.XGBoostRegressor(n_jobs=1)\n</pre> mapping = getml.preprocessors.Mapping()  fast_prop = getml.feature_learning.FastProp(     aggregation=[\"AVG\", \"SUM\"],     loss_function=getml.feature_learning.loss_functions.SquareLoss,     num_threads=1,     n_most_frequent=5,      )  relboost = getml.feature_learning.Relboost(     loss_function=getml.feature_learning.loss_functions.SquareLoss,     num_threads=1,     num_subfeatures=10, )  predictor = getml.predictors.XGBoostRegressor(n_jobs=1) <p>Build the pipeline</p> In\u00a0[17]: Copied! <pre>pipe1 = getml.Pipeline(\n    tags=['fast_prop'],\n    data_model=dm,\n    feature_learners=[fast_prop],\n    predictors=[predictor],\n    include_categorical=True,\n)\n\npipe1\n</pre> pipe1 = getml.Pipeline(     tags=['fast_prop'],     data_model=dm,     feature_learners=[fast_prop],     predictors=[predictor],     include_categorical=True, )  pipe1 Out[17]: <pre>Pipeline(data_model='users',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=['badges', 'posts', 'votes'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['fast_prop'])</pre> In\u00a0[18]: Copied! <pre>pipe2 = getml.Pipeline(\n    tags=['relboost'],\n    data_model=dm,\n    preprocessors=[mapping],\n    feature_learners=[relboost],\n    predictors=[predictor],    \n    include_categorical=True,\n)\n\npipe2\n</pre> pipe2 = getml.Pipeline(     tags=['relboost'],     data_model=dm,     preprocessors=[mapping],     feature_learners=[relboost],     predictors=[predictor],         include_categorical=True, )  pipe2 Out[18]: <pre>Pipeline(data_model='users',\n         feature_learners=['Relboost'],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=['badges', 'posts', 'votes'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['relboost'])</pre> In\u00a0[19]: Copied! <pre>pipe1.check(container.train)\n</pre> pipe1.check(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 3 issues labeled INFO and 3 issues labeled WARNING.\n</pre> Out[19]: type    label                   message                          0 WARNING COLUMN SHOULD BE UNUSED All non-NULL entries in column 't3__posttypeid' in POSTS__STAGING_TABLE_3 are equal to each other. You should consider setting its role to unused_string or using it for comparison only (you can do the latter by setting a unit that contains 'comparison only'). 1 WARNING COLUMN SHOULD BE UNUSED 93.147125% of all entries in column 't3__favoritecount' in POSTS__STAGING_TABLE_3 are NULL values. You should consider setting its role to unused_float. 2 WARNING COLUMN SHOULD BE UNUSED 99.468396% of all entries in column 'bountyamount' in VOTES__STAGING_TABLE_4 are NULL values. You should consider setting its role to unused_float. 3 INFO FOREIGN KEYS NOT FOUND When joining USERS__STAGING_TABLE_1 and BADGES__STAGING_TABLE_2 over 'Id' and 'UserId', there are no corresponding entries for 37.794204% of entries in 'Id' in 'USERS__STAGING_TABLE_1'. You might want to double-check your join keys. 4 INFO FOREIGN KEYS NOT FOUND When joining USERS__STAGING_TABLE_1 and POSTS__STAGING_TABLE_3 over 'Id' and 'OwnerUserId', there are no corresponding entries for 45.600470% of entries in 'Id' in 'USERS__STAGING_TABLE_1'. You might want to double-check your join keys. 5 INFO FOREIGN KEYS NOT FOUND When joining POSTS__STAGING_TABLE_3 and VOTES__STAGING_TABLE_4 over 'Id' and 'PostId', there are no corresponding entries for 16.464078% of entries in 'Id' in 'POSTS__STAGING_TABLE_3'. You might want to double-check your join keys. In\u00a0[20]: Copied! <pre>pipe1.fit(container.train)\n</pre> pipe1.fit(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 3 issues labeled INFO and 3 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 522 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:06, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:10, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:18.11942\n\n</pre> Out[20]: <pre>Pipeline(data_model='users',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=['badges', 'posts', 'votes'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['fast_prop', 'container-YZzVTV'])</pre> In\u00a0[21]: Copied! <pre>pipe2.check(container.train)\n</pre> pipe2.check(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 3 issues labeled INFO and 3 issues labeled WARNING.\n</pre> Out[21]: type    label                   message                          0 WARNING COLUMN SHOULD BE UNUSED All non-NULL entries in column 't3__posttypeid' in POSTS__STAGING_TABLE_3 are equal to each other. You should consider setting its role to unused_string or using it for comparison only (you can do the latter by setting a unit that contains 'comparison only'). 1 WARNING COLUMN SHOULD BE UNUSED 93.147125% of all entries in column 't3__favoritecount' in POSTS__STAGING_TABLE_3 are NULL values. You should consider setting its role to unused_float. 2 WARNING COLUMN SHOULD BE UNUSED 99.468396% of all entries in column 'bountyamount' in VOTES__STAGING_TABLE_4 are NULL values. You should consider setting its role to unused_float. 3 INFO FOREIGN KEYS NOT FOUND When joining USERS__STAGING_TABLE_1 and BADGES__STAGING_TABLE_2 over 'Id' and 'UserId', there are no corresponding entries for 37.794204% of entries in 'Id' in 'USERS__STAGING_TABLE_1'. You might want to double-check your join keys. 4 INFO FOREIGN KEYS NOT FOUND When joining USERS__STAGING_TABLE_1 and POSTS__STAGING_TABLE_3 over 'Id' and 'OwnerUserId', there are no corresponding entries for 45.600470% of entries in 'Id' in 'USERS__STAGING_TABLE_1'. You might want to double-check your join keys. 5 INFO FOREIGN KEYS NOT FOUND When joining POSTS__STAGING_TABLE_3 and VOTES__STAGING_TABLE_4 over 'Id' and 'PostId', there are no corresponding entries for 16.464078% of entries in 'Id' in 'POSTS__STAGING_TABLE_3'. You might want to double-check your join keys. In\u00a0[22]: Copied! <pre>pipe2.fit(container.train)\n</pre> pipe2.fit(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 3 issues labeled INFO and 3 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:26, remaining: 00:00]          \nRelboost: Training subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:28, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Training features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 01:42, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:04, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:04, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:2m:46.974973\n\n</pre> Out[22]: <pre>Pipeline(data_model='users',\n         feature_learners=['Relboost'],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=['badges', 'posts', 'votes'],\n         predictors=['XGBoostRegressor'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['relboost', 'container-YZzVTV'])</pre> In\u00a0[23]: Copied! <pre>fastprop_score = pipe1.score(container.test)\nfastprop_score\n</pre> fastprop_score = pipe1.score(container.test) fastprop_score <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[23]: date time           set used target          mae     rmse rsquared 0 2024-02-21 15:11:18 train Reputation 31.8897 43.6771 0.9974 1 2024-02-21 15:14:07 test Reputation 33.6064 65.3332 0.9777 In\u00a0[24]: Copied! <pre>relboost_score = pipe2.score(container.test)\nrelboost_score\n</pre> relboost_score = pipe2.score(container.test) relboost_score <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nRelboost: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\n</pre> Out[24]: date time           set used target          mae     rmse rsquared 0 2024-02-21 15:14:06 train Reputation 29.318 42.6186 0.9975 1 2024-02-21 15:14:08 test Reputation 31.1388 60.7643 0.9809 In\u00a0[25]: Copied! <pre>population_train_pd = container.train.population.to_pandas()\npopulation_test_pd = container.test.population.to_pandas()\n</pre> population_train_pd = container.train.population.to_pandas() population_test_pd = container.test.population.to_pandas() In\u00a0[26]: Copied! <pre>badges_pd = badges.drop(badges.roles.unused).to_pandas()\nposts_pd = posts.drop(posts.roles.unused).to_pandas()\nvotes_pd = votes.drop(votes.roles.unused).to_pandas()\n</pre> badges_pd = badges.drop(badges.roles.unused).to_pandas() posts_pd = posts.drop(posts.roles.unused).to_pandas() votes_pd = votes.drop(votes.roles.unused).to_pandas() In\u00a0[27]: Copied! <pre>population_pd_logical_types = {\n    \"Id\": ww.logical_types.IntegerNullable,\n    \"Views\": ww.logical_types.Integer,\n    \"UpVotes\": ww.logical_types.Integer,\n    \"DownVotes\": ww.logical_types.Integer,\n    \"Reputation\": ww.logical_types.Integer,\n    \"CreationDate\": ww.logical_types.Datetime,\n    \"LastAccessDate\": ww.logical_types.Datetime\n}\npopulation_train_pd.ww.init(logical_types=population_pd_logical_types, index=\"Id\", name=\"population\")\npopulation_test_pd.ww.init(logical_types=population_pd_logical_types, index=\"Id\", name=\"population\")\n\nbadges_pd_logical_types = {\n    \"Name\": ww.logical_types.Categorical,\n    \"Id\": ww.logical_types.Integer,\n    \"UserId\": ww.logical_types.IntegerNullable,\n    \"Date\": ww.logical_types.Datetime\n}\nbadges_pd.ww.init(logical_types=badges_pd_logical_types, index=\"Id\", name=\"badges\")\n\nposts_pd_logical_types = {\n    \"PostTypeId\": ww.logical_types.Integer,\n    \"Id\": ww.logical_types.Integer,\n    \"AcceptedAnswerId\": ww.logical_types.IntegerNullable,\n    \"OwnerUserId\": ww.logical_types.IntegerNullable,\n    \"Score\": ww.logical_types.Integer,\n    \"ViewCount\": ww.logical_types.IntegerNullable,\n    \"AnswerCount\": ww.logical_types.IntegerNullable,\n    \"CommentCount\": ww.logical_types.Integer,\n    \"FavoriteCount\": ww.logical_types.IntegerNullable,\n    \"LastEditorUserId\": ww.logical_types.IntegerNullable\n}\nposts_pd.ww.init(logical_types=posts_pd_logical_types, index=\"Id\", name=\"posts\")\n\nvotes_pd_logical_types = {\n    \"VoteTypeId\": ww.logical_types.Categorical,\n    \"Id\": ww.logical_types.Integer,\n    \"PostId\": ww.logical_types.Integer,\n    \"BountyAmount\": ww.logical_types.IntegerNullable,\n    \"CreationDate\": ww.logical_types.Datetime\n}\nvotes_pd.ww.init(logical_types=votes_pd_logical_types, index=\"Id\", name=\"votes\")\n</pre> population_pd_logical_types = {     \"Id\": ww.logical_types.IntegerNullable,     \"Views\": ww.logical_types.Integer,     \"UpVotes\": ww.logical_types.Integer,     \"DownVotes\": ww.logical_types.Integer,     \"Reputation\": ww.logical_types.Integer,     \"CreationDate\": ww.logical_types.Datetime,     \"LastAccessDate\": ww.logical_types.Datetime } population_train_pd.ww.init(logical_types=population_pd_logical_types, index=\"Id\", name=\"population\") population_test_pd.ww.init(logical_types=population_pd_logical_types, index=\"Id\", name=\"population\")  badges_pd_logical_types = {     \"Name\": ww.logical_types.Categorical,     \"Id\": ww.logical_types.Integer,     \"UserId\": ww.logical_types.IntegerNullable,     \"Date\": ww.logical_types.Datetime } badges_pd.ww.init(logical_types=badges_pd_logical_types, index=\"Id\", name=\"badges\")  posts_pd_logical_types = {     \"PostTypeId\": ww.logical_types.Integer,     \"Id\": ww.logical_types.Integer,     \"AcceptedAnswerId\": ww.logical_types.IntegerNullable,     \"OwnerUserId\": ww.logical_types.IntegerNullable,     \"Score\": ww.logical_types.Integer,     \"ViewCount\": ww.logical_types.IntegerNullable,     \"AnswerCount\": ww.logical_types.IntegerNullable,     \"CommentCount\": ww.logical_types.Integer,     \"FavoriteCount\": ww.logical_types.IntegerNullable,     \"LastEditorUserId\": ww.logical_types.IntegerNullable } posts_pd.ww.init(logical_types=posts_pd_logical_types, index=\"Id\", name=\"posts\")  votes_pd_logical_types = {     \"VoteTypeId\": ww.logical_types.Categorical,     \"Id\": ww.logical_types.Integer,     \"PostId\": ww.logical_types.Integer,     \"BountyAmount\": ww.logical_types.IntegerNullable,     \"CreationDate\": ww.logical_types.Datetime } votes_pd.ww.init(logical_types=votes_pd_logical_types, index=\"Id\", name=\"votes\") In\u00a0[28]: Copied! <pre>dataframes_train = {\n    \"population\" : (population_train_pd, ),\n    \"badges\": (badges_pd, ),\n    \"posts\": (posts_pd, ),\n    \"votes\": (votes_pd, ),\n}\n</pre> dataframes_train = {     \"population\" : (population_train_pd, ),     \"badges\": (badges_pd, ),     \"posts\": (posts_pd, ),     \"votes\": (votes_pd, ), } In\u00a0[29]: Copied! <pre>dataframes_test = {\n    \"population\" : (population_test_pd, ),\n    \"badges\": (badges_pd, ),\n    \"posts\": (posts_pd, ),\n    \"votes\": (votes_pd, ),\n}\n</pre> dataframes_test = {     \"population\" : (population_test_pd, ),     \"badges\": (badges_pd, ),     \"posts\": (posts_pd, ),     \"votes\": (votes_pd, ), } In\u00a0[30]: Copied! <pre>relationships = [\n    (\"population\", \"Id\", \"badges\", \"UserId\"),\n    (\"population\", \"Id\", \"posts\", \"OwnerUserId\"),\n    (\"posts\", \"Id\", \"votes\", \"PostId\"),\n]\n</pre> relationships = [     (\"population\", \"Id\", \"badges\", \"UserId\"),     (\"population\", \"Id\", \"posts\", \"OwnerUserId\"),     (\"posts\", \"Id\", \"votes\", \"PostId\"), ] In\u00a0[31]: Copied! <pre>featuretools_train_pd = featuretools.dfs(\n    dataframes=dataframes_train,\n    relationships=relationships,\n    target_dataframe_name=\"population\")[0]\n</pre> featuretools_train_pd = featuretools.dfs(     dataframes=dataframes_train,     relationships=relationships,     target_dataframe_name=\"population\")[0] In\u00a0[32]: Copied! <pre>featuretools_test_pd = featuretools.dfs(\n    dataframes=dataframes_test,\n    relationships=relationships,\n    target_dataframe_name=\"population\")[0]\n</pre> featuretools_test_pd = featuretools.dfs(     dataframes=dataframes_test,     relationships=relationships,     target_dataframe_name=\"population\")[0] In\u00a0[33]: Copied! <pre>featuretools_train = getml.data.DataFrame.from_pandas(featuretools_train_pd, \"featuretools_train\")\nfeaturetools_test = getml.data.DataFrame.from_pandas(featuretools_test_pd, \"featuretools_test\")\n</pre> featuretools_train = getml.data.DataFrame.from_pandas(featuretools_train_pd, \"featuretools_train\") featuretools_test = getml.data.DataFrame.from_pandas(featuretools_test_pd, \"featuretools_test\") In\u00a0[34]: Copied! <pre>featuretools_train.set_role(\"Reputation\", getml.data.roles.target)\nfeaturetools_train.set_role(featuretools_train.roles.unused_float, getml.data.roles.numerical)\nfeaturetools_train.set_role(featuretools_train.roles.unused_string, getml.data.roles.categorical)\nfeaturetools_train.set_role([\"SKEW(posts.STD(votes.BountyAmount))\", \"STD(posts.SKEW(votes.BountyAmount))\"], getml.data.roles.unused_float)\n\nfeaturetools_train\n</pre> featuretools_train.set_role(\"Reputation\", getml.data.roles.target) featuretools_train.set_role(featuretools_train.roles.unused_float, getml.data.roles.numerical) featuretools_train.set_role(featuretools_train.roles.unused_string, getml.data.roles.categorical) featuretools_train.set_role([\"SKEW(posts.STD(votes.BountyAmount))\", \"STD(posts.SKEW(votes.BountyAmount))\"], getml.data.roles.unused_float)  featuretools_train Out[34]:  name Reputation COUNT(badges) MODE(badges.Name) NUM_UNIQUE(badges.Name) COUNT(posts) COUNT(votes) MODE(votes.VoteTypeId) NUM_UNIQUE(votes.VoteTypeId) DAY(CreationDate) DAY(LastAccessDate) MONTH(CreationDate) MONTH(LastAccessDate) WEEKDAY(CreationDate) WEEKDAY(LastAccessDate) YEAR(CreationDate) YEAR(LastAccessDate) MODE(badges.DAY(Date)) MODE(badges.MONTH(Date)) MODE(badges.WEEKDAY(Date)) MODE(badges.YEAR(Date)) NUM_UNIQUE(badges.DAY(Date)) NUM_UNIQUE(badges.MONTH(Date)) NUM_UNIQUE(badges.WEEKDAY(Date)) NUM_UNIQUE(badges.YEAR(Date)) MODE(posts.MODE(votes.VoteTypeId)) NUM_UNIQUE(posts.MODE(votes.VoteTypeId))     Views   UpVotes DownVotes MAX(posts.AcceptedAnswerId) MAX(posts.AnswerCount) MAX(posts.CommentCount) MAX(posts.FavoriteCount) MAX(posts.LastEditorUserId) MAX(posts.PostTypeId) MAX(posts.Score) MAX(posts.ViewCount) MEAN(posts.AcceptedAnswerId) MEAN(posts.AnswerCount) MEAN(posts.CommentCount) MEAN(posts.FavoriteCount) MEAN(posts.LastEditorUserId) MEAN(posts.PostTypeId) MEAN(posts.Score) MEAN(posts.ViewCount) MIN(posts.AcceptedAnswerId) MIN(posts.AnswerCount) MIN(posts.CommentCount) MIN(posts.FavoriteCount) MIN(posts.LastEditorUserId) MIN(posts.PostTypeId) MIN(posts.Score) MIN(posts.ViewCount) SKEW(posts.AcceptedAnswerId) SKEW(posts.AnswerCount) SKEW(posts.CommentCount) SKEW(posts.FavoriteCount) SKEW(posts.LastEditorUserId) SKEW(posts.PostTypeId) SKEW(posts.Score) SKEW(posts.ViewCount) STD(posts.AcceptedAnswerId) STD(posts.AnswerCount) STD(posts.CommentCount) STD(posts.FavoriteCount) STD(posts.LastEditorUserId) STD(posts.PostTypeId) STD(posts.Score) STD(posts.ViewCount) SUM(posts.AcceptedAnswerId) SUM(posts.AnswerCount) SUM(posts.CommentCount) SUM(posts.FavoriteCount) SUM(posts.LastEditorUserId) SUM(posts.PostTypeId) SUM(posts.Score) SUM(posts.ViewCount) MAX(votes.BountyAmount) MEAN(votes.BountyAmount) MIN(votes.BountyAmount) SKEW(votes.BountyAmount) STD(votes.BountyAmount) SUM(votes.BountyAmount) MAX(posts.COUNT(votes)) MAX(posts.MEAN(votes.BountyAmount)) MAX(posts.MIN(votes.BountyAmount)) MAX(posts.NUM_UNIQUE(votes.VoteTypeId)) MAX(posts.SKEW(votes.BountyAmount)) MAX(posts.STD(votes.BountyAmount)) MAX(posts.SUM(votes.BountyAmount)) MEAN(posts.COUNT(votes)) MEAN(posts.MAX(votes.BountyAmount)) MEAN(posts.MEAN(votes.BountyAmount)) MEAN(posts.MIN(votes.BountyAmount)) MEAN(posts.NUM_UNIQUE(votes.VoteTypeId)) MEAN(posts.SKEW(votes.BountyAmount)) MEAN(posts.STD(votes.BountyAmount)) MEAN(posts.SUM(votes.BountyAmount)) MIN(posts.COUNT(votes)) MIN(posts.MAX(votes.BountyAmount)) MIN(posts.MEAN(votes.BountyAmount)) MIN(posts.NUM_UNIQUE(votes.VoteTypeId)) MIN(posts.SKEW(votes.BountyAmount)) MIN(posts.STD(votes.BountyAmount)) MIN(posts.SUM(votes.BountyAmount)) SKEW(posts.COUNT(votes)) SKEW(posts.MAX(votes.BountyAmount)) SKEW(posts.MEAN(votes.BountyAmount)) SKEW(posts.MIN(votes.BountyAmount)) SKEW(posts.NUM_UNIQUE(votes.VoteTypeId)) SKEW(posts.SUM(votes.BountyAmount)) STD(posts.COUNT(votes)) STD(posts.MAX(votes.BountyAmount)) STD(posts.MEAN(votes.BountyAmount)) STD(posts.MIN(votes.BountyAmount)) STD(posts.NUM_UNIQUE(votes.VoteTypeId)) STD(posts.SUM(votes.BountyAmount)) SUM(posts.MAX(votes.BountyAmount)) SUM(posts.MEAN(votes.BountyAmount)) SUM(posts.MIN(votes.BountyAmount)) SUM(posts.NUM_UNIQUE(votes.VoteTypeId)) SUM(posts.SKEW(votes.BountyAmount)) SUM(posts.STD(votes.BountyAmount)) MAX(votes.posts.AcceptedAnswerId) MAX(votes.posts.AnswerCount) MAX(votes.posts.CommentCount) MAX(votes.posts.FavoriteCount) MAX(votes.posts.LastEditorUserId) MAX(votes.posts.PostTypeId) MAX(votes.posts.Score) MAX(votes.posts.ViewCount) MEAN(votes.posts.AcceptedAnswerId) MEAN(votes.posts.AnswerCount) MEAN(votes.posts.CommentCount) MEAN(votes.posts.FavoriteCount) MEAN(votes.posts.LastEditorUserId) MEAN(votes.posts.PostTypeId) MEAN(votes.posts.Score) MEAN(votes.posts.ViewCount) MIN(votes.posts.AcceptedAnswerId) MIN(votes.posts.AnswerCount) MIN(votes.posts.CommentCount) MIN(votes.posts.FavoriteCount) MIN(votes.posts.LastEditorUserId) MIN(votes.posts.PostTypeId) MIN(votes.posts.Score) MIN(votes.posts.ViewCount) SKEW(votes.posts.AcceptedAnswerId) SKEW(votes.posts.AnswerCount) SKEW(votes.posts.CommentCount) SKEW(votes.posts.FavoriteCount) SKEW(votes.posts.LastEditorUserId) SKEW(votes.posts.PostTypeId) SKEW(votes.posts.Score) SKEW(votes.posts.ViewCount) STD(votes.posts.AcceptedAnswerId) STD(votes.posts.AnswerCount) STD(votes.posts.CommentCount) STD(votes.posts.FavoriteCount) STD(votes.posts.LastEditorUserId) STD(votes.posts.PostTypeId) STD(votes.posts.Score) STD(votes.posts.ViewCount) SUM(votes.posts.AcceptedAnswerId) SUM(votes.posts.AnswerCount) SUM(votes.posts.CommentCount) SUM(votes.posts.FavoriteCount) SUM(votes.posts.LastEditorUserId) SUM(votes.posts.PostTypeId) SUM(votes.posts.Score) SUM(votes.posts.ViewCount) SKEW(posts.STD(votes.BountyAmount)) STD(posts.SKEW(votes.BountyAmount))  role     target categorical   categorical       categorical             categorical  categorical  categorical            categorical                  categorical       categorical         categorical         categorical           categorical           categorical             categorical        categorical          categorical            categorical              categorical                categorical             categorical                  categorical                    categorical                      categorical                   categorical                      categorical                      numerical numerical numerical                   numerical              numerical               numerical                numerical                   numerical             numerical        numerical            numerical                    numerical               numerical                numerical                 numerical                    numerical              numerical         numerical             numerical                   numerical              numerical               numerical                numerical                   numerical             numerical        numerical            numerical                    numerical               numerical                numerical                 numerical                    numerical              numerical         numerical             numerical                   numerical              numerical               numerical                numerical                   numerical             numerical        numerical            numerical                   numerical              numerical               numerical                numerical                   numerical             numerical        numerical            numerical               numerical                numerical               numerical                numerical               numerical               numerical               numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical               numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                numerical                        numerical                        numerical                        numerical                        numerical                        numerical               numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                    numerical                     numerical                      numerical                        numerical                   numerical              numerical                  numerical                        numerical                     numerical                      numerical                       numerical                        numerical                    numerical               numerical                   numerical                        numerical                    numerical                     numerical                      numerical                        numerical                   numerical              numerical                  numerical                        numerical                     numerical                      numerical                       numerical                        numerical                    numerical               numerical                   numerical                        numerical                    numerical                     numerical                      numerical                        numerical                   numerical              numerical                  numerical                        numerical                    numerical                     numerical                      numerical                        numerical                   numerical              numerical                  numerical                     unused_float                     unused_float 0 1 0 NULL NULL 211 10 16 1 19 19 7 7 0 0 2010 2010 NULL NULL NULL NULL NULL NULL NULL NULL 16 1 0 5007 1920 nan nan 0 nan 8588 7 0 nan nan nan 0 nan 306.1634 4.9005 0 nan nan nan 0 nan -1 3 0 nan nan nan 0 nan 4.9461 -0.2648 0 nan nan nan 0 nan 1339.1554 0.5386 0 nan 0 0 0 0 61845 1034 0 0 nan nan nan nan nan 0 1 nan nan 1 nan nan 0 0.04739 nan nan nan 1 nan nan 0 0 nan nan 1 nan nan 0 4.2908 nan nan nan 0 0 0.213 nan nan nan 0 0 0 0 0 10 0 0 nan nan 0 nan 2116 5 0 nan nan nan 0 nan 2116 4.4 0 nan nan nan 0 nan 2116 3 0 nan nan nan 0 nan nan -0.7801 0 nan nan nan 0 nan nan 0.6992 0 nan 0 0 0 0 2116 44 0 0 nan nan 1 101 3 Autobiographer 3 0 0 NULL NULL 19 12 7 11 0 1 2010 2013 11 7 5 2010 3 3 2 1 NULL NULL 25 3 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 nan nan nan nan nan 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 nan nan 2 101 3 Autobiographer 3 0 0 NULL NULL 19 8 7 8 0 4 2010 2014 2 4 0 2010 3 3 3 2 NULL NULL 22 19 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 nan nan nan nan nan 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 nan nan 3 6792 131 Nice Answer 45 117 2016 2 7 19 13 7 8 0 2 2010 2014 19 7 3 2010 30 12 7 5 2 3 1145 662 5 2251 56 7 137 22047 2 156 64481 1793.6667 10.7059 1.5812 29.8125 1004.902 1.8547 12.5043 8898.2941 566 1 0 2 -1 1 1 286 -1.5227 2.6791 1.2464 1.8953 4.868 -2.0393 4.9894 2.5979 616.7514 13.4618 1.7628 45.5869 3514.2488 0.3539 22.4828 17078.287 16143 182 185 477 51250 217 1463 151271 50 50 50 nan nan 50 303 50 50 4 nan nan 50 17.2308 50 50 50 1.5299 nan nan 0.4274 1 50 50 1 nan nan 0 5.7832 nan nan nan 0.9689 10.8167 41.2949 nan nan nan 0.6238 4.6225 50 50 50 179 0 0 2251 56 7 137 22047 2 156 64481 1862.9954 25.5357 3.0511 90.5613 5358.5862 1.4648 64.5496 30912.241 566 1 0 2 -1 1 1 286 -1.6226 0.6212 0.3534 -0.561 1.3063 0.1413 0.5721 0.2586 554.8324 20.2126 2.4432 54.3408 8826.2062 0.4989 62.094 23812.9928 402407 27553 6151 97444 7276960 2953 130132 33354308 nan nan 4 457 19 Announcer 16 12 94 2 3 19 7 7 8 0 3 2010 2014 19 7 2 2010 11 8 5 5 2 2 114 47 0 39226 2 5 4 930 2 40 981 22179 1 1.8333 2.3333 187.3333 1.5 7.0833 507.6667 3832 0 0 1 6 1 0 122 -0.267 0 0.613 0.9352 2.3842 0 2.814 0.2603 14488.6332 0.6325 1.6422 1.5275 366.0419 0.5222 11.0327 399.5571 88716 6 22 7 1124 18 85 3046 nan nan nan nan nan 0 40 nan nan 2 nan nan 0 7.8333 nan nan nan 1.3333 nan nan 0 1 nan nan 1 nan nan 0 2.6072 nan nan nan 0.8124 0 11.044 nan nan nan 0.4924 0 0 0 0 16 0 0 39226 2 5 4 930 2 40 981 23376.875 1.1 1.4681 2.9545 96 1.6809 21.2553 706.7 3832 0 0 1 6 1 0 122 -0.2995 0.8834 0.9674 -0.5825 3.1415 -0.7886 0.1943 -0.8643 13635.7689 0.4026 1.3496 1.3266 245.0523 0.4686 16.5282 334.6668 374030 33 138 65 6240 158 1998 21201 nan nan ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 32328 1 0 NULL NULL 0 0 NULL NULL 13 13 9 9 5 5 2014 2014 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 nan nan nan nan nan 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 nan nan 32329 101 1 Supporter 1 0 0 NULL NULL 13 13 9 9 5 5 2014 2014 13 9 5 2014 1 1 1 1 NULL NULL 0 1 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 nan nan nan nan nan 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 nan nan 32330 6 1 Student 1 1 2 2 2 13 13 9 9 5 5 2014 2014 13 9 5 2014 1 1 1 1 2 1 0 0 0 nan 1 0 1 nan 1 1 17 nan 1 0 1 nan 1 1 17 nan 1 0 1 nan 1 1 17 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 1 0 1 0 1 1 17 nan nan nan nan nan 0 2 nan nan 2 nan nan 0 2 nan nan nan 2 nan nan 0 2 nan nan 2 nan nan 0 nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 2 0 0 nan 1 0 1 nan 1 1 17 nan 1 0 1 nan 1 1 17 nan 1 0 1 nan 1 1 17 nan nan nan nan nan nan nan nan nan 0 0 0 nan 0 0 0 0 2 0 2 0 2 2 34 nan nan 32331 101 0 NULL NULL 0 0 NULL NULL 13 13 9 9 5 5 2014 2014 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 nan nan nan nan nan 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 nan nan 32332 1 0 NULL NULL 0 0 NULL NULL 14 14 9 9 6 6 2014 2014 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 nan nan nan nan nan 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 nan nan <p>     32333 rows x 173 columns     memory usage: 41.52 MB     name: featuretools_train     type: getml.DataFrame </p> In\u00a0[35]: Copied! <pre>featuretools_test.set_role(\"Reputation\", getml.data.roles.target)\nfeaturetools_test.set_role(featuretools_test.roles.unused_float, getml.data.roles.numerical)\nfeaturetools_test.set_role(featuretools_test.roles.unused_string, getml.data.roles.categorical)\nfeaturetools_test.set_role([\"SKEW(posts.STD(votes.BountyAmount))\", \"STD(posts.SKEW(votes.BountyAmount))\"], getml.data.roles.unused_float)\n\nfeaturetools_test\n</pre> featuretools_test.set_role(\"Reputation\", getml.data.roles.target) featuretools_test.set_role(featuretools_test.roles.unused_float, getml.data.roles.numerical) featuretools_test.set_role(featuretools_test.roles.unused_string, getml.data.roles.categorical) featuretools_test.set_role([\"SKEW(posts.STD(votes.BountyAmount))\", \"STD(posts.SKEW(votes.BountyAmount))\"], getml.data.roles.unused_float)  featuretools_test Out[35]: name Reputation COUNT(badges) MODE(badges.Name) NUM_UNIQUE(badges.Name) COUNT(posts) COUNT(votes) MODE(votes.VoteTypeId) NUM_UNIQUE(votes.VoteTypeId) DAY(CreationDate) DAY(LastAccessDate) MONTH(CreationDate) MONTH(LastAccessDate) WEEKDAY(CreationDate) WEEKDAY(LastAccessDate) YEAR(CreationDate) YEAR(LastAccessDate) MODE(badges.DAY(Date)) MODE(badges.MONTH(Date)) MODE(badges.WEEKDAY(Date)) MODE(badges.YEAR(Date)) NUM_UNIQUE(badges.DAY(Date)) NUM_UNIQUE(badges.MONTH(Date)) NUM_UNIQUE(badges.WEEKDAY(Date)) NUM_UNIQUE(badges.YEAR(Date)) MODE(posts.MODE(votes.VoteTypeId)) NUM_UNIQUE(posts.MODE(votes.VoteTypeId))     Views   UpVotes DownVotes MAX(posts.AcceptedAnswerId) MAX(posts.AnswerCount) MAX(posts.CommentCount) MAX(posts.FavoriteCount) MAX(posts.LastEditorUserId) MAX(posts.PostTypeId) MAX(posts.Score) MAX(posts.ViewCount) MEAN(posts.AcceptedAnswerId) MEAN(posts.AnswerCount) MEAN(posts.CommentCount) MEAN(posts.FavoriteCount) MEAN(posts.LastEditorUserId) MEAN(posts.PostTypeId) MEAN(posts.Score) MEAN(posts.ViewCount) MIN(posts.AcceptedAnswerId) MIN(posts.AnswerCount) MIN(posts.CommentCount) MIN(posts.FavoriteCount) MIN(posts.LastEditorUserId) MIN(posts.PostTypeId) MIN(posts.Score) MIN(posts.ViewCount) SKEW(posts.AcceptedAnswerId) SKEW(posts.AnswerCount) SKEW(posts.CommentCount) SKEW(posts.FavoriteCount) SKEW(posts.LastEditorUserId) SKEW(posts.PostTypeId) SKEW(posts.Score) SKEW(posts.ViewCount) STD(posts.AcceptedAnswerId) STD(posts.AnswerCount) STD(posts.CommentCount) STD(posts.FavoriteCount) STD(posts.LastEditorUserId) STD(posts.PostTypeId) STD(posts.Score) STD(posts.ViewCount) SUM(posts.AcceptedAnswerId) SUM(posts.AnswerCount) SUM(posts.CommentCount) SUM(posts.FavoriteCount) SUM(posts.LastEditorUserId) SUM(posts.PostTypeId) SUM(posts.Score) SUM(posts.ViewCount) MAX(votes.BountyAmount) MEAN(votes.BountyAmount) MIN(votes.BountyAmount) SKEW(votes.BountyAmount) STD(votes.BountyAmount) SUM(votes.BountyAmount) MAX(posts.COUNT(votes)) MAX(posts.MEAN(votes.BountyAmount)) MAX(posts.MIN(votes.BountyAmount)) MAX(posts.NUM_UNIQUE(votes.VoteTypeId)) MAX(posts.SKEW(votes.BountyAmount)) MAX(posts.STD(votes.BountyAmount)) MAX(posts.SUM(votes.BountyAmount)) MEAN(posts.COUNT(votes)) MEAN(posts.MAX(votes.BountyAmount)) MEAN(posts.MEAN(votes.BountyAmount)) MEAN(posts.MIN(votes.BountyAmount)) MEAN(posts.NUM_UNIQUE(votes.VoteTypeId)) MEAN(posts.SKEW(votes.BountyAmount)) MEAN(posts.STD(votes.BountyAmount)) MEAN(posts.SUM(votes.BountyAmount)) MIN(posts.COUNT(votes)) MIN(posts.MAX(votes.BountyAmount)) MIN(posts.MEAN(votes.BountyAmount)) MIN(posts.NUM_UNIQUE(votes.VoteTypeId)) MIN(posts.SKEW(votes.BountyAmount)) MIN(posts.STD(votes.BountyAmount)) MIN(posts.SUM(votes.BountyAmount)) SKEW(posts.COUNT(votes)) SKEW(posts.MAX(votes.BountyAmount)) SKEW(posts.MEAN(votes.BountyAmount)) SKEW(posts.MIN(votes.BountyAmount)) SKEW(posts.NUM_UNIQUE(votes.VoteTypeId)) SKEW(posts.SUM(votes.BountyAmount)) STD(posts.COUNT(votes)) STD(posts.MAX(votes.BountyAmount)) STD(posts.MEAN(votes.BountyAmount)) STD(posts.MIN(votes.BountyAmount)) STD(posts.NUM_UNIQUE(votes.VoteTypeId)) STD(posts.SUM(votes.BountyAmount)) SUM(posts.MAX(votes.BountyAmount)) SUM(posts.MEAN(votes.BountyAmount)) SUM(posts.MIN(votes.BountyAmount)) SUM(posts.NUM_UNIQUE(votes.VoteTypeId)) SUM(posts.SKEW(votes.BountyAmount)) SUM(posts.STD(votes.BountyAmount)) MAX(votes.posts.AcceptedAnswerId) MAX(votes.posts.AnswerCount) MAX(votes.posts.CommentCount) MAX(votes.posts.FavoriteCount) MAX(votes.posts.LastEditorUserId) MAX(votes.posts.PostTypeId) MAX(votes.posts.Score) MAX(votes.posts.ViewCount) MEAN(votes.posts.AcceptedAnswerId) MEAN(votes.posts.AnswerCount) MEAN(votes.posts.CommentCount) MEAN(votes.posts.FavoriteCount) MEAN(votes.posts.LastEditorUserId) MEAN(votes.posts.PostTypeId) MEAN(votes.posts.Score) MEAN(votes.posts.ViewCount) MIN(votes.posts.AcceptedAnswerId) MIN(votes.posts.AnswerCount) MIN(votes.posts.CommentCount) MIN(votes.posts.FavoriteCount) MIN(votes.posts.LastEditorUserId) MIN(votes.posts.PostTypeId) MIN(votes.posts.Score) MIN(votes.posts.ViewCount) SKEW(votes.posts.AcceptedAnswerId) SKEW(votes.posts.AnswerCount) SKEW(votes.posts.CommentCount) SKEW(votes.posts.FavoriteCount) SKEW(votes.posts.LastEditorUserId) SKEW(votes.posts.PostTypeId) SKEW(votes.posts.Score) SKEW(votes.posts.ViewCount) STD(votes.posts.AcceptedAnswerId) STD(votes.posts.AnswerCount) STD(votes.posts.CommentCount) STD(votes.posts.FavoriteCount) STD(votes.posts.LastEditorUserId) STD(votes.posts.PostTypeId) STD(votes.posts.Score) STD(votes.posts.ViewCount) SUM(votes.posts.AcceptedAnswerId) SUM(votes.posts.AnswerCount) SUM(votes.posts.CommentCount) SUM(votes.posts.FavoriteCount) SUM(votes.posts.LastEditorUserId) SUM(votes.posts.PostTypeId) SUM(votes.posts.Score) SUM(votes.posts.ViewCount) SKEW(posts.STD(votes.BountyAmount)) STD(posts.SKEW(votes.BountyAmount)) role     target categorical   categorical       categorical             categorical  categorical  categorical            categorical                  categorical       categorical         categorical         categorical           categorical           categorical             categorical        categorical          categorical            categorical              categorical                categorical             categorical                  categorical                    categorical                      categorical                   categorical                      categorical                      numerical numerical numerical                   numerical              numerical               numerical                numerical                   numerical             numerical        numerical            numerical                    numerical               numerical                numerical                 numerical                    numerical              numerical         numerical             numerical                   numerical              numerical               numerical                numerical                   numerical             numerical        numerical            numerical                    numerical               numerical                numerical                 numerical                    numerical              numerical         numerical             numerical                   numerical              numerical               numerical                numerical                   numerical             numerical        numerical            numerical                   numerical              numerical               numerical                numerical                   numerical             numerical        numerical            numerical               numerical                numerical               numerical                numerical               numerical               numerical               numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical               numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                numerical                        numerical                        numerical                        numerical                        numerical                        numerical               numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                        numerical                    numerical                     numerical                      numerical                        numerical                   numerical              numerical                  numerical                        numerical                     numerical                      numerical                       numerical                        numerical                    numerical               numerical                   numerical                        numerical                    numerical                     numerical                      numerical                        numerical                   numerical              numerical                  numerical                        numerical                     numerical                      numerical                       numerical                        numerical                    numerical               numerical                   numerical                        numerical                    numerical                     numerical                      numerical                        numerical                   numerical              numerical                  numerical                        numerical                    numerical                     numerical                      numerical                        numerical                   numerical              numerical                  numerical                     unused_float                     unused_float 0 101 2 Autobiographer 2 0 0 NULL NULL 19 2 7 1 0 3 2010 2014 8 6 5 2011 2 2 1 2 NULL NULL 11 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 nan nan nan nan nan 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 nan nan 1 429 10 Yearling 9 2 59 2 4 19 10 7 9 0 2 2010 2014 19 7 2 2010 8 6 5 4 2 1 56 20 0 nan 7 4 7 17230 2 26 1582 nan 7 3 7 8659 1.5 23.5 1582 nan 7 2 7 88 1 21 1582 nan nan nan nan nan nan nan nan nan nan 1.4142 nan 12121.2244 0.7071 3.5355 nan 0 7 6 7 17318 3 47 1582 nan nan nan nan nan 0 30 nan nan 3 nan nan 0 29.5 nan nan nan 3 nan nan 0 29 nan nan 3 nan nan 0 nan nan nan nan nan nan 0.7071 nan nan nan 0 0 0 0 0 6 0 0 nan 7 4 7 17230 2 26 1582 nan 7 2.9831 7 8513.7288 1.4915 23.4576 1582 nan 7 2 7 88 1 21 1582 nan 0 0.03479 0 0.03479 0.03479 0.03479 0 nan 0 1.0084 0 8643.3304 0.5042 2.5211 0 0 210 176 210 502310 88 1384 47460 nan nan 2 101 5 Autobiographer 5 1 2 2 1 19 9 7 9 0 1 2010 2014 19 7 0 2010 4 3 4 2 2 1 10 5 0 nan nan 0 nan 615 2 2 nan nan nan 0 nan 615 2 2 nan nan nan 0 nan 615 2 2 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 615 2 2 0 nan nan nan nan nan 0 2 nan nan 1 nan nan 0 2 nan nan nan 1 nan nan 0 2 nan nan 1 nan nan 0 nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 1 0 0 nan nan 0 nan 615 2 2 nan nan nan 0 nan 615 2 2 nan nan nan 0 nan 615 2 2 nan nan nan nan nan nan nan nan nan nan nan 0 nan 0 0 0 nan 0 0 0 0 1230 4 4 0 nan nan 3 2185 44 Announcer 20 39 499 2 5 19 27 7 8 0 2 2010 2014 10 7 1 2012 23 12 7 5 2 2 312 40 3 15396 11 10 21 28666 2 74 1926 7502.75 2.4 2.1282 5.7692 3279.1053 1.4872 10.5641 655.75 844 0 0 1 30 1 0 73 0.3969 2.2973 1.2348 1.4673 2.7866 0.05337 3.4381 1.0282 4550.3906 2.5215 2.4299 7.0018 7949.6098 0.5064 15.5306 588.8215 60022 48 83 75 62303 58 412 13115 nan nan nan nan nan 0 74 nan nan 4 nan nan 0 12.7949 nan nan nan 1.5789 nan nan 0 0 nan nan 1 nan nan 0 2.6117 nan nan nan 1.3039 0 17.2095 nan nan nan 0.7215 0 0 0 0 60 0 0 15396 11 10 21 28666 2 74 1926 6143.6699 4.4669 2.6934 11.1415 8242.8584 1.515 30.1603 1191.1488 844 0 0 1 30 1 1 73 0.5685 0.9218 0.7248 -0.0435 0.8061 -0.06033 0.7466 -0.3801 3575.4252 3.6326 2.4585 8.0147 11448.7075 0.5003 27.7176 636.2425 632798 1081 1344 2362 2736629 756 15050 288258 nan nan 4 1651 22 Nice Answer 14 14 172 2 4 19 7 7 8 0 3 2010 2014 19 7 0 2010 12 9 7 5 2 1 255 12 0 nan 4 5 6 805 2 37 413 nan 2.5 1.7857 4.5 102.8333 1.8571 11.1429 376 nan 1 0 3 39 1 1 339 nan nan 0.8296 nan 3.4641 -2.2948 1.703 nan nan 2.1213 1.8051 2.1213 221.1252 0.3631 9.5905 52.3259 0 5 25 9 1234 26 156 752 nan nan nan nan nan 0 37 nan nan 3 nan nan 0 12.2857 nan nan nan 1.6429 nan nan 0 1 nan nan 1 nan nan 0 1.3957 nan nan nan 0.4326 0 9.7067 nan nan nan 0.6333 0 0 0 0 23 0 0 nan 4 5 6 805 2 37 413 nan 2.4483 1.9419 4.5517 177.3056 1.8314 18.0756 377.2759 nan 1 0 3 39 1 1 339 nan 0.07283 0.7614 -0.07283 1.6785 -1.7859 0.6603 -0.07283 nan 1.5256 1.6533 1.5256 295.6699 0.3755 11.4253 37.6325 0 71 334 132 25532 315 3109 10941 nan nan ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 7987 1 0 NULL NULL 0 0 NULL NULL 13 13 9 9 5 5 2014 2014 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 nan nan nan nan nan 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 nan nan 7988 11 1 Student 1 1 2 2 1 13 14 9 9 5 6 2014 2014 13 9 5 2014 1 1 1 1 2 1 0 0 0 nan 0 4 nan nan 1 2 40 nan 0 4 nan nan 1 2 40 nan 0 4 nan nan 1 2 40 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 4 0 0 1 2 40 nan nan nan nan nan 0 2 nan nan 1 nan nan 0 2 nan nan nan 1 nan nan 0 2 nan nan 1 nan nan 0 nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 1 0 0 nan 0 4 nan nan 1 2 40 nan 0 4 nan nan 1 2 40 nan 0 4 nan nan 1 2 40 nan nan nan nan nan nan nan nan nan 0 0 nan nan 0 0 0 0 0 8 0 0 2 4 80 nan nan 7989 1 0 NULL NULL 0 0 NULL NULL 13 13 9 9 5 5 2014 2014 NULL NULL NULL NULL NULL NULL NULL NULL NULL NULL 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 nan nan nan nan nan 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 0 0 0 0 0 nan nan 7990 6 1 Student 1 1 1 2 1 13 13 9 9 5 5 2014 2014 13 9 5 2014 1 1 1 1 2 1 1 0 0 nan 0 2 nan nan 1 1 13 nan 0 2 nan nan 1 1 13 nan 0 2 nan nan 1 1 13 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 2 0 0 1 1 13 nan nan nan nan nan 0 1 nan nan 1 nan nan 0 1 nan nan nan 1 nan nan 0 1 nan nan 1 nan nan 0 nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 1 0 0 nan 0 2 nan nan 1 1 13 nan 0 2 nan nan 1 1 13 nan 0 2 nan nan 1 1 13 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 2 0 0 1 1 13 nan nan 7991 106 1 Student 1 1 1 2 1 14 14 9 9 6 6 2014 2014 14 9 6 2014 1 1 1 1 2 1 1 0 0 nan 0 2 nan 7290 1 1 5 nan 0 2 nan 7290 1 1 5 nan 0 2 nan 7290 1 1 5 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 2 0 7290 1 1 5 nan nan nan nan nan 0 1 nan nan 1 nan nan 0 1 nan nan nan 1 nan nan 0 1 nan nan 1 nan nan 0 nan nan nan nan nan nan nan nan nan nan nan nan 0 0 0 1 0 0 nan 0 2 nan 7290 1 1 5 nan 0 2 nan 7290 1 1 5 nan 0 2 nan 7290 1 1 5 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan 0 0 2 0 7290 1 1 5 nan nan <p>     7992 rows x 173 columns     memory usage: 10.26 MB     name: featuretools_test     type: getml.DataFrame </p> <p>We train an untuned XGBoostRegressor on top of featuretools' features, just like we have done for getML's features.</p> <p>Since some of featuretools features are categorical, we allow the pipeline to include these features as well. Other features contain NaN values, which is why we also apply getML's Imputation preprocessor.</p> In\u00a0[36]: Copied! <pre>imputation = getml.preprocessors.Imputation()\n\npredictor = getml.predictors.XGBoostRegressor(n_jobs=1)\n\npipe3 = getml.Pipeline(\n    tags=['featuretools'],\n    preprocessors=[imputation],\n    predictors=[predictor],\n    include_categorical=True,\n)\n\npipe3\n</pre> imputation = getml.preprocessors.Imputation()  predictor = getml.predictors.XGBoostRegressor(n_jobs=1)  pipe3 = getml.Pipeline(     tags=['featuretools'],     preprocessors=[imputation],     predictors=[predictor],     include_categorical=True, )  pipe3 Out[36]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=['Imputation'],\n         share_selected_features=0.5,\n         tags=['featuretools'])</pre> In\u00a0[37]: Copied! <pre>pipe3.fit(featuretools_train)\n</pre> pipe3.fit(featuretools_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:08, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:8.388461\n\n</pre> Out[37]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=True,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=['Imputation'],\n         share_selected_features=0.5,\n         tags=['featuretools'])</pre> In\u00a0[38]: Copied! <pre>featuretools_score = pipe3.score(featuretools_test)\nfeaturetools_score\n</pre> featuretools_score = pipe3.score(featuretools_test) featuretools_score <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[38]: date time           set used           target          mae     rmse rsquared 0 2024-02-21 15:15:21 featuretools_train Reputation 32.3818 48.2617 0.9969 1 2024-02-21 15:15:21 featuretools_test Reputation 34.219 78.0558 0.9684 In\u00a0[39]: Copied! <pre>pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name]\n</pre> pipe1.features.to_sql()[pipe1.features.sort(by=\"importances\")[0].name] Out[39]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_87\";\n\nCREATE TABLE \"FEATURE_1_87\" AS\nSELECT SUM( t2.\"score\" ) AS \"feature_1_87\",\n       t1.rowid AS rownum\nFROM \"USERS__STAGING_TABLE_1\" t1\nINNER JOIN \"POSTS__STAGING_TABLE_3\" t2\nON t1.\"id\" = t2.\"owneruserid\"\nWHERE t2.\"posttypeid\" = '2'\nGROUP BY t1.rowid;\n</pre> In\u00a0[40]: Copied! <pre>pipe2.features.to_sql()[pipe2.features.sort(by=\"importances\")[0].name]\n</pre> pipe2.features.to_sql()[pipe2.features.sort(by=\"importances\")[0].name] Out[40]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_1\";\n\nCREATE TABLE \"FEATURE_1_1\" AS\nSELECT SUM( \n    CASE\n        WHEN ( f_1_2.\"feature_1_2_13\" &gt; 52.410975 ) AND ( t2.\"t3__commentcount\" &gt; 18.000000 ) AND ( t2.\"lasteditoruserid__mapping_2_target_1_avg\" &gt; 18893.105802 ) THEN 2232.219909292027\n        WHEN ( f_1_2.\"feature_1_2_13\" &gt; 52.410975 ) AND ( t2.\"t3__commentcount\" &gt; 18.000000 ) AND ( t2.\"lasteditoruserid__mapping_2_target_1_avg\" &lt;= 18893.105802 OR t2.\"lasteditoruserid__mapping_2_target_1_avg\" IS NULL ) THEN 1532.385271802465\n        WHEN ( f_1_2.\"feature_1_2_13\" &gt; 52.410975 ) AND ( t2.\"t3__commentcount\" &lt;= 18.000000 OR t2.\"t3__commentcount\" IS NULL ) AND ( t2.\"answercount__mapping_2_target_1_avg\" &gt; 474.581818 ) THEN 68.63615059480811\n        WHEN ( f_1_2.\"feature_1_2_13\" &gt; 52.410975 ) AND ( t2.\"t3__commentcount\" &lt;= 18.000000 OR t2.\"t3__commentcount\" IS NULL ) AND ( t2.\"answercount__mapping_2_target_1_avg\" &lt;= 474.581818 OR t2.\"answercount__mapping_2_target_1_avg\" IS NULL ) THEN 151.3502309330784\n        WHEN ( f_1_2.\"feature_1_2_13\" &lt;= 52.410975 ) AND ( f_1_2.\"feature_1_2_13\" &gt; 1.500560 ) AND ( t2.\"lasteditoruserid__mapping_2_target_1_avg\" &gt; 9890.350688 ) THEN 15.59546887560193\n        WHEN ( f_1_2.\"feature_1_2_13\" &lt;= 52.410975 ) AND ( f_1_2.\"feature_1_2_13\" &gt; 1.500560 ) AND ( t2.\"lasteditoruserid__mapping_2_target_1_avg\" &lt;= 9890.350688 OR t2.\"lasteditoruserid__mapping_2_target_1_avg\" IS NULL ) THEN 18.23250276402033\n        WHEN ( f_1_2.\"feature_1_2_13\" &lt;= 52.410975 ) AND ( f_1_2.\"feature_1_2_13\" &lt;= 1.500560 ) AND ( t1.\"upvotes\" &gt; 8562.000000 ) THEN 8.786831543345656\n        WHEN ( f_1_2.\"feature_1_2_13\" &lt;= 52.410975 ) AND ( f_1_2.\"feature_1_2_13\" &lt;= 1.500560 ) AND ( t1.\"upvotes\" &lt;= 8562.000000 OR t1.\"upvotes\" IS NULL ) THEN -0.3702761379749883\n        ELSE NULL\n    END\n) AS \"feature_1_1\",\n       t1.rowid AS rownum\nFROM \"USERS__STAGING_TABLE_1\" t1\nINNER JOIN \"POSTS__STAGING_TABLE_3\" t2\nON t1.\"id\" = t2.\"owneruserid\"\nLEFT JOIN \"FEATURES_1_2\" f_1_2\nON t2.rowid = f_1_2.\"rownum\"\nGROUP BY t1.rowid;\n</pre> In\u00a0[41]: Copied! <pre># Creates a folder named stats_pipeline containing\n# the SQL code.\npipe2.features.to_sql().save(\"stats_pipeline\")\n</pre> # Creates a folder named stats_pipeline containing # the SQL code. pipe2.features.to_sql().save(\"stats_pipeline\") In\u00a0[42]: Copied! <pre>pipe2.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"stats_spark\")\n</pre> pipe2.features.to_sql(dialect=getml.pipeline.dialect.spark_sql).save(\"stats_spark\") In\u00a0[43]: Copied! <pre>scores = [fastprop_score, relboost_score, featuretools_score]\npd.DataFrame(data={\n    'Name': ['getML: FastProp', 'getML: Relboost', 'featuretools'],\n    'R-squared': [f'{score.rsquared:.2%}' for score in scores],\n    'RMSE': [f'{score.rmse:,.2f}' for score in scores],\n    'MAE': [f'{score.mae:,.2f}' for score in scores]\n})\n</pre> scores = [fastprop_score, relboost_score, featuretools_score] pd.DataFrame(data={     'Name': ['getML: FastProp', 'getML: Relboost', 'featuretools'],     'R-squared': [f'{score.rsquared:.2%}' for score in scores],     'RMSE': [f'{score.rmse:,.2f}' for score in scores],     'MAE': [f'{score.mae:,.2f}' for score in scores] }) Out[43]: Name R-squared RMSE MAE 0 getML: FastProp 97.77% 65.33 33.61 1 getML: Relboost 98.09% 60.76 31.14 2 featuretools 96.84% 78.06 34.22","boost":0.8},{"location":"examples/enterprise-notebooks/stats/#statsexchange-predicting-users-reputations","title":"StatsExchange - Predicting users' reputations\u00b6","text":"<p>In this notebook, we will use relational learning techniques to predict users' reputation on StackExchange</p> <p>Summary:</p> <ul> <li>Prediction type: Regression model</li> <li>Domain: Internet</li> <li>Prediction target: Reputation</li> <li>Population size: 41793</li> </ul>","boost":0.8},{"location":"examples/enterprise-notebooks/stats/#background","title":"Background\u00b6","text":"<p>StatsExchange is a website similar to StackOverflow, but based on statistics and machine learning. Much like StackOverflow, it has a complicated system of calculating users' reputation. In this notebook, we will benchmark relational learning techniques based on there ability to reverse-engineer said system.</p> <p>The dataset has been downloaded from the CTU Prague relational learning repository (Motl and Schulte, 2015)(Now residing at relational-data.org.).</p> <p>We will benchmark getML 's feature learning algorithms against featuretools, an open-source implementation of the propositionalization algorithm, similar to getML's FastProp.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/stats/#analysis","title":"Analysis\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/stats/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/stats/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>We begin by downloading the data:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/stats/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/stats/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"<p>We loaded the data and defined the roles and units. Next, we create a getML pipeline for relational learning.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/stats/#21-define-relational-model","title":"2.1 Define relational model\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/stats/#22-getml-pipeline","title":"2.2 getML pipeline\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/stats/#23-model-training","title":"2.3 Model training\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/stats/#24-model-evaluation","title":"2.4 Model evaluation\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/stats/#25-featuretools","title":"2.5 featuretools\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/stats/#26-features","title":"2.6 Features\u00b6","text":"<p>The most important features look as follows:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/stats/#27-productionization","title":"2.7 Productionization\u00b6","text":"<p>It is possible to productionize the pipeline by transpiling the features into production-ready SQL code. Please also refer to getML's <code>sqlite3</code> and <code>spark</code> modules.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/stats/#28-discussion","title":"2.8 Discussion\u00b6","text":"<p>For a more convenient overview, we summarize our results into a table.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/stats/#3-conclusion","title":"3. Conclusion\u00b6","text":"<p>In this notebook, we have benchmarked several relational learning algorithms based on their ability to reverse-engineer StatsExchange's system of calculating users' reputation. We have found that all relational learning algorithm can reverse-engineer the system with high levels of accuracy, but we have also found that getML nonetheless outperforms featuretools.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/stats/#references","title":"References\u00b6","text":"<p>Motl, Jan, and Oliver Schulte. \"The CTU prague relational learning repository.\" arXiv preprint arXiv:1511.03086 (2015).</p>","boost":0.8},{"location":"examples/enterprise-notebooks/fastprop_benchmark/","title":"FastProp Benchmarks","text":"<p>A compilation of benchmark notebooks that demonstrate and compare the performance of getML's <code>FastProp</code> algorithm against various implementations of other propositionalization algorithms across different datasets.</p> <p>Note</p> <p><code>FastProp</code> (short for fast propositionalization) is an open source Feature Learner and available in the getML Community Edition.</p>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/#results","title":"Results","text":"Faster vs. featuretools Faster vs. tsfresh Remarks Air pollution ~65x ~33x The predictive accuracy can be significantly improved by using RelMT instead of propositionalization approaches, please refer to this notebook. Dodgers ~42x ~75x The predictive accuracy can be significantly improved by using the mapping preprocessor and/or more advanced feature learning algorithms, please refer to this notebook. Interstate94 ~55x Occupancy ~87x ~41x Robot ~162x ~77x","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/#source","title":"Source","text":"<p>These notebooks are published on the getml-demo repository.</p>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/air_pollution_prop/","title":"<span class=\"ntitle\">air_pollution_prop.ipynb</span> <span class=\"ndesc\">Why feature learning is better than simple propositionalization</span>","text":"<ol> <li>Loading data</li> <li>Predictive modeling</li> <li>Comparison</li> <li>Conclusion</li> </ol> In\u00a0[1]: Copied! <pre>import datetime\nimport os\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\nfrom pathlib import Path\n\nimport sys\nimport time\nfrom urllib import request\n\nimport getml\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import pearsonr\n\n%matplotlib inline\n</pre> import datetime import os os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\" from pathlib import Path  import sys import time from urllib import request  import getml import matplotlib.pyplot as plt import numpy as np import pandas as pd from scipy.stats import pearsonr  %matplotlib inline In\u00a0[2]: Copied! <pre>parent = Path(os.getcwd()).parent.as_posix()\n\nif parent not in sys.path:\n   sys.path.append(parent) \n\nfrom utils import Benchmark, FTTimeSeriesBuilder, TSFreshBuilder\n</pre> parent = Path(os.getcwd()).parent.as_posix()  if parent not in sys.path:    sys.path.append(parent)   from utils import Benchmark, FTTimeSeriesBuilder, TSFreshBuilder In\u00a0[3]: Copied! <pre>FEATURETOOLS_FILES = [\"featuretools_training.csv\", \"featuretools_test.csv\"]\n\nfor fname in FEATURETOOLS_FILES:\n    if not os.path.exists(fname):\n        fname, res = request.urlretrieve(\n            \"https://static.getml.com/datasets/air_pollution/featuretools/\" + fname,\n            fname,\n        )\n</pre> FEATURETOOLS_FILES = [\"featuretools_training.csv\", \"featuretools_test.csv\"]  for fname in FEATURETOOLS_FILES:     if not os.path.exists(fname):         fname, res = request.urlretrieve(             \"https://static.getml.com/datasets/air_pollution/featuretools/\" + fname,             fname,         ) In\u00a0[4]: Copied! <pre>TSFRESH_FILES = [\"tsfresh_training.csv\", \"tsfresh_test.csv\"]\n\nfor fname in TSFRESH_FILES:\n    if not os.path.exists(fname):\n        fname, res = request.urlretrieve(\n            \"https://static.getml.com/datasets/air_pollution/tsfresh/\" + fname, fname\n        )\n</pre> TSFRESH_FILES = [\"tsfresh_training.csv\", \"tsfresh_test.csv\"]  for fname in TSFRESH_FILES:     if not os.path.exists(fname):         fname, res = request.urlretrieve(             \"https://static.getml.com/datasets/air_pollution/tsfresh/\" + fname, fname         ) In\u00a0[5]: Copied! <pre>fname = \"PRSA_data_2010.1.1-2014.12.31.csv\"\n\nif not os.path.exists(fname):\n    fname, res = request.urlretrieve(\n        \"https://archive.ics.uci.edu/ml/machine-learning-databases/00381/\" + fname,\n        fname,\n    )\n</pre> fname = \"PRSA_data_2010.1.1-2014.12.31.csv\"  if not os.path.exists(fname):     fname, res = request.urlretrieve(         \"https://archive.ics.uci.edu/ml/machine-learning-databases/00381/\" + fname,         fname,     ) In\u00a0[6]: Copied! <pre>data_full_pandas = pd.read_csv(fname)\n\ndata_full_pandas = data_full_pandas[\n    data_full_pandas[\"pm2.5\"] == data_full_pandas[\"pm2.5\"]\n]\n</pre> data_full_pandas = pd.read_csv(fname)  data_full_pandas = data_full_pandas[     data_full_pandas[\"pm2.5\"] == data_full_pandas[\"pm2.5\"] ] <p>tsfresh requires a date column, so we build one.</p> In\u00a0[7]: Copied! <pre>def add_leading_zero(val):\n    if len(str(val)) == 1:\n        return \"0\" + str(val)\n    return str(val)\n\n\ndata_full_pandas[\"month\"] = [add_leading_zero(val) for val in data_full_pandas[\"month\"]]\n\ndata_full_pandas[\"day\"] = [add_leading_zero(val) for val in data_full_pandas[\"day\"]]\n\ndata_full_pandas[\"hour\"] = [add_leading_zero(val) for val in data_full_pandas[\"hour\"]]\n\n\ndef make_date(year, month, day, hour):\n    return year + \"-\" + month + \"-\" + day + \" \" + hour + \":00:00\"\n\n\ndata_full_pandas[\"date\"] = [\n    make_date(str(year), month, day, hour)\n    for year, month, day, hour in zip(\n        data_full_pandas[\"year\"],\n        data_full_pandas[\"month\"],\n        data_full_pandas[\"day\"],\n        data_full_pandas[\"hour\"],\n    )\n]\n</pre> def add_leading_zero(val):     if len(str(val)) == 1:         return \"0\" + str(val)     return str(val)   data_full_pandas[\"month\"] = [add_leading_zero(val) for val in data_full_pandas[\"month\"]]  data_full_pandas[\"day\"] = [add_leading_zero(val) for val in data_full_pandas[\"day\"]]  data_full_pandas[\"hour\"] = [add_leading_zero(val) for val in data_full_pandas[\"hour\"]]   def make_date(year, month, day, hour):     return year + \"-\" + month + \"-\" + day + \" \" + hour + \":00:00\"   data_full_pandas[\"date\"] = [     make_date(str(year), month, day, hour)     for year, month, day, hour in zip(         data_full_pandas[\"year\"],         data_full_pandas[\"month\"],         data_full_pandas[\"day\"],         data_full_pandas[\"hour\"],     ) ] <p>tsfresh also requires the time series to have ids. Since there is only a single time series, that series has the same id.</p> In\u00a0[8]: Copied! <pre>data_full_pandas[\"id\"] = 1\n</pre> data_full_pandas[\"id\"] = 1 <p>The dataset now contains many columns that we do not need or that tsfresh cannot process. For instance, cbwd might actually contain useful information, but it is a categorical variable, which is difficult to handle for tsfresh, so we remove it.</p> <p>We also want to split our data into a training and testing set.</p> In\u00a0[9]: Copied! <pre>data_train_pandas = data_full_pandas[data_full_pandas[\"year\"] &lt; 2014]\ndata_test_pandas = data_full_pandas[data_full_pandas[\"year\"] == 2014]\ndata_full_pandas = data_full_pandas\n</pre> data_train_pandas = data_full_pandas[data_full_pandas[\"year\"] &lt; 2014] data_test_pandas = data_full_pandas[data_full_pandas[\"year\"] == 2014] data_full_pandas = data_full_pandas In\u00a0[10]: Copied! <pre>def remove_unwanted_columns(df):\n    del df[\"cbwd\"]\n    del df[\"year\"]\n    del df[\"month\"]\n    del df[\"day\"]\n    del df[\"hour\"]\n    del df[\"No\"]\n    return df\n\n\ndata_full_pandas = remove_unwanted_columns(data_full_pandas)\ndata_train_pandas = remove_unwanted_columns(data_train_pandas)\ndata_test_pandas = remove_unwanted_columns(data_test_pandas)\n</pre> def remove_unwanted_columns(df):     del df[\"cbwd\"]     del df[\"year\"]     del df[\"month\"]     del df[\"day\"]     del df[\"hour\"]     del df[\"No\"]     return df   data_full_pandas = remove_unwanted_columns(data_full_pandas) data_train_pandas = remove_unwanted_columns(data_train_pandas) data_test_pandas = remove_unwanted_columns(data_test_pandas) In\u00a0[11]: Copied! <pre>data_full_pandas\n</pre> data_full_pandas Out[11]: pm2.5 DEWP TEMP PRES Iws Is Ir date id 24 129.0 -16 -4.0 1020.0 1.79 0 0 2010-01-02 00:00:00 1 25 148.0 -15 -4.0 1020.0 2.68 0 0 2010-01-02 01:00:00 1 26 159.0 -11 -5.0 1021.0 3.57 0 0 2010-01-02 02:00:00 1 27 181.0 -7 -5.0 1022.0 5.36 1 0 2010-01-02 03:00:00 1 28 138.0 -7 -5.0 1022.0 6.25 2 0 2010-01-02 04:00:00 1 ... ... ... ... ... ... ... ... ... ... 43819 8.0 -23 -2.0 1034.0 231.97 0 0 2014-12-31 19:00:00 1 43820 10.0 -22 -3.0 1034.0 237.78 0 0 2014-12-31 20:00:00 1 43821 10.0 -22 -3.0 1034.0 242.70 0 0 2014-12-31 21:00:00 1 43822 8.0 -22 -4.0 1034.0 246.72 0 0 2014-12-31 22:00:00 1 43823 12.0 -21 -3.0 1034.0 249.85 0 0 2014-12-31 23:00:00 1 <p>41757 rows \u00d7 9 columns</p> <p>We then load the data into the getML engine. We begin by setting a project.</p> In\u00a0[12]: Copied! <pre>getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.engine.set_project(\"air_pollution\")\n</pre> getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.engine.set_project(\"air_pollution\") <pre>getML engine is already running.\nLoading pipelines... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\nConnected to project 'air_pollution'\n</pre> In\u00a0[13]: Copied! <pre>df_full = getml.data.DataFrame.from_pandas(data_full_pandas, name=\"full\")\ndf_full[\"date\"] = df_full[\"date\"].as_ts()\n</pre> df_full = getml.data.DataFrame.from_pandas(data_full_pandas, name=\"full\") df_full[\"date\"] = df_full[\"date\"].as_ts() <p>We need to assign roles to the columns, such as defining the target column.</p> In\u00a0[14]: Copied! <pre>df_full.set_role([\"date\"], getml.data.roles.time_stamp)\ndf_full.set_role([\"pm2.5\"], getml.data.roles.target)\ndf_full.set_role(\n    [\"DEWP\", \"TEMP\", \"PRES\", \"Iws\", \"Is\", \"Ir\"], getml.data.roles.numerical\n)\ndf_full\n</pre> df_full.set_role([\"date\"], getml.data.roles.time_stamp) df_full.set_role([\"pm2.5\"], getml.data.roles.target) df_full.set_role(     [\"DEWP\", \"TEMP\", \"PRES\", \"Iws\", \"Is\", \"Ir\"], getml.data.roles.numerical ) df_full Out[14]:  name                        date  pm2.5      DEWP      TEMP      PRES       Iws        Is        Ir           id  role                  time_stamp target numerical numerical numerical numerical numerical numerical unused_float  unit time stamp, comparison only 0 2010-01-02 129 -16 -4 1020 1.79 0 0 1 1 2010-01-02 01:00:00 148 -15 -4 1020 2.68 0 0 1 2 2010-01-02 02:00:00 159 -11 -5 1021 3.57 0 0 1 3 2010-01-02 03:00:00 181 -7 -5 1022 5.36 1 0 1 4 2010-01-02 04:00:00 138 -7 -5 1022 6.25 2 0 1 ... ... ... ... ... ... ... ... ... 41752 2014-12-31 19:00:00 8 -23 -2 1034 231.97 0 0 1 41753 2014-12-31 20:00:00 10 -22 -3 1034 237.78 0 0 1 41754 2014-12-31 21:00:00 10 -22 -3 1034 242.7 0 0 1 41755 2014-12-31 22:00:00 8 -22 -4 1034 246.72 0 0 1 41756 2014-12-31 23:00:00 12 -21 -3 1034 249.85 0 0 1 <p>     41757 rows x 9 columns     memory usage: 3.01 MB     name: full     type: getml.DataFrame </p> In\u00a0[15]: Copied! <pre>split = getml.data.split.time(\n    population=df_full, time_stamp=\"date\", test=getml.data.time.datetime(2014, 1, 1)\n)\nsplit\n</pre> split = getml.data.split.time(     population=df_full, time_stamp=\"date\", test=getml.data.time.datetime(2014, 1, 1) ) split Out[15]: 0 train 1 train 2 train 3 train 4 train ... <p>     41757 rows          type: StringColumnView </p> In\u00a0[16]: Copied! <pre>time_series = getml.data.TimeSeries(\n    population=df_full,\n    alias=\"population\",\n    split=split,\n    time_stamps=\"date\",\n    memory=getml.data.time.days(1),\n)\n\ntime_series\n</pre> time_series = getml.data.TimeSeries(     population=df_full,     alias=\"population\",     split=split,     time_stamps=\"date\",     memory=getml.data.time.days(1), )  time_series Out[16]: data model diagram fullpopulationdate &lt;= dateMemory: 1.0 days staging data frames staging table               0 population POPULATION__STAGING_TABLE_1 1 full FULL__STAGING_TABLE_2 container population subset name  rows type 0 test full 8661 View 1 train full 33096 View peripheral name  rows type      0 full 41757 DataFrame In\u00a0[17]: Copied! <pre>fast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    num_threads=1,\n    aggregation=getml.feature_learning.FastProp.agg_sets.All,\n)\n\npipe_fp_fl = getml.pipeline.Pipeline(\n    tags=[\"memory: 1d\", \"simple features\"],\n    data_model=time_series.data_model,\n    feature_learners=[fast_prop],\n)\n\npipe_fp_fl\n</pre> fast_prop = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.SquareLoss,     num_threads=1,     aggregation=getml.feature_learning.FastProp.agg_sets.All, )  pipe_fp_fl = getml.pipeline.Pipeline(     tags=[\"memory: 1d\", \"simple features\"],     data_model=time_series.data_model,     feature_learners=[fast_prop], )  pipe_fp_fl Out[17]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=['full'],\n         predictors=[],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['memory: 1d', 'simple features'])</pre> In\u00a0[18]: Copied! <pre>pipe_fp_fl.check(time_series.train)\n</pre> pipe_fp_fl.check(time_series.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \n\nOK.\n</pre> In\u00a0[19]: Copied! <pre>benchmark = Benchmark()\n</pre> benchmark = Benchmark() In\u00a0[20]: Copied! <pre>with benchmark(\"fastprop\"):\n    pipe_fp_fl.fit(time_series.train)\n    fastprop_train = pipe_fp_fl.transform(time_series.train, df_name=\"fastprop_train\")\n</pre> with benchmark(\"fastprop\"):     pipe_fp_fl.fit(time_series.train)     fastprop_train = pipe_fp_fl.transform(time_series.train, df_name=\"fastprop_train\") <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 331 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:05, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:5.370066\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:03, remaining: 00:00]          \n\n</pre> In\u00a0[21]: Copied! <pre>fastprop_test = pipe_fp_fl.transform(time_series.test, df_name=\"fastprop_test\")\n</pre> fastprop_test = pipe_fp_fl.transform(time_series.test, df_name=\"fastprop_test\") <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\n</pre> In\u00a0[22]: Copied! <pre>predictor = getml.predictors.XGBoostRegressor()\n\npipe_fp_pr = getml.pipeline.Pipeline(\n    tags=[\"prediction\", \"fastprop\"], predictors=[predictor]\n)\n</pre> predictor = getml.predictors.XGBoostRegressor()  pipe_fp_pr = getml.pipeline.Pipeline(     tags=[\"prediction\", \"fastprop\"], predictors=[predictor] ) In\u00a0[23]: Copied! <pre>pipe_fp_pr.fit(fastprop_train)\n</pre> pipe_fp_pr.fit(fastprop_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:16, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:16.267519\n\n</pre> Out[23]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['prediction', 'fastprop'])</pre> In\u00a0[24]: Copied! <pre>pipe_fp_pr.score(fastprop_test)\n</pre> pipe_fp_pr.score(fastprop_test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[24]: date time           set used       target      mae     rmse rsquared 0 2024-02-21 16:13:14 fastprop_train pm2.5 38.3028 55.2472 0.6438 1 2024-02-21 16:13:14 fastprop_test pm2.5 44.2526 63.4191 0.5462 In\u00a0[25]: Copied! <pre>ft_builder = FTTimeSeriesBuilder(\n    num_features=200,\n    horizon=pd.Timedelta(days=0),\n    memory=pd.Timedelta(days=1),\n    column_id=\"id\",\n    time_stamp=\"date\",\n    target=\"pm2.5\",\n)\n</pre> ft_builder = FTTimeSeriesBuilder(     num_features=200,     horizon=pd.Timedelta(days=0),     memory=pd.Timedelta(days=1),     column_id=\"id\",     time_stamp=\"date\",     target=\"pm2.5\", ) In\u00a0[26]: Copied! <pre>with benchmark(\"featuretools\"):\n    featuretools_training = ft_builder.fit(data_train_pandas)\n\nfeaturetools_test = ft_builder.transform(data_test_pandas)\n</pre> with benchmark(\"featuretools\"):     featuretools_training = ft_builder.fit(data_train_pandas)  featuretools_test = ft_builder.transform(data_test_pandas) <pre>featuretools: Trying features...\nSelecting the best out of 298 features...\nTime taken: 0h:34m:1.838204\n\n</pre> In\u00a0[27]: Copied! <pre>df_featuretools_training = getml.data.DataFrame.from_pandas(\n    featuretools_training, name=\"featuretools_training\"\n)\ndf_featuretools_test = getml.data.DataFrame.from_pandas(\n    featuretools_test, name=\"featuretools_test\"\n)\n</pre> df_featuretools_training = getml.data.DataFrame.from_pandas(     featuretools_training, name=\"featuretools_training\" ) df_featuretools_test = getml.data.DataFrame.from_pandas(     featuretools_test, name=\"featuretools_test\" ) In\u00a0[28]: Copied! <pre>def set_roles_featuretools(df):\n    df[\"date\"] = df[\"date\"].as_ts()\n    df.set_role([\"pm2.5\"], getml.data.roles.target)\n    df.set_role([\"date\"], getml.data.roles.time_stamp)\n    df.set_role(df.roles.unused, getml.data.roles.numerical)\n    df.set_role([\"id\"], getml.data.roles.unused_float)\n    return df\n\n\ndf_featuretools_training = set_roles_featuretools(df_featuretools_training)\ndf_featuretools_test = set_roles_featuretools(df_featuretools_test)\n</pre> def set_roles_featuretools(df):     df[\"date\"] = df[\"date\"].as_ts()     df.set_role([\"pm2.5\"], getml.data.roles.target)     df.set_role([\"date\"], getml.data.roles.time_stamp)     df.set_role(df.roles.unused, getml.data.roles.numerical)     df.set_role([\"id\"], getml.data.roles.unused_float)     return df   df_featuretools_training = set_roles_featuretools(df_featuretools_training) df_featuretools_test = set_roles_featuretools(df_featuretools_test) In\u00a0[29]: Copied! <pre>predictor = getml.predictors.XGBoostRegressor()\n\npipe_ft_pr = getml.pipeline.Pipeline(\n    tags=[\"featuretools\", \"memory: 1d\"], predictors=[predictor]\n)\n\npipe_ft_pr\n</pre> predictor = getml.predictors.XGBoostRegressor()  pipe_ft_pr = getml.pipeline.Pipeline(     tags=[\"featuretools\", \"memory: 1d\"], predictors=[predictor] )  pipe_ft_pr Out[29]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['featuretools', 'memory: 1d'])</pre> In\u00a0[30]: Copied! <pre>pipe_ft_pr.check(df_featuretools_training)\n</pre> pipe_ft_pr.check(df_featuretools_training) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\n</pre> In\u00a0[31]: Copied! <pre>pipe_ft_pr.fit(df_featuretools_training)\n</pre> pipe_ft_pr.fit(df_featuretools_training) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:10, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:10.460135\n\n</pre> Out[31]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['featuretools', 'memory: 1d'])</pre> In\u00a0[32]: Copied! <pre>pipe_ft_pr.score(df_featuretools_test)\n</pre> pipe_ft_pr.score(df_featuretools_test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[32]: date time           set used              target      mae     rmse rsquared 0 2024-02-21 16:56:07 featuretools_training pm2.5 38.277 54.8781 0.6506 1 2024-02-21 16:56:07 featuretools_test pm2.5 43.9151 62.5672 0.5594 In\u00a0[33]: Copied! <pre>data_train_pandas\n</pre> data_train_pandas Out[33]: pm2.5 DEWP TEMP PRES Iws Is Ir date id 24 129.0 -16 -4.0 1020.0 1.79 0 0 2010-01-02 00:00:00 1 25 148.0 -15 -4.0 1020.0 2.68 0 0 2010-01-02 01:00:00 1 26 159.0 -11 -5.0 1021.0 3.57 0 0 2010-01-02 02:00:00 1 27 181.0 -7 -5.0 1022.0 5.36 1 0 2010-01-02 03:00:00 1 28 138.0 -7 -5.0 1022.0 6.25 2 0 2010-01-02 04:00:00 1 ... ... ... ... ... ... ... ... ... ... 35059 22.0 -19 7.0 1013.0 114.87 0 0 2013-12-31 19:00:00 1 35060 18.0 -21 7.0 1014.0 119.79 0 0 2013-12-31 20:00:00 1 35061 23.0 -21 7.0 1014.0 125.60 0 0 2013-12-31 21:00:00 1 35062 20.0 -21 6.0 1014.0 130.52 0 0 2013-12-31 22:00:00 1 35063 23.0 -20 7.0 1014.0 137.67 0 0 2013-12-31 23:00:00 1 <p>33096 rows \u00d7 9 columns</p> In\u00a0[34]: Copied! <pre>tsfresh_builder = TSFreshBuilder(\n    num_features=200, memory=24, column_id=\"id\", time_stamp=\"date\", target=\"pm2.5\"\n)\n\nwith benchmark(\"tsfresh\"):\n    tsfresh_training = tsfresh_builder.fit(data_train_pandas)\n\ntsfresh_test = tsfresh_builder.transform(data_test_pandas)\n</pre> tsfresh_builder = TSFreshBuilder(     num_features=200, memory=24, column_id=\"id\", time_stamp=\"date\", target=\"pm2.5\" )  with benchmark(\"tsfresh\"):     tsfresh_training = tsfresh_builder.fit(data_train_pandas)  tsfresh_test = tsfresh_builder.transform(data_test_pandas) <pre>Rolling: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:22&lt;00:00,  1.82it/s]\nFeature Extraction: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:26&lt;00:00,  1.48it/s]\nFeature Extraction: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:29&lt;00:00,  1.35it/s]\n</pre> <pre>Selecting the best out of 78 features...\nTime taken: 0h:1m:30.572509\n\n</pre> <pre>Rolling: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:03&lt;00:00, 10.93it/s]\nFeature Extraction: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:06&lt;00:00,  5.84it/s]\nFeature Extraction: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:07&lt;00:00,  5.61it/s]\n</pre> <p>tsfresh does not contain built-in machine learning algorithms. In order to ensure a fair comparison, we use the exact same machine learning algorithm we have also used for getML: An XGBoost regressor with all hyperparameters set to their default value.</p> <p>In order to do so, we load the tsfresh features into the getML engine.</p> In\u00a0[35]: Copied! <pre>df_tsfresh_training = getml.data.DataFrame.from_pandas(\n    tsfresh_training, name=\"tsfresh_training\"\n)\ndf_tsfresh_test = getml.data.DataFrame.from_pandas(tsfresh_test, name=\"tsfresh_test\")\n</pre> df_tsfresh_training = getml.data.DataFrame.from_pandas(     tsfresh_training, name=\"tsfresh_training\" ) df_tsfresh_test = getml.data.DataFrame.from_pandas(tsfresh_test, name=\"tsfresh_test\") <p>As usual, we need to set roles:</p> In\u00a0[36]: Copied! <pre>def set_roles_tsfresh(df):\n    df[\"date\"] = df[\"date\"].as_ts()\n    df.set_role([\"pm2.5\"], getml.data.roles.target)\n    df.set_role([\"date\"], getml.data.roles.time_stamp)\n    df.set_role(df.roles.unused, getml.data.roles.numerical)\n    df.set_role([\"id\"], getml.data.roles.unused_float)\n    return df\n\n\ndf_tsfresh_training = set_roles_tsfresh(df_tsfresh_training)\ndf_tsfresh_test = set_roles_tsfresh(df_tsfresh_test)\n</pre> def set_roles_tsfresh(df):     df[\"date\"] = df[\"date\"].as_ts()     df.set_role([\"pm2.5\"], getml.data.roles.target)     df.set_role([\"date\"], getml.data.roles.time_stamp)     df.set_role(df.roles.unused, getml.data.roles.numerical)     df.set_role([\"id\"], getml.data.roles.unused_float)     return df   df_tsfresh_training = set_roles_tsfresh(df_tsfresh_training) df_tsfresh_test = set_roles_tsfresh(df_tsfresh_test) <p>In this case, our pipeline is very simple. It only consists of a single XGBoostRegressor.</p> In\u00a0[37]: Copied! <pre>predictor = getml.predictors.XGBoostRegressor()\n\npipe_tsf_pr = getml.pipeline.Pipeline(\n    tags=[\"tsfresh\", \"memory: 1d\"], predictors=[predictor]\n)\n\npipe_tsf_pr\n</pre> predictor = getml.predictors.XGBoostRegressor()  pipe_tsf_pr = getml.pipeline.Pipeline(     tags=[\"tsfresh\", \"memory: 1d\"], predictors=[predictor] )  pipe_tsf_pr Out[37]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['tsfresh', 'memory: 1d'])</pre> In\u00a0[38]: Copied! <pre>pipe_tsf_pr.check(df_tsfresh_training)\n</pre> pipe_tsf_pr.check(df_tsfresh_training) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\n</pre> In\u00a0[39]: Copied! <pre>pipe_tsf_pr.fit(df_tsfresh_training)\n</pre> pipe_tsf_pr.fit(df_tsfresh_training) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:06, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:5.783919\n\n</pre> Out[39]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['tsfresh', 'memory: 1d'])</pre> In\u00a0[40]: Copied! <pre>pipe_tsf_pr.score(df_tsfresh_test)\n</pre> pipe_tsf_pr.score(df_tsfresh_test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[40]: date time           set used         target      mae     rmse rsquared 0 2024-02-21 16:58:06 tsfresh_training pm2.5 40.8917 57.9517 0.6099 1 2024-02-21 16:58:06 tsfresh_test pm2.5 47.1106 66.6 0.5015 In\u00a0[41]: Copied! <pre>num_features = dict(\n    fastprop=289,\n    featuretools=114,\n    tsfresh=72,\n)\n\nruntime_per_feature = [\n    benchmark.runtimes[\"fastprop\"] / num_features[\"fastprop\"],\n    benchmark.runtimes[\"featuretools\"] / num_features[\"featuretools\"],\n    benchmark.runtimes[\"tsfresh\"] / num_features[\"tsfresh\"],\n]\n\nfeatures_per_second = [1.0 / r.total_seconds() for r in runtime_per_feature]\n\nnormalized_runtime_per_feature = [\n    r / runtime_per_feature[0] for r in runtime_per_feature\n]\n\ncomparison = pd.DataFrame(\n    dict(\n        runtime=[\n            benchmark.runtimes[\"fastprop\"],\n            benchmark.runtimes[\"featuretools\"],\n            benchmark.runtimes[\"tsfresh\"],\n        ],\n        num_features=num_features.values(),\n        features_per_second=features_per_second,\n        normalized_runtime=[\n            1,\n            benchmark.runtimes[\"featuretools\"] / benchmark.runtimes[\"fastprop\"],\n            benchmark.runtimes[\"tsfresh\"] / benchmark.runtimes[\"fastprop\"],\n        ],\n        normalized_runtime_per_feature=normalized_runtime_per_feature,\n        mae=[pipe_fp_pr.mae, pipe_ft_pr.mae, pipe_tsf_pr.mae],\n        rmse=[pipe_fp_pr.rmse, pipe_ft_pr.rmse, pipe_tsf_pr.rmse],\n        rsquared=[pipe_fp_pr.rsquared, pipe_ft_pr.rsquared, pipe_tsf_pr.rsquared],\n    )\n)\n\ncomparison.index = [\"getML: FastProp\", \"featuretools\", \"tsfresh\"]\n\ncomparison\n</pre> num_features = dict(     fastprop=289,     featuretools=114,     tsfresh=72, )  runtime_per_feature = [     benchmark.runtimes[\"fastprop\"] / num_features[\"fastprop\"],     benchmark.runtimes[\"featuretools\"] / num_features[\"featuretools\"],     benchmark.runtimes[\"tsfresh\"] / num_features[\"tsfresh\"], ]  features_per_second = [1.0 / r.total_seconds() for r in runtime_per_feature]  normalized_runtime_per_feature = [     r / runtime_per_feature[0] for r in runtime_per_feature ]  comparison = pd.DataFrame(     dict(         runtime=[             benchmark.runtimes[\"fastprop\"],             benchmark.runtimes[\"featuretools\"],             benchmark.runtimes[\"tsfresh\"],         ],         num_features=num_features.values(),         features_per_second=features_per_second,         normalized_runtime=[             1,             benchmark.runtimes[\"featuretools\"] / benchmark.runtimes[\"fastprop\"],             benchmark.runtimes[\"tsfresh\"] / benchmark.runtimes[\"fastprop\"],         ],         normalized_runtime_per_feature=normalized_runtime_per_feature,         mae=[pipe_fp_pr.mae, pipe_ft_pr.mae, pipe_tsf_pr.mae],         rmse=[pipe_fp_pr.rmse, pipe_ft_pr.rmse, pipe_tsf_pr.rmse],         rsquared=[pipe_fp_pr.rsquared, pipe_ft_pr.rsquared, pipe_tsf_pr.rsquared],     ) )  comparison.index = [\"getML: FastProp\", \"featuretools\", \"tsfresh\"]  comparison Out[41]: runtime num_features features_per_second normalized_runtime normalized_runtime_per_feature mae rmse rsquared getML: FastProp 0 days 00:00:08.637513 289 33.458244 1.000000 1.000000 44.252635 63.419113 0.546164 featuretools 0 days 00:34:01.839960 114 0.055832 236.392114 599.266495 43.915071 62.567175 0.559369 tsfresh 0 days 00:01:30.572946 72 0.794939 10.485998 42.089066 47.110594 66.599982 0.501524 In\u00a0[42]: Copied! <pre># export for further use\ncomparison.to_csv(\"comparisons/air_pollution.csv\")\n</pre> # export for further use comparison.to_csv(\"comparisons/air_pollution.csv\")","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/air_pollution_prop/#air-polution-predicting-air-pollution-in-beijing","title":"Air Polution - Predicting air pollution in Beijing\u00b6","text":"<p>In this notebook we will compare getML to featuretools and tsfresh, both of which open-source libraries for feature engineering. We find that advanced algorithms featured in getML yield significantly better predictions on this dataset. We then discuss why that is.</p> <p>Summary:</p> <ul> <li>Prediction type: Regression model</li> <li>Domain: Air pollution</li> <li>Prediction target: pm 2.5 concentration</li> <li>Source data: Multivariate time series</li> <li>Population size: 41757</li> </ul>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/air_pollution_prop/#background","title":"Background\u00b6","text":"<p>A common approach to feature engineering is to generate attribute-value representations from relational data by applying a fixed set of aggregations to columns of interest and perform a feature selection on the (possibly large) set of generated features afterwards. In academia, this approach is called propositionalization.</p> <p>getML's FastProp is an implementation of this propositionalization approach that has been optimized for speed and memory efficiency. In this notebook, we want to demonstrate how \u2013 well \u2013 fast FastProp is. To this end, we will benchmark FastProp against the popular feature engineering libraries featuretools and tsfresh. Both of these libraries use propositionalization approaches for feature engineering.</p> <p>As our example dataset, we use a publicly available dataset on air pollution in Beijing, China (https://archive.ics.uci.edu/dataset/381/beijing+pm2+5+data). For further details about the data set refer to the full notebook.</p>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/air_pollution_prop/#analysis","title":"Analysis\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/air_pollution_prop/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/air_pollution_prop/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>We begin by downloading the data from the UCI Machine Learning repository.</p>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/air_pollution_prop/#12-prepare-data-for-tsfresh-and-getml","title":"1.2 Prepare data for tsfresh and getML\u00b6","text":"<p>Our our goal is to predict the pm2.5 concentration from factors such as weather or time of day. However, there are some missing entries for pm2.5, so we get rid of them.</p>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/air_pollution_prop/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/air_pollution_prop/#21-propositionalization-with-getmls-fastprop","title":"2.1 Propositionalization with getML's FastProp\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/air_pollution_prop/#22-using-featuretools","title":"2.2 Using featuretools\u00b6","text":"<p>To make things a bit easier, we have written a high-level wrapper around featuretools which we placed in a separate module (<code>utils</code>).</p>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/air_pollution_prop/#23-using-tsfresh","title":"2.3 Using tsfresh\u00b6","text":"<p>tsfresh is a rather low-level library. To make things a bit easier, we have written a high-level wrapper which we placed in a separate module (<code>utils</code>).</p> <p>To limit the memory consumption, we undertake the following steps:</p> <ul> <li>We limit ourselves to a memory of 1 day from any point in time. This is necessary, because tsfresh duplicates records for every time stamp. That means that looking back 7 days instead of one day, the memory consumption would be  seven times as high.</li> <li>We extract only tsfresh's MinimalFCParameters and IndexBasedFCParameters (the latter is a superset of TimeBasedFCParameters).</li> </ul> <p>In order to make sure that tsfresh's features can be compared to getML's features, we also do the following:</p> <ul> <li>We apply tsfresh's built-in feature selection algorithm.</li> <li>Of the remaining features, we only keep the 40 features most correlated with the target (in terms of the absolute value of the correlation).</li> <li>We add the original columns as additional features.</li> </ul>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/air_pollution_prop/#3-comparison","title":"3. Comparison\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/air_pollution_prop/#4-conclusion","title":"4. Conclusion\u00b6","text":"<p>We have compared getML's feature learning algorithms to tsfresh's brute-force feature engineering approaches on a data set related to air pollution in China. We found that getML significantly outperforms featuretools and tsfresh. These results are consistent with the view that feature learning can yield significant improvements over simple propositionalization approaches.</p> <p>However, there are other datasets on which simple propositionalization performs well. Our suggestion is therefore to think of algorithms like <code>FastProp</code> and <code>RelMT</code> as tools in a toolbox. If a simple tool like <code>FastProp</code> gets the job done, then use that. But when you need more advanced approaches, like <code>RelMT</code>, you should have them at your disposal as well.</p> <p>You are encouraged to reproduce these results.</p>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/dodgers_prop/","title":"<span class=\"ntitle\">dodgers_prop.ipynb</span> <span class=\"ndesc\">Traffic volume prediction</span>","text":"<ol> <li>Loading data</li> <li>Predictive modeling</li> <li>Comparison</li> </ol> <p>Let's get started with the analysis and set-up your session:</p> In\u00a0[1]: Copied! <pre>import datetime\nimport gc\nimport os\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\nfrom pathlib import Path\n\nimport sys\nimport time\nfrom urllib import request\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport scipy\nfrom IPython.display import Image\nfrom scipy.stats import pearsonr\n\n%matplotlib inline\n</pre> import datetime import gc import os os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\" from pathlib import Path  import sys import time from urllib import request  import matplotlib.pyplot as plt import seaborn as sns import numpy as np import pandas as pd import scipy from IPython.display import Image from scipy.stats import pearsonr  %matplotlib inline In\u00a0[2]: Copied! <pre>parent = Path(os.getcwd()).parent.as_posix()\n\nif parent not in sys.path:\n   sys.path.append(parent) \n\nfrom utils import Benchmark, FTTimeSeriesBuilder, TSFreshBuilder\n</pre> parent = Path(os.getcwd()).parent.as_posix()  if parent not in sys.path:    sys.path.append(parent)   from utils import Benchmark, FTTimeSeriesBuilder, TSFreshBuilder In\u00a0[3]: Copied! <pre>import getml\n\ngetml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.engine.set_project(\"dodgers\")\n</pre> import getml  getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.engine.set_project(\"dodgers\") <pre>Launching ./getML --allow-push-notifications=true --allow-remote-ips=true --home-directory=/home/getml --in-memory=true --install=false --launch-browser=true --log=false --token=token in /home/getml/.getML/getml-1.4.0-x64-linux...\nLaunched the getML engine. The log output will be stored in /home/getml/.getML/logs/20240221161224.log.\nLoading pipelines... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\nConnected to project 'dodgers'\n</pre> In\u00a0[4]: Copied! <pre>fname = \"Dodgers.data\"\n\nif not os.path.exists(fname):\n    fname, res = request.urlretrieve(\n        \"https://archive.ics.uci.edu/ml/machine-learning-databases/event-detection/\"\n        + fname,\n        fname,\n    )\n\ndata_full_pandas = pd.read_csv(fname, header=None)\ndata_full_pandas.columns = [\"ds\", \"y\"]\n</pre> fname = \"Dodgers.data\"  if not os.path.exists(fname):     fname, res = request.urlretrieve(         \"https://archive.ics.uci.edu/ml/machine-learning-databases/event-detection/\"         + fname,         fname,     )  data_full_pandas = pd.read_csv(fname, header=None) data_full_pandas.columns = [\"ds\", \"y\"] In\u00a0[5]: Copied! <pre>data_full_pandas[\"ds\"] = [\n    datetime.datetime.strptime(dt, \"%m/%d/%Y %H:%M\") for dt in data_full_pandas[\"ds\"]\n]\n</pre> data_full_pandas[\"ds\"] = [     datetime.datetime.strptime(dt, \"%m/%d/%Y %H:%M\") for dt in data_full_pandas[\"ds\"] ] In\u00a0[6]: Copied! <pre>data_full_pandas\n</pre> data_full_pandas Out[6]: ds y 0 2005-04-10 00:00:00 -1 1 2005-04-10 00:05:00 -1 2 2005-04-10 00:10:00 -1 3 2005-04-10 00:15:00 -1 4 2005-04-10 00:20:00 -1 ... ... ... 50395 2005-10-01 23:35:00 -1 50396 2005-10-01 23:40:00 -1 50397 2005-10-01 23:45:00 -1 50398 2005-10-01 23:50:00 -1 50399 2005-10-01 23:55:00 -1 <p>50400 rows \u00d7 2 columns</p> In\u00a0[7]: Copied! <pre>data_full = getml.data.DataFrame.from_pandas(data_full_pandas, \"data_full\")\n</pre> data_full = getml.data.DataFrame.from_pandas(data_full_pandas, \"data_full\") In\u00a0[8]: Copied! <pre>data_full.set_role(\"y\", getml.data.roles.target)\ndata_full.set_role(\"ds\", getml.data.roles.time_stamp)\n</pre> data_full.set_role(\"y\", getml.data.roles.target) data_full.set_role(\"ds\", getml.data.roles.time_stamp) In\u00a0[9]: Copied! <pre>data_full\n</pre> data_full Out[9]:  name                          ds      y  role                  time_stamp target  unit time stamp, comparison only 0 2005-04-10 -1 1 2005-04-10 00:05:00 -1 2 2005-04-10 00:10:00 -1 3 2005-04-10 00:15:00 -1 4 2005-04-10 00:20:00 -1 ... ... 50395 2005-10-01 23:35:00 -1 50396 2005-10-01 23:40:00 -1 50397 2005-10-01 23:45:00 -1 50398 2005-10-01 23:50:00 -1 50399 2005-10-01 23:55:00 -1 <p>     50400 rows x 2 columns     memory usage: 0.81 MB     name: data_full     type: getml.DataFrame </p> In\u00a0[10]: Copied! <pre>split = getml.data.split.time(\n    population=data_full, time_stamp=\"ds\", test=getml.data.time.datetime(2005, 8, 20)\n)\nsplit\n</pre> split = getml.data.split.time(     population=data_full, time_stamp=\"ds\", test=getml.data.time.datetime(2005, 8, 20) ) split Out[10]: 0 train 1 train 2 train 3 train 4 train ... <p>     50400 rows          type: StringColumnView </p> In\u00a0[11]: Copied! <pre># 1. The horizon is 1 hour (we predict the traffic volume in one hour).\n# 2. The memory is 2 hours, so we allow the algorithm to\n#    use information from up to 2 hours ago.\n# 3. We allow lagged targets. Thus, the algorithm can\n#    identify autoregressive processes.\n\ntime_series = getml.data.TimeSeries(\n    population=data_full,\n    alias=\"population\",\n    split=split,\n    time_stamps=\"ds\",\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.hours(2),\n    lagged_targets=True,\n)\n\ntime_series\n</pre> # 1. The horizon is 1 hour (we predict the traffic volume in one hour). # 2. The memory is 2 hours, so we allow the algorithm to #    use information from up to 2 hours ago. # 3. We allow lagged targets. Thus, the algorithm can #    identify autoregressive processes.  time_series = getml.data.TimeSeries(     population=data_full,     alias=\"population\",     split=split,     time_stamps=\"ds\",     horizon=getml.data.time.hours(1),     memory=getml.data.time.hours(2),     lagged_targets=True, )  time_series Out[11]: data model diagram data_fullpopulationds &lt;= dsMemory: 2.0 hoursHorizon: 1.0 hoursLagged targets allowed staging data frames staging table               0 population POPULATION__STAGING_TABLE_1 1 data_full DATA_FULL__STAGING_TABLE_2 container population subset name       rows type 0 test data_full 12384 View 1 train data_full 38016 View peripheral name       rows type      0 data_full 50400 DataFrame In\u00a0[12]: Copied! <pre>seasonal = getml.preprocessors.Seasonal()\n\nfast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    num_threads=1,\n)\n</pre> seasonal = getml.preprocessors.Seasonal()  fast_prop = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.SquareLoss,     num_threads=1, ) <p>Build the pipeline</p> In\u00a0[13]: Copied! <pre>pipe_fp_fl = getml.pipeline.Pipeline(\n    preprocessors=[seasonal],\n    feature_learners=[fast_prop],\n    data_model=time_series.data_model,\n    tags=[\"feature learning\", \"fastprop\"],\n)\n\npipe_fp_fl\n</pre> pipe_fp_fl = getml.pipeline.Pipeline(     preprocessors=[seasonal],     feature_learners=[fast_prop],     data_model=time_series.data_model,     tags=[\"feature learning\", \"fastprop\"], )  pipe_fp_fl Out[13]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=['data_full'],\n         predictors=[],\n         preprocessors=['Seasonal'],\n         share_selected_features=0.5,\n         tags=['feature learning', 'fastprop'])</pre> In\u00a0[14]: Copied! <pre>pipe_fp_fl.check(time_series.train)\n</pre> pipe_fp_fl.check(time_series.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \n\nOK.\n</pre> In\u00a0[15]: Copied! <pre>benchmark = Benchmark()\n</pre> benchmark = Benchmark() In\u00a0[16]: Copied! <pre>with benchmark(\"fastprop\"):\n    pipe_fp_fl.fit(time_series.train)\n    fastprop_train = pipe_fp_fl.transform(time_series.train, df_name=\"fastprop_train\")\n</pre> with benchmark(\"fastprop\"):     pipe_fp_fl.fit(time_series.train)     fastprop_train = pipe_fp_fl.transform(time_series.train, df_name=\"fastprop_train\") <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 526 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:06, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:6.317863\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:03, remaining: 00:00]          \n\n</pre> In\u00a0[17]: Copied! <pre>fastprop_test = pipe_fp_fl.transform(time_series.test, df_name=\"fastprop_test\")\n</pre> fastprop_test = pipe_fp_fl.transform(time_series.test, df_name=\"fastprop_test\") <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\n</pre> In\u00a0[18]: Copied! <pre>predictor = getml.predictors.XGBoostRegressor()\n\npipe_fp_pr = getml.pipeline.Pipeline(\n    tags=[\"prediction\", \"fastprop\"], predictors=[predictor]\n)\n</pre> predictor = getml.predictors.XGBoostRegressor()  pipe_fp_pr = getml.pipeline.Pipeline(     tags=[\"prediction\", \"fastprop\"], predictors=[predictor] ) In\u00a0[19]: Copied! <pre>pipe_fp_pr.fit(fastprop_train)\n</pre> pipe_fp_pr.fit(fastprop_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:09, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:9.613381\n\n</pre> Out[19]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['prediction', 'fastprop'])</pre> In\u00a0[20]: Copied! <pre>pipe_fp_pr.score(fastprop_test)\n</pre> pipe_fp_pr.score(fastprop_test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[20]: date time           set used       target     mae    rmse rsquared 0 2024-02-21 16:12:55 fastprop_train y 5.4188 7.5347 0.699 1 2024-02-21 16:12:55 fastprop_test y 5.6151 7.8243 0.6747 In\u00a0[21]: Copied! <pre>data_train = time_series.train.population.to_df(\"data_train\")\ndata_test = time_series.test.population.to_df(\"data_test\")\n</pre> data_train = time_series.train.population.to_df(\"data_train\") data_test = time_series.test.population.to_df(\"data_test\") In\u00a0[22]: Copied! <pre>dfs_pandas = {}\n\nfor df in getml.project.data_frames:\n    dfs_pandas[df.name] = df.to_pandas()\n    dfs_pandas[df.name][\"id\"] = 1\n</pre> dfs_pandas = {}  for df in getml.project.data_frames:     dfs_pandas[df.name] = df.to_pandas()     dfs_pandas[df.name][\"id\"] = 1 In\u00a0[23]: Copied! <pre>ft_builder = FTTimeSeriesBuilder(\n    num_features=200,\n    horizon=pd.Timedelta(hours=1),\n    memory=pd.Timedelta(hours=2),\n    column_id=\"id\",\n    time_stamp=\"ds\",\n    target=\"y\",\n    allow_lagged_targets=True,\n)\n</pre> ft_builder = FTTimeSeriesBuilder(     num_features=200,     horizon=pd.Timedelta(hours=1),     memory=pd.Timedelta(hours=2),     column_id=\"id\",     time_stamp=\"ds\",     target=\"y\",     allow_lagged_targets=True, ) In\u00a0[24]: Copied! <pre>with benchmark(\"featuretools\"):\n    featuretools_train = ft_builder.fit(dfs_pandas[\"data_train\"])\n\nfeaturetools_test = ft_builder.transform(dfs_pandas[\"data_test\"])\n</pre> with benchmark(\"featuretools\"):     featuretools_train = ft_builder.fit(dfs_pandas[\"data_train\"])  featuretools_test = ft_builder.transform(dfs_pandas[\"data_test\"]) <pre>featuretools: Trying features...\nSelecting the best out of 118 features...\nTime taken: 0h:9m:19.75259\n\n</pre> In\u00a0[25]: Copied! <pre>df_featuretools_train = getml.data.DataFrame.from_pandas(\n    featuretools_train, name=\"featuretools_train\", roles=data_train.roles\n)\n\ndf_featuretools_test = getml.data.DataFrame.from_pandas(\n    featuretools_test, name=\"featuretools_test\", roles=data_train.roles\n)\n</pre> df_featuretools_train = getml.data.DataFrame.from_pandas(     featuretools_train, name=\"featuretools_train\", roles=data_train.roles )  df_featuretools_test = getml.data.DataFrame.from_pandas(     featuretools_test, name=\"featuretools_test\", roles=data_train.roles ) In\u00a0[26]: Copied! <pre>df_featuretools_train.set_role(\n    df_featuretools_train.roles.unused, getml.data.roles.numerical\n)\n\ndf_featuretools_test.set_role(\n    df_featuretools_test.roles.unused, getml.data.roles.numerical\n)\n</pre> df_featuretools_train.set_role(     df_featuretools_train.roles.unused, getml.data.roles.numerical )  df_featuretools_test.set_role(     df_featuretools_test.roles.unused, getml.data.roles.numerical ) In\u00a0[27]: Copied! <pre>predictor = getml.predictors.XGBoostRegressor()\n\npipe_ft_pr = getml.pipeline.Pipeline(\n    tags=[\"prediction\", \"featuretools\"], predictors=[predictor]\n)\n\npipe_ft_pr\n</pre> predictor = getml.predictors.XGBoostRegressor()  pipe_ft_pr = getml.pipeline.Pipeline(     tags=[\"prediction\", \"featuretools\"], predictors=[predictor] )  pipe_ft_pr Out[27]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['prediction', 'featuretools'])</pre> In\u00a0[28]: Copied! <pre>pipe_ft_pr.check(df_featuretools_train)\n</pre> pipe_ft_pr.check(df_featuretools_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 0 issues labeled INFO and 1 issues labeled WARNING.\n</pre> Out[28]: type    label                   message                          0 WARNING COLUMN SHOULD BE UNUSED All non-NULL entries in column 'id' in POPULATION__STAGING_TABLE_1 are equal to each other. You should consider setting its role to unused_float or using it for comparison only (you can do the latter by setting a unit that contains 'comparison only'). In\u00a0[29]: Copied! <pre>pipe_ft_pr.fit(df_featuretools_train)\n</pre> pipe_ft_pr.fit(df_featuretools_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 0 issues labeled INFO and 1 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:04, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:4.092266\n\n</pre> Out[29]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['prediction', 'featuretools'])</pre> In\u00a0[30]: Copied! <pre>pipe_ft_pr.score(df_featuretools_test)\n</pre> pipe_ft_pr.score(df_featuretools_test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[30]: date time           set used           target     mae    rmse rsquared 0 2024-02-21 16:25:31 featuretools_train y 5.4482 7.568 0.6962 1 2024-02-21 16:25:31 featuretools_test y 6.0863 8.5009 0.6498 In\u00a0[31]: Copied! <pre>tsfresh_builder = TSFreshBuilder(\n    num_features=200,\n    horizon=20,\n    memory=60,\n    column_id=\"id\",\n    time_stamp=\"ds\",\n    target=\"y\",\n    allow_lagged_targets=True,\n)\n</pre> tsfresh_builder = TSFreshBuilder(     num_features=200,     horizon=20,     memory=60,     column_id=\"id\",     time_stamp=\"ds\",     target=\"y\",     allow_lagged_targets=True, ) In\u00a0[32]: Copied! <pre>with benchmark(\"tsfresh\"):\n    tsfresh_train = tsfresh_builder.fit(dfs_pandas[\"data_train\"])\n\ntsfresh_test = tsfresh_builder.transform(dfs_pandas[\"data_test\"])\n</pre> with benchmark(\"tsfresh\"):     tsfresh_train = tsfresh_builder.fit(dfs_pandas[\"data_train\"])  tsfresh_test = tsfresh_builder.transform(dfs_pandas[\"data_test\"]) <pre>Rolling: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:19&lt;00:00,  2.06it/s]\nFeature Extraction: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:08&lt;00:00,  4.69it/s]\nFeature Extraction: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:08&lt;00:00,  4.71it/s]\n</pre> <pre>Selecting the best out of 13 features...\nTime taken: 0h:0m:46.114942\n\n</pre> <pre>Rolling: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:05&lt;00:00,  7.69it/s]\nFeature Extraction: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:02&lt;00:00, 13.49it/s]\nFeature Extraction: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:03&lt;00:00, 12.04it/s]\n</pre> In\u00a0[33]: Copied! <pre>df_tsfresh_train = getml.data.DataFrame.from_pandas(\n    tsfresh_train, name=\"tsfresh_train\", roles=data_train.roles\n)\n\ndf_tsfresh_test = getml.data.DataFrame.from_pandas(\n    tsfresh_test, name=\"tsfresh_test\", roles=data_train.roles\n)\n</pre> df_tsfresh_train = getml.data.DataFrame.from_pandas(     tsfresh_train, name=\"tsfresh_train\", roles=data_train.roles )  df_tsfresh_test = getml.data.DataFrame.from_pandas(     tsfresh_test, name=\"tsfresh_test\", roles=data_train.roles ) In\u00a0[34]: Copied! <pre>df_tsfresh_train.set_role(df_tsfresh_train.roles.unused, getml.data.roles.numerical)\n\ndf_tsfresh_test.set_role(df_tsfresh_test.roles.unused, getml.data.roles.numerical)\n</pre> df_tsfresh_train.set_role(df_tsfresh_train.roles.unused, getml.data.roles.numerical)  df_tsfresh_test.set_role(df_tsfresh_test.roles.unused, getml.data.roles.numerical) In\u00a0[35]: Copied! <pre>pipe_tsf_pr = getml.pipeline.Pipeline(\n    tags=[\"predicition\", \"tsfresh\"], predictors=[predictor]\n)\n\npipe_tsf_pr\n</pre> pipe_tsf_pr = getml.pipeline.Pipeline(     tags=[\"predicition\", \"tsfresh\"], predictors=[predictor] )  pipe_tsf_pr Out[35]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['predicition', 'tsfresh'])</pre> In\u00a0[36]: Copied! <pre>pipe_tsf_pr.fit(df_tsfresh_train)\n</pre> pipe_tsf_pr.fit(df_tsfresh_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 0 issues labeled INFO and 1 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:1.790984\n\n</pre> Out[36]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['predicition', 'tsfresh'])</pre> In\u00a0[37]: Copied! <pre>pipe_tsf_pr.score(df_tsfresh_test)\n</pre> pipe_tsf_pr.score(df_tsfresh_test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[37]: date time           set used      target     mae    rmse rsquared 0 2024-02-21 16:26:34 tsfresh_train y 6.3146 8.2348 0.6418 1 2024-02-21 16:26:34 tsfresh_test y 6.7886 8.9134 0.5778 In\u00a0[38]: Copied! <pre>num_features = dict(\n    fastprop=526,\n    featuretools=59,\n    tsfresh=12,\n)\n\nruntime_per_feature = [\n    benchmark.runtimes[\"fastprop\"] / num_features[\"fastprop\"],\n    benchmark.runtimes[\"featuretools\"] / num_features[\"featuretools\"],\n    benchmark.runtimes[\"tsfresh\"] / num_features[\"tsfresh\"],\n]\n\nfeatures_per_second = [1.0 / r.total_seconds() for r in runtime_per_feature]\n\nnormalized_runtime_per_feature = [\n    r / runtime_per_feature[0] for r in runtime_per_feature\n]\n\ncomparison = pd.DataFrame(\n    dict(\n        runtime=[\n            benchmark.runtimes[\"fastprop\"],\n            benchmark.runtimes[\"featuretools\"],\n            benchmark.runtimes[\"tsfresh\"],\n        ],\n        num_features=num_features.values(),\n        features_per_second=features_per_second,\n        normalized_runtime=[\n            1,\n            benchmark.runtimes[\"featuretools\"] / benchmark.runtimes[\"fastprop\"],\n            benchmark.runtimes[\"tsfresh\"] / benchmark.runtimes[\"fastprop\"],\n        ],\n        normalized_runtime_per_feature=normalized_runtime_per_feature,\n        rsquared=[pipe_fp_pr.rsquared, pipe_ft_pr.rsquared, pipe_tsf_pr.rsquared],\n        rmse=[pipe_fp_pr.rmse, pipe_ft_pr.rmse, pipe_tsf_pr.rmse],\n        mae=[pipe_fp_pr.mae, pipe_ft_pr.mae, pipe_tsf_pr.mae],\n    )\n)\n\ncomparison.index = [\"getML: FastProp\", \"featuretools\", \"tsfresh\"]\n</pre> num_features = dict(     fastprop=526,     featuretools=59,     tsfresh=12, )  runtime_per_feature = [     benchmark.runtimes[\"fastprop\"] / num_features[\"fastprop\"],     benchmark.runtimes[\"featuretools\"] / num_features[\"featuretools\"],     benchmark.runtimes[\"tsfresh\"] / num_features[\"tsfresh\"], ]  features_per_second = [1.0 / r.total_seconds() for r in runtime_per_feature]  normalized_runtime_per_feature = [     r / runtime_per_feature[0] for r in runtime_per_feature ]  comparison = pd.DataFrame(     dict(         runtime=[             benchmark.runtimes[\"fastprop\"],             benchmark.runtimes[\"featuretools\"],             benchmark.runtimes[\"tsfresh\"],         ],         num_features=num_features.values(),         features_per_second=features_per_second,         normalized_runtime=[             1,             benchmark.runtimes[\"featuretools\"] / benchmark.runtimes[\"fastprop\"],             benchmark.runtimes[\"tsfresh\"] / benchmark.runtimes[\"fastprop\"],         ],         normalized_runtime_per_feature=normalized_runtime_per_feature,         rsquared=[pipe_fp_pr.rsquared, pipe_ft_pr.rsquared, pipe_tsf_pr.rsquared],         rmse=[pipe_fp_pr.rmse, pipe_ft_pr.rmse, pipe_tsf_pr.rmse],         mae=[pipe_fp_pr.mae, pipe_ft_pr.mae, pipe_tsf_pr.mae],     ) )  comparison.index = [\"getML: FastProp\", \"featuretools\", \"tsfresh\"] In\u00a0[39]: Copied! <pre>comparison\n</pre> comparison Out[39]: runtime num_features features_per_second normalized_runtime normalized_runtime_per_feature rsquared rmse mae getML: FastProp 0 days 00:00:09.406415 526 55.919029 1.000000 1.000000 0.674740 7.824273 5.615138 featuretools 0 days 00:09:19.754041 59 0.105403 59.507691 530.523794 0.649768 8.500887 6.086277 tsfresh 0 days 00:00:46.115063 12 0.260219 4.902512 214.892468 0.577811 8.913408 6.788610 In\u00a0[40]: Copied! <pre># export for further use\ncomparison.to_csv(\"comparisons/dodgers.csv\")\n</pre> # export for further use comparison.to_csv(\"comparisons/dodgers.csv\")","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/dodgers_prop/#dodgers-traffic-near-dodgers-stadium","title":"Dodgers - Traffic near Dodgers' stadium\u00b6","text":"<p>In this notebook, we compare getML's FastProp against well-known feature engineering libraries featuretools and tsfresh.</p> <p>Summary:</p> <ul> <li>Prediction type: Regression model</li> <li>Domain: Transportation</li> <li>Prediction target: traffic volume</li> <li>Source data: Univariate time series</li> <li>Population size: 47497</li> </ul>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/dodgers_prop/#background","title":"Background\u00b6","text":"<p>A common approach to feature engineering is to generate attribute-value representations from relational data by applying a fixed set of aggregations to columns of interest and perform a feature selection on the (possibly large) set of generated features afterwards. In academia, this approach is called propositionalization.</p> <p>getML's FastProp is an implementation of this propositionalization approach that has been optimized for speed and memory efficiency. In this notebook, we want to demonstrate how \u2013 well \u2013 fast FastProp is. To this end, we will benchmark FastProp against the popular feature engineering libraries featuretools and tsfresh. Both of these libraries use propositionalization approaches for feature engineering.</p> <p>In this notebook, we use traffic data that was collected for the Glendale on ramp for the 101 North freeway in Los Angeles. For further details about the data set refer to the full notebook.</p>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/dodgers_prop/#analysis","title":"Analysis\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/dodgers_prop/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/dodgers_prop/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>We begin by downloading the data from the UC Irvine Machine Learning repository:</p>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/dodgers_prop/#12-prepare-data-for-getml","title":"1.2 Prepare data for getML\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/dodgers_prop/#13-define-relational-model","title":"1.3 Define relational model\u00b6","text":"<p>To start with relational learning, we need to specify the data model. We manually replicate the appropriate time series structure by setting time series related join conditions (<code>horizon</code>, <code>memory</code> and <code>allow_lagged_targets</code>). This is done abstractly using Placeholders</p> <p>The data model consists of two tables:</p> <ul> <li>Population table <code>traffic_{test/train}</code>: holds target and the contemporarily available time-based components</li> <li>Peripheral table <code>traffic</code>: same table as the population table</li> <li>Join between both placeholders specifies (<code>horizon</code>) to prevent leaks and (<code>memory</code>) that keeps the computations feasible</li> </ul>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/dodgers_prop/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"<p>We loaded the data, defined the roles, units and the abstract data model. Next, we create a getML pipeline for relational learning.</p>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/dodgers_prop/#21-propositionalization-with-getmls-fastprop","title":"2.1 Propositionalization with getML's FastProp\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/dodgers_prop/#22-propositionalization-with-featuretools","title":"2.2 Propositionalization with featuretools\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/dodgers_prop/#23-propositionalization-with-tsfresh","title":"2.3 Propositionalization with tsfresh\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/dodgers_prop/#3-comparison","title":"3. Comparison\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/interstate94_prop/","title":"<span class=\"ntitle\">interstate94_prop.ipynb</span> <span class=\"ndesc\">Multivariate time series prediction</span>","text":"<ol> <li>Loading data</li> <li>Predictive modeling</li> <li>Comparison</li> </ol> <p>Let's get started with the analysis and set-up your session:</p> In\u00a0[1]: Copied! <pre>import datetime\nimport os\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\nfrom pathlib import Path\n\nimport sys\nimport time\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image\n\n%matplotlib inline\n\nimport getml\n\nprint(f\"getML API version: {getml.__version__}\\n\")\n\ngetml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.engine.set_project(\"interstate94\")\n</pre> import datetime import os os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\" from pathlib import Path  import sys import time  import matplotlib.pyplot as plt import seaborn as sns import numpy as np import pandas as pd from IPython.display import Image  %matplotlib inline  import getml  print(f\"getML API version: {getml.__version__}\\n\")  getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.engine.set_project(\"interstate94\") <pre>getML API version: 1.4.0\n\ngetML engine is already running.\nLoading pipelines... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nConnected to project 'interstate94'\n</pre> In\u00a0[2]: Copied! <pre>parent = Path(os.getcwd()).parent.as_posix()\n\nif parent not in sys.path:\n   sys.path.append(parent) \n\nfrom utils import Benchmark, FTTimeSeriesBuilder, TSFreshBuilder\nimport woodwork as ww\n</pre> parent = Path(os.getcwd()).parent.as_posix()  if parent not in sys.path:    sys.path.append(parent)   from utils import Benchmark, FTTimeSeriesBuilder, TSFreshBuilder import woodwork as ww In\u00a0[3]: Copied! <pre>traffic = getml.datasets.load_interstate94(roles=True, units=True)\n</pre> traffic = getml.datasets.load_interstate94(roles=True, units=True) <pre>\nLoading traffic...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n</pre> In\u00a0[4]: Copied! <pre>traffic.set_role(traffic.roles.categorical, getml.data.roles.unused_string)\n</pre> traffic.set_role(traffic.roles.categorical, getml.data.roles.unused_string) In\u00a0[5]: Copied! <pre>traffic\n</pre> traffic Out[5]:  name                          ds traffic_volume holiday       day           month         weekday       hour          year           role                  time_stamp         target unused_string unused_string unused_string unused_string unused_string unused_string  unit time stamp, comparison only day           month         weekday       hour          year          0 2016-01-01 1513 New Years Day 1 1 4 0 2016 1 2016-01-01 01:00:00 1550 New Years Day 1 1 4 1 2016 2 2016-01-01 02:00:00 993 New Years Day 1 1 4 2 2016 3 2016-01-01 03:00:00 719 New Years Day 1 1 4 3 2016 4 2016-01-01 04:00:00 533 New Years Day 1 1 4 4 2016 ... ... ... ... ... ... ... ... 24091 2018-09-30 19:00:00 3543 No holiday 30 9 6 19 2018 24092 2018-09-30 20:00:00 2781 No holiday 30 9 6 20 2018 24093 2018-09-30 21:00:00 2159 No holiday 30 9 6 21 2018 24094 2018-09-30 22:00:00 1450 No holiday 30 9 6 22 2018 24095 2018-09-30 23:00:00 954 No holiday 30 9 6 23 2018 <p>     24096 rows x 8 columns     memory usage: 2.16 MB     name: traffic     type: getml.DataFrame </p> In\u00a0[6]: Copied! <pre>split = getml.data.split.time(traffic, \"ds\", test=getml.data.time.datetime(2018, 3, 15))\n</pre> split = getml.data.split.time(traffic, \"ds\", test=getml.data.time.datetime(2018, 3, 15)) In\u00a0[7]: Copied! <pre>time_series = getml.data.TimeSeries(\n    population=traffic,\n    split=split,\n    alias=\"traffic\",\n    time_stamps=\"ds\",\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.hours(24),\n    lagged_targets=True,\n)\n\ntime_series\n</pre> time_series = getml.data.TimeSeries(     population=traffic,     split=split,     alias=\"traffic\",     time_stamps=\"ds\",     horizon=getml.data.time.hours(1),     memory=getml.data.time.hours(24),     lagged_targets=True, )  time_series Out[7]: data model diagram traffictrafficds &lt;= dsMemory: 1.0 daysHorizon: 1.0 hoursLagged targets allowed staging data frames staging table            0 traffic TRAFFIC__STAGING_TABLE_1 1 traffic TRAFFIC__STAGING_TABLE_2 container population subset name     rows type 0 test traffic 4800 View 1 train traffic 19296 View peripheral name     rows type      0 traffic 24096 DataFrame In\u00a0[8]: Copied! <pre>seasonal = getml.preprocessors.Seasonal()\n\nfast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n    num_threads=1,\n)\n</pre> seasonal = getml.preprocessors.Seasonal()  fast_prop = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.SquareLoss,     num_threads=1, ) <p>Build the pipeline</p> In\u00a0[9]: Copied! <pre>pipe_fp_fl = getml.pipeline.Pipeline(\n    preprocessors=[seasonal],\n    feature_learners=[fast_prop],\n    data_model=time_series.data_model,\n    tags=[\"feature learning\", \"fastprop\"],\n)\n\npipe_fp_fl\n</pre> pipe_fp_fl = getml.pipeline.Pipeline(     preprocessors=[seasonal],     feature_learners=[fast_prop],     data_model=time_series.data_model,     tags=[\"feature learning\", \"fastprop\"], )  pipe_fp_fl Out[9]: <pre>Pipeline(data_model='traffic',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=['traffic'],\n         predictors=[],\n         preprocessors=['Seasonal'],\n         share_selected_features=0.5,\n         tags=['feature learning', 'fastprop'])</pre> In\u00a0[10]: Copied! <pre>pipe_fp_fl.check(time_series.train)\n</pre> pipe_fp_fl.check(time_series.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\nOK.\n</pre> In\u00a0[11]: Copied! <pre>benchmark = Benchmark()\n</pre> benchmark = Benchmark() In\u00a0[12]: Copied! <pre>with benchmark(\"fastprop\"):\n    pipe_fp_fl.fit(time_series.train)\n    fastprop_train = pipe_fp_fl.transform(time_series.train, df_name=\"fastprop_train\")\n</pre> with benchmark(\"fastprop\"):     pipe_fp_fl.fit(time_series.train)     fastprop_train = pipe_fp_fl.transform(time_series.train, df_name=\"fastprop_train\") <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 365 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:2.505792\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\n</pre> In\u00a0[13]: Copied! <pre>fastprop_test = pipe_fp_fl.transform(time_series.test, df_name=\"fastprop_test\")\n</pre> fastprop_test = pipe_fp_fl.transform(time_series.test, df_name=\"fastprop_test\") <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> In\u00a0[14]: Copied! <pre>predictor = getml.predictors.XGBoostRegressor()\n\npipe_fp_pr = getml.pipeline.Pipeline(\n    tags=[\"prediction\", \"fastprop\"], predictors=[predictor]\n)\n</pre> predictor = getml.predictors.XGBoostRegressor()  pipe_fp_pr = getml.pipeline.Pipeline(     tags=[\"prediction\", \"fastprop\"], predictors=[predictor] ) In\u00a0[15]: Copied! <pre>pipe_fp_pr.fit(fastprop_train)\n</pre> pipe_fp_pr.fit(fastprop_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:05, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:5.173095\n\n</pre> Out[15]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['prediction', 'fastprop'])</pre> In\u00a0[16]: Copied! <pre>pipe_fp_pr.score(fastprop_test)\n</pre> pipe_fp_pr.score(fastprop_test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[16]: date time           set used       target               mae      rmse rsquared 0 2024-02-21 16:13:15 fastprop_train traffic_volume 198.9482 292.2493 0.9779 1 2024-02-21 16:13:15 fastprop_test traffic_volume 180.4867 261.9389 0.9827 In\u00a0[17]: Copied! <pre>traffic_train = time_series.train.population\ntraffic_test = time_series.test.population\n</pre> traffic_train = time_series.train.population traffic_test = time_series.test.population In\u00a0[18]: Copied! <pre>dfs_pandas = {}\n\nfor df in [traffic_train, traffic_test, traffic]:\n    dfs_pandas[df.name] = df.drop(df.roles.unused).to_pandas()\n    dfs_pandas[df.name][\"join_key\"] = 1\n</pre> dfs_pandas = {}  for df in [traffic_train, traffic_test, traffic]:     dfs_pandas[df.name] = df.drop(df.roles.unused).to_pandas()     dfs_pandas[df.name][\"join_key\"] = 1 In\u00a0[19]: Copied! <pre>ft_builder = FTTimeSeriesBuilder(\n    num_features=200,\n    horizon=pd.Timedelta(hours=1),\n    memory=pd.Timedelta(hours=24),\n    column_id=\"join_key\",\n    time_stamp=\"ds\",\n    target=\"traffic_volume\",\n    allow_lagged_targets=True,\n)\n</pre> ft_builder = FTTimeSeriesBuilder(     num_features=200,     horizon=pd.Timedelta(hours=1),     memory=pd.Timedelta(hours=24),     column_id=\"join_key\",     time_stamp=\"ds\",     target=\"traffic_volume\",     allow_lagged_targets=True, ) In\u00a0[20]: Copied! <pre>with benchmark(\"featuretools\"):\n    featuretools_train = ft_builder.fit(dfs_pandas[\"train\"])\n\nfeaturetools_test = ft_builder.transform(dfs_pandas[\"test\"])\n</pre> with benchmark(\"featuretools\"):     featuretools_train = ft_builder.fit(dfs_pandas[\"train\"])  featuretools_test = ft_builder.transform(dfs_pandas[\"test\"]) <pre>featuretools: Trying features...\nSelecting the best out of 118 features...\nTime taken: 0h:4m:46.444548\n\n</pre> In\u00a0[21]: Copied! <pre>roles = {\n    getml.data.roles.join_key: [\"join_key\"],\n    getml.data.roles.target: [\"traffic_volume\"],\n    getml.data.roles.time_stamp: [\"ds\"],\n}\n\ndf_featuretools_train = getml.data.DataFrame.from_pandas(\n    featuretools_train, name=\"featuretools_train\", roles=roles\n)\n\ndf_featuretools_test = getml.data.DataFrame.from_pandas(\n    featuretools_test, name=\"featuretools_test\", roles=roles\n)\n</pre> roles = {     getml.data.roles.join_key: [\"join_key\"],     getml.data.roles.target: [\"traffic_volume\"],     getml.data.roles.time_stamp: [\"ds\"], }  df_featuretools_train = getml.data.DataFrame.from_pandas(     featuretools_train, name=\"featuretools_train\", roles=roles )  df_featuretools_test = getml.data.DataFrame.from_pandas(     featuretools_test, name=\"featuretools_test\", roles=roles ) In\u00a0[22]: Copied! <pre>df_featuretools_train.set_role(\n    df_featuretools_train.roles.unused, getml.data.roles.numerical\n)\n\ndf_featuretools_test.set_role(\n    df_featuretools_test.roles.unused, getml.data.roles.numerical\n)\n</pre> df_featuretools_train.set_role(     df_featuretools_train.roles.unused, getml.data.roles.numerical )  df_featuretools_test.set_role(     df_featuretools_test.roles.unused, getml.data.roles.numerical ) In\u00a0[23]: Copied! <pre>predictor = getml.predictors.XGBoostRegressor()\n\npipe_ft_pr = getml.pipeline.Pipeline(\n    tags=[\"prediction\", \"featuretools\"], predictors=[predictor]\n)\n\npipe_ft_pr\n</pre> predictor = getml.predictors.XGBoostRegressor()  pipe_ft_pr = getml.pipeline.Pipeline(     tags=[\"prediction\", \"featuretools\"], predictors=[predictor] )  pipe_ft_pr Out[23]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['prediction', 'featuretools'])</pre> In\u00a0[24]: Copied! <pre>pipe_ft_pr.check(df_featuretools_train)\n</pre> pipe_ft_pr.check(df_featuretools_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\n</pre> In\u00a0[25]: Copied! <pre>pipe_ft_pr.fit(df_featuretools_train)\n</pre> pipe_ft_pr.fit(df_featuretools_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:2.260849\n\n</pre> Out[25]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['prediction', 'featuretools'])</pre> In\u00a0[26]: Copied! <pre>pipe_ft_pr.score(df_featuretools_test)\n</pre> pipe_ft_pr.score(df_featuretools_test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[26]: date time           set used           target               mae      rmse rsquared 0 2024-02-21 16:19:18 featuretools_train traffic_volume 220.4023 321.1657 0.9734 1 2024-02-21 16:19:18 featuretools_test traffic_volume 210.1988 317.52 0.9746 In\u00a0[27]: Copied! <pre>num_features = dict(\n    fastprop=461,\n    featuretools=59,\n)\n\nruntime_per_feature = [\n    benchmark.runtimes[\"fastprop\"] / num_features[\"fastprop\"],\n    benchmark.runtimes[\"featuretools\"] / num_features[\"featuretools\"],\n]\n\nfeatures_per_second = [1.0 / r.total_seconds() for r in runtime_per_feature]\n\nnormalized_runtime_per_feature = [\n    r / runtime_per_feature[0] for r in runtime_per_feature\n]\n\ncomparison = pd.DataFrame(\n    dict(\n        runtime=[benchmark.runtimes[\"fastprop\"], benchmark.runtimes[\"featuretools\"]],\n        num_features=num_features.values(),\n        features_per_second=features_per_second,\n        normalized_runtime=[\n            1,\n            benchmark.runtimes[\"featuretools\"] / benchmark.runtimes[\"fastprop\"],\n        ],\n        normalized_runtime_per_feature=normalized_runtime_per_feature,\n        rsquared=[pipe_fp_pr.rsquared, pipe_ft_pr.rsquared],\n        rmse=[pipe_fp_pr.rmse, pipe_ft_pr.rmse],\n        mae=[pipe_fp_pr.mae, pipe_ft_pr.mae],\n    )\n)\n\ncomparison.index = [\"getML: FastProp\", \"featuretools\"]\n</pre> num_features = dict(     fastprop=461,     featuretools=59, )  runtime_per_feature = [     benchmark.runtimes[\"fastprop\"] / num_features[\"fastprop\"],     benchmark.runtimes[\"featuretools\"] / num_features[\"featuretools\"], ]  features_per_second = [1.0 / r.total_seconds() for r in runtime_per_feature]  normalized_runtime_per_feature = [     r / runtime_per_feature[0] for r in runtime_per_feature ]  comparison = pd.DataFrame(     dict(         runtime=[benchmark.runtimes[\"fastprop\"], benchmark.runtimes[\"featuretools\"]],         num_features=num_features.values(),         features_per_second=features_per_second,         normalized_runtime=[             1,             benchmark.runtimes[\"featuretools\"] / benchmark.runtimes[\"fastprop\"],         ],         normalized_runtime_per_feature=normalized_runtime_per_feature,         rsquared=[pipe_fp_pr.rsquared, pipe_ft_pr.rsquared],         rmse=[pipe_fp_pr.rmse, pipe_ft_pr.rmse],         mae=[pipe_fp_pr.mae, pipe_ft_pr.mae],     ) )  comparison.index = [\"getML: FastProp\", \"featuretools\"] In\u00a0[28]: Copied! <pre>comparison\n</pre> comparison Out[28]: runtime num_features features_per_second normalized_runtime normalized_runtime_per_feature rsquared rmse mae getML: FastProp 0 days 00:00:03.963151 461 116.319646 1.000000 1.000000 0.982678 261.938873 180.486734 featuretools 0 days 00:04:46.446138 59 0.205972 72.277372 564.734093 0.974582 317.519976 210.198793 In\u00a0[29]: Copied! <pre>comparison.to_csv(\"comparisons/interstate94.csv\")\n</pre> comparison.to_csv(\"comparisons/interstate94.csv\")","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/interstate94_prop/#interstate-94-multivariate-time-series-prediction","title":"Interstate 94 - Multivariate time series prediction\u00b6","text":"<p>In this notebbok, we compare getML's FastProp against well-known feature engineering libraries featuretools and tsfresh.</p> <p>Summary:</p> <ul> <li>Prediction type: Regression model</li> <li>Domain: Transportation</li> <li>Prediction target: Hourly traffic volume</li> <li>Source data: Multivariate time series, 5 components</li> <li>Population size: 24096</li> </ul>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/interstate94_prop/#background","title":"Background\u00b6","text":"<p>A common approach to feature engineering is to generate attribute-value representations from relational data by applying a fixed set of aggregations to columns of interest and perform a feature selection on the (possibly large) set of generated features afterwards. In academia, this approach is called propositionalization.</p> <p>getML's FastProp is an implementation of this propositionalization approach that has been optimized for speed and memory efficiency. In this notebook, we want to demonstrate how \u2013 well \u2013 fast FastProp is. To this end, we will benchmark FastProp against the popular feature engineering libraries featuretools and tsfresh. Both of these libraries use propositionalization approaches for feature engineering.</p> <p>In this notebook, we predict the hourly traffic volume on I-94 westbound from Minneapolis-St Paul. The analysis is built on top of a dataset provided by the MN Department of Transportation, with some data preparation done by John Hogue. For further details about the data set refer to the full notebook.</p>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/interstate94_prop/#analysis","title":"Analysis\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/interstate94_prop/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/interstate94_prop/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"<p>We begin by downloading the data from the UC Irvine Machine Learning repository:</p>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/interstate94_prop/#12-define-relational-model","title":"1.2 Define relational model\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/interstate94_prop/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"<p>We loaded the data, defined the roles, units and the abstract data model. Next, we create a getML pipeline for relational learning.</p>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/interstate94_prop/#21-propositionalization-with-getmls-fastprop","title":"2.1 Propositionalization with getML's FastProp\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/interstate94_prop/#22-propositionalization-with-featuretools","title":"2.2 Propositionalization with featuretools\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/interstate94_prop/#23-propositionalization-with-tsfresh","title":"2.3 Propositionalization with tsfresh\u00b6","text":"<p>tsfresh failed to run through due to an apparent bug in the tsfresh library and is therefore excluded from this analysis.</p>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/interstate94_prop/#3-comparison","title":"3. Comparison\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/occupancy_prop/","title":"<span class=\"ntitle\">occupancy_prop.ipynb</span> <span class=\"ndesc\">A multivariate time series example</span>","text":"<ol> <li>Loading data</li> <li>Predictive modeling</li> <li>Comparison</li> </ol> <p>Let's get started with the analysis and set-up your session:</p> In\u00a0[1]: Copied! <pre>import datetime\nimport os\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\nfrom pathlib import Path\n\nimport sys\nimport time\nfrom urllib import request\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\n%matplotlib inline\n\nimport getml\n\nprint(f\"getML API version: {getml.__version__}\\n\")\n\ngetml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.engine.set_project(\"occupancy\")\n</pre> import datetime import os os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\" from pathlib import Path  import sys import time from urllib import request  import matplotlib.pyplot as plt import seaborn as sns import numpy as np import pandas as pd  %matplotlib inline  import getml  print(f\"getML API version: {getml.__version__}\\n\")  getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.engine.set_project(\"occupancy\") <pre>getML API version: 1.4.0\n\ngetML engine is already running.\nLoading pipelines... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nLoading hyperopts... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nConnected to project 'occupancy'\n</pre> In\u00a0[2]: Copied! <pre>parent = Path(os.getcwd()).parent.as_posix()\n\nif parent not in sys.path:\n   sys.path.append(parent) \n\nfrom utils import Benchmark, FTTimeSeriesBuilder, TSFreshBuilder\n</pre> parent = Path(os.getcwd()).parent.as_posix()  if parent not in sys.path:    sys.path.append(parent)   from utils import Benchmark, FTTimeSeriesBuilder, TSFreshBuilder <p>The data set can be downloaded directly from GitHub. It is conveniently separated into a train, a validation and a testing set. This allows us to directly benchmark our results against the results of the original paper later.</p> In\u00a0[3]: Copied! <pre>data_test, data_train, data_validate = getml.datasets.load_occupancy(roles=True)\n</pre> data_test, data_train, data_validate = getml.datasets.load_occupancy(roles=True) <pre>\nLoading population_train...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nLoading population_test...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nLoading population_validation...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n</pre> In\u00a0[4]: Copied! <pre>data_all, split = getml.data.split.concat(\n    \"data_all\",\n    train=data_train,\n    validation=data_validate,\n    test=data_test,\n)\n</pre> data_all, split = getml.data.split.concat(     \"data_all\",     train=data_train,     validation=data_validate,     test=data_test, ) <p>The train set looks like this:</p> In\u00a0[5]: hide_input Copied! <pre>data_train\n</pre> data_train Out[5]: name                date Occupancy Temperature  Humidity     Light       CO2 HumidityRatio role          time_stamp    target   numerical numerical numerical numerical     numerical unit          time stamp 0 2015-02-11 14:48:00 1 21.76 31.1333 437.3333 1029.6667 0.005021 1 2015-02-11 14:49:00 1 21.79 31 437.3333 1000 0.005009 2 2015-02-11 14:50:00 1 21.7675 31.1225 434 1003.75 0.005022 3 2015-02-11 14:51:00 1 21.7675 31.1225 439 1009.5 0.005022 4 2015-02-11 14:51:59 1 21.79 31.1333 437.3333 1005.6667 0.00503 ... ... ... ... ... ... ... 9747 2015-02-18 09:15:00 1 20.815 27.7175 429.75 1505.25 0.004213 9748 2015-02-18 09:16:00 1 20.865 27.745 423.5 1514.5 0.00423 9749 2015-02-18 09:16:59 1 20.89 27.745 423.5 1521.5 0.004237 9750 2015-02-18 09:17:59 1 20.89 28.0225 418.75 1632 0.004279 9751 2015-02-18 09:19:00 1 21 28.1 409 1864 0.004321 <p>     9752 rows x 7 columns     memory usage: 0.55 MB     name: population_test     type: getml.DataFrame </p> <p>We use all possible aggregations. Because tsfresh and featuretools are single-threaded, we limit our FastProp algorithm to one thread as well, to ensure a fair comparison.</p> In\u00a0[6]: Copied! <pre># Our forecast horizon is 0.\n# We do not predict the future, instead we infer\n# the present state from current and past sensor data.\nhorizon = 0.0\n\n# We do not allow the time series features\n# to use target values from the past.\n# (Otherwise, we would need the horizon to\n# be greater than 0.0).\nallow_lagged_targets = False\n\n# We want our time series features to only use\n# data from the last 15 minutes\nmemory = getml.data.time.minutes(15)\n\ntime_series = getml.data.TimeSeries(\n    population=data_all,\n    split=split,\n    time_stamps=\"date\",\n    horizon=horizon,\n    memory=memory,\n    lagged_targets=allow_lagged_targets,\n)\n\ntime_series\n</pre> # Our forecast horizon is 0. # We do not predict the future, instead we infer # the present state from current and past sensor data. horizon = 0.0  # We do not allow the time series features # to use target values from the past. # (Otherwise, we would need the horizon to # be greater than 0.0). allow_lagged_targets = False  # We want our time series features to only use # data from the last 15 minutes memory = getml.data.time.minutes(15)  time_series = getml.data.TimeSeries(     population=data_all,     split=split,     time_stamps=\"date\",     horizon=horizon,     memory=memory,     lagged_targets=allow_lagged_targets, )  time_series Out[6]: data model diagram data_allpopulationdate &lt;= dateMemory: 15.0 minutes staging data frames staging table               0 population POPULATION__STAGING_TABLE_1 1 data_all DATA_ALL__STAGING_TABLE_2 container population subset     name     rows type 0 test data_all 8142 View 1 train data_all 9753 View 2 validation data_all 2665 View peripheral name      rows type      0 data_all 20560 DataFrame In\u00a0[7]: Copied! <pre>feature_learner = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    aggregation=getml.feature_learning.FastProp.agg_sets.All,\n    num_threads=1,\n)\n</pre> feature_learner = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     aggregation=getml.feature_learning.FastProp.agg_sets.All,     num_threads=1, ) <p>Next, we create the pipeline. In contrast to our usual approach, we create two pipelines in this notebook. One for feature learning (suffix <code>_fl</code>) and one for predicition (suffix <code>_pr</code>). This allows for a fair comparison of runtimes.</p> In\u00a0[8]: Copied! <pre>pipe_fp_fl = getml.pipeline.Pipeline(\n    feature_learners=[feature_learner],\n    data_model=time_series.data_model,\n    tags=[\"feature learning\", \"fastprop\"],\n)\n</pre> pipe_fp_fl = getml.pipeline.Pipeline(     feature_learners=[feature_learner],     data_model=time_series.data_model,     tags=[\"feature learning\", \"fastprop\"], ) In\u00a0[9]: Copied! <pre>pipe_fp_fl.check(time_series.train)\n</pre> pipe_fp_fl.check(time_series.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\n</pre> <p>The wrappers around featuretools and tsfresh fit on the training set and then return the training features. We therefore measure the time it takes getML's FastProp algorithm to fit on the training set and create the training features.</p> In\u00a0[10]: Copied! <pre>benchmark = Benchmark()\n</pre> benchmark = Benchmark() In\u00a0[11]: Copied! <pre>with benchmark(\"fastprop\"):\n    pipe_fp_fl.fit(time_series.train)\n    fastprop_train = pipe_fp_fl.transform(time_series.train, df_name=\"fastprop_train\")\n</pre> with benchmark(\"fastprop\"):     pipe_fp_fl.fit(time_series.train)     fastprop_train = pipe_fp_fl.transform(time_series.train, df_name=\"fastprop_train\") <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 331 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:1.177664\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\n</pre> In\u00a0[12]: Copied! <pre>fastprop_test = pipe_fp_fl.transform(time_series.test, df_name=\"fastprop_test\")\n</pre> fastprop_test = pipe_fp_fl.transform(time_series.test, df_name=\"fastprop_test\") <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\n</pre> <p>Now we create a dedicated prediction pipeline and provide the fast prop features (contrained in <code>fastprop_train</code> and <code>fastprop_test</code>.)</p> In\u00a0[13]: Copied! <pre>predictor = getml.predictors.XGBoostClassifier()\n\npipe_fp_pr = getml.pipeline.Pipeline(\n    tags=[\"prediction\", \"fastprop\"], predictors=[predictor]\n)\n</pre> predictor = getml.predictors.XGBoostClassifier()  pipe_fp_pr = getml.pipeline.Pipeline(     tags=[\"prediction\", \"fastprop\"], predictors=[predictor] ) In\u00a0[14]: Copied! <pre>pipe_fp_pr.check(fastprop_train)\n\npipe_fp_pr.fit(fastprop_train)\n</pre> pipe_fp_pr.check(fastprop_train)  pipe_fp_pr.fit(fastprop_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nChecking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:05, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:5.153885\n\n</pre> Out[14]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostClassifier'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['prediction', 'fastprop'])</pre> In\u00a0[15]: Copied! <pre>pipe_fp_pr.score(fastprop_test)\n</pre> pipe_fp_pr.score(fastprop_test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[15]: date time           set used       target    accuracy     auc cross entropy 0 2024-02-21 16:13:26 fastprop_train Occupancy 0.9995 1. 0.004464 1 2024-02-21 16:13:26 fastprop_test Occupancy 0.9888 0.9982 0.044271 In\u00a0[16]: Copied! <pre>data_train = time_series.train.population.to_df(\"train\")\ndata_test = time_series.test.population.to_df(\"test\")\n</pre> data_train = time_series.train.population.to_df(\"train\") data_test = time_series.test.population.to_df(\"test\") In\u00a0[17]: Copied! <pre>dfs_pandas = {}\n\nfor df in getml.project.data_frames:\n    dfs_pandas[df.name] = df.to_pandas()\n    dfs_pandas[df.name][\"id\"] = 1\n</pre> dfs_pandas = {}  for df in getml.project.data_frames:     dfs_pandas[df.name] = df.to_pandas()     dfs_pandas[df.name][\"id\"] = 1 In\u00a0[18]: Copied! <pre>ft_builder = FTTimeSeriesBuilder(\n    num_features=200,\n    horizon=pd.Timedelta(minutes=0),\n    memory=pd.Timedelta(minutes=15),\n    column_id=\"id\",\n    time_stamp=\"date\",\n    target=\"Occupancy\",\n)\n</pre> ft_builder = FTTimeSeriesBuilder(     num_features=200,     horizon=pd.Timedelta(minutes=0),     memory=pd.Timedelta(minutes=15),     column_id=\"id\",     time_stamp=\"date\",     target=\"Occupancy\", ) <p>The <code>FTTimeSeriesBuilder</code> provides a <code>fit</code> method that is designed to be equivilant to to the <code>fit</code> method of the predictorless getML pipeline above.</p> In\u00a0[19]: Copied! <pre>with benchmark(\"featuretools\"):\n    featuretools_train = ft_builder.fit(dfs_pandas[\"train\"])\n\nfeaturetools_test = ft_builder.transform(dfs_pandas[\"test\"])\n\ndf_featuretools_train = getml.data.DataFrame.from_pandas(\n    featuretools_train, name=\"featuretools_train\", roles=data_train.roles\n)\ndf_featuretools_test = getml.data.DataFrame.from_pandas(\n    featuretools_test, name=\"featuretools_test\", roles=data_train.roles\n)\n\ndf_featuretools_train.set_role(\n    df_featuretools_train.roles.unused, getml.data.roles.numerical\n)\n\ndf_featuretools_test.set_role(\n    df_featuretools_test.roles.unused, getml.data.roles.numerical\n)\n</pre> with benchmark(\"featuretools\"):     featuretools_train = ft_builder.fit(dfs_pandas[\"train\"])  featuretools_test = ft_builder.transform(dfs_pandas[\"test\"])  df_featuretools_train = getml.data.DataFrame.from_pandas(     featuretools_train, name=\"featuretools_train\", roles=data_train.roles ) df_featuretools_test = getml.data.DataFrame.from_pandas(     featuretools_test, name=\"featuretools_test\", roles=data_train.roles )  df_featuretools_train.set_role(     df_featuretools_train.roles.unused, getml.data.roles.numerical )  df_featuretools_test.set_role(     df_featuretools_test.roles.unused, getml.data.roles.numerical ) <pre>featuretools: Trying features...\nSelecting the best out of 262 features...\nTime taken: 0h:8m:2.659652\n\n</pre> In\u00a0[20]: Copied! <pre>predictor = getml.predictors.XGBoostClassifier()\n\npipe_ft_pr = getml.pipeline.Pipeline(\n    tags=[\"prediction\", \"featuretools\"], predictors=[predictor]\n)\n\npipe_ft_pr\n</pre> predictor = getml.predictors.XGBoostClassifier()  pipe_ft_pr = getml.pipeline.Pipeline(     tags=[\"prediction\", \"featuretools\"], predictors=[predictor] )  pipe_ft_pr Out[20]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostClassifier'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['prediction', 'featuretools'])</pre> In\u00a0[21]: Copied! <pre>pipe_ft_pr.check(df_featuretools_train)\n</pre> pipe_ft_pr.check(df_featuretools_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 0 issues labeled INFO and 1 issues labeled WARNING.\n</pre> Out[21]: type    label                   message                          0 WARNING COLUMN SHOULD BE UNUSED All non-NULL entries in column 'id' in POPULATION__STAGING_TABLE_1 are equal to each other. You should consider setting its role to unused_float or using it for comparison only (you can do the latter by setting a unit that contains 'comparison only'). In\u00a0[22]: Copied! <pre>pipe_ft_pr.fit(df_featuretools_train)\n</pre> pipe_ft_pr.fit(df_featuretools_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 0 issues labeled INFO and 1 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:04, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:4.049619\n\n</pre> Out[22]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostClassifier'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['prediction', 'featuretools'])</pre> In\u00a0[23]: Copied! <pre>pipe_ft_pr.score(df_featuretools_test)\n</pre> pipe_ft_pr.score(df_featuretools_test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[23]: date time           set used           target    accuracy     auc cross entropy 0 2024-02-21 16:29:04 featuretools_train Occupancy 0.9994 1. 0.004997 1 2024-02-21 16:29:04 featuretools_test Occupancy 0.9886 0.9973 0.05008 In\u00a0[24]: Copied! <pre>tsfresh_builder = TSFreshBuilder(\n    num_features=200, memory=15, column_id=\"id\", time_stamp=\"date\", target=\"Occupancy\"\n)\n\nwith benchmark(\"tsfresh\"):\n    tsfresh_train = tsfresh_builder.fit(dfs_pandas[\"train\"])\n\ntsfresh_test = tsfresh_builder.transform(dfs_pandas[\"test\"])\n\ndf_tsfresh_train = getml.data.DataFrame.from_pandas(\n    tsfresh_train, name=\"tsfresh_train\", roles=data_train.roles\n)\ndf_tsfresh_test = getml.data.DataFrame.from_pandas(\n    tsfresh_test, name=\"tsfresh_test\", roles=data_train.roles\n)\n\ndf_tsfresh_train.set_role(df_tsfresh_train.roles.unused, getml.data.roles.numerical)\n\ndf_tsfresh_test.set_role(df_tsfresh_test.roles.unused, getml.data.roles.numerical)\n</pre> tsfresh_builder = TSFreshBuilder(     num_features=200, memory=15, column_id=\"id\", time_stamp=\"date\", target=\"Occupancy\" )  with benchmark(\"tsfresh\"):     tsfresh_train = tsfresh_builder.fit(dfs_pandas[\"train\"])  tsfresh_test = tsfresh_builder.transform(dfs_pandas[\"test\"])  df_tsfresh_train = getml.data.DataFrame.from_pandas(     tsfresh_train, name=\"tsfresh_train\", roles=data_train.roles ) df_tsfresh_test = getml.data.DataFrame.from_pandas(     tsfresh_test, name=\"tsfresh_test\", roles=data_train.roles )  df_tsfresh_train.set_role(df_tsfresh_train.roles.unused, getml.data.roles.numerical)  df_tsfresh_test.set_role(df_tsfresh_test.roles.unused, getml.data.roles.numerical) <pre>Rolling: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:03&lt;00:00, 10.42it/s]\nFeature Extraction: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:06&lt;00:00,  5.95it/s]\nFeature Extraction: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:06&lt;00:00,  5.80it/s]\n</pre> <pre>Selecting the best out of 65 features...\nTime taken: 0h:0m:20.577953\n\n</pre> <pre>Rolling: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:03&lt;00:00, 12.87it/s]\nFeature Extraction: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:05&lt;00:00,  6.77it/s]\nFeature Extraction: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:05&lt;00:00,  7.10it/s]\n</pre> In\u00a0[25]: Copied! <pre>pipe_tsf_pr = getml.pipeline.Pipeline(\n    tags=[\"predicition\", \"tsfresh\"], predictors=[predictor]\n)\n\npipe_tsf_pr\n</pre> pipe_tsf_pr = getml.pipeline.Pipeline(     tags=[\"predicition\", \"tsfresh\"], predictors=[predictor] )  pipe_tsf_pr Out[25]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostClassifier'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['predicition', 'tsfresh'])</pre> In\u00a0[26]: Copied! <pre>pipe_tsf_pr.check(df_tsfresh_train)\n</pre> pipe_tsf_pr.check(df_tsfresh_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 0 issues labeled INFO and 1 issues labeled WARNING.\n</pre> Out[26]: type    label                   message                          0 WARNING COLUMN SHOULD BE UNUSED All non-NULL entries in column 'id' in POPULATION__STAGING_TABLE_1 are equal to each other. You should consider setting its role to unused_float or using it for comparison only (you can do the latter by setting a unit that contains 'comparison only'). In\u00a0[27]: Copied! <pre>pipe_tsf_pr.fit(df_tsfresh_train)\n</pre> pipe_tsf_pr.fit(df_tsfresh_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 0 issues labeled INFO and 1 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:02, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:1.896389\n\n</pre> Out[27]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostClassifier'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['predicition', 'tsfresh'])</pre> In\u00a0[28]: Copied! <pre>pipe_tsf_pr.score(df_tsfresh_test)\n</pre> pipe_tsf_pr.score(df_tsfresh_test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[28]: date time           set used      target    accuracy     auc cross entropy 0 2024-02-21 16:29:44 tsfresh_train Occupancy 0.9985 1. 0.006898 1 2024-02-21 16:29:44 tsfresh_test Occupancy 0.9877 0.9979 0.049359 In\u00a0[29]: Copied! <pre>num_features = dict(\n    fastprop=289,\n    featuretools=103,\n    tsfresh=60,\n)\n\nruntime_per_feature = [\n    benchmark.runtimes[\"fastprop\"] / num_features[\"fastprop\"],\n    benchmark.runtimes[\"featuretools\"] / num_features[\"featuretools\"],\n    benchmark.runtimes[\"tsfresh\"] / num_features[\"tsfresh\"],\n]\n\nfeatures_per_second = [1.0 / r.total_seconds() for r in runtime_per_feature]\n\nnormalized_runtime_per_feature = [\n    r / runtime_per_feature[0] for r in runtime_per_feature\n]\n\ncomparison = pd.DataFrame(\n    dict(\n        runtime=[\n            benchmark.runtimes[\"fastprop\"],\n            benchmark.runtimes[\"featuretools\"],\n            benchmark.runtimes[\"tsfresh\"],\n        ],\n        num_features=num_features.values(),\n        features_per_second=features_per_second,\n        normalized_runtime=[\n            1,\n            benchmark.runtimes[\"featuretools\"] / benchmark.runtimes[\"fastprop\"],\n            benchmark.runtimes[\"tsfresh\"] / benchmark.runtimes[\"fastprop\"],\n        ],\n        normalized_runtime_per_feature=normalized_runtime_per_feature,\n        accuracy=[pipe_fp_pr.accuracy, pipe_ft_pr.accuracy, pipe_tsf_pr.accuracy],\n        auc=[pipe_fp_pr.auc, pipe_ft_pr.auc, pipe_tsf_pr.auc],\n        cross_entropy=[\n            pipe_fp_pr.cross_entropy,\n            pipe_ft_pr.cross_entropy,\n            pipe_tsf_pr.cross_entropy,\n        ],\n    )\n)\n\ncomparison.index = [\"getML: FastProp\", \"featuretools\", \"tsfresh\"]\n</pre> num_features = dict(     fastprop=289,     featuretools=103,     tsfresh=60, )  runtime_per_feature = [     benchmark.runtimes[\"fastprop\"] / num_features[\"fastprop\"],     benchmark.runtimes[\"featuretools\"] / num_features[\"featuretools\"],     benchmark.runtimes[\"tsfresh\"] / num_features[\"tsfresh\"], ]  features_per_second = [1.0 / r.total_seconds() for r in runtime_per_feature]  normalized_runtime_per_feature = [     r / runtime_per_feature[0] for r in runtime_per_feature ]  comparison = pd.DataFrame(     dict(         runtime=[             benchmark.runtimes[\"fastprop\"],             benchmark.runtimes[\"featuretools\"],             benchmark.runtimes[\"tsfresh\"],         ],         num_features=num_features.values(),         features_per_second=features_per_second,         normalized_runtime=[             1,             benchmark.runtimes[\"featuretools\"] / benchmark.runtimes[\"fastprop\"],             benchmark.runtimes[\"tsfresh\"] / benchmark.runtimes[\"fastprop\"],         ],         normalized_runtime_per_feature=normalized_runtime_per_feature,         accuracy=[pipe_fp_pr.accuracy, pipe_ft_pr.accuracy, pipe_tsf_pr.accuracy],         auc=[pipe_fp_pr.auc, pipe_ft_pr.auc, pipe_tsf_pr.auc],         cross_entropy=[             pipe_fp_pr.cross_entropy,             pipe_ft_pr.cross_entropy,             pipe_tsf_pr.cross_entropy,         ],     ) )  comparison.index = [\"getML: FastProp\", \"featuretools\", \"tsfresh\"] In\u00a0[30]: Copied! <pre>comparison\n</pre> comparison Out[30]: runtime num_features features_per_second normalized_runtime normalized_runtime_per_feature accuracy auc cross_entropy getML: FastProp 0 days 00:00:01.918944 289 150.602410 1.000000 1.000000 0.988823 0.998166 0.044271 featuretools 0 days 00:08:02.660716 103 0.213400 251.524128 705.726807 0.988578 0.997259 0.050080 tsfresh 0 days 00:00:20.578092 60 2.915724 10.723654 51.651807 0.987718 0.997861 0.049359 In\u00a0[31]: Copied! <pre># export for further use\ncomparison.to_csv(\"comparisons/occupancy.csv\")\n</pre> # export for further use comparison.to_csv(\"comparisons/occupancy.csv\")","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/occupancy_prop/#occupancy-a-multivariate-time-series-example","title":"Occupancy - A multivariate time series example\u00b6","text":"<p>In this notebbok, we compare getML's FastProp against well-known feature engineering libraries featuretools and tsfresh.</p> <p>Summary:</p> <ul> <li>Prediction type: Binary classification</li> <li>Domain: Energy</li> <li>Prediction target: Room occupancy</li> <li>Source data: 1 table, 32k rows</li> <li>Population size: 32k</li> </ul>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/occupancy_prop/#background","title":"Background\u00b6","text":"<p>A common approach to feature engineering is to generate attribute-value representations from relational data by applying a fixed set of aggregations to columns of interest and perform a feature selection on the (possibly large) set of generated features afterwards. In academia, this approach is called propositionalization.</p> <p>getML's FastProp is an implementation of this propositionalization approach that has been optimized for speed and memory efficiency. In this notebook, we want to demonstrate how \u2013 well \u2013 fast FastProp is. To this end, we will benchmark FastProp against the popular feature engineering libraries featuretools and tsfresh. Both of these libraries use propositionalization approaches for feature engineering.</p> <p>Our use case here is a public domain data set for predicting room occupancy from sensor data. For further details about the data set refer to the full notebook.</p>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/occupancy_prop/#analysis","title":"Analysis\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/occupancy_prop/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/occupancy_prop/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"<p>We loaded the data, defined the roles, units and the abstract data model. Next, we create a getML pipeline for relational learning.</p>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/occupancy_prop/#21-propositionalization-with-getmls-fastprop","title":"2.1 Propositionalization with getML's FastProp\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/occupancy_prop/#22-propositionalization-with-featuretools","title":"2.2 Propositionalization with featuretools\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/occupancy_prop/#23-propositionalization-with-tsfresh","title":"2.3 Propositionalization with tsfresh\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/occupancy_prop/#3-comparison","title":"3. Comparison\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/robot_prop/","title":"<span class=\"ntitle\">robot_prop.ipynb</span> <span class=\"ndesc\">Feature engineering on sensor data</span>","text":"<ol> <li>Loading data</li> <li>Predictive modeling</li> <li>Comparison</li> </ol> <p>We begin by importing the libraries and setting the project.</p> In\u00a0[1]: Copied! <pre>import datetime\nimport os\nos.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\nfrom pathlib import Path\n\nimport sys\nimport time\nfrom urllib import request\n\nimport getml\nimport getml.data as data\nimport getml.data.roles as roles\nimport getml.database as database\nimport getml.engine as engine\nimport getml.feature_learning.aggregations as agg\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image\n\n%matplotlib inline\n\ngetml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token')\ngetml.engine.set_project(\"robot\")\n</pre> import datetime import os os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\" from pathlib import Path  import sys import time from urllib import request  import getml import getml.data as data import getml.data.roles as roles import getml.database as database import getml.engine as engine import getml.feature_learning.aggregations as agg import matplotlib.pyplot as plt import seaborn as sns import numpy as np import pandas as pd from IPython.display import Image  %matplotlib inline  getml.engine.launch(home_directory=Path.home(), allow_remote_ips=True, token='token') getml.engine.set_project(\"robot\") <pre>getML engine is already running.\nLoading pipelines... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nConnected to project 'robot'\n</pre> In\u00a0[2]: Copied! <pre>parent = Path(os.getcwd()).parent.as_posix()\n\nif parent not in sys.path:\n   sys.path.append(parent) \n\nfrom utils import Benchmark, FTTimeSeriesBuilder, TSFreshBuilder\n</pre> parent = Path(os.getcwd()).parent.as_posix()  if parent not in sys.path:    sys.path.append(parent)   from utils import Benchmark, FTTimeSeriesBuilder, TSFreshBuilder In\u00a0[3]: Copied! <pre>data_all = getml.data.DataFrame.from_csv(\n    \"https://static.getml.com/datasets/robotarm/robot-demo.csv\", \"data_all\"\n)\n</pre> data_all = getml.data.DataFrame.from_csv(     \"https://static.getml.com/datasets/robotarm/robot-demo.csv\", \"data_all\" ) <pre>Downloading robot-demo.csv...\n 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n</pre> In\u00a0[4]: Copied! <pre>data_all\n</pre> data_all Out[4]:  name            3            4            5            6            7            8            9           10           11           12           13           14           15           16           17           18           19           20           21           22           23           24           25           26           27           28           29           30           31           32           33           34           35           36           37           38           39           40           41           42           43           44           45           46           47           48           49           50           51           52           53           54           55           56           57           58           59           60           61           62           63           64           65           66           67           68           69           70           71           72           73           74           75           76           77           78           79           80           81           82           83           84           85           86           98           99          100          101          102          103          104          105          106          f_x          f_y          f_z  role unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float 0 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3274 0.9605 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1233 -6.5483 -2.8045 -0.8296 0.07625 -0.1906 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1983 0.7699 0.41 0.08279 -1.4094 0.786 -0.3682 0 0 0 0 0 0 -22.654 -11.503 -18.673 -3.5155 5.8354 -2.05 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.069 48.009 0.9668 47.834 47.925 47.818 47.834 47.955 47.971 -11.03 6.9 -7.33 1 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3274 0.9604 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1188 -6.5506 -2.8404 -0.8281 0.06405 -0.1998 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1983 0.7699 0.41 0.0828 -1.4094 0.7859 -0.3682 0 0 0 0 0 0 -21.627 -11.046 -18.66 -3.5395 5.7577 -1.9805 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.009 48.009 0.8594 47.834 47.925 47.818 47.834 47.955 47.971 -10.848 6.7218 -7.4427 2 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3274 0.9605 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1099 -6.5438 -2.8 -0.8205 0.07473 -0.183 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1922 0.7699 0.41 0.08279 -1.4094 0.7859 -0.3682 0 0 0 0 0 0 -23.843 -12.127 -18.393 -3.6453 5.978 -1.9978 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.009 48.069 0.931 47.879 47.925 47.818 47.834 47.955 47.971 -10.666 6.5436 -7.5555 3 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3273 0.9604 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1233 -6.5483 -2.8224 -0.8266 0.07168 -0.1998 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1967 0.7699 0.41 0.08275 -1.4094 0.786 -0.3681 0 0 0 0 0 0 -21.772 -10.872 -18.691 -3.5512 5.6648 -1.9976 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.069 48.069 0.931 47.879 47.925 47.818 47.834 47.955 47.971 -10.507 6.4533 -7.65 4 3.4098 -0.3274 0.9604 -3.7436 -1.0191 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 -1.2042 0.02167 0 3.4098 -0.3274 0.9604 -3.7437 -1.0191 -6.0205 0 0 0 0 0 0 0.1255 -6.5394 -2.8 -0.8327 0.07473 -0.1952 0.1211 -6.5483 -2.8157 -0.8327 0.07015 -0.1922 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 -22.823 -11.645 -18.524 -3.5305 5.8712 -2.0096 0.7699 0.41 0.08278 -1.4094 0.786 -0.3681 0 0 0 0 0 0 48.069 48.069 0.8952 47.879 47.925 47.818 47.834 47.955 47.971 -10.413 6.6267 -7.69 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 14996 3.0837 -0.8836 1.4501 -2.2102 -1.559 -5.3265 -0.03151 -0.05375 0.04732 0.1482 -0.05218 0.06706 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3694 -4.1879 -1.1847 -0.09441 -0.1568 0.1898 1.1605 -42.951 -19.023 -2.6343 0.1551 -0.1338 3.0836 -0.8836 1.4503 -2.2101 -1.5591 -5.3263 -0.03347 -0.05585 0.04805 0.151 -0.05513 0.07114 -0.3564 -6.0394 -2.3001 -0.2181 -0.1159 0.09608 -0.3632 -6.0394 -2.3023 -0.212 -0.125 0.1113 0.7116 0.06957 0.06036 -0.8506 2.9515 -0.03352 -0.03558 -0.03029 0.002444 -0.04208 0.1458 -0.1098 -0.8784 -0.07291 -37.584 0.0001132 -2.1031 0.03318 0.7117 0.0697 0.06044 -0.8511 2.951 -0.03356 -0.03508 -0.02849 0.001571 -0.03951 0.1442 -0.1036 48.069 48.009 0.8952 47.818 47.834 47.818 47.803 47.94 47.94 10.84 -1.41 16.14 14997 3.0835 -0.884 1.4505 -2.2091 -1.5594 -5.326 -0.02913 -0.0497 0.04376 0.137 -0.04825 0.062 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3677 -4.1837 -1.1874 -0.09682 -0.1562 0.189 1.1592 -42.937 -19.023 -2.6331 0.1545 -0.1338 3.0833 -0.8841 1.4507 -2.209 -1.5596 -5.3258 -0.02909 -0.04989 0.04198 0.1481 -0.05465 0.06249 -0.3161 -6.1179 -2.253 -0.3752 -0.03965 0.08693 -0.3273 -6.1022 -2.2597 -0.366 -0.05033 0.0915 0.7114 0.06932 0.06039 -0.8497 2.953 -0.03359 -0.0335 -0.02723 0.001208 -0.04242 0.1428 -0.0967 -2.7137 0.8552 -38.514 -0.6088 -3.2383 -0.9666 0.7114 0.06948 0.06045 -0.8503 2.9525 -0.03359 -0.03246 -0.02633 0.001469 -0.03657 0.1333 -0.09571 48.009 48.009 0.8594 47.818 47.834 47.818 47.803 47.94 47.94 10.857 -1.52 15.943 14998 3.0833 -0.8844 1.4508 -2.208 -1.5598 -5.3256 -0.02676 -0.04565 0.04019 0.1258 -0.04431 0.05695 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3659 -4.1797 -1.1901 -0.09922 -0.1555 0.1881 1.1579 -42.924 -19.023 -2.6321 0.154 -0.1338 3.0831 -0.8844 1.451 -2.2078 -1.56 -5.3253 -0.02776 -0.04382 0.03652 0.1295 -0.05064 0.04818 -0.343 -6.2569 -2.1566 -0.3035 0.00305 0.1434 -0.3385 -6.2322 -2.1589 -0.302 -0.00915 0.1571 0.7111 0.06912 0.06039 -0.849 2.9544 -0.0337 -0.02911 -0.02589 0.001292 -0.04046 0.1246 -0.08058 4.2749 1.0128 -36.412 -1.2811 -0.4296 -1.1013 0.7112 0.06928 0.06046 -0.8495 2.9538 -0.03362 -0.02984 -0.02417 0.001364 -0.03362 0.1224 -0.08786 48.009 48.009 0.931 47.818 47.834 47.818 47.803 47.94 47.94 10.89 -1.74 15.55 14999 3.0831 -0.8847 1.4511 -2.2071 -1.5602 -5.3251 -0.02438 -0.0416 0.03662 0.1147 -0.04038 0.0519 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3642 -4.1758 -1.1928 -0.1016 -0.1548 0.1873 1.1568 -42.912 -19.023 -2.6311 0.1535 -0.1338 3.0829 -0.8848 1.4513 -2.2068 -1.5604 -5.3249 -0.02149 -0.04059 0.03417 0.1202 -0.0395 0.04178 -0.4237 -6.2703 -2.0939 -0.302 -0.01372 0.1739 -0.4125 -6.2569 -2.0916 -0.2943 -0.02898 0.1891 0.7109 0.06894 0.06039 -0.8484 2.9557 -0.03384 -0.02738 -0.01982 0.001031 -0.03028 0.1157 -0.06702 11.518 1.5002 -39.314 -1.8671 -0.3734 -0.5733 0.7109 0.06909 0.06047 -0.8488 2.955 -0.03364 -0.02721 -0.02201 0.001255 -0.03067 0.1115 -0.08003 48.009 48.009 0.931 47.818 47.834 47.818 47.803 47.94 47.94 11.29 -1.4601 15.743 15000 3.0829 -0.885 1.4514 -2.2062 -1.5605 -5.3247 -0.02201 -0.03755 0.03305 0.1035 -0.03645 0.04684 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3624 -4.172 -1.1955 -0.1041 -0.1542 0.1864 1.1558 -42.901 -19.023 -2.6302 0.1531 -0.1338 3.0827 -0.8851 1.4516 -2.2059 -1.5607 -5.3246 -0.02096 -0.03808 0.02958 0.1171 -0.03289 0.03883 -0.417 -6.2434 -2.058 -0.4102 -0.04728 0.1967 -0.4237 -6.2367 -2.0714 -0.4163 -0.0671 0.2059 0.7107 0.06878 0.06041 -0.8478 2.9567 -0.03382 -0.02535 -0.01854 0.001614 -0.02421 0.11 -0.06304 15.099 2.936 -39.068 -1.9402 0.139 -0.2674 0.7107 0.06893 0.06048 -0.8482 2.9561 -0.03367 -0.02458 -0.01986 0.001142 -0.0277 0.1007 -0.07221 48.009 48.069 0.8952 47.818 47.834 47.818 47.803 47.94 47.955 11.69 -1.1801 15.937 <p>     15001 rows x 96 columns     memory usage: 11.52 MB     name: data_all     type: getml.DataFrame </p> In\u00a0[5]: Copied! <pre>only_use = [\"30\", \"34\", \"37\", \"38\", \"4\", \"59\", \"61\", \"7\", \"77\", \"78\"]\n\ndata_all.set_role([\"f_x\"], getml.data.roles.target)\ndata_all.set_role(only_use, getml.data.roles.numerical)\n</pre> only_use = [\"30\", \"34\", \"37\", \"38\", \"4\", \"59\", \"61\", \"7\", \"77\", \"78\"]  data_all.set_role([\"f_x\"], getml.data.roles.target) data_all.set_role(only_use, getml.data.roles.numerical) <p>This is what the data set looks like:</p> In\u00a0[6]: Copied! <pre>data_all\n</pre> data_all Out[6]:  name     f_x        30        34        37        38         4        59        61         7        77        78            3            5            6            8            9           10           11           12           13           14           15           16           17           18           19           20           21           22           23           24           25           26           27           28           29           31           32           33           35           36           39           40           41           42           43           44           45           46           47           48           49           50           51           52           53           54           55           56           57           58           60           62           63           64           65           66           67           68           69           70           71           72           73           74           75           76           79           80           81           82           83           84           85           86           98           99          100          101          102          103          104          105          106          f_y          f_z  role  target numerical numerical numerical numerical numerical numerical numerical numerical numerical numerical unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float unused_float 0 -11.03 -1.2042 -0.3274 -1.0191 -6.0205 -0.3274 0.08279 0.786 -1.0191 0.08278 -1.4094 3.4098 0.9604 -3.7436 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 0.02167 0 3.4098 0.9605 -3.7437 0 0 0 0 0 0 0.1233 -6.5483 -2.8045 -0.8296 0.07625 -0.1906 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1983 0.7699 0.41 -1.4094 -0.3682 0 0 0 0 0 0 -22.654 -11.503 -18.673 -3.5155 5.8354 -2.05 0.7699 0.41 0.786 -0.3681 0 0 0 0 0 0 48.069 48.009 0.9668 47.834 47.925 47.818 47.834 47.955 47.971 6.9 -7.33 1 -10.848 -1.2042 -0.3274 -1.0191 -6.0205 -0.3274 0.0828 0.7859 -1.0191 0.08278 -1.4094 3.4098 0.9604 -3.7436 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 0.02167 0 3.4098 0.9604 -3.7437 0 0 0 0 0 0 0.1188 -6.5506 -2.8404 -0.8281 0.06405 -0.1998 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1983 0.7699 0.41 -1.4094 -0.3682 0 0 0 0 0 0 -21.627 -11.046 -18.66 -3.5395 5.7577 -1.9805 0.7699 0.41 0.786 -0.3681 0 0 0 0 0 0 48.009 48.009 0.8594 47.834 47.925 47.818 47.834 47.955 47.971 6.7218 -7.4427 2 -10.666 -1.2042 -0.3274 -1.0191 -6.0205 -0.3274 0.08279 0.7859 -1.0191 0.08278 -1.4094 3.4098 0.9604 -3.7436 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 0.02167 0 3.4098 0.9605 -3.7437 0 0 0 0 0 0 0.1099 -6.5438 -2.8 -0.8205 0.07473 -0.183 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1922 0.7699 0.41 -1.4094 -0.3682 0 0 0 0 0 0 -23.843 -12.127 -18.393 -3.6453 5.978 -1.9978 0.7699 0.41 0.786 -0.3681 0 0 0 0 0 0 48.009 48.069 0.931 47.879 47.925 47.818 47.834 47.955 47.971 6.5436 -7.5555 3 -10.507 -1.2042 -0.3273 -1.0191 -6.0205 -0.3274 0.08275 0.786 -1.0191 0.08278 -1.4094 3.4098 0.9604 -3.7436 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 0.02167 0 3.4098 0.9604 -3.7437 0 0 0 0 0 0 0.1233 -6.5483 -2.8224 -0.8266 0.07168 -0.1998 0.1211 -6.5483 -2.8157 -0.8281 0.07015 -0.1967 0.7699 0.41 -1.4094 -0.3681 0 0 0 0 0 0 -21.772 -10.872 -18.691 -3.5512 5.6648 -1.9976 0.7699 0.41 0.786 -0.3681 0 0 0 0 0 0 48.069 48.069 0.931 47.879 47.925 47.818 47.834 47.955 47.971 6.4533 -7.65 4 -10.413 -1.2042 -0.3274 -1.0191 -6.0205 -0.3274 0.08278 0.786 -1.0191 0.08278 -1.4094 3.4098 0.9604 -3.7436 -6.0205 0 0 0 0 0 0 0 0 0 0 0 0 8.38e-17 -4.8116 -1.4033 -0.1369 0.002472 0 9.803e-16 -55.642 -16.312 0.02167 0 3.4098 0.9604 -3.7437 0 0 0 0 0 0 0.1255 -6.5394 -2.8 -0.8327 0.07473 -0.1952 0.1211 -6.5483 -2.8157 -0.8327 0.07015 -0.1922 0.7699 0.41 -1.4094 -0.3681 0 0 0 0 0 0 -22.823 -11.645 -18.524 -3.5305 5.8712 -2.0096 0.7699 0.41 0.786 -0.3681 0 0 0 0 0 0 48.069 48.069 0.8952 47.879 47.925 47.818 47.834 47.955 47.971 6.6267 -7.69 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 14996 10.84 -2.6343 -0.8836 -1.5591 -5.3263 -0.8836 0.06036 2.9515 -1.559 0.06044 -0.8511 3.0837 1.4501 -2.2102 -5.3265 -0.03151 -0.05375 0.04732 0.1482 -0.05218 0.06706 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3694 -4.1879 -1.1847 -0.09441 -0.1568 0.1898 1.1605 -42.951 -19.023 0.1551 -0.1338 3.0836 1.4503 -2.2101 -0.03347 -0.05585 0.04805 0.151 -0.05513 0.07114 -0.3564 -6.0394 -2.3001 -0.2181 -0.1159 0.09608 -0.3632 -6.0394 -2.3023 -0.212 -0.125 0.1113 0.7116 0.06957 -0.8506 -0.03352 -0.03558 -0.03029 0.002444 -0.04208 0.1458 -0.1098 -0.8784 -0.07291 -37.584 0.0001132 -2.1031 0.03318 0.7117 0.0697 2.951 -0.03356 -0.03508 -0.02849 0.001571 -0.03951 0.1442 -0.1036 48.069 48.009 0.8952 47.818 47.834 47.818 47.803 47.94 47.94 -1.41 16.14 14997 10.857 -2.6331 -0.8841 -1.5596 -5.3258 -0.884 0.06039 2.953 -1.5594 0.06045 -0.8503 3.0835 1.4505 -2.2091 -5.326 -0.02913 -0.0497 0.04376 0.137 -0.04825 0.062 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3677 -4.1837 -1.1874 -0.09682 -0.1562 0.189 1.1592 -42.937 -19.023 0.1545 -0.1338 3.0833 1.4507 -2.209 -0.02909 -0.04989 0.04198 0.1481 -0.05465 0.06249 -0.3161 -6.1179 -2.253 -0.3752 -0.03965 0.08693 -0.3273 -6.1022 -2.2597 -0.366 -0.05033 0.0915 0.7114 0.06932 -0.8497 -0.03359 -0.0335 -0.02723 0.001208 -0.04242 0.1428 -0.0967 -2.7137 0.8552 -38.514 -0.6088 -3.2383 -0.9666 0.7114 0.06948 2.9525 -0.03359 -0.03246 -0.02633 0.001469 -0.03657 0.1333 -0.09571 48.009 48.009 0.8594 47.818 47.834 47.818 47.803 47.94 47.94 -1.52 15.943 14998 10.89 -2.6321 -0.8844 -1.56 -5.3253 -0.8844 0.06039 2.9544 -1.5598 0.06046 -0.8495 3.0833 1.4508 -2.208 -5.3256 -0.02676 -0.04565 0.04019 0.1258 -0.04431 0.05695 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3659 -4.1797 -1.1901 -0.09922 -0.1555 0.1881 1.1579 -42.924 -19.023 0.154 -0.1338 3.0831 1.451 -2.2078 -0.02776 -0.04382 0.03652 0.1295 -0.05064 0.04818 -0.343 -6.2569 -2.1566 -0.3035 0.00305 0.1434 -0.3385 -6.2322 -2.1589 -0.302 -0.00915 0.1571 0.7111 0.06912 -0.849 -0.0337 -0.02911 -0.02589 0.001292 -0.04046 0.1246 -0.08058 4.2749 1.0128 -36.412 -1.2811 -0.4296 -1.1013 0.7112 0.06928 2.9538 -0.03362 -0.02984 -0.02417 0.001364 -0.03362 0.1224 -0.08786 48.009 48.009 0.931 47.818 47.834 47.818 47.803 47.94 47.94 -1.74 15.55 14999 11.29 -2.6311 -0.8848 -1.5604 -5.3249 -0.8847 0.06039 2.9557 -1.5602 0.06047 -0.8488 3.0831 1.4511 -2.2071 -5.3251 -0.02438 -0.0416 0.03662 0.1147 -0.04038 0.0519 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3642 -4.1758 -1.1928 -0.1016 -0.1548 0.1873 1.1568 -42.912 -19.023 0.1535 -0.1338 3.0829 1.4513 -2.2068 -0.02149 -0.04059 0.03417 0.1202 -0.0395 0.04178 -0.4237 -6.2703 -2.0939 -0.302 -0.01372 0.1739 -0.4125 -6.2569 -2.0916 -0.2943 -0.02898 0.1891 0.7109 0.06894 -0.8484 -0.03384 -0.02738 -0.01982 0.001031 -0.03028 0.1157 -0.06702 11.518 1.5002 -39.314 -1.8671 -0.3734 -0.5733 0.7109 0.06909 2.955 -0.03364 -0.02721 -0.02201 0.001255 -0.03067 0.1115 -0.08003 48.009 48.009 0.931 47.818 47.834 47.818 47.803 47.94 47.94 -1.4601 15.743 15000 11.69 -2.6302 -0.8851 -1.5607 -5.3246 -0.885 0.06041 2.9567 -1.5605 0.06048 -0.8482 3.0829 1.4514 -2.2062 -5.3247 -0.02201 -0.03755 0.03305 0.1035 -0.03645 0.04684 0.2969 0.5065 -0.4459 -1.3963 0.4916 -0.6319 -0.3624 -4.172 -1.1955 -0.1041 -0.1542 0.1864 1.1558 -42.901 -19.023 0.1531 -0.1338 3.0827 1.4516 -2.2059 -0.02096 -0.03808 0.02958 0.1171 -0.03289 0.03883 -0.417 -6.2434 -2.058 -0.4102 -0.04728 0.1967 -0.4237 -6.2367 -2.0714 -0.4163 -0.0671 0.2059 0.7107 0.06878 -0.8478 -0.03382 -0.02535 -0.01854 0.001614 -0.02421 0.11 -0.06304 15.099 2.936 -39.068 -1.9402 0.139 -0.2674 0.7107 0.06893 2.9561 -0.03367 -0.02458 -0.01986 0.001142 -0.0277 0.1007 -0.07221 48.009 48.069 0.8952 47.818 47.834 47.818 47.803 47.94 47.955 -1.1801 15.937 <p>     15001 rows x 96 columns     memory usage: 11.52 MB     name: data_all     type: getml.DataFrame </p> In\u00a0[7]: Copied! <pre>split = getml.data.split.time(data_all, \"rowid\", test=10500)\nsplit\n</pre> split = getml.data.split.time(data_all, \"rowid\", test=10500) split Out[7]: 0 train 1 train 2 train 3 train 4 train ... <p>     15001 rows          type: StringColumnView </p> In\u00a0[8]: Copied! <pre>time_series = getml.data.TimeSeries(\n    population=data_all,\n    split=split,\n    time_stamps=\"rowid\",\n    lagged_targets=False,\n    memory=30,\n)\n\ntime_series\n</pre> time_series = getml.data.TimeSeries(     population=data_all,     split=split,     time_stamps=\"rowid\",     lagged_targets=False,     memory=30, )  time_series Out[8]: data model diagram data_allpopulationrowid &lt;= rowidMemory: 30 time steps staging data frames staging table               0 population POPULATION__STAGING_TABLE_1 1 data_all DATA_ALL__STAGING_TABLE_2 container population subset name      rows type 0 test data_all 4501 View 1 train data_all 10500 View peripheral name      rows type 0 data_all 15001 View In\u00a0[9]: Copied! <pre>fast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.SquareLoss,\n)\n</pre> fast_prop = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.SquareLoss, ) In\u00a0[10]: Copied! <pre>pipe_fp_fl = getml.pipeline.Pipeline(\n    data_model=time_series.data_model,\n    feature_learners=[fast_prop],\n    tags=[\"feature learning\", \"fastprop\"],\n)\n</pre> pipe_fp_fl = getml.pipeline.Pipeline(     data_model=time_series.data_model,     feature_learners=[fast_prop],     tags=[\"feature learning\", \"fastprop\"], ) In\u00a0[11]: Copied! <pre>pipe_fp_fl.check(time_series.train)\n</pre> pipe_fp_fl.check(time_series.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\n</pre> In\u00a0[12]: Copied! <pre>benchmark = Benchmark()\n</pre> benchmark = Benchmark() In\u00a0[13]: Copied! <pre>with benchmark(\"fastprop\"):\n    pipe_fp_fl.fit(time_series.train)\n    fastprop_train = pipe_fp_fl.transform(time_series.train, df_name=\"fastprop_train\")\n</pre> with benchmark(\"fastprop\"):     pipe_fp_fl.fit(time_series.train)     fastprop_train = pipe_fp_fl.transform(time_series.train, df_name=\"fastprop_train\") <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 134 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:0.012022\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> In\u00a0[14]: Copied! <pre>fastprop_test = pipe_fp_fl.transform(time_series.test, df_name=\"fastprop_test\")\n</pre> fastprop_test = pipe_fp_fl.transform(time_series.test, df_name=\"fastprop_test\") <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> In\u00a0[15]: Copied! <pre>predictor = getml.predictors.XGBoostRegressor()\n\npipe_fp_pr = getml.pipeline.Pipeline(\n    tags=[\"prediction\", \"fastprop\"], predictors=[predictor]\n)\n</pre> predictor = getml.predictors.XGBoostRegressor()  pipe_fp_pr = getml.pipeline.Pipeline(     tags=[\"prediction\", \"fastprop\"], predictors=[predictor] ) In\u00a0[16]: Copied! <pre>pipe_fp_pr.check(fastprop_train)\n</pre> pipe_fp_pr.check(fastprop_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 0 issues labeled INFO and 5 issues labeled WARNING.\n</pre> Out[16]: type    label                   message                          0 WARNING COLUMN SHOULD BE UNUSED All non-NULL entries in column 'feature_1_123' in POPULATION__STAGING_TABLE_1 are equal to each other. You should consider setting its role to unused_float or using it for comparison only (you can do the latter by setting a unit that contains 'comparison only'). 1 WARNING COLUMN SHOULD BE UNUSED All non-NULL entries in column 'feature_1_125' in POPULATION__STAGING_TABLE_1 are equal to each other. You should consider setting its role to unused_float or using it for comparison only (you can do the latter by setting a unit that contains 'comparison only'). 2 WARNING COLUMN SHOULD BE UNUSED All non-NULL entries in column 'feature_1_128' in POPULATION__STAGING_TABLE_1 are equal to each other. You should consider setting its role to unused_float or using it for comparison only (you can do the latter by setting a unit that contains 'comparison only'). 3 WARNING COLUMN SHOULD BE UNUSED All non-NULL entries in column 'feature_1_129' in POPULATION__STAGING_TABLE_1 are equal to each other. You should consider setting its role to unused_float or using it for comparison only (you can do the latter by setting a unit that contains 'comparison only'). 4 WARNING COLUMN SHOULD BE UNUSED All non-NULL entries in column 'feature_1_132' in POPULATION__STAGING_TABLE_1 are equal to each other. You should consider setting its role to unused_float or using it for comparison only (you can do the latter by setting a unit that contains 'comparison only'). In\u00a0[17]: Copied! <pre>pipe_fp_pr.fit(fastprop_train)\n</pre> pipe_fp_pr.fit(fastprop_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 0 issues labeled INFO and 5 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:05, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:4.613744\n\n</pre> Out[17]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['prediction', 'fastprop'])</pre> In\u00a0[18]: Copied! <pre>pipe_fp_pr.score(fastprop_test)\n</pre> pipe_fp_pr.score(fastprop_test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[18]: date time           set used       target     mae    rmse rsquared 0 2024-02-21 16:13:41 fastprop_train f_x 0.4383 0.5764 0.9963 1 2024-02-21 16:13:41 fastprop_test f_x 0.5515 0.7236 0.9951 In\u00a0[19]: Copied! <pre>data_train = time_series.train.population.to_df(\"data_train\")\ndata_test = time_series.test.population.to_df(\"data_test\")\n</pre> data_train = time_series.train.population.to_df(\"data_train\") data_test = time_series.test.population.to_df(\"data_test\") In\u00a0[20]: Copied! <pre>dfs_pandas = {}\n\nfor df in [data_train, data_test, data_all]:\n    dfs_pandas[df.name] = df.to_pandas()\n    delete_columns = [\n        col for col in dfs_pandas[df.name].columns if col not in only_use + [\"f_x\"]\n    ]\n    for col in delete_columns:\n        del dfs_pandas[df.name][col]\n    dfs_pandas[df.name][\"id\"] = 1\n    dfs_pandas[df.name][\"ds\"] = pd.to_datetime(\n        np.arange(0, dfs_pandas[df.name].shape[0]), unit=\"s\"\n    )\n</pre> dfs_pandas = {}  for df in [data_train, data_test, data_all]:     dfs_pandas[df.name] = df.to_pandas()     delete_columns = [         col for col in dfs_pandas[df.name].columns if col not in only_use + [\"f_x\"]     ]     for col in delete_columns:         del dfs_pandas[df.name][col]     dfs_pandas[df.name][\"id\"] = 1     dfs_pandas[df.name][\"ds\"] = pd.to_datetime(         np.arange(0, dfs_pandas[df.name].shape[0]), unit=\"s\"     ) In\u00a0[21]: Copied! <pre>dfs_pandas[\"data_train\"]\n</pre> dfs_pandas[\"data_train\"] Out[21]: 30 34 37 38 4 59 61 7 77 78 f_x id ds 0 -1.2042 -0.32739 -1.0191 -6.0205 -0.32737 0.082791 0.78597 -1.0191 0.082782 -1.4094 -11.0300 1 1970-01-01 00:00:00 1 -1.2042 -0.32739 -1.0191 -6.0205 -0.32737 0.082800 0.78592 -1.0191 0.082782 -1.4094 -10.8480 1 1970-01-01 00:00:01 2 -1.2042 -0.32737 -1.0191 -6.0205 -0.32737 0.082786 0.78594 -1.0191 0.082782 -1.4094 -10.6660 1 1970-01-01 00:00:02 3 -1.2042 -0.32734 -1.0191 -6.0205 -0.32737 0.082755 0.78599 -1.0191 0.082782 -1.4094 -10.5070 1 1970-01-01 00:00:03 4 -1.2042 -0.32736 -1.0191 -6.0205 -0.32737 0.082782 0.78597 -1.0191 0.082782 -1.4094 -10.4130 1 1970-01-01 00:00:04 ... ... ... ... ... ... ... ... ... ... ... ... ... ... 10495 -1.1446 -0.37311 -1.0486 -5.9532 -0.37326 0.087343 0.90793 -1.0488 0.087468 -1.4162 -9.7673 1 1970-01-01 02:54:55 10496 -1.1349 -0.37103 -1.0472 -5.9564 -0.37108 0.087241 0.90199 -1.0474 0.087274 -1.4160 -9.9200 1 1970-01-01 02:54:56 10497 -1.1255 -0.36889 -1.0458 -5.9596 -0.36896 0.087055 0.89618 -1.0460 0.087082 -1.4158 -9.7743 1 1970-01-01 02:54:57 10498 -1.1163 -0.36680 -1.0444 -5.9627 -0.36689 0.086907 0.89034 -1.0447 0.086893 -1.4155 -8.6109 1 1970-01-01 02:54:58 10499 -1.1072 -0.36477 -1.0430 -5.9657 -0.36487 0.086720 0.88476 -1.0434 0.086706 -1.4153 -8.4345 1 1970-01-01 02:54:59 <p>10500 rows \u00d7 13 columns</p> In\u00a0[22]: Copied! <pre>ft_builder = FTTimeSeriesBuilder(\n    num_features=200,\n    horizon=pd.Timedelta(seconds=0),\n    memory=pd.Timedelta(seconds=15),\n    column_id=\"id\",\n    time_stamp=\"ds\",\n    target=\"f_x\",\n)\n</pre> ft_builder = FTTimeSeriesBuilder(     num_features=200,     horizon=pd.Timedelta(seconds=0),     memory=pd.Timedelta(seconds=15),     column_id=\"id\",     time_stamp=\"ds\",     target=\"f_x\", ) In\u00a0[23]: Copied! <pre>with benchmark(\"featuretools\"):\n    featuretools_train = ft_builder.fit(dfs_pandas[\"data_train\"])\n\nfeaturetools_test = ft_builder.transform(dfs_pandas[\"data_test\"])\n</pre> with benchmark(\"featuretools\"):     featuretools_train = ft_builder.fit(dfs_pandas[\"data_train\"])  featuretools_test = ft_builder.transform(dfs_pandas[\"data_test\"]) <pre>featuretools: Trying features...\nSelecting the best out of 442 features...\nTime taken: 0h:16m:25.820024\n\n</pre> In\u00a0[24]: Copied! <pre>featuretools_train\n</pre> featuretools_train Out[24]: MIN(peripheral.37) MIN(peripheral.7) FIRST(peripheral.37) FIRST(peripheral.7) MEDIAN(peripheral.37) MEDIAN(peripheral.7) MEAN(peripheral.37) MEAN(peripheral.7) SUM(peripheral.37) SUM(peripheral.7) ... NUM_ZERO_CROSSINGS(peripheral.77) NUM_ZERO_CROSSINGS(peripheral.59) IS_MONOTONICALLY_DECREASING(peripheral.77) TIME_SINCE_LAST_MAX(peripheral.ds, 4) TIME_SINCE_LAST_MAX(peripheral.ds, 30) TIME_SINCE_LAST_MAX(peripheral.ds, 77) TIME_SINCE_LAST_MAX(peripheral.ds, 7) f_x id ds _featuretools_index 0 -1.0191 -1.0191 -1.0191 -1.0191 -1.0191 -1.0191 -1.019100 -1.019100 -1.0191 -1.0191 ... 0 0 True 1.708532e+09 1.708532e+09 1.708532e+09 1.708532e+09 -11.0300 1 1970-01-01 00:00:00 1 -1.0191 -1.0191 -1.0191 -1.0191 -1.0191 -1.0191 -1.019100 -1.019100 -2.0382 -2.0382 ... 0 0 True 1.708532e+09 1.708532e+09 1.708532e+09 1.708532e+09 -10.8480 1 1970-01-01 00:00:01 2 -1.0191 -1.0191 -1.0191 -1.0191 -1.0191 -1.0191 -1.019100 -1.019100 -3.0573 -3.0573 ... 0 0 True 1.708532e+09 1.708532e+09 1.708532e+09 1.708532e+09 -10.6660 1 1970-01-01 00:00:02 3 -1.0191 -1.0191 -1.0191 -1.0191 -1.0191 -1.0191 -1.019100 -1.019100 -4.0764 -4.0764 ... 0 0 True 1.708532e+09 1.708532e+09 1.708532e+09 1.708532e+09 -10.5070 1 1970-01-01 00:00:03 4 -1.0191 -1.0191 -1.0191 -1.0191 -1.0191 -1.0191 -1.019100 -1.019100 -5.0955 -5.0955 ... 0 0 True 1.708532e+09 1.708532e+09 1.708532e+09 1.708532e+09 -10.4130 1 1970-01-01 00:00:04 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 10495 -1.0716 -1.0721 -1.0716 -1.0721 -1.0596 -1.0596 -1.059793 -1.059927 -15.8969 -15.8989 ... 0 0 True 1.708522e+09 1.708522e+09 1.708522e+09 1.708522e+09 -9.7673 1 1970-01-01 02:54:55 10496 -1.0698 -1.0702 -1.0698 -1.0702 -1.0580 -1.0580 -1.058167 -1.058280 -15.8725 -15.8742 ... 0 0 True 1.708522e+09 1.708522e+09 1.708522e+09 1.708522e+09 -9.9200 1 1970-01-01 02:54:56 10497 -1.0681 -1.0684 -1.0681 -1.0684 -1.0565 -1.0563 -1.056567 -1.056667 -15.8485 -15.8500 ... 0 0 True 1.708522e+09 1.708522e+09 1.708522e+09 1.708522e+09 -9.7743 1 1970-01-01 02:54:57 10498 -1.0662 -1.0665 -1.0662 -1.0665 -1.0549 -1.0548 -1.054987 -1.055087 -15.8248 -15.8263 ... 0 0 True 1.708522e+09 1.708522e+09 1.708522e+09 1.708522e+09 -8.6109 1 1970-01-01 02:54:58 10499 -1.0644 -1.0648 -1.0644 -1.0648 -1.0534 -1.0532 -1.053440 -1.053547 -15.8016 -15.8032 ... 0 0 True 1.708522e+09 1.708522e+09 1.708522e+09 1.708522e+09 -8.4345 1 1970-01-01 02:54:59 <p>10500 rows \u00d7 203 columns</p> In\u00a0[25]: Copied! <pre>roles = {\n    getml.data.roles.target: [\"f_x\"],\n    getml.data.roles.join_key: [\"id\"],\n    getml.data.roles.time_stamp: [\"ds\"],\n}\n\ndf_featuretools_train = getml.data.DataFrame.from_pandas(\n    featuretools_train, name=\"featuretools_train\", roles=roles\n)\n\ndf_featuretools_test = getml.data.DataFrame.from_pandas(\n    featuretools_test, name=\"featuretools_test\", roles=roles\n)\n</pre> roles = {     getml.data.roles.target: [\"f_x\"],     getml.data.roles.join_key: [\"id\"],     getml.data.roles.time_stamp: [\"ds\"], }  df_featuretools_train = getml.data.DataFrame.from_pandas(     featuretools_train, name=\"featuretools_train\", roles=roles )  df_featuretools_test = getml.data.DataFrame.from_pandas(     featuretools_test, name=\"featuretools_test\", roles=roles ) In\u00a0[26]: Copied! <pre>df_featuretools_train.set_role(\n    df_featuretools_train.roles.unused, getml.data.roles.numerical\n)\n\ndf_featuretools_test.set_role(\n    df_featuretools_test.roles.unused, getml.data.roles.numerical\n)\n</pre> df_featuretools_train.set_role(     df_featuretools_train.roles.unused, getml.data.roles.numerical )  df_featuretools_test.set_role(     df_featuretools_test.roles.unused, getml.data.roles.numerical ) In\u00a0[27]: Copied! <pre>predictor = getml.predictors.XGBoostRegressor()\n\npipe_ft_pr = getml.pipeline.Pipeline(\n    tags=[\"prediction\", \"featuretools\"], predictors=[predictor]\n)\n\npipe_ft_pr\n</pre> predictor = getml.predictors.XGBoostRegressor()  pipe_ft_pr = getml.pipeline.Pipeline(     tags=[\"prediction\", \"featuretools\"], predictors=[predictor] )  pipe_ft_pr Out[27]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['prediction', 'featuretools'])</pre> In\u00a0[28]: Copied! <pre>pipe_ft_pr.fit(df_featuretools_train)\n</pre> pipe_ft_pr.fit(df_featuretools_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:05, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:5.114944\n\n</pre> Out[28]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['prediction', 'featuretools'])</pre> In\u00a0[29]: Copied! <pre>pipe_ft_pr.score(df_featuretools_test)\n</pre> pipe_ft_pr.score(df_featuretools_test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[29]: date time           set used           target     mae    rmse rsquared 0 2024-02-21 16:37:13 featuretools_train f_x 0.4387 0.5826 0.9962 1 2024-02-21 16:37:14 featuretools_test f_x 0.5718 0.7481 0.9948 In\u00a0[30]: Copied! <pre>tsfresh_builder = TSFreshBuilder(\n    num_features=200,\n    memory=15,\n    column_id=\"id\",\n    time_stamp=\"ds\",\n    target=\"f_x\",\n)\n</pre> tsfresh_builder = TSFreshBuilder(     num_features=200,     memory=15,     column_id=\"id\",     time_stamp=\"ds\",     target=\"f_x\", ) In\u00a0[31]: Copied! <pre>with benchmark(\"tsfresh\"):\n    tsfresh_train = tsfresh_builder.fit(dfs_pandas[\"data_train\"])\n\ntsfresh_test = tsfresh_builder.transform(dfs_pandas[\"data_test\"])\n</pre> with benchmark(\"tsfresh\"):     tsfresh_train = tsfresh_builder.fit(dfs_pandas[\"data_train\"])  tsfresh_test = tsfresh_builder.transform(dfs_pandas[\"data_test\"]) <pre>Rolling: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:03&lt;00:00, 10.71it/s]\nFeature Extraction: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:13&lt;00:00,  3.01it/s]\nFeature Extraction: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:13&lt;00:00,  3.08it/s]\n</pre> <pre>Selecting the best out of 130 features...\nTime taken: 0h:0m:34.618933\n\n</pre> <pre>Rolling: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:01&lt;00:00, 26.85it/s]\nFeature Extraction: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:05&lt;00:00,  7.43it/s]\nFeature Extraction: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40/40 [00:05&lt;00:00,  7.22it/s]\n</pre> In\u00a0[32]: Copied! <pre>roles = {\n    getml.data.roles.target: [\"f_x\"],\n    getml.data.roles.join_key: [\"id\"],\n    getml.data.roles.time_stamp: [\"ds\"],\n}\n\ndf_tsfresh_train = getml.data.DataFrame.from_pandas(\n    tsfresh_train, name=\"tsfresh_train\", roles=roles\n)\n\ndf_tsfresh_test = getml.data.DataFrame.from_pandas(\n    tsfresh_test, name=\"tsfresh_test\", roles=roles\n)\n</pre> roles = {     getml.data.roles.target: [\"f_x\"],     getml.data.roles.join_key: [\"id\"],     getml.data.roles.time_stamp: [\"ds\"], }  df_tsfresh_train = getml.data.DataFrame.from_pandas(     tsfresh_train, name=\"tsfresh_train\", roles=roles )  df_tsfresh_test = getml.data.DataFrame.from_pandas(     tsfresh_test, name=\"tsfresh_test\", roles=roles ) In\u00a0[33]: Copied! <pre>df_tsfresh_train.set_role(df_tsfresh_train.roles.unused, getml.data.roles.numerical)\n\ndf_tsfresh_test.set_role(df_tsfresh_test.roles.unused, getml.data.roles.numerical)\n</pre> df_tsfresh_train.set_role(df_tsfresh_train.roles.unused, getml.data.roles.numerical)  df_tsfresh_test.set_role(df_tsfresh_test.roles.unused, getml.data.roles.numerical) In\u00a0[34]: Copied! <pre>pipe_tsf_pr = getml.pipeline.Pipeline(\n    tags=[\"predicition\", \"tsfresh\"], predictors=[predictor]\n)\n\npipe_tsf_pr\n</pre> pipe_tsf_pr = getml.pipeline.Pipeline(     tags=[\"predicition\", \"tsfresh\"], predictors=[predictor] )  pipe_tsf_pr Out[34]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['predicition', 'tsfresh'])</pre> In\u00a0[35]: Copied! <pre>pipe_tsf_pr.check(df_tsfresh_train)\n</pre> pipe_tsf_pr.check(df_tsfresh_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\n</pre> In\u00a0[36]: Copied! <pre>pipe_tsf_pr.fit(df_tsfresh_train)\n</pre> pipe_tsf_pr.fit(df_tsfresh_train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nOK.\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:04, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:4.40497\n\n</pre> Out[36]: <pre>Pipeline(data_model='population',\n         feature_learners=[],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=[],\n         predictors=['XGBoostRegressor'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['predicition', 'tsfresh'])</pre> In\u00a0[37]: Copied! <pre>pipe_tsf_pr.score(df_tsfresh_test)\n</pre> pipe_tsf_pr.score(df_tsfresh_test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[37]: date time           set used      target     mae    rmse rsquared 0 2024-02-21 16:38:08 tsfresh_train f_x 0.4916 0.6636 0.9951 1 2024-02-21 16:38:08 tsfresh_test f_x 0.5986 0.7906 0.9938 In\u00a0[38]: Copied! <pre>num_features = dict(\n    fastprop=134,\n    featuretools=158,\n    tsfresh=120,\n)\n\nruntime_per_feature = [\n    benchmark.runtimes[\"fastprop\"] / num_features[\"fastprop\"],\n    benchmark.runtimes[\"featuretools\"] / num_features[\"featuretools\"],\n    benchmark.runtimes[\"tsfresh\"] / num_features[\"tsfresh\"],\n]\n\nfeatures_per_second = [1.0 / r.total_seconds() for r in runtime_per_feature]\n\nnormalized_runtime_per_feature = [\n    r / runtime_per_feature[0] for r in runtime_per_feature\n]\n\ncomparison = pd.DataFrame(\n    dict(\n        runtime=[\n            benchmark.runtimes[\"fastprop\"],\n            benchmark.runtimes[\"featuretools\"],\n            benchmark.runtimes[\"tsfresh\"],\n        ],\n        num_features=num_features.values(),\n        features_per_second=features_per_second,\n        normalized_runtime=[\n            1,\n            benchmark.runtimes[\"featuretools\"] / benchmark.runtimes[\"fastprop\"],\n            benchmark.runtimes[\"tsfresh\"] / benchmark.runtimes[\"fastprop\"],\n        ],\n        normalized_runtime_per_feature=normalized_runtime_per_feature,\n        rsquared=[pipe_fp_pr.rsquared, pipe_ft_pr.rsquared, pipe_tsf_pr.rsquared],\n        rmse=[pipe_fp_pr.rmse, pipe_ft_pr.rmse, pipe_tsf_pr.rmse],\n        mae=[pipe_fp_pr.mae, pipe_ft_pr.mae, pipe_tsf_pr.mae],\n    )\n)\n\ncomparison.index = [\"getML: FastProp\", \"featuretools\", \"tsfresh\"]\n</pre> num_features = dict(     fastprop=134,     featuretools=158,     tsfresh=120, )  runtime_per_feature = [     benchmark.runtimes[\"fastprop\"] / num_features[\"fastprop\"],     benchmark.runtimes[\"featuretools\"] / num_features[\"featuretools\"],     benchmark.runtimes[\"tsfresh\"] / num_features[\"tsfresh\"], ]  features_per_second = [1.0 / r.total_seconds() for r in runtime_per_feature]  normalized_runtime_per_feature = [     r / runtime_per_feature[0] for r in runtime_per_feature ]  comparison = pd.DataFrame(     dict(         runtime=[             benchmark.runtimes[\"fastprop\"],             benchmark.runtimes[\"featuretools\"],             benchmark.runtimes[\"tsfresh\"],         ],         num_features=num_features.values(),         features_per_second=features_per_second,         normalized_runtime=[             1,             benchmark.runtimes[\"featuretools\"] / benchmark.runtimes[\"fastprop\"],             benchmark.runtimes[\"tsfresh\"] / benchmark.runtimes[\"fastprop\"],         ],         normalized_runtime_per_feature=normalized_runtime_per_feature,         rsquared=[pipe_fp_pr.rsquared, pipe_ft_pr.rsquared, pipe_tsf_pr.rsquared],         rmse=[pipe_fp_pr.rmse, pipe_ft_pr.rmse, pipe_tsf_pr.rmse],         mae=[pipe_fp_pr.mae, pipe_ft_pr.mae, pipe_tsf_pr.mae],     ) )  comparison.index = [\"getML: FastProp\", \"featuretools\", \"tsfresh\"] In\u00a0[39]: Copied! <pre>comparison\n</pre> comparison Out[39]: runtime num_features features_per_second normalized_runtime normalized_runtime_per_feature rsquared rmse mae getML: FastProp 0 days 00:00:00.278417 134 481.231954 1.000000 1.000000 0.995059 0.723622 0.551521 featuretools 0 days 00:16:25.821720 158 0.160272 3540.810080 3002.588065 0.994791 0.748083 0.571803 tsfresh 0 days 00:00:34.619104 120 3.466289 124.342637 138.832050 0.993836 0.790602 0.598600 In\u00a0[40]: Copied! <pre>comparison.to_csv(\"comparisons/robot.csv\")\n</pre> comparison.to_csv(\"comparisons/robot.csv\")","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/robot_prop/#robot-feature-engineering-on-sensor-data","title":"Robot - Feature engineering on sensor data\u00b6","text":"<p>In this notebook, we compare getML's FastProp against well-known feature engineering libraries featuretools and tsfresh.</p> <p>Summary:</p> <ul> <li>Prediction type: Regression</li> <li>Domain: Robotics</li> <li>Prediction target: The force vector on the robot's arm</li> <li>Population size: 15001</li> </ul>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/robot_prop/#background","title":"Background\u00b6","text":"<p>A common approach to feature engineering is to generate attribute-value representations from relational data by applying a fixed set of aggregations to columns of interest and perform a feature selection on the (possibly large) set of generated features afterwards. In academia, this approach is called propositionalization.</p> <p>getML's FastProp is an implementation of this propositionalization approach that has been optimized for speed and memory efficiency. In this notebook, we want to demonstrate how \u2013 well \u2013 fast FastProp is. To this end, we will benchmark FastProp against the popular feature engineering libraries featuretools and tsfresh. Both of these libraries use propositionalization approaches for feature engineering.</p> <p>The data set has been generously provided by Erik Berger who originally collected it for his dissertation:</p> <p>Berger, E. (2018). Behavior-Specific Proprioception Models for Robotic Force Estimation: A Machine Learning Approach. Freiberg, Germany: Technische Universitaet Bergakademie Freiberg.</p>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/robot_prop/#analysis","title":"Analysis\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/robot_prop/#1-loading-data","title":"1. Loading data\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/robot_prop/#11-download-from-source","title":"1.1 Download from source\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/robot_prop/#12-prepare-data","title":"1.2 Prepare data\u00b6","text":"<p>The force vector consists of three component (f_x, f_y and f_z), meaning that we have three targets. For this comparison, we only predict the first component (f_x).</p> <p>Also, we want to speed things up a little, so we only use 10 columns. A previous analysis has revealed that the predictive power is mainly extracted from these 10 columns:</p>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/robot_prop/#13-separate-data-into-a-training-and-testing-set","title":"1.3 Separate data into a training and testing set\u00b6","text":"<p>We also want to separate the data set into a training and testing set. We do so by using the first 10,500 measurements for training and then using the remainder for testing.</p>","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/robot_prop/#2-predictive-modeling","title":"2. Predictive modeling\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/robot_prop/#21-propositionalization-with-getmls-fastprop","title":"2.1 Propositionalization with getML's FastProp\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/robot_prop/#22-propositionalization-with-featuretools","title":"2.2 Propositionalization with featuretools\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/robot_prop/#23-propositionalization-with-tsfresh","title":"2.3 Propositionalization with tsfresh\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/fastprop_benchmark/robot_prop/#3-comparison","title":"3. Comparison\u00b6","text":"","boost":0.9},{"location":"examples/enterprise-notebooks/kaggle_notebooks/","title":"Kaggle Notebooks","text":"<p>Explore a curated selection of Jupyter Notebooks that demonstrate the powerful capabilities of getML on Kaggle datasets. These notebooks showcase practical applications, highlighting benchmarks of various machine learning techniques, including Graph Neural Networks (GNNs).</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/#epileptic-seizure-recognition","title":"Epileptic Seizure Recognition","text":"<p>Dive into the classification of epileptic seizures using advanced machine learning techniques. This notebook covers the complete workflow, from data preprocessing and feature extraction to model training, providing a comprehensive approach to seizure recognition. [Source on Kaggle]</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/#getml-and-gnns-a-natural-symbiosis","title":"getML and GNNs: A Natural Symbiosis","text":"<p>Discover the synergy between getML and Graph Neural Networks (GNNs). This notebook illustrates how integrating these technologies can significantly enhance both data processing and model performance. [Source on Kaggle]</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/#cora-relational-learning-vs-gnns","title":"CORA: Relational Learning vs. GNNs","text":"<p>Explore a comparative analysis of getML and GNNs on the CORA dataset, a well-known benchmark in graph-based learning. This notebook provides valuable insights into the relative strengths and limitations of each approach. [Source on Kaggle]</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/cora_getml_vs_gnn/","title":"<span class=\"ntitle\">cora_getml_vs_gnn.ipynb</span> <span class=\"ndesc\">Relational Learning vs Graph Neural Networks</span>","text":"In\u00a0[1]: Copied! <pre># Install multiple Python packages in a single command for efficiency\n%pip install -q \"getml==1.4.0\" \"torch-geometric~=2.5\" \"pandas~=2.2\" \"matplotlib~=3.9\" \"seaborn~=0.13\" \"numpy~=1.26\" \"torch~=2.4\"\n\n# Download and extract getML software\n!wget -q https://static.getml.com/download/1.4.0/getml-1.4.0-x64-linux.tar.gz\n!tar -xzf getml-1.4.0-x64-linux.tar.gz &gt;/dev/null 2&gt;&amp;1\n\n# Install getML\n!./getml-1.4.0-x64-linux/getML install\n</pre> # Install multiple Python packages in a single command for efficiency %pip install -q \"getml==1.4.0\" \"torch-geometric~=2.5\" \"pandas~=2.2\" \"matplotlib~=3.9\" \"seaborn~=0.13\" \"numpy~=1.26\" \"torch~=2.4\"  # Download and extract getML software !wget -q https://static.getml.com/download/1.4.0/getml-1.4.0-x64-linux.tar.gz !tar -xzf getml-1.4.0-x64-linux.tar.gz &gt;/dev/null 2&gt;&amp;1  # Install getML !./getml-1.4.0-x64-linux/getML install <pre>\n[notice] A new release of pip is available: 24.0 -&gt; 24.2\n[notice] To update, run: pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\nInstalling getML...\nCould not install into '/usr/local': mkdir /usr/local/getML: permission denied\nGlobal installation failed, most likely due to missing root rights. Trying local installation instead.\nInstalling getML...\nSuccessfully installed getML into '/home/alex/.getML/getml-1.4.0-x64-linux'.\n</pre> In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport collections\nimport seaborn as sns\n\nimport torch\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GCNConv\n\nimport getml\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt import collections import seaborn as sns  import torch from torch_geometric.data import Data from torch_geometric.nn import GCNConv  import getml In\u00a0[3]: Copied! <pre>getml.engine.launch()\ngetml.engine.set_project(\"cora\")\n</pre> getml.engine.launch() getml.engine.set_project(\"cora\") <pre>Launching ./getML --allow-push-notifications=true --allow-remote-ips=false --home-directory=/home/alex --in-memory=true --install=false --launch-browser=true --log=false in /home/alex/.getML/getml-1.4.0-x64-linux...\nLaunched the getML engine. The log output will be stored in /home/alex/.getML/logs/20240826151248.log.\n\nConnected to project 'cora'\n</pre> In\u00a0[4]: Copied! <pre>conn = getml.database.connect_mysql(\n    host=\"db.relational-data.org\",\n    dbname=\"CORA\",\n    port=3306,\n    user=\"guest\",\n    password=\"relational\",\n)\n</pre> conn = getml.database.connect_mysql(     host=\"db.relational-data.org\",     dbname=\"CORA\",     port=3306,     user=\"guest\",     password=\"relational\", ) <p>To ensure comparability, the data set will be loaded from a single source. In real life, the provision of data in the appropriate graph structure is extremely unlikely. Hence, to mimic a typical setting of data availability, we choose to set out with the data in tabular format.</p> In\u00a0[5]: Copied! <pre>def load_if_needed(name):\n    if not getml.data.exists(name):\n        data_frame = getml.data.DataFrame.from_db(name=name, table_name=name, conn=conn)\n        data_frame.save()\n    else:\n        data_frame = getml.data.load_data_frame(name)\n    return data_frame\n</pre> def load_if_needed(name):     if not getml.data.exists(name):         data_frame = getml.data.DataFrame.from_db(name=name, table_name=name, conn=conn)         data_frame.save()     else:         data_frame = getml.data.load_data_frame(name)     return data_frame In\u00a0[6]: Copied! <pre>paper = load_if_needed(\"paper\")\ncites = load_if_needed(\"cites\")\ncontent = load_if_needed(\"content\")\n</pre> paper = load_if_needed(\"paper\") cites = load_if_needed(\"cites\") content = load_if_needed(\"content\") In\u00a0[7]: Copied! <pre>paper_df = paper.to_pandas()\ncites_df = cites.to_pandas()\ncontent_df = content.to_pandas()\n</pre> paper_df = paper.to_pandas() cites_df = cites.to_pandas() content_df = content.to_pandas() <p>Check out the image at https://graphsandnetworks.com/the-cora-dataset/. It shows the entire Cora graph, including all papers and all references.</p> In\u00a0[8]: Copied! <pre>def vectorize(word_list):\n    word_vector = np.zeros(vocab_size, dtype=int)\n    word_vector[word_list] = 1\n    return word_vector\n\n\nvocab_size = content_df[\"word_cited_id\"].nunique() + 1\n\ncontent_df[\"paper_id\"] = content_df[\"paper_id\"].astype(int)\ncontent_df[\"word_list\"] = content_df[\"word_cited_id\"].apply(lambda x: int(x[4:]) - 1)\nvector_content_df = content_df.groupby(\"paper_id\").agg(list)\nvector_content_df[\"word_vector\"] = vector_content_df[\"word_list\"].apply(\n    lambda x: vectorize(x)\n)\n</pre> def vectorize(word_list):     word_vector = np.zeros(vocab_size, dtype=int)     word_vector[word_list] = 1     return word_vector   vocab_size = content_df[\"word_cited_id\"].nunique() + 1  content_df[\"paper_id\"] = content_df[\"paper_id\"].astype(int) content_df[\"word_list\"] = content_df[\"word_cited_id\"].apply(lambda x: int(x[4:]) - 1) vector_content_df = content_df.groupby(\"paper_id\").agg(list) vector_content_df[\"word_vector\"] = vector_content_df[\"word_list\"].apply(     lambda x: vectorize(x) ) <p>The class labels we want to predict also need to be properly encoded. For convenience the mapping between code and label name is saved in a dictionary.</p> In\u00a0[9]: Copied! <pre>paper_df[\"paper_id\"] = paper_df[\"paper_id\"].astype(int)\npaper_df[\"class_label_enc\"] = paper_df[\"class_label\"].astype(\"category\").cat.codes\nnum_classes = paper_df[\"class_label_enc\"].nunique()\n\n_df = paper_df[[\"class_label_enc\", \"class_label\"]].drop_duplicates()\n_df.index = _df.class_label_enc\nlabel_mapping = _df.sort_index().to_dict()[\"class_label\"]\n</pre> paper_df[\"paper_id\"] = paper_df[\"paper_id\"].astype(int) paper_df[\"class_label_enc\"] = paper_df[\"class_label\"].astype(\"category\").cat.codes num_classes = paper_df[\"class_label_enc\"].nunique()  _df = paper_df[[\"class_label_enc\", \"class_label\"]].drop_duplicates() _df.index = _df.class_label_enc label_mapping = _df.sort_index().to_dict()[\"class_label\"] In\u00a0[10]: Copied! <pre>index_alligned_df = paper_df.merge(vector_content_df, on=\"paper_id\")\nindex_alligned_df = index_alligned_df.sample(frac=1, random_state=1234).reset_index(\n    drop=True\n)  # Randomization step\nindex_alligned_df[\"node_index\"] = index_alligned_df.index\n</pre> index_alligned_df = paper_df.merge(vector_content_df, on=\"paper_id\") index_alligned_df = index_alligned_df.sample(frac=1, random_state=1234).reset_index(     drop=True )  # Randomization step index_alligned_df[\"node_index\"] = index_alligned_df.index In\u00a0[11]: Copied! <pre>cites_df = cites_df.astype({\"cited_paper_id\": int, \"citing_paper_id\": int})\n\ncites_df = cites_df.merge(\n    index_alligned_df[[\"paper_id\", \"node_index\"]],\n    how=\"left\",\n    left_on=\"cited_paper_id\",\n    right_on=\"paper_id\",\n    copy=False,\n)\ncites_df.rename(columns={\"node_index\": \"cited_node_index\"}, inplace=True)\ncites_df.drop(\"paper_id\", axis=1, inplace=True)\n\ncites_df = cites_df.merge(\n    index_alligned_df[[\"paper_id\", \"node_index\"]],\n    how=\"left\",\n    left_on=\"citing_paper_id\",\n    right_on=\"paper_id\",\n    copy=False,\n)\ncites_df.rename(columns={\"node_index\": \"citing_node_index\"}, inplace=True)\ncites_df.drop(\"paper_id\", axis=1, inplace=True)\ncites_df\n</pre> cites_df = cites_df.astype({\"cited_paper_id\": int, \"citing_paper_id\": int})  cites_df = cites_df.merge(     index_alligned_df[[\"paper_id\", \"node_index\"]],     how=\"left\",     left_on=\"cited_paper_id\",     right_on=\"paper_id\",     copy=False, ) cites_df.rename(columns={\"node_index\": \"cited_node_index\"}, inplace=True) cites_df.drop(\"paper_id\", axis=1, inplace=True)  cites_df = cites_df.merge(     index_alligned_df[[\"paper_id\", \"node_index\"]],     how=\"left\",     left_on=\"citing_paper_id\",     right_on=\"paper_id\",     copy=False, ) cites_df.rename(columns={\"node_index\": \"citing_node_index\"}, inplace=True) cites_df.drop(\"paper_id\", axis=1, inplace=True) cites_df Out[11]: cited_paper_id citing_paper_id cited_node_index citing_node_index 0 35 887 545 77 1 35 1033 545 249 2 35 1688 545 1409 3 35 1956 545 1010 4 35 8865 545 2073 ... ... ... ... ... 5424 853116 19621 491 709 5425 853116 853155 491 2440 5426 853118 1140289 116 2375 5427 853155 853118 2440 116 5428 954315 1155073 845 2503 <p>5429 rows \u00d7 4 columns</p> In\u00a0[12]: Copied! <pre>_cited_node = cites_df[\"cited_node_index\"].values\n_citing_node = cites_df[\"citing_node_index\"].values\n\ncited_node = np.concatenate((_citing_node, _cited_node))\nciting_node = np.concatenate((_cited_node, _citing_node))\n</pre> _cited_node = cites_df[\"cited_node_index\"].values _citing_node = cites_df[\"citing_node_index\"].values  cited_node = np.concatenate((_citing_node, _cited_node)) citing_node = np.concatenate((_cited_node, _citing_node)) In\u00a0[13]: Copied! <pre>train_share = 0.7\nn_papers = len(index_alligned_df)\ncut_off = int(n_papers * train_share)\ntrain_mask = n_papers * [False]\ntrain_mask[:cut_off] = cut_off * [True]\ntest_mask = [not e for e in train_mask]\n</pre> train_share = 0.7 n_papers = len(index_alligned_df) cut_off = int(n_papers * train_share) train_mask = n_papers * [False] train_mask[:cut_off] = cut_off * [True] test_mask = [not e for e in train_mask] In\u00a0[14]: Copied! <pre>x = torch.tensor(\n    np.array(index_alligned_df.word_vector.values.tolist()), dtype=torch.float32\n)\n\ny = torch.tensor(\n    np.array(index_alligned_df[\"class_label_enc\"].values), dtype=torch.long\n)\nedge_index = torch.tensor(np.array([cited_node, citing_node]), dtype=torch.int64)\n\ngraph_object = Data(x=x, edge_index=edge_index, y=y)\ngraph_object.train_mask = torch.tensor(train_mask)\ngraph_object.test_mask = torch.tensor(test_mask)\n</pre> x = torch.tensor(     np.array(index_alligned_df.word_vector.values.tolist()), dtype=torch.float32 )  y = torch.tensor(     np.array(index_alligned_df[\"class_label_enc\"].values), dtype=torch.long ) edge_index = torch.tensor(np.array([cited_node, citing_node]), dtype=torch.int64)  graph_object = Data(x=x, edge_index=edge_index, y=y) graph_object.train_mask = torch.tensor(train_mask) graph_object.test_mask = torch.tensor(test_mask) In\u00a0[15]: Copied! <pre>print(graph_object)\nprint(\"==============================================================\")\n\nprint(f\"Number of nodes: {graph_object.num_nodes}\")\nprint(f\"Number of edges: {int(graph_object.num_edges/2)}\")\nprint(f\"Average node degree: {(graph_object.num_edges) / graph_object.num_nodes:.2f}\")\nprint(f\"Number of training nodes: {graph_object.train_mask.sum()}\")\nprint(f\"Number of test nodes: {graph_object.test_mask.sum()}\")\nprint(\n    f\"Training node label rate: {int(graph_object.train_mask.sum()) / graph_object.num_nodes:.2f}\"\n)\nprint(\n    f\"Test node label rate: {int(graph_object.test_mask.sum()) / graph_object.num_nodes:.2f}\"\n)\nprint(f\"Contains isolated nodes: {graph_object.has_isolated_nodes()}\")\nprint(f\"Contains self-loops: {graph_object.has_self_loops()}\")\nprint(f\"Is undirected: {graph_object.is_undirected()}\")\n</pre> print(graph_object) print(\"==============================================================\")  print(f\"Number of nodes: {graph_object.num_nodes}\") print(f\"Number of edges: {int(graph_object.num_edges/2)}\") print(f\"Average node degree: {(graph_object.num_edges) / graph_object.num_nodes:.2f}\") print(f\"Number of training nodes: {graph_object.train_mask.sum()}\") print(f\"Number of test nodes: {graph_object.test_mask.sum()}\") print(     f\"Training node label rate: {int(graph_object.train_mask.sum()) / graph_object.num_nodes:.2f}\" ) print(     f\"Test node label rate: {int(graph_object.test_mask.sum()) / graph_object.num_nodes:.2f}\" ) print(f\"Contains isolated nodes: {graph_object.has_isolated_nodes()}\") print(f\"Contains self-loops: {graph_object.has_self_loops()}\") print(f\"Is undirected: {graph_object.is_undirected()}\") <pre>Data(x=[2708, 1433], edge_index=[2, 10858], y=[2708], train_mask=[2708], test_mask=[2708])\n==============================================================\nNumber of nodes: 2708\nNumber of edges: 5429\nAverage node degree: 4.01\nNumber of training nodes: 1895\nNumber of test nodes: 813\nTraining node label rate: 0.70\nTest node label rate: 0.30\nContains isolated nodes: False\nContains self-loops: False\nIs undirected: True\n</pre> <p>It would be interesting to learn what is the connectivity of papers across topics. The following heatmap shows the share of references within a domain on its diagonal, and across domains off the diagonal. \"Theory\" seems to be most cross-referenced of all domains, which makes sense since it often forms the fundament for more advanced research.</p> In\u00a0[16]: Copied! <pre>labels = graph_object.y.numpy()\nconnected_labels_set = list(map(lambda x: labels[x], graph_object.edge_index.numpy()))\nconnected_labels_set = np.array(connected_labels_set)\n\nlabel_connection_counts = []\nfor i in range(7):\n    connected_labels = connected_labels_set[\n        :, np.where(connected_labels_set[0] == i)[0]\n    ]\n    counter = collections.Counter(connected_labels[1])\n    counter = dict(counter)\n    items = sorted(counter.items())\n    items = [x[1] for x in items]\n    label_connection_counts.append(items)\nlabel_connection_counts = np.array(label_connection_counts)\n\n\ndef scaling(array):\n    return array / sum(array)\n\n\nlabel_connection_counts_scaled = np.apply_along_axis(\n    scaling, 1, label_connection_counts\n)\nconnection_df = pd.DataFrame(\n    label_connection_counts_scaled,\n    columns=label_mapping.items(),\n    index=label_mapping.items(),\n)\nconnection_df = connection_df.droplevel(level=0, axis=0)\nconnection_df = connection_df.droplevel(level=0, axis=1)\n\nplt.figure(figsize=(9, 7))\nhm = sns.heatmap(\n    connection_df, annot=True, cmap=\"hot_r\", fmt=\"1.2f\", cbar=True, square=True\n)\n\nplt.tight_layout()\nplt.show()\n</pre> labels = graph_object.y.numpy() connected_labels_set = list(map(lambda x: labels[x], graph_object.edge_index.numpy())) connected_labels_set = np.array(connected_labels_set)  label_connection_counts = [] for i in range(7):     connected_labels = connected_labels_set[         :, np.where(connected_labels_set[0] == i)[0]     ]     counter = collections.Counter(connected_labels[1])     counter = dict(counter)     items = sorted(counter.items())     items = [x[1] for x in items]     label_connection_counts.append(items) label_connection_counts = np.array(label_connection_counts)   def scaling(array):     return array / sum(array)   label_connection_counts_scaled = np.apply_along_axis(     scaling, 1, label_connection_counts ) connection_df = pd.DataFrame(     label_connection_counts_scaled,     columns=label_mapping.items(),     index=label_mapping.items(), ) connection_df = connection_df.droplevel(level=0, axis=0) connection_df = connection_df.droplevel(level=0, axis=1)  plt.figure(figsize=(9, 7)) hm = sns.heatmap(     connection_df, annot=True, cmap=\"hot_r\", fmt=\"1.2f\", cbar=True, square=True )  plt.tight_layout() plt.show() In\u00a0[17]: Copied! <pre>class GNNTrain:\n    def __init__(self, graph_object, nn_model):\n        self.graph_object = graph_object\n        self.nn_model = nn_model\n        self.loss_function = torch.nn.CrossEntropyLoss()\n        self.optimizer = torch.optim.Adam(\n            nn_model.parameters(), lr=0.01, weight_decay=5e-4\n        )\n\n    def train(self):\n        self.nn_model.train()\n        self.optimizer.zero_grad()\n        out = self.nn_model(self.graph_object.x, self.graph_object.edge_index)\n        loss = self.loss_function(\n            out[self.graph_object.train_mask],\n            self.graph_object.y[self.graph_object.train_mask],\n        )\n        loss.backward()  # compute loss\n        self.optimizer.step()  # apply grad\n        return loss\n\n    def test(self, mask):\n        self.nn_model.eval()\n        out = self.nn_model(self.graph_object.x, self.graph_object.edge_index)\n        pred = out.argmax(dim=1)\n        correct = pred[mask] == self.graph_object.y[mask]\n        acc = int(correct.sum()) / int(mask.sum())\n        return acc\n\n    def run(self):\n        max_test_acc = 0\n        test_acc_list = []\n        for epoch in range(1, 1001):\n            self.train()\n            train_acc = self.test(self.graph_object.train_mask)\n            test_acc = self.test(self.graph_object.test_mask)\n            test_acc_list.append(test_acc)\n            if test_acc &gt; max_test_acc:\n                max_test_acc = test_acc\n            if epoch % 100 == 0:\n                print(\n                    f\"Epoch: {epoch:03d}, Train acc: {train_acc:.4f}\"\n                    f\", Test acc: {test_acc:.4f}\"\n                )\n        self.test_acc_list = test_acc_list\n        self.max_test_acc = max_test_acc\n</pre> class GNNTrain:     def __init__(self, graph_object, nn_model):         self.graph_object = graph_object         self.nn_model = nn_model         self.loss_function = torch.nn.CrossEntropyLoss()         self.optimizer = torch.optim.Adam(             nn_model.parameters(), lr=0.01, weight_decay=5e-4         )      def train(self):         self.nn_model.train()         self.optimizer.zero_grad()         out = self.nn_model(self.graph_object.x, self.graph_object.edge_index)         loss = self.loss_function(             out[self.graph_object.train_mask],             self.graph_object.y[self.graph_object.train_mask],         )         loss.backward()  # compute loss         self.optimizer.step()  # apply grad         return loss      def test(self, mask):         self.nn_model.eval()         out = self.nn_model(self.graph_object.x, self.graph_object.edge_index)         pred = out.argmax(dim=1)         correct = pred[mask] == self.graph_object.y[mask]         acc = int(correct.sum()) / int(mask.sum())         return acc      def run(self):         max_test_acc = 0         test_acc_list = []         for epoch in range(1, 1001):             self.train()             train_acc = self.test(self.graph_object.train_mask)             test_acc = self.test(self.graph_object.test_mask)             test_acc_list.append(test_acc)             if test_acc &gt; max_test_acc:                 max_test_acc = test_acc             if epoch % 100 == 0:                 print(                     f\"Epoch: {epoch:03d}, Train acc: {train_acc:.4f}\"                     f\", Test acc: {test_acc:.4f}\"                 )         self.test_acc_list = test_acc_list         self.max_test_acc = max_test_acc <p>Now, the GNNTrain object is instantiated. We choose a neural layer supported by pytorch.geometric, pass the previousy created graph objected and let the .run() do its job.</p> In\u00a0[18]: Copied! <pre>nn_model = GCNConv(\n    in_channels=graph_object.num_features, out_channels=len(graph_object.y.unique())\n)\ngcn = GNNTrain(graph_object=graph_object, nn_model=nn_model)\ngcn.run()\nprint(\"Maximum Test Accuracy: \", gcn.max_test_acc)\n</pre> nn_model = GCNConv(     in_channels=graph_object.num_features, out_channels=len(graph_object.y.unique()) ) gcn = GNNTrain(graph_object=graph_object, nn_model=nn_model) gcn.run() print(\"Maximum Test Accuracy: \", gcn.max_test_acc) <pre>Epoch: 100, Train acc: 0.9588, Test acc: 0.8721\nEpoch: 200, Train acc: 0.9631, Test acc: 0.8733\nEpoch: 300, Train acc: 0.9636, Test acc: 0.8696\nEpoch: 400, Train acc: 0.9662, Test acc: 0.8721\nEpoch: 500, Train acc: 0.9673, Test acc: 0.8696\nEpoch: 600, Train acc: 0.9673, Test acc: 0.8708\nEpoch: 700, Train acc: 0.9673, Test acc: 0.8721\nEpoch: 800, Train acc: 0.9678, Test acc: 0.8708\nEpoch: 900, Train acc: 0.9678, Test acc: 0.8708\nEpoch: 1000, Train acc: 0.9678, Test acc: 0.8708\nMaximum Test Accuracy:  0.8782287822878229\n</pre> <p>Using the graph convolutional operater GCNConv, the best accuracy that has been attained on the test data set is 87.82%. Let's turn to relation feature engineering employed by getML and examine its performance under identical conditions.</p> In\u00a0[19]: Copied! <pre>data_train = getml.DataFrame.from_pandas(\n    index_alligned_df[[\"paper_id\", \"class_label\"]][train_mask], name=\"data_train\"\n)\ndata_test = getml.DataFrame.from_pandas(\n    index_alligned_df[[\"paper_id\", \"class_label\"]][test_mask], name=\"data_test\"\n)\n\npaper, split = getml.data.split.concat(\"population\", train=data_train, test=data_test)\n</pre> data_train = getml.DataFrame.from_pandas(     index_alligned_df[[\"paper_id\", \"class_label\"]][train_mask], name=\"data_train\" ) data_test = getml.DataFrame.from_pandas(     index_alligned_df[[\"paper_id\", \"class_label\"]][test_mask], name=\"data_test\" )  paper, split = getml.data.split.concat(\"population\", train=data_train, test=data_test) In\u00a0[20]: Copied! <pre>getml.DataFrame.from_pandas(\n    index_alligned_df[[\"paper_id\", \"class_label\"]][train_mask], name=\"data_train\"\n)\n</pre> getml.DataFrame.from_pandas(     index_alligned_df[[\"paper_id\", \"class_label\"]][train_mask], name=\"data_train\" ) Out[20]: name     paper_id class_label            role unused_float unused_string          0 1124844 Neural_Networks 1 17208 Reinforcement_Learning 2 63931 Rule_Learning 3 12165 Theory 4 1122425 Reinforcement_Learning ... ... 1890 1154176 Genetic_Algorithms 1891 1113742 Genetic_Algorithms 1892 1952 Case_Based 1893 503893 Genetic_Algorithms 1894 51866 Theory <p>     1895 rows x 2 columns     memory usage: 0.06 MB     name: data_train     type: getml.DataFrame </p> In\u00a0[21]: Copied! <pre>paper.set_role(\"paper_id\", getml.data.roles.join_key)\npaper.set_role(\"class_label\", getml.data.roles.categorical)\n\ncites.set_role([\"cited_paper_id\", \"citing_paper_id\"], getml.data.roles.join_key)\n\ncontent.set_role(\"paper_id\", getml.data.roles.join_key)\ncontent.set_role(\"word_cited_id\", getml.data.roles.categorical)\n\ndata_full = getml.data.make_target_columns(paper, \"class_label\")\n</pre> paper.set_role(\"paper_id\", getml.data.roles.join_key) paper.set_role(\"class_label\", getml.data.roles.categorical)  cites.set_role([\"cited_paper_id\", \"citing_paper_id\"], getml.data.roles.join_key)  content.set_role(\"paper_id\", getml.data.roles.join_key) content.set_role(\"word_cited_id\", getml.data.roles.categorical)  data_full = getml.data.make_target_columns(paper, \"class_label\") In\u00a0[22]: Copied! <pre>container = getml.data.Container(population=data_full, split=split)\ncontainer.add(cites=cites, content=content, paper=paper)\ncontainer.freeze()\n</pre> container = getml.data.Container(population=data_full, split=split) container.add(cites=cites, content=content, paper=paper) container.freeze() In\u00a0[23]: Copied! <pre>dm = getml.data.DataModel(paper.to_placeholder(\"population\"))\n\n# We need two different placeholders for cites.\ndm.add(getml.data.to_placeholder(cites=[cites] * 2, content=content, paper=paper))\n\ndm.population.join(dm.cites[0], on=(\"paper_id\", \"cited_paper_id\"))\n\ndm.cites[0].join(dm.content, on=(\"citing_paper_id\", \"paper_id\"))\n\ndm.cites[0].join(\n    dm.paper,\n    on=(\"citing_paper_id\", \"paper_id\"),\n    relationship=getml.data.relationship.many_to_one,\n)\n\ndm.population.join(dm.cites[1], on=(\"paper_id\", \"citing_paper_id\"))\n\ndm.cites[1].join(dm.content, on=(\"cited_paper_id\", \"paper_id\"))\n\ndm.cites[1].join(\n    dm.paper,\n    on=(\"cited_paper_id\", \"paper_id\"),\n    relationship=getml.data.relationship.many_to_one,\n)\n\ndm.population.join(dm.content, on=\"paper_id\")\n\ndm\n</pre> dm = getml.data.DataModel(paper.to_placeholder(\"population\"))  # We need two different placeholders for cites. dm.add(getml.data.to_placeholder(cites=[cites] * 2, content=content, paper=paper))  dm.population.join(dm.cites[0], on=(\"paper_id\", \"cited_paper_id\"))  dm.cites[0].join(dm.content, on=(\"citing_paper_id\", \"paper_id\"))  dm.cites[0].join(     dm.paper,     on=(\"citing_paper_id\", \"paper_id\"),     relationship=getml.data.relationship.many_to_one, )  dm.population.join(dm.cites[1], on=(\"paper_id\", \"citing_paper_id\"))  dm.cites[1].join(dm.content, on=(\"cited_paper_id\", \"paper_id\"))  dm.cites[1].join(     dm.paper,     on=(\"cited_paper_id\", \"paper_id\"),     relationship=getml.data.relationship.many_to_one, )  dm.population.join(dm.content, on=\"paper_id\")  dm Out[23]: diagram contentpapercitescontentpapercitescontentpopulationpaper_id = citing_paper_idpaper_id = citing_paper_idRelationship: many-to-onepaper_id = cited_paper_idpaper_id = cited_paper_idRelationship: many-to-onecited_paper_id = paper_idciting_paper_id = paper_idpaper_id = paper_id staging data frames  staging table               0 population POPULATION__STAGING_TABLE_1 1 cites, paper CITES__STAGING_TABLE_2 2 cites, paper CITES__STAGING_TABLE_3 3 content CONTENT__STAGING_TABLE_4 <p>We use the FastProp algorithm for feature engineering and XGBoost for classification.</p> In\u00a0[24]: Copied! <pre>mapping = getml.preprocessors.Mapping()\n\nfast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    num_threads=1,\n)\n\npredictor = getml.predictors.XGBoostClassifier()\n</pre> mapping = getml.preprocessors.Mapping()  fast_prop = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     num_threads=1, )  predictor = getml.predictors.XGBoostClassifier() In\u00a0[25]: Copied! <pre>pipe1 = getml.pipeline.Pipeline(\n    tags=[\"fast_prop\"],\n    data_model=dm,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    predictors=[predictor],\n)\n\npipe1\n</pre> pipe1 = getml.pipeline.Pipeline(     tags=[\"fast_prop\"],     data_model=dm,     preprocessors=[mapping],     feature_learners=[fast_prop],     predictors=[predictor], )  pipe1 Out[25]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['cites', 'content', 'paper'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['fast_prop'])</pre> In\u00a0[26]: Copied! <pre>pipe1.fit(container.train)\n</pre> pipe1.fit(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 3 issues labeled INFO and 0 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 3780 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:08, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:15.826725\n\n</pre> Out[26]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['cites', 'content', 'paper'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['fast_prop', 'container-1UHYiW'])</pre> In\u00a0[27]: Copied! <pre>probabilities1 = pipe1.predict(container.test)\nclass_label = paper.class_label.unique()\n\nix_max = np.argmax(probabilities1, axis=1)\npredicted_labels1 = np.asarray([class_label[ix] for ix in ix_max])\n\nactual_labels = paper[split == \"test\"].class_label.to_numpy()\n\nprint(\"Share of accurately predicted class labels (pipe1):\")\nprint((actual_labels == predicted_labels1).sum() / len(actual_labels))\n</pre> probabilities1 = pipe1.predict(container.test) class_label = paper.class_label.unique()  ix_max = np.argmax(probabilities1, axis=1) predicted_labels1 = np.asarray([class_label[ix] for ix in ix_max])  actual_labels = paper[split == \"test\"].class_label.to_numpy()  print(\"Share of accurately predicted class labels (pipe1):\") print((actual_labels == predicted_labels1).sum() / len(actual_labels)) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nShare of accurately predicted class labels (pipe1):\n0.8879310344827587\n</pre>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/cora_getml_vs_gnn/#getml-and-graph-neural-networks-comparative-study-on-the-cora-data-set","title":"getML and Graph Neural Networks: Comparative Study on the CORA Data Set\u00b6","text":"<p>In recent years, Graph Neural Networks have gained a lot of attention in the data science community. An established benchmark to track and compare the progress of its development is the CORA classification data set. It consists of academic papers, their domain, word content of their abstracts, and how they reference each other. The challenge posed by the CORA data set is: Determine the domain of a paper, based on its references and the word content of its abstracts.</p> <p>The paper's references give rise to a complex graph structure, while the word content form an appropriate input for neural networks. Hence, GNN's are extremely well suited to be applied to the data set. The popularity of CORA among GNN researchers is underscored by the fact, that the renowned tracking site Papers with Code lists 69 papers benchmarking on CORA since 2016 (https://paperswithcode.com/sota/node-classification-on-cora), the vast majority of which are GNN based. Therefore, it makes sense to put the power of getML's automatic feature engineering to test by directly comparing it to state of the art GNN approaches in the context of the CORA data set.</p> <p>This notebook looks at efforts required and performance in terms of accuracy to compare both approaches.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/cora_getml_vs_gnn/#setup-and-download-of-data","title":"Setup and Download of Data\u00b6","text":"<p>First install the dependencies. Don't be discouraged, that may take a few minutes. Make sure you use Python 3.9.19.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/cora_getml_vs_gnn/#graph-neural-networks","title":"Graph Neural Networks\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/cora_getml_vs_gnn/#nodes","title":"Nodes\u00b6","text":"<p>Node attribute: Vectorization of words</p> <p>In order to model the CORA dataset as a graph structure, we need to define its constituents: nodes and edges. Nodes represent papers and edges represent references. As an additional source of information, the word composition of the paper's abstracts play a major role in determining the paper's class (i.e. domain). To make the word information accessible to the Neural Networks within the GNN architecture, the words lists need to be represented as a matrix. Using a bag of words approach, the words are being one-hot-encoded. Within GNN terminology, the resulting vectors of constant length form the node attributes.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/cora_getml_vs_gnn/#node-index","title":"Node index\u00b6","text":"<p>Each paper forms one node. Within the GNN architecture, nodes are identified by their index. Therefore it is crucial to create a single source of truth, where the node attributes have the same index. The paper and content dataframes are merged to ensure that congruency.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/cora_getml_vs_gnn/#edges","title":"Edges\u00b6","text":"<p>Edges constitute the other basic building block of a graph. An edge is simply defined as a tuple of two node indices: the sending node and the receiving node. The citation dataframe holds this information. To extent the newly imposed node_index nomenclatura, the citation dataframe is updated with the correct node indices.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/cora_getml_vs_gnn/#edge-index","title":"Edge Index\u00b6","text":"<p>The edge_index tensor is the collection of all edges and represents the node connectivity of a graph. Edges can be directed or undirected. In the current case, we do not differentiate whether a paper cites another paper or is cited. That is why we treat every connection as undirectional, append the edge_index with the reverse of the already existing directed edge.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/cora_getml_vs_gnn/#train-test-split","title":"Train Test Split\u00b6","text":"<p>In order to maintain comparability, we ensure that for each run, test and training data are identical for both the GNN getML approach. The masks are easily integrable with the graph object, as well as with the getML semantics in a later step. The actual randomization takes place during the defintion of the node_index a few lines above.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/cora_getml_vs_gnn/#graph-object","title":"Graph Object\u00b6","text":"<p>Now, the actual work on the graph object can begin. Predictors, targets and edge index are converted to torch tensors. The Graph object is instantiated and populated with the data and masks.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/cora_getml_vs_gnn/#exploratory-analysis","title":"Exploratory Analysis\u00b6","text":"<p>To get a better grasp of the graph object we examine several of its features.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/cora_getml_vs_gnn/#modeling","title":"Modeling\u00b6","text":"<p>The GNNTrain object contains the training routine that trains and tests the model every epoch.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/cora_getml_vs_gnn/#getml-implementation","title":"getML Implementation\u00b6","text":"<p>For a more detailed description and in depth feature analysis of the getML approach to the CORA data set check out: https://github.com/getml/getml-demo/blob/master/cora.ipynb</p> <p>Unlike in a standalone approach of a getML solution, we do not randomize train and test data. Instead, we utilize the identical train test split that was used for the GNN implementation. To this end, the index_aligned_df and the identical masks from previously are used.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/cora_getml_vs_gnn/#role-setting","title":"Role Setting\u00b6","text":"<p>getML requires that we define roles for each of the columns.</p> <p>Also, the goal is to predict seven different labels. We generate a target column for each of those labels.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/cora_getml_vs_gnn/#data-model","title":"Data Model\u00b6","text":"<p>To get started with relational learning, we need to specify the data model. Even though the data set itself is quite simple with only three tables and six columns in total, the resulting data model is actually quite complicated.</p> <p>That is because the class label can be predicted using three different pieces of information:</p> <ul> <li>The keywords used by the paper</li> <li>The keywords used by papers it cites and by papers that cite the paper</li> <li>The class label of papers it cites and by papers that cite the paper</li> </ul> <p>The main challenge here is that <code>cites</code> is used twice, once to connect the cited papers and then to connect the citing papers. To resolve this, we need two placeholders on <code>cites</code>.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/cora_getml_vs_gnn/#build-the-pipeline","title":"Build the Pipeline\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/cora_getml_vs_gnn/#model-training","title":"Model training\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/cora_getml_vs_gnn/#out-of-sample-prediction","title":"Out of Sample Prediction\u00b6","text":"<p>In order to obtain the accuracy of our model we need to transform the computed probabilities in actual categorical predictions and compare them with the ground truth.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/cora_getml_vs_gnn/#conclusion","title":"Conclusion\u00b6","text":"<p>With automatic feature engineering we attain 88.79% accuracy. Randomizing over train test split we found in large scale test runs, that on average getML feature engineering reaches an accuracy of 88.3%, whereas GNN's attain on average 87.6%. That means with little effort, getML FastProp algorithm outperforms off-the-shelf GNN implementations. And even compared with cutting edge GNN research, getML ranks well within the TOP 10 among all papers ever published.</p> <p>Stay tuned for our next notebook, where we combine getML feature engineering and off-the-shelf GNN implementaions and vastly outperform cutting edge GNN implementations.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/epilepsy_recognition/","title":"<span class=\"ntitle\">epilepsy_recognition.ipynb</span> <span class=\"ndesc\">Recognizing epilepsy in EEG data</span>","text":"In\u00a0[1]: Copied! <pre>%pip install -q \"getml==1.4.0\" \"numpy&lt;2.0.0\" \"matplotlib~=3.9\" \"seaborn~=0.13\"\n</pre> %pip install -q \"getml==1.4.0\" \"numpy&lt;2.0.0\" \"matplotlib~=3.9\" \"seaborn~=0.13\" <pre>\n[notice] A new release of pip is available: 24.0 -&gt; 24.2\n[notice] To update, run: pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\n\nimport getml\n</pre> import numpy as np import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns  sns.set_style(\"whitegrid\")  import getml <p>All your work is organized into projects. You can easily set any name for your current project. The engine will create a new project or use an existing one if the project name already exists. It will also provide you with a direct link to the project within the monitor.</p> In\u00a0[3]: Copied! <pre>getml.engine.launch()\ngetml.engine.set_project(\"epilepsy_recognition\")\n</pre> getml.engine.launch() getml.engine.set_project(\"epilepsy_recognition\") <pre>Launching ./getML --allow-push-notifications=true --allow-remote-ips=false --home-directory=/home/alex/.local/share/hatch/env/virtual/getml-demo/txflr3_Z/getml-demo/lib/python3.10/site-packages/getml --in-memory=true --install=false --launch-browser=true --log=false in /home/alex/.local/share/hatch/env/virtual/getml-demo/txflr3_Z/getml-demo/lib/python3.10/site-packages/getml/.getML/getml-1.4.0-x64-community-edition-linux...\nLaunched the getML engine. The log output will be stored in /home/alex/.getML/logs/20240826151418.log.\nLoading pipelines... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nConnected to project 'epilepsy_recognition'\n</pre> <p>You can manage your projects conveniently through the monitor web interface or directly using Python commands. For example, you can suspend your current project to free resources using <code>getml.project.suspend()</code>, switch to another project on the fly using <code>getml.project.switch('new project name')</code>, or restart using <code>getml.project.restart()</code> should something go wrong. You can even save your current project to disk using <code>getml.project.save('filename')</code> and load it with <code>getml.project.load('filename')</code>.</p> In\u00a0[4]: Copied! <pre>data = pd.read_csv(\"data/Epileptic Seizure Recognition.csv\")\n\n# view first 5 rows of the data\ndata.head()\n</pre> data = pd.read_csv(\"data/Epileptic Seizure Recognition.csv\")  # view first 5 rows of the data data.head() Out[4]: Unnamed X1 X2 X3 X4 X5 X6 X7 X8 X9 ... X170 X171 X172 X173 X174 X175 X176 X177 X178 y 0 X21.V1.791 135 190 229 223 192 125 55 -9 -33 ... -17 -15 -31 -77 -103 -127 -116 -83 -51 4 1 X15.V1.924 386 382 356 331 320 315 307 272 244 ... 164 150 146 152 157 156 154 143 129 1 2 X8.V1.1 -32 -39 -47 -37 -32 -36 -57 -73 -85 ... 57 64 48 19 -12 -30 -35 -35 -36 5 3 X16.V1.60 -105 -101 -96 -92 -89 -95 -102 -100 -87 ... -82 -81 -80 -77 -85 -77 -72 -69 -65 5 4 X20.V1.54 -9 -65 -98 -102 -78 -48 -16 0 -21 ... 4 2 -12 -32 -41 -65 -83 -89 -73 5 <p>5 rows \u00d7 180 columns</p> <p>The first column contains extraneous metadata and can be dropped. The last column is named <code>y</code> and contains the class labels. As described above, we will do a binary classification into epileptic seizure (label 1) or not (label 2-5). Thus, we can set the labels 2-5 to 0, representing a non-epileptic instance, and 1 for epileptic seizure.</p> In\u00a0[5]: Copied! <pre>data.drop(\"Unnamed\", axis=1, inplace=True)\n</pre> data.drop(\"Unnamed\", axis=1, inplace=True) In\u00a0[6]: Copied! <pre># classify having epileptic seizure or not\nclass_relabeling = {1: 1, 2: 0, 3: 0, 4: 0, 5: 0}\ndata.replace({\"y\": class_relabeling}, inplace=True)\n</pre> # classify having epileptic seizure or not class_relabeling = {1: 1, 2: 0, 3: 0, 4: 0, 5: 0} data.replace({\"y\": class_relabeling}, inplace=True) <p>Now we can check which values we have in the label column and the DataFrame in general.</p> In\u00a0[7]: Copied! <pre>counts = data[\"y\"].value_counts()\nprint(f\"Number of records epileptic {counts[1]} vs non-epileptic {counts[0]}\")\n</pre> counts = data[\"y\"].value_counts() print(f\"Number of records epileptic {counts[1]} vs non-epileptic {counts[0]}\") <pre>Number of records epileptic 2300 vs non-epileptic 9200\n</pre> In\u00a0[8]: Copied! <pre>data.head()\n</pre> data.head() Out[8]: X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 ... X170 X171 X172 X173 X174 X175 X176 X177 X178 y 0 135 190 229 223 192 125 55 -9 -33 -38 ... -17 -15 -31 -77 -103 -127 -116 -83 -51 0 1 386 382 356 331 320 315 307 272 244 232 ... 164 150 146 152 157 156 154 143 129 1 2 -32 -39 -47 -37 -32 -36 -57 -73 -85 -94 ... 57 64 48 19 -12 -30 -35 -35 -36 0 3 -105 -101 -96 -92 -89 -95 -102 -100 -87 -79 ... -82 -81 -80 -77 -85 -77 -72 -69 -65 0 4 -9 -65 -98 -102 -78 -48 -16 0 -21 -59 ... 4 2 -12 -32 -41 -65 -83 -89 -73 0 <p>5 rows \u00d7 179 columns</p> In\u00a0[9]: Copied! <pre># describe non-epileptic data\ndata[data[\"y\"] == 0].describe().T\n</pre> # describe non-epileptic data data[data[\"y\"] == 0].describe().T Out[9]: count mean std min 25% 50% 75% max X1 9200.0 -8.992609 70.455286 -566.0 -44.0 -7.0 26.0 1726.0 X2 9200.0 -8.877174 70.560110 -609.0 -44.0 -7.0 27.0 1713.0 X3 9200.0 -8.910435 70.372582 -594.0 -45.0 -7.0 28.0 1697.0 X4 9200.0 -8.969783 70.030409 -549.0 -45.0 -8.0 27.0 1612.0 X5 9200.0 -9.085326 69.377958 -603.0 -45.0 -8.0 27.0 1437.0 ... ... ... ... ... ... ... ... ... X175 9200.0 -9.848587 69.550894 -570.0 -45.0 -9.0 27.0 1958.0 X176 9200.0 -9.620435 70.353607 -594.0 -46.0 -8.0 27.0 2047.0 X177 9200.0 -9.395435 70.934300 -563.0 -45.0 -9.0 27.0 2047.0 X178 9200.0 -9.240435 71.185850 -559.0 -45.0 -8.0 27.0 1915.0 y 9200.0 0.000000 0.000000 0.0 0.0 0.0 0.0 0.0 <p>179 rows \u00d7 8 columns</p> In\u00a0[10]: Copied! <pre># describe epileptic data\ndata[data[\"y\"] == 1].describe().T\n</pre> # describe epileptic data data[data[\"y\"] == 1].describe().T Out[10]: count mean std min 25% 50% 75% max X1 2300.0 -21.936522 342.361939 -1839.0 -193.25 -16.0 159.00 1314.0 X2 2300.0 -19.049130 343.398782 -1838.0 -191.25 -18.0 168.25 1356.0 X3 2300.0 -15.293913 337.489643 -1835.0 -187.00 -12.5 169.25 1274.0 X4 2300.0 -9.836087 332.354833 -1845.0 -184.00 -6.0 166.25 1226.0 X5 2300.0 -3.707391 332.211163 -1791.0 -174.25 -12.0 170.00 1518.0 ... ... ... ... ... ... ... ... ... X175 2300.0 -25.830870 339.650467 -1863.0 -195.00 -14.5 153.25 1205.0 X176 2300.0 -25.043913 335.747017 -1781.0 -192.00 -18.0 150.00 1371.0 X177 2300.0 -24.548261 335.244512 -1727.0 -190.25 -21.5 151.25 1445.0 X178 2300.0 -24.016522 339.819309 -1829.0 -189.00 -23.0 157.25 1380.0 y 2300.0 1.000000 0.000000 1.0 1.00 1.0 1.00 1.0 <p>179 rows \u00d7 8 columns</p> <p>The data in its current form is cumbersome to work with, both for data exploration and for applying machine learning later on. Thus, we reshape our data into a more compliant form.</p> <p>First, we reshape our data from its pivoted form into a well-structured table using pandas' <code>melt</code> function. The original index represents each sample, so we preserve it as the <code>sample_index</code>. We then extract the number from the <code>X</code> column names, which represents a timestamp (we call it <code>time_index</code> here) along each window. Finally, we sort the resulting DataFrame so we have a nicely structured table containing each window and corresponding metadata.</p> In\u00a0[11]: Copied! <pre># data is in 1s per row format\n# first unpivot into single time series, preserve target y, then take the original index, which is the\n# \"sample index\" of each sample\ndata_unpivoted = (\n    data.melt(\n        id_vars=[\"y\"], var_name=\"time_label\", value_name=\"eeg\", ignore_index=False\n    )\n    .reset_index()\n    .rename(columns={\"index\": \"sample_index\"})\n)\n\n# the time index is the index over the 1s time period in each original row in data\ndata_unpivoted[\"time_index\"] = (\n    data_unpivoted[\"time_label\"].str.extract(r\"(\\d+)\", expand=False).astype(int)\n)\n\n# sort each window according to the sample and time and re-order columns\ndata_unpivoted = data_unpivoted.sort_values(by=[\"sample_index\", \"time_index\"]).reindex(\n    [\"sample_index\", \"time_index\", \"eeg\", \"y\"], axis=1\n)\n</pre> # data is in 1s per row format # first unpivot into single time series, preserve target y, then take the original index, which is the # \"sample index\" of each sample data_unpivoted = (     data.melt(         id_vars=[\"y\"], var_name=\"time_label\", value_name=\"eeg\", ignore_index=False     )     .reset_index()     .rename(columns={\"index\": \"sample_index\"}) )  # the time index is the index over the 1s time period in each original row in data data_unpivoted[\"time_index\"] = (     data_unpivoted[\"time_label\"].str.extract(r\"(\\d+)\", expand=False).astype(int) )  # sort each window according to the sample and time and re-order columns data_unpivoted = data_unpivoted.sort_values(by=[\"sample_index\", \"time_index\"]).reindex(     [\"sample_index\", \"time_index\", \"eeg\", \"y\"], axis=1 ) <p>Let's have a look at our new DataFrame and the first recording.</p> In\u00a0[12]: Copied! <pre>data_unpivoted\n</pre> data_unpivoted Out[12]: sample_index time_index eeg y 0 0 1 135 0 11500 0 2 190 0 23000 0 3 229 0 34500 0 4 223 0 46000 0 5 192 0 ... ... ... ... ... 2000999 11499 174 5 0 2012499 11499 175 4 0 2023999 11499 176 -2 0 2035499 11499 177 2 0 2046999 11499 178 20 0 <p>2047000 rows \u00d7 4 columns</p> In\u00a0[13]: Copied! <pre>data_unpivoted[data_unpivoted[\"sample_index\"] == 0]\n</pre> data_unpivoted[data_unpivoted[\"sample_index\"] == 0] Out[13]: sample_index time_index eeg y 0 0 1 135 0 11500 0 2 190 0 23000 0 3 229 0 34500 0 4 223 0 46000 0 5 192 0 ... ... ... ... ... 1989500 0 174 -103 0 2001000 0 175 -127 0 2012500 0 176 -116 0 2024000 0 177 -83 0 2035500 0 178 -51 0 <p>178 rows \u00d7 4 columns</p> <p>We then can have a look at some of the EEG signals and get a feel for what we are dealing with. We pick the first <code>n</code> (we chose 5, use any number you like) samples of every class and plot the EEG signals side-by-side.</p> In\u00a0[14]: Copied! <pre>n = 5\n\nindex_n_epileptic = data_unpivoted[data_unpivoted[\"y\"] == 1][\"sample_index\"].unique()[\n    :n\n]\nindex_n_nonepileptic = data_unpivoted[data_unpivoted[\"y\"] == 0][\n    \"sample_index\"\n].unique()[:n]\n\nsamples_to_show = np.concatenate((index_n_epileptic, index_n_nonepileptic))\n</pre> n = 5  index_n_epileptic = data_unpivoted[data_unpivoted[\"y\"] == 1][\"sample_index\"].unique()[     :n ] index_n_nonepileptic = data_unpivoted[data_unpivoted[\"y\"] == 0][     \"sample_index\" ].unique()[:n]  samples_to_show = np.concatenate((index_n_epileptic, index_n_nonepileptic)) In\u00a0[15]: Copied! <pre>g = sns.relplot(\n    data=data_unpivoted[data_unpivoted[\"sample_index\"].isin(samples_to_show)],\n    kind=\"line\",\n    x=\"time_index\",\n    y=\"eeg\",\n    col=\"y\",\n    hue=\"sample_index\",\n    legend=\"full\",\n    palette=sns.color_palette(),\n)\n</pre> g = sns.relplot(     data=data_unpivoted[data_unpivoted[\"sample_index\"].isin(samples_to_show)],     kind=\"line\",     x=\"time_index\",     y=\"eeg\",     col=\"y\",     hue=\"sample_index\",     legend=\"full\",     palette=sns.color_palette(), ) <p>We can already guess that epileptic signals seem to be a lot more deviating than non-epileptic signals. Let's have a look at the standard deviation of EEG values per class label and compare them:</p> In\u00a0[16]: Copied! <pre>g = sns.catplot(\n    data=data_unpivoted.groupby([\"sample_index\", \"y\"]).std().reset_index(),\n    kind=\"box\",\n    x=\"y\",\n    y=\"eeg\",\n)\ng.set_ylabels(\"std of eeg\")\n</pre> g = sns.catplot(     data=data_unpivoted.groupby([\"sample_index\", \"y\"]).std().reset_index(),     kind=\"box\",     x=\"y\",     y=\"eeg\", ) g.set_ylabels(\"std of eeg\") Out[16]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7969f37261a0&gt;</pre> In\u00a0[17]: Copied! <pre>sns.displot(\n    data=data_unpivoted.groupby([\"sample_index\", \"y\"]).std().reset_index(),\n    kind=\"kde\",\n    x=\"eeg\",\n    hue=\"y\",\n)\n</pre> sns.displot(     data=data_unpivoted.groupby([\"sample_index\", \"y\"]).std().reset_index(),     kind=\"kde\",     x=\"eeg\",     hue=\"y\", ) Out[17]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7969f238c550&gt;</pre> <p>At this point, it is fairly obvious that standard deviation is going to be a major feature in this analysis.</p> In\u00a0[18]: Copied! <pre>population_df = pd.DataFrame(\n    {\n        \"sample_index\": data.index.values,\n        \"y\": data.y,\n    }\n)\n</pre> population_df = pd.DataFrame(     {         \"sample_index\": data.index.values,         \"y\": data.y,     } ) In\u00a0[19]: Copied! <pre>population_df\n</pre> population_df Out[19]: sample_index y 0 0 0 1 1 1 2 2 0 3 3 0 4 4 0 ... ... ... 11495 11495 0 11496 11496 1 11497 11497 0 11498 11498 0 11499 11499 0 <p>11500 rows \u00d7 2 columns</p> <p>A getML.DataFrame can be created from a pandas DataFrame as follows. As this getML.DataFrame is a representation of data internally handled by the engine, we need to specify an internal name. DataFrames are represented by these names in the monitor.</p> In\u00a0[20]: Copied! <pre>population = getml.DataFrame.from_pandas(population_df, name=\"population\")\n</pre> population = getml.DataFrame.from_pandas(population_df, name=\"population\") In\u00a0[21]: Copied! <pre>population\n</pre> population Out[21]:  name sample_index            y  role unused_float unused_float 0 0 0 1 1 1 2 2 0 3 3 0 4 4 0 ... ... 11495 11495 0 11496 11496 1 11497 11497 0 11498 11498 0 11499 11499 0 <p>     11500 rows x 2 columns     memory usage: 0.18 MB     name: population     type: getml.DataFrame </p> <p>As you can see, our data is now stored inside the engine and represented by a getML.DataFrame (data is of course the same). The Python API provides a link to the getML.DataFrame in the monitor, where you can conveniently explore your data.</p> <p>Now we need to annotate our data so the engine knows what to do with it.</p> <p>A key aspect of using getML.DataFrame are roles. Every column with relevant data to our data model needs to have a certain role specified. As you can see, both of our columns have the <code>unused_float</code> role for now. One of the most important roles is <code>getml.data.roles.target</code>, specifying that the data in this column is our target variable, the value that we want to train our machine learning model on. In our case, the column <code>y</code> containing the class label is our target. Let's tell the engine exactly that:</p> In\u00a0[22]: Copied! <pre>population.set_role([\"y\"], getml.data.roles.target)\n</pre> population.set_role([\"y\"], getml.data.roles.target) <p>As you may have noticed, our population getML.DataFrame does not contain any actual data, specifically no EEG signals. We utilize one of getML's core strengths here: relational data and time-series.</p> <p>Our data can in fact be interpreted as a relational time-series. We have a label for each window with a unique window id (<code>sample_index</code>) that stands in relation to its actual data, the corresponding EEG signal of each individual window. Each window has the EEG signal values along with the unique window id (<code>sample_index</code>). In other words: we can utilize a very efficient data model by joining a peripheral table containing the EEG values onto the window labels.</p> <p>Thus, the next step is to specify the <code>sample_index</code> as the join key using <code>getml.data.roles.join_key</code> in our population table:</p> In\u00a0[23]: Copied! <pre>population.set_role([\"sample_index\"], getml.data.roles.join_key)\n</pre> population.set_role([\"sample_index\"], getml.data.roles.join_key) In\u00a0[24]: Copied! <pre>population\n</pre> population Out[24]:  name sample_index      y  role     join_key target 0 0 0 1 1 1 2 2 0 3 3 0 4 4 0 ... ... 11495 11495 0 11496 11496 1 11497 11497 0 11498 11498 0 11499 11499 0 <p>     11500 rows x 2 columns     memory usage: 0.14 MB     name: population     type: getml.DataFrame </p> <p>We now create the peripheral table containing the time-series data corresponding to the 1s window labels in the population table just like we did previously with our population table:</p> In\u00a0[25]: Copied! <pre>peripheral = getml.DataFrame.from_pandas(data_unpivoted, name=\"peripheral\")\n</pre> peripheral = getml.DataFrame.from_pandas(data_unpivoted, name=\"peripheral\") <p>Now we need to specify the column roles in the peripheral table. This getML.DataFrame contains the <code>sample_index</code> as well and we need to set it as our join key, as described above. Subsequently, as this table will contain our actual data in the form of EEG signal values, we specify the role of this column as numerical (<code>getml.data.roles.numerical</code>), something we can train our machine learning model on:</p> In\u00a0[26]: Copied! <pre>peripheral.set_role([\"sample_index\"], getml.data.roles.join_key)\nperipheral.set_role([\"eeg\"], getml.data.roles.numerical)\n</pre> peripheral.set_role([\"sample_index\"], getml.data.roles.join_key) peripheral.set_role([\"eeg\"], getml.data.roles.numerical) In\u00a0[27]: Copied! <pre>peripheral\n</pre> peripheral Out[27]:    name sample_index       eeg   time_index            y    role     join_key numerical unused_float unused_float 0 0 135 1 0 1 0 190 2 0 2 0 229 3 0 3 0 223 4 0 4 0 192 5 0 ... ... ... ... 2046995 11499 5 174 0 2046996 11499 4 175 0 2046997 11499 -2 176 0 2046998 11499 2 177 0 2046999 11499 20 178 0 <p>     2047000 rows x 4 columns     memory usage: 57.32 MB     name: peripheral     type: getml.DataFrame </p> <p>As you may have noticed, there are still <code>unused_float</code> columns left. This data is present, but we do not use or need it in our machine learning efforts. This unused data is not considered and can just be ignored.</p> In\u00a0[28]: Copied! <pre>train, test = 0.9, 0.1\n\nsplit = getml.data.split.random(seed=5849, train=train, test=test)\n</pre> train, test = 0.9, 0.1  split = getml.data.split.random(seed=5849, train=train, test=test) <p>Second, we create our data model. We create a StarSchema containing our population getML.DataFrame as the population table and specify the split of our dataset into train and test set. We then join our peripheral table to our time series on the join key, in this case <code>sample_index</code>:</p> In\u00a0[29]: Copied! <pre>time_series = getml.data.StarSchema(population=population, split=split)\n\ntime_series.join(\n    peripheral,\n    on=\"sample_index\",\n)\n</pre> time_series = getml.data.StarSchema(population=population, split=split)  time_series.join(     peripheral,     on=\"sample_index\", ) <p>getML provides a convenient view to our data model. We can look at a diagram representation of our data model with table names and specific joins, as well as the staging tables and statistics about the underlying data container.</p> In\u00a0[30]: Copied! <pre>time_series\n</pre> time_series Out[30]: data model diagram peripheralpopulationsample_index = sample_index staging data frames staging table               0 population POPULATION__STAGING_TABLE_1 1 peripheral PERIPHERAL__STAGING_TABLE_2 container population subset name        rows type 0 test population 1086 View 1 train population 10414 View peripheral name          rows type      0 peripheral 2047000 DataFrame <p>This is an overview of your data model in the getML engine. At the top you can see a visual representation in the form of a diagram. Here you can easily see how your data and the specific joins is structured. Next you are presented the so called staging tables. This is a list of the relevant data frames and staging table names. At last, you can see an overview of all the data containers. This includes the split in train and test set of your population table as well as the peripheral tables.</p> <p>In this simple example, the diagram consists of a single join of the peripheral table onto the population table via the <code>sample_index</code> as a join key. The population table is split into 90% train and 10% test set. The peripheral talbe contains all the EEG signal values and has over 2 million rows.</p> In\u00a0[31]: Copied! <pre>feature_learner = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    aggregation=getml.feature_learning.FastProp.agg_sets.All,\n)\n\npredictor = getml.predictors.XGBoostClassifier()\n</pre> feature_learner = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     aggregation=getml.feature_learning.FastProp.agg_sets.All, )  predictor = getml.predictors.XGBoostClassifier() <p>Now that we have our data model and machine learning components defined in just a few lines of code, we declare our machine learning pipeline as simple as the following. For convenience, we specify some free to choose tags. These are shown in the monitor and can be used to efficiently and easily distinguish different pipelines and their performance.</p> In\u00a0[32]: Copied! <pre>pipe = getml.pipeline.Pipeline(\n    data_model=time_series.data_model,\n    tags=[\"FastProp+AggAll\", \"XGBoost\", f\"split={train}/{test}\"],\n    feature_learners=[feature_learner],\n    predictors=[predictor],\n)\n</pre> pipe = getml.pipeline.Pipeline(     data_model=time_series.data_model,     tags=[\"FastProp+AggAll\", \"XGBoost\", f\"split={train}/{test}\"],     feature_learners=[feature_learner],     predictors=[predictor], ) <p>Now all we need to do is train our model:</p> In\u00a0[33]: Copied! <pre>pipe.fit(time_series.train, check=False)\n</pre> pipe.fit(time_series.train, check=False) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 25 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nXGBoost: Training as predictor... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:01, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:1.434615\n\n</pre> Out[33]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['peripheral'],\n         predictors=['XGBoostClassifier'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['FastProp+AggAll', 'XGBoost', 'split=0.9/0.1', 'container-Mq37qh'])</pre> <p>This is all it takes. FastProp found features in just 1 second and XGBoost trained our model in just 2 seconds. The whole process took less than 4 seconds!</p> <p>Now, let's look at how well our model performs. Again, getML does everything for you. We score our pipeline on the test set:</p> In\u00a0[34]: Copied! <pre>pipe.score(time_series.test)\n</pre> pipe.score(time_series.test) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> Out[34]: date time           set used target accuracy     auc cross entropy 0 2024-08-26 15:14:26 train y 0.98 0.9974 0.05586 1 2024-08-26 15:14:26 test y 0.9696 0.994 0.0793 <p>Let's have a look at some key machine learning metrics: Accuracy and Area Under Curve (AUC):</p> In\u00a0[35]: Copied! <pre>print(f\"Accuracy: {pipe.scores.accuracy*100:.2f}%, AUC: {pipe.scores.auc:.4f}\")\n</pre> print(f\"Accuracy: {pipe.scores.accuracy*100:.2f}%, AUC: {pipe.scores.auc:.4f}\") <pre>Accuracy: 96.96%, AUC: 0.9940\n</pre> <p>There are several other, more complex metrics to understand the performance of a machine learning model. The most prominent being Receiver Operating Characteristic (ROC) curve, precision-recall curve and lift curve. getML has these already calculated for you:</p> In\u00a0[36]: Copied! <pre># Refers to the data from the last time we called .score(...)\nfpr, tpr = pipe.plots.roc_curve()\nrecall, precision = pipe.plots.precision_recall_curve()\nproportion, lift = pipe.plots.lift_curve()\n\nfig, ax = plt.subplots(ncols=3, figsize=(12, 4))\n\nax[0].plot(fpr, tpr, color=\"#6829c2\")\nax[0].set_title(\"receiver operating characteristic (ROC)\")\nax[0].set_xlabel(\"false positive rate\")\nax[0].set_ylabel(\"true positive rate\")\n\nax[1].plot(recall, precision, color=\"#6829c2\")\nax[1].set_title(\"precision-recall curve\")\nax[1].set_xlabel(\"recall (true positive rate)\")\nax[1].set_ylabel(\"precision\")\n\nax[2].plot(proportion, lift, color=\"#6829c2\")\nax[2].set_title(\"lift curve\")\nax[2].set_xlabel(\"proportion\")\nax[2].set_ylabel(\"lift\")\n\nplt.show()\n</pre> # Refers to the data from the last time we called .score(...) fpr, tpr = pipe.plots.roc_curve() recall, precision = pipe.plots.precision_recall_curve() proportion, lift = pipe.plots.lift_curve()  fig, ax = plt.subplots(ncols=3, figsize=(12, 4))  ax[0].plot(fpr, tpr, color=\"#6829c2\") ax[0].set_title(\"receiver operating characteristic (ROC)\") ax[0].set_xlabel(\"false positive rate\") ax[0].set_ylabel(\"true positive rate\")  ax[1].plot(recall, precision, color=\"#6829c2\") ax[1].set_title(\"precision-recall curve\") ax[1].set_xlabel(\"recall (true positive rate)\") ax[1].set_ylabel(\"precision\")  ax[2].plot(proportion, lift, color=\"#6829c2\") ax[2].set_title(\"lift curve\") ax[2].set_xlabel(\"proportion\") ax[2].set_ylabel(\"lift\")  plt.show() <p>The Receiver-Operator-Characteristic (ROC) curve is a diagram that shows the diagnostic ability of a binary classifier for all classification thresholds. It shows the tradeoff between sensitivity (true positive rate or TPR) and specificity (1 - FPR or false positive rate). Easily put, the closer the curve to the top left (meaning the larger the area under curve or AUC) , the more accurate the classifier. A 45\u00b0 diagonal would be a random classifier.</p> <p>Much like the ROC curve, a precision-recall curve (PR curve) is used to evaluate the performance of a binary classifier. It is often used when dealing with heavily imbalanced classes. It is desired that your machine learning model has both high precision and high recall. However, we often end up with a trade-off between the two. Similar to a ROC curve, the higher the area und the curve the better performing our binary classifier.</p> <p>The\u00a0Lift curve\u00a0shows the relation between the number of instances which were predicted positive and those that are indeed positive and thus, like ROC and PR curves, measures the effectiveness of a chosen classifier against a random classifier. In our example, the patients with the highest probability of having an epileptic seizure appear on the left of the Lift curve along with high Lift scores. This point is called the Maximum Lift Point: the higher this point, the better our model performs. Also, it is generally considered that the longer the flat part on the right of the Lift curve, the more reliable the model is.</p> <p>All three performance diagrams measure the performance of a binary classifier against a random classifier. As a rule of thumb, the higher ROC and PR curves the better, while a Lift curve is desired to be high in the left and preferably flat on the right.</p> In\u00a0[37]: Copied! <pre>pipe.features\n</pre> pipe.features Out[37]: target name         correlation importance 0 y feature_1_1 0.0358 0.0087 1 y feature_1_2 0.7295 0.018 2 y feature_1_3 -0.7295 0.0 3 y feature_1_4 0.7646 0.008 4 y feature_1_5 0.0516 0.0041 ... ... ... ... 20 y feature_1_21 0.7634 0.0012 21 y feature_1_22 0.7615 0.0017 22 y feature_1_23 0.0186 0.0189 23 y feature_1_24 -0.0433 0.0026 24 y feature_1_25 0.0 0.0 <p>We can look at feature correlations:</p> In\u00a0[38]: Copied! <pre>names, correlations = pipe.features.correlations()\n\nplt.subplots(figsize=(8, 4))\n\nplt.bar(names, correlations, color=\"#6829c2\")\n\nplt.title(\"feature correlations\")\nplt.xlabel(\"features\")\nplt.ylabel(\"correlations\")\nplt.xticks(rotation=\"vertical\")\n\nplt.show()\n</pre> names, correlations = pipe.features.correlations()  plt.subplots(figsize=(8, 4))  plt.bar(names, correlations, color=\"#6829c2\")  plt.title(\"feature correlations\") plt.xlabel(\"features\") plt.ylabel(\"correlations\") plt.xticks(rotation=\"vertical\")  plt.show() <p>Or we can have a look at the feature importances. This is particularly interesting if we want to understand what generated features are most important for our machine learning model.</p> <p>The feature importance is calculated by XGBoost based on the improvement of the optimizing criterium at each split in the decision tree and is normalized to 100%.</p> In\u00a0[39]: Copied! <pre>names, importances = pipe.features.importances()\n\nplt.subplots(figsize=(8, 4))\n\nplt.bar(names, importances, color=\"#6829c2\")\n\nplt.title(\"feature importances\")\nplt.xlabel(\"features\")\nplt.ylabel(\"importance\")\nplt.xticks(rotation=\"vertical\")\n\nplt.show()\n</pre> names, importances = pipe.features.importances()  plt.subplots(figsize=(8, 4))  plt.bar(names, importances, color=\"#6829c2\")  plt.title(\"feature importances\") plt.xlabel(\"features\") plt.ylabel(\"importance\") plt.xticks(rotation=\"vertical\")  plt.show() <p>This is intriguing! It seems that one particular feature stands out in its importance for classification predictions.</p> <p>As we already discussed above, we view our time series data from a relation data point of view. In fact, relational learning is one of getML's core strengths. Particularly, getML is able to transpile features into database queries that can be used in production database environments without the need of any other software component.</p> <p>Let's have a look at the SQL code of our most important feature:</p> In\u00a0[40]: Copied! <pre>pipe.features.to_sql()[names[0]]\n</pre> pipe.features.to_sql()[names[0]] Out[40]: <pre>DROP TABLE IF EXISTS \"FEATURE_1_7\";\n\nCREATE TABLE \"FEATURE_1_7\" AS\nSELECT STDDEV( t2.\"eeg\" ) AS \"feature_1_7\",\n       t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"PERIPHERAL__STAGING_TABLE_2\" t2\nON t1.\"sample_index\" = t2.\"sample_index\"\nGROUP BY t1.rowid;\n</pre> <p>As you can see, the most important feature in seizure recognition from EEG signals seems to be the standard deviation. Just like we guessed in the beginning of this notebook.</p> <p>However, relevant features are by far not always so obvious as in this particular example dataset. In fact, most of the time feature engineering takes a lot of effort and domain knowledge from domain experts. As we discussed above, manual feature engineering is a cumbersome, time consuming and error prone process.</p> <p>Novel machine learning libraries like getML with automatic feature learning, flexible data models and machine learning pipelines, all wrapped inside an easy to use Python API, backed by an efficient and fast C++ backend make this task a lot easier and way more efficient for data scientists.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/epilepsy_recognition/#using-getml-on-eeg-data-to-classify-epileptic-seizures","title":"Using getML on EEG data to classify epileptic seizures\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/epilepsy_recognition/#introduction","title":"Introduction\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/epilepsy_recognition/#a-primer-on-time-series","title":"A Primer on Time Series\u00b6","text":"<p>Wikipedia defines a time series as follows:</p> <p>In mathematics, a time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time.</p> <p>The first sentence defines a fundamental feature of time series: your data can be ordered in time. This does not necessitate the existence of a specific time stamp. It only means that data points should be in chronological order.</p> <p>Consider the following three examples. All of them are time series. The first example shows force measurements on a robot's arm. We do not know the specific time and date when these measurements were taken. But what we do know is that the data points are indexed according to chronological order. The second example shows hourly traffic volume on interstate 94 (a highway in the United States of America) in the first week of April 2016. This data has an exact date and time when the measurements were taken. The third example is a bio-signal, a recording of an EEG signal of human brain activity. For this kind of data, a specific time stamp can be useful, for example sleep data over longer time periods such as months, but commonly is not necessary for short term bio-signals such as EEG readings or movement data.</p> <p></p> <p>The second sentence in Wikipedia's definition says that points in time are most commonly equally spaced. This in fact holds true for most time series data. We know for a fact that traffic volume was measured once every hour. Although we do not have a time stamp for the robot arm data, we can assume that the data recording sensor measured in equally spaced time intervals.</p> <p>As a matter of fact, even these equidistant time intervals might be an assumption in data sets like the above examples, as all sensors physically record data points with some form of timely variance due to multiple reasons such as sensor or data saving lags and so on. Additionally, there might be an inherent variance; for example a person never sleeps at exactly the same time every day. However, in many real-world data sets you will find that the assumption of equally spaced data points in time is violated and you will have to deal with that.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/epilepsy_recognition/#time-series-prediction-tasks","title":"Time Series Prediction Tasks\u00b6","text":"<p>There are several kinds of time series problems, but they can be roughly subdivided into two kinds: Forecasts and Nowcasts.</p> <ul> <li><p>A forecast tries to predict the future. For instance, when dealing with the Interstate 94 data, you will most likely want to predict the traffic volume in one hour from now or one week from now. This time period is often called the forecast horizon.</p> </li> <li><p>A nowcast tries to predict another time series or class-label based on some time series. For instance, when confronted with the robot data, you might want to predict the force vector based on other sensor data. This is useful when other sensor data are easier to measure than the force vector. On the other hand, a very common task for bio-signals is classifying some state based on these signals, such as sleep quality or epileptic seizure occurance.</p> </li> </ul> <p>A very interesting question is also whether you are allowed to use lagged targets. For instance, if you want to predict the traffic volume for the next hour, are you allowed to use the traffic volume for the last three hours? In this case, probably yes, but if you want to do a nowcast on the force vector of the robot's arm, probably no, because the entire idea is that measuring the force vector is difficult and therefore we cannot assume that we have the force vector data of the recent past when we are making a prediction. This question is also very relevant for classification tasks, as you face the question of how far in the past data is relevant to classifying a specific time point or interval. For example, in sleep data classification, events that occurred several days, weeks, or even months in the past might have an influence, while an epileptic seizure might be detectable in the short term.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/epilepsy_recognition/#why-we-need-feature-engineering","title":"Why We Need Feature Engineering\u00b6","text":"<p>The next question is: why do we need feature engineering in the first place? Isn't machine learning supposed to magically extract the information it needs from the data? Unfortunately, it is not quite that simple. Standard machine learning algorithms require the data to be presented in a flat table.</p> <p>We say standard machine learning and that means that there are exceptions. Deep learning can automatically extract features from unstructured data like images, text and speech. On the other hand, relational learning can automatically extract features from structured relational data.</p> <p>In the absence of these two approaches, we have to manually engineer features. This involves interactions between data scientists, who understand statistics, and domain experts, who understand the underlying problem domain. For example, domain experts provide domain know-how to data scientists, who write code and software that needs to be evaluated and optimized by both parties. The result is a flat table containing features to be used in machine learning. However, the process of getting to that table is cumbersome, time consuming and error prone.</p> <p></p> <p>In this figure, we refer to relational data. Relational data is structured data that you would usually find in relational databases such as PostgreSQL, MySQL, MariaDB, BigQuery or Redshift. However, the same holds true for time series. In fact, time series can be considered a special case of relational data. Below, we will see why that is.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/epilepsy_recognition/#interesting-features-for-time-series","title":"Interesting Features for Time Series\u00b6","text":"<p>Two important characteristics are trend and seasonality.</p> <ul> <li><p>The trend indicates whether your time series increases or decreases over time.</p> </li> <li><p>The seasonality indicates any patterns that vary by hour, day etc. For instance, the Interstate 94 dataset has very obvious seasonal patterns: Traffic volume is highest in the morning (when people drive to work) and in the evening (when people get home from work), very low at night and overall lower on weekends.</p> </li> </ul> <p>But there are many other important characteristics, such as the average, median, max, min and standard deviation of your observations over time. So when you build features for time series, a very common approach is to use a sliding window technique. Whenever you want to make a prediction you take all the values over a fixed period of time right up to when you want to make the prediction and then apply all sorts of aggregations to this. For classification tasks, every window or time point is classified by a certain label.</p> <p></p> <p>This depiction shows how we can engineer a very simple feature for a time series. As we would for relational data, we define some kind of criterion over which we identify the data we are interested in (in this case, the last five days), which we then aggregate using some aggregation function (in this case, the average). This is a very simple example how feature engineering for time series usually works. But if you engineer features for time series in this way, you are effectively thinking of time series as relational data: You are identifying relevant data from your data set and aggregating it, just like you would for relational data. In fact, what we are doing is effectively a self join, because we are joining a table to itself.</p> <p>This is just a simple example. But there are many more features you can generate. For instance, for the EEG data, you build features like this:</p> <ul> <li>Average EEG value in the last second</li> <li>Maximum EEG value in the last second</li> <li>Minimum EEG value in the last second</li> <li>Median EEG value in the last second</li> <li>First measurement of EEG value in the last second</li> <li>Last measurement of EEG value in the last second</li> <li>Variance of the EEG value in the last second</li> <li>Standard deviation of the EEG value in the last second</li> <li>Exponentially weighted moving average of the EEG value in the last second</li> <li>1%-quantile of the EEG value in the last second</li> <li>5%-quantile of the EEG value in the last second</li> <li>...</li> </ul> <p>Of course, we mustn't just assume that one second is the right period of time to use. So we could take all of these features and calculate them for the last minute or last three hours.</p> <p>Moreover, the features we have discussed so far don't really take seasonality into account (with the exception of first measurement of EEG value in the last second, because this is the measurement from exactly one second ago).</p> <p>On the other hand, the Interstate 94 dataset is strongly seasonal, we should take that into account as well. So we could calculate features like this:</p> <ul> <li>Average traffic volume in the last four weeks, but only where weekday equals the weekday the point in time we want to predict.</li> <li>Average traffic volume in the last four weeks, but only where hour of the day equals the hour of the point in time we want to predict.</li> <li>Average traffic volume in the last four weeks, but only where both the weekday and the hour of the day equal the point in time we want to predict.</li> <li>Maximum traffic volume in the last four weeks, but only where weekday equals the weekday the point in time we want to predict.</li> <li>...</li> </ul> <p>What should be very obvious at this point is that there are many features that you can generate like this, even for a very simple time series problem. When you have a multivariate time series (meaning you have more than one input variable), you can apply these techniques to every single column in your input data and you will get many, many features. You would very likely have to apply some kind of feature selection techniques to focus on the most useful features.</p> <p>The beauty of this approach is that it is very flexible and uses few assumptions. We have noted above that many time series are not equally-spaced. This would be a problem for classical time series analyses like ARIMA or ARMA. Not here. Nothing about this approach makes any assumptions about the spacing.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/epilepsy_recognition/#automated-feature-engineering-for-time-series","title":"Automated Feature Engineering for Time Series\u00b6","text":"<p>You will find a lot of examples where people conduct this work manually (mainly using pandas). But today, we are not limited to manual feature engineering. There are numerous tools and libraries which can automate away this kind of work.</p> <p>Unfortunately, automated feature engineering has been getting a bit of a bad rap, mainly for taking too long. And it isn't wrong: Some of the more well-known libraries like featuretools or tsfresh are slow and not very memory-efficient.</p> <p>Overall, the features extracted from time series by such libraries are quite similar [Henderson &amp; Fulcher]. However, the stark differences in terms of runtime and memory consumption make it worthwhile taking a closer look as some of the newer tools and libraries like getML or tsflex are highly optimized and can generate many features in a short period of time.</p> <p>In the following, we would like to introduce time series classification using the getML Pyhon API. Above, we have discussed how time series can be seen as a form of relational data and introduced the term relational learning. We can utilize a very simple relational learning approach by interpreting the time series as a form of relational data and conduct a self join.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/epilepsy_recognition/#an-introduction-to-univariate-time-series-classification-with-getml","title":"An Introduction to Univariate Time Series Classification with getML\u00b6","text":"<p>In this tutorial, you will learn how to use getML to classify univariate time series. We first explore the data and learn what we are dealing with. Subsequently, we demonstrate how to efficiently build a full fledged machine learning data model and how to use getML's automatic feature learning algorithm FastProp.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/epilepsy_recognition/#about-the-dataset","title":"About the Dataset\u00b6","text":"<p>The original dataset from the reference comprises 500 files, each file representing a single person/subject. Each recording contains the EEG signal value of brain activity for 23.6s sampled into 4096 data points. These recordings have been split into 1s windows. This results in 23 x 500 = 11500 windows of EEG data over time in 178 datapoints and each window is categorized into 5 labels:</p> <ol> <li>Seizure activity</li> <li>EEG recorded at tumor site</li> <li>EEG recorded in healthy brain area</li> <li>eyes closed during recording</li> <li>eyes open during recording</li> </ol> <p>Subjects labeled with classes 2-5 did not have epileptic seizures. We can thus do a binary classification of subjects suffering an epileptic seizure or not, meaning classes 1 or 0, respectively.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/epilepsy_recognition/#acknowledgements","title":"Acknowledgements\u00b6","text":"<p>Andrzejak RG, Lehnertz K, Rieke C, Mormann F, David P, Elger CE (2001) Indications of nonlinear deterministic and finite dimensional structures in time series of brain electrical activity: Dependence on recording region and brain state, Phys. Rev. E, 64, 061907</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/epilepsy_recognition/#setup","title":"Setup\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/epilepsy_recognition/#start-up-getml","title":"Start up getML\u00b6","text":"<p>First, we import the necessary libraries  and launch the getML engine. The engine runs in the background and takes care of all the heavy lifting for you. This includes things like our powerful database engine and efficient algorithms as well as the getML monitor, which you can access by pointing your browser to http://localhost:1709/#/</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/epilepsy_recognition/#load-data","title":"Load Data\u00b6","text":"<p>The original dataset was hosted on the UCI repository but was unfortunately removed. You can get the dataset via Kaggle, here.</p> <p>The dataset we will be working on is stored in a CSV file located on disk. As we will perform data exploration, we will first load the data into a pandas DataFrame, as usual, and examine the raw data.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/epilepsy_recognition/#explore-data","title":"Explore Data\u00b6","text":"<p>We first have a look at some common statistics of our data divided into both classes.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/epilepsy_recognition/#creating-a-getml-data-model","title":"Creating a GetML Data Model\u00b6","text":"<p>Now that we have explored our data, let's do some machine learning. GetML uses a highly sophisticated engine that runs in the background and takes away a lot of hassle in machine learning applications.</p> <p>Let's take a look at loading data into your getML project. First, let's learn how we work with data in getML. Data is represented by getML's custom DataFrame that behaves similarly to a pandas DataFrame. However, a getML.DataFrame is a representation of our data inside getML's highly efficient C++ database engine that runs in the background. We can load data from various sources such as pandas DataFrames (<code>getml.DataFrame.from_pandas</code>), from CSV files (<code>getml.DataFrame.from_csv</code>), or load from remote databases (<code>getml.DataFrame.from_db</code>) or even S3 buckets (<code>getml.DataFrame.from_s3</code>).</p> <p>Let's create a population DataFrame that contains our main goal: classify a 1s window. This means that we only need a DataFrame that holds the class labels of each window and a unique id, which in this case can just be the <code>sample_index</code>.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/epilepsy_recognition/#the-getml-dataframes","title":"The getML DataFrames\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/epilepsy_recognition/#the-getml-data-model","title":"The getML Data Model\u00b6","text":"<p>Now that we have our data efficiently stored in getML.DataFrame, we continue to construct our data model.</p> <p>This is very easily done by using one of getML's many DataModels. We put our time-series data in a relational context and can utilze for example a simple StarSchema data model to accomplish this. Easily put, we see our windows (the time-series data) as splits into many individual samples that are joined onto the window labels. This way, we are effectively thinking of time series as relational data: we are identifying relevant information from our data and aggragate it into a single label. In fact, what we are doing is effectively a\u00a0self join, because we are joining a table to itself. This allows for very efficient calculation.</p> <p>First, we define a random data split:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/epilepsy_recognition/#the-getml-machine-learning-pipeline","title":"The getML machine learning pipeline\u00b6","text":"<p>Complex machine learning models are represented by getML pipelines. A pipeline contains the data model (including complex data relations), data preprocessors, feature learners, predictors and so on.</p> <p>In our approach, we will use getML's very own FastProp automatic feature learner for feature engineering. We specify a loss function suitable for classification. As we are only dealing with a univariate time-series, we want to use all possible aggregation functions.</p> <p>We use the highly efficient XGBoost classifier algorithm as a predictor.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/epilepsy_recognition/#model-evaluation","title":"Model Evaluation\u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/epilepsy_recognition/#studying-the-features","title":"Studying the Features\u00b6","text":"<p>Finally, we can have a look at the features our relational learning algorithm has extracted. We can view them conveniently in the getML monitor under the respective pipeline, or print them directly in Python:</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/getml-and-gnns-a-natural-symbiosis/","title":"<span class=\"ntitle\">getml-and-gnns-a-natural-symbiosis.ipynb</span> <span class=\"ndesc\">getML and GNNs: A Natural Symbiosis</span>","text":"<p>Table of Contents</p> <ul> <li>Setup and Download of Data</li> <li>Data Preparation</li> <li>Feature Learning with getML</li> <li>Graph Neural Network with getML Engineered Features</li> <li>Comparison of Approaches</li> <li>Conclusion</li> </ul> In\u00a0[1]: Copied! <pre># You might need to restart the kernel after the installs\n%pip install -q \"getml==1.4.0\" \"torch-geometric~=2.5\" \"pandas~=2.2\" \"matplotlib~=3.9\" \"seaborn~=0.13\" \"numpy~=1.26\" \"torch~=2.4\"\n\n# Download and extract getML software\n!wget -q https://static.getml.com/download/1.4.0/getml-1.4.0-x64-linux.tar.gz\n!tar -xzf getml-1.4.0-x64-linux.tar.gz &gt;/dev/null 2&gt;&amp;1\n\n# Install getML\n!./getml-1.4.0-x64-linux/getML install &gt;/dev/null 2&gt;&amp;1\n</pre> # You might need to restart the kernel after the installs %pip install -q \"getml==1.4.0\" \"torch-geometric~=2.5\" \"pandas~=2.2\" \"matplotlib~=3.9\" \"seaborn~=0.13\" \"numpy~=1.26\" \"torch~=2.4\"  # Download and extract getML software !wget -q https://static.getml.com/download/1.4.0/getml-1.4.0-x64-linux.tar.gz !tar -xzf getml-1.4.0-x64-linux.tar.gz &gt;/dev/null 2&gt;&amp;1  # Install getML !./getml-1.4.0-x64-linux/getML install &gt;/dev/null 2&gt;&amp;1 <pre>\n[notice] A new release of pip is available: 24.0 -&gt; 24.2\n[notice] To update, run: pip install --upgrade pip\nNote: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\n\nimport torch\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GCNConv\n\nimport getml\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.manifold import TSNE import seaborn as sns  import torch from torch_geometric.data import Data from torch_geometric.nn import GCNConv  import getml In\u00a0[3]: Copied! <pre>getml.engine.launch()\ngetml.engine.set_project(\"getml_gnn_cora\")\n</pre> getml.engine.launch() getml.engine.set_project(\"getml_gnn_cora\") <pre>Launching ./getML --allow-push-notifications=true --allow-remote-ips=false --home-directory=/home/alex --in-memory=true --install=false --launch-browser=true --log=false in /home/alex/.getML/getml-1.4.0-x64-linux...\nLaunched the getML engine. The log output will be stored in /home/alex/.getML/logs/20240826151709.log.\n\nConnected to project 'getml_gnn_cora'\n</pre> In\u00a0[4]: Copied! <pre>conn = getml.database.connect_mysql(\n    host=\"db.relational-data.org\",\n    dbname=\"CORA\",\n    port=3306,\n    user=\"guest\",\n    password=\"relational\",\n)\n</pre> conn = getml.database.connect_mysql(     host=\"db.relational-data.org\",     dbname=\"CORA\",     port=3306,     user=\"guest\",     password=\"relational\", ) In\u00a0[5]: Copied! <pre>def load_if_needed(name):\n    if not getml.data.exists(name):\n        data_frame = getml.data.DataFrame.from_db(name=name, table_name=name, conn=conn)\n        data_frame.save()\n    else:\n        data_frame = getml.data.load_data_frame(name)\n    return data_frame\n</pre> def load_if_needed(name):     if not getml.data.exists(name):         data_frame = getml.data.DataFrame.from_db(name=name, table_name=name, conn=conn)         data_frame.save()     else:         data_frame = getml.data.load_data_frame(name)     return data_frame In\u00a0[6]: Copied! <pre>paper = load_if_needed(\"paper\")\ncites = load_if_needed(\"cites\")\ncontent = load_if_needed(\"content\")\n</pre> paper = load_if_needed(\"paper\") cites = load_if_needed(\"cites\") content = load_if_needed(\"content\") In\u00a0[7]: Copied! <pre>paper_df = paper.to_pandas()\ncites_df = cites.to_pandas()\ncontent_df = content.to_pandas()\n</pre> paper_df = paper.to_pandas() cites_df = cites.to_pandas() content_df = content.to_pandas() In\u00a0[8]: Copied! <pre>def vectorize(word_list):\n    word_vector = np.zeros(vocab_size, dtype=int)\n    word_vector[word_list] = 1\n    return word_vector\n\n\nvocab_size = content_df[\"word_cited_id\"].nunique() + 1\n\ncontent_df[\"paper_id\"] = content_df[\"paper_id\"].astype(int)\ncontent_df[\"word_list\"] = content_df[\"word_cited_id\"].apply(lambda x: int(x[4:]) - 1)\nvector_content_df = content_df.groupby(\"paper_id\").agg(list)\nvector_content_df[\"word_vector\"] = vector_content_df[\"word_list\"].apply(\n    lambda x: vectorize(x)\n)\n</pre> def vectorize(word_list):     word_vector = np.zeros(vocab_size, dtype=int)     word_vector[word_list] = 1     return word_vector   vocab_size = content_df[\"word_cited_id\"].nunique() + 1  content_df[\"paper_id\"] = content_df[\"paper_id\"].astype(int) content_df[\"word_list\"] = content_df[\"word_cited_id\"].apply(lambda x: int(x[4:]) - 1) vector_content_df = content_df.groupby(\"paper_id\").agg(list) vector_content_df[\"word_vector\"] = vector_content_df[\"word_list\"].apply(     lambda x: vectorize(x) ) In\u00a0[9]: Copied! <pre>paper_df[\"paper_id\"] = paper_df[\"paper_id\"].astype(int)\npaper_df[\"class_label_enc\"] = paper_df[\"class_label\"].astype(\"category\").cat.codes\nnum_classes = paper_df[\"class_label_enc\"].nunique()\n\n_df = paper_df[[\"class_label_enc\", \"class_label\"]].drop_duplicates()\n_df.index = _df.class_label_enc\nlabel_mapping = _df.sort_index().to_dict()[\"class_label\"]\n</pre> paper_df[\"paper_id\"] = paper_df[\"paper_id\"].astype(int) paper_df[\"class_label_enc\"] = paper_df[\"class_label\"].astype(\"category\").cat.codes num_classes = paper_df[\"class_label_enc\"].nunique()  _df = paper_df[[\"class_label_enc\", \"class_label\"]].drop_duplicates() _df.index = _df.class_label_enc label_mapping = _df.sort_index().to_dict()[\"class_label\"] In\u00a0[10]: Copied! <pre>index_aligned_df = paper_df.merge(vector_content_df, on=\"paper_id\")\nindex_aligned_df = index_aligned_df.sample(frac=1, random_state=1).reset_index(\n    drop=True\n)  # Randomization step\nindex_aligned_df[\"node_index\"] = index_aligned_df.index\n</pre> index_aligned_df = paper_df.merge(vector_content_df, on=\"paper_id\") index_aligned_df = index_aligned_df.sample(frac=1, random_state=1).reset_index(     drop=True )  # Randomization step index_aligned_df[\"node_index\"] = index_aligned_df.index <p>Citations across papers constitute the edges of the Network. Now we align them with the index of the nodes.</p> In\u00a0[11]: Copied! <pre>cites_df = cites_df.astype({\"cited_paper_id\": int, \"citing_paper_id\": int})\n\ncites_df = cites_df.merge(\n    index_aligned_df[[\"paper_id\", \"node_index\"]],\n    how=\"left\",\n    left_on=\"cited_paper_id\",\n    right_on=\"paper_id\",\n    copy=False,\n)\ncites_df.rename(columns={\"node_index\": \"cited_node_index\"}, inplace=True)\ncites_df.drop(\"paper_id\", axis=1, inplace=True)\n\ncites_df = cites_df.merge(\n    index_aligned_df[[\"paper_id\", \"node_index\"]],\n    how=\"left\",\n    left_on=\"citing_paper_id\",\n    right_on=\"paper_id\",\n    copy=False,\n)\ncites_df.rename(columns={\"node_index\": \"citing_node_index\"}, inplace=True)\ncites_df.drop(\"paper_id\", axis=1, inplace=True)\ncites_df\n</pre> cites_df = cites_df.astype({\"cited_paper_id\": int, \"citing_paper_id\": int})  cites_df = cites_df.merge(     index_aligned_df[[\"paper_id\", \"node_index\"]],     how=\"left\",     left_on=\"cited_paper_id\",     right_on=\"paper_id\",     copy=False, ) cites_df.rename(columns={\"node_index\": \"cited_node_index\"}, inplace=True) cites_df.drop(\"paper_id\", axis=1, inplace=True)  cites_df = cites_df.merge(     index_aligned_df[[\"paper_id\", \"node_index\"]],     how=\"left\",     left_on=\"citing_paper_id\",     right_on=\"paper_id\",     copy=False, ) cites_df.rename(columns={\"node_index\": \"citing_node_index\"}, inplace=True) cites_df.drop(\"paper_id\", axis=1, inplace=True) cites_df Out[11]: cited_paper_id citing_paper_id cited_node_index citing_node_index 0 35 887 1217 1930 1 35 1033 1217 1804 2 35 1688 1217 2112 3 35 1956 1217 2333 4 35 8865 1217 726 ... ... ... ... ... 5424 853116 19621 113 1967 5425 853116 853155 113 2577 5426 853118 1140289 2591 683 5427 853155 853118 2577 2591 5428 954315 1155073 520 129 <p>5429 rows \u00d7 4 columns</p> In\u00a0[12]: Copied! <pre>_cited_node = cites_df[\"cited_node_index\"].values\n_citing_node = cites_df[\"citing_node_index\"].values\n\ncited_node = np.concatenate((_citing_node, _cited_node))\nciting_node = np.concatenate((_cited_node, _citing_node))\n</pre> _cited_node = cites_df[\"cited_node_index\"].values _citing_node = cites_df[\"citing_node_index\"].values  cited_node = np.concatenate((_citing_node, _cited_node)) citing_node = np.concatenate((_cited_node, _citing_node)) In\u00a0[13]: Copied! <pre>train_share = 0.7\nn_papers = len(index_aligned_df)\ncut_off = int(n_papers * train_share)\ntrain_mask = n_papers * [False]\ntrain_mask[:cut_off] = cut_off * [True]\ntest_mask = [not e for e in train_mask]\n</pre> train_share = 0.7 n_papers = len(index_aligned_df) cut_off = int(n_papers * train_share) train_mask = n_papers * [False] train_mask[:cut_off] = cut_off * [True] test_mask = [not e for e in train_mask] <p>Now that we have an index aligned feature matrix, label vector (class_label_enc) and an edge index that builds on the aligned node indices, we can break from the established GNN routine: we enter getML territory. The procedure is very similar to the getML implementation of the previous notebook. The difference lies in the fact that we won't apply a predictor, but leave it with feature learners.</p> <p>There is one more important detail to caution for. Even though we \"only\" learn the features with the getML routine, we have to guard us from subtle data leakage. We must only use data for feature learning, that is used later on in GNN modeling. This is also the main reason, why the getML feature learning doesn't precede data preparation in its entirety: to ensure the train test split is identical for both processing steps.</p> In\u00a0[14]: Copied! <pre>data_train = getml.DataFrame.from_pandas(\n    index_aligned_df[[\"paper_id\", \"class_label\"]][train_mask], name=\"data_train\"\n)\ndata_test = getml.DataFrame.from_pandas(\n    index_aligned_df[[\"paper_id\", \"class_label\"]][test_mask], name=\"data_test\"\n)\n\npaper, split = getml.data.split.concat(\"population\", train=data_train, test=data_test)\n</pre> data_train = getml.DataFrame.from_pandas(     index_aligned_df[[\"paper_id\", \"class_label\"]][train_mask], name=\"data_train\" ) data_test = getml.DataFrame.from_pandas(     index_aligned_df[[\"paper_id\", \"class_label\"]][test_mask], name=\"data_test\" )  paper, split = getml.data.split.concat(\"population\", train=data_train, test=data_test) <p>getML requires that we define roles for each of the columns.</p> <p>Also, the goal is to predict seven different labels. We generate a target column for each of those labels.</p> In\u00a0[15]: Copied! <pre>paper.set_role(\"paper_id\", getml.data.roles.join_key)\npaper.set_role(\"class_label\", getml.data.roles.categorical)\n\ncites.set_role([\"cited_paper_id\", \"citing_paper_id\"], getml.data.roles.join_key)\n\ncontent.set_role(\"paper_id\", getml.data.roles.join_key)\ncontent.set_role(\"word_cited_id\", getml.data.roles.categorical)\n\ndata_full = getml.data.make_target_columns(paper, \"class_label\")\n</pre> paper.set_role(\"paper_id\", getml.data.roles.join_key) paper.set_role(\"class_label\", getml.data.roles.categorical)  cites.set_role([\"cited_paper_id\", \"citing_paper_id\"], getml.data.roles.join_key)  content.set_role(\"paper_id\", getml.data.roles.join_key) content.set_role(\"word_cited_id\", getml.data.roles.categorical)  data_full = getml.data.make_target_columns(paper, \"class_label\") In\u00a0[16]: Copied! <pre>container = getml.data.Container(population=data_full, split=split)\ncontainer.add(cites=cites, content=content, paper=paper)\ncontainer.freeze()\n</pre> container = getml.data.Container(population=data_full, split=split) container.add(cites=cites, content=content, paper=paper) container.freeze() <p>To get started with relational learning, we need to specify the data model. Even though the data set itself is quite simple with only three tables and six columns in total, the resulting data model is actually quite complicated.</p> <p>That is because the class label can be predicted using three different pieces of information:</p> <ul> <li>The keywords used by the paper</li> <li>The keywords used by papers it cites and by papers that cite the paper</li> <li>The class label of papers it cites and by papers that cite the paper</li> </ul> <p>The main challenge here is that <code>cites</code> is used twice, once to connect the cited papers and then to connect the citing papers. To resolve this, we need two placeholders on <code>cites</code>.</p> In\u00a0[17]: Copied! <pre>dm = getml.data.DataModel(paper.to_placeholder(\"population\"))\n\n# We need two different placeholders for cites.\ndm.add(getml.data.to_placeholder(cites=[cites] * 2, content=content, paper=paper))\n\ndm.population.join(dm.cites[0], on=(\"paper_id\", \"cited_paper_id\"))\n\ndm.cites[0].join(dm.content, on=(\"citing_paper_id\", \"paper_id\"))\n\ndm.cites[0].join(\n    dm.paper,\n    on=(\"citing_paper_id\", \"paper_id\"),\n    relationship=getml.data.relationship.many_to_one,\n)\n\ndm.population.join(dm.cites[1], on=(\"paper_id\", \"citing_paper_id\"))\n\ndm.cites[1].join(dm.content, on=(\"cited_paper_id\", \"paper_id\"))\n\ndm.cites[1].join(\n    dm.paper,\n    on=(\"cited_paper_id\", \"paper_id\"),\n    relationship=getml.data.relationship.many_to_one,\n)\n\ndm.population.join(dm.content, on=\"paper_id\")\n\ndm\n</pre> dm = getml.data.DataModel(paper.to_placeholder(\"population\"))  # We need two different placeholders for cites. dm.add(getml.data.to_placeholder(cites=[cites] * 2, content=content, paper=paper))  dm.population.join(dm.cites[0], on=(\"paper_id\", \"cited_paper_id\"))  dm.cites[0].join(dm.content, on=(\"citing_paper_id\", \"paper_id\"))  dm.cites[0].join(     dm.paper,     on=(\"citing_paper_id\", \"paper_id\"),     relationship=getml.data.relationship.many_to_one, )  dm.population.join(dm.cites[1], on=(\"paper_id\", \"citing_paper_id\"))  dm.cites[1].join(dm.content, on=(\"cited_paper_id\", \"paper_id\"))  dm.cites[1].join(     dm.paper,     on=(\"cited_paper_id\", \"paper_id\"),     relationship=getml.data.relationship.many_to_one, )  dm.population.join(dm.content, on=\"paper_id\")  dm Out[17]: diagram contentpapercitescontentpapercitescontentpopulationpaper_id = citing_paper_idpaper_id = citing_paper_idRelationship: many-to-onepaper_id = cited_paper_idpaper_id = cited_paper_idRelationship: many-to-onecited_paper_id = paper_idciting_paper_id = paper_idpaper_id = paper_id staging data frames  staging table               0 population POPULATION__STAGING_TABLE_1 1 cites, paper CITES__STAGING_TABLE_2 2 cites, paper CITES__STAGING_TABLE_3 3 content CONTENT__STAGING_TABLE_4 <p>We use the FastProp algorithm for feature learning. Again, no predictor is required, and the pipeline is built without one.</p> In\u00a0[18]: Copied! <pre>mapping = getml.preprocessors.Mapping()\n\nfast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,\n    num_threads=1,\n)\n</pre> mapping = getml.preprocessors.Mapping()  fast_prop = getml.feature_learning.FastProp(     loss_function=getml.feature_learning.loss_functions.CrossEntropyLoss,     num_threads=1, ) In\u00a0[19]: Copied! <pre>pipe1 = getml.pipeline.Pipeline(\n    tags=[\"fast_prop\"],\n    data_model=dm,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n)\n\npipe1\n</pre> pipe1 = getml.pipeline.Pipeline(     tags=[\"fast_prop\"],     data_model=dm,     preprocessors=[mapping],     feature_learners=[fast_prop], )  pipe1 Out[19]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['cites', 'content', 'paper'],\n         predictors=[],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['fast_prop'])</pre> <p>Model training</p> In\u00a0[20]: Copied! <pre>pipe1.fit(container.train)\n</pre> pipe1.fit(container.train) <pre>Checking data model...\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nChecking... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nThe pipeline check generated 3 issues labeled INFO and 0 issues labeled WARNING.\nTo see the issues in full, run .check() on the pipeline.\n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Trying 3780 features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:08, remaining: 00:00]          \n\nTrained pipeline.\nTime taken: 0h:0m:8.093514\n\n</pre> Out[20]: <pre>Pipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='CrossEntropyLoss',\n         peripheral=['cites', 'content', 'paper'],\n         predictors=[],\n         preprocessors=['Mapping'],\n         share_selected_features=0.5,\n         tags=['fast_prop', 'container-HXC9EH'])</pre> <p>Now comes the crucial trick! Instead of applying the pipeline on our test data to make predictions, we apply the pipeline on our train and test data and transform them to the learned (or optimal) features. It is these optimized features that will replace the word vectors as node attributes in the graph neural network.</p> In\u00a0[21]: Copied! <pre>trans_feat_train = pipe1.transform(container.train)\ntrans_feat_test = pipe1.transform(container.test)\n\noptimized_features = np.concatenate((trans_feat_train, trans_feat_test))\n</pre> trans_feat_train = pipe1.transform(container.train) trans_feat_test = pipe1.transform(container.test)  optimized_features = np.concatenate((trans_feat_train, trans_feat_test)) <pre>Staging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\nStaging... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nPreprocessing... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building subfeatures... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \nFastProp: Building features... 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| [elapsed: 00:00, remaining: 00:00]          \n\n</pre> <p>From here on out it is smooth sailing, because everything happens exactly as in the previous notebook. We create the graph object and populate it with the nodes containing the optimized features, the edges and masks.</p> In\u00a0[22]: Copied! <pre>optimized_x = torch.tensor(optimized_features, dtype=torch.float32)\n\ny = torch.tensor(np.array(index_aligned_df[\"class_label_enc\"].values), dtype=torch.long)\nedge_index = torch.tensor([cited_node, citing_node], dtype=torch.int64)\n\noptimized_graph_object = Data(x=optimized_x, edge_index=edge_index, y=y)\noptimized_graph_object.train_mask = torch.tensor(train_mask)\noptimized_graph_object.test_mask = torch.tensor(test_mask)\n</pre> optimized_x = torch.tensor(optimized_features, dtype=torch.float32)  y = torch.tensor(np.array(index_aligned_df[\"class_label_enc\"].values), dtype=torch.long) edge_index = torch.tensor([cited_node, citing_node], dtype=torch.int64)  optimized_graph_object = Data(x=optimized_x, edge_index=edge_index, y=y) optimized_graph_object.train_mask = torch.tensor(train_mask) optimized_graph_object.test_mask = torch.tensor(test_mask) <pre>/tmp/ipykernel_248862/41776833.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  edge_index = torch.tensor([cited_node, citing_node], dtype=torch.int64)\n</pre> In\u00a0[23]: Copied! <pre>print(optimized_graph_object)\nprint(\"==============================================================\")\n\nprint(f\"Number of nodes: {optimized_graph_object.num_nodes}\")\nprint(f\"Number of edges: {int(optimized_graph_object.num_edges/2)}\")\nprint(\n    f\"Average node degree: {(optimized_graph_object.num_edges) / optimized_graph_object.num_nodes:.2f}\"\n)\nprint(f\"Number of training nodes: {optimized_graph_object.train_mask.sum()}\")\nprint(f\"Number of test nodes: {optimized_graph_object.test_mask.sum()}\")\nprint(\n    f\"Training node label rate: {int(optimized_graph_object.train_mask.sum()) / optimized_graph_object.num_nodes:.2f}\"\n)\nprint(\n    f\"Test node label rate: {int(optimized_graph_object.test_mask.sum()) / optimized_graph_object.num_nodes:.2f}\"\n)\nprint(f\"Contains isolated nodes: {optimized_graph_object.has_isolated_nodes()}\")\nprint(f\"Contains self-loops: {optimized_graph_object.has_self_loops()}\")\nprint(f\"Is undirected: {optimized_graph_object.is_undirected()}\")\n</pre> print(optimized_graph_object) print(\"==============================================================\")  print(f\"Number of nodes: {optimized_graph_object.num_nodes}\") print(f\"Number of edges: {int(optimized_graph_object.num_edges/2)}\") print(     f\"Average node degree: {(optimized_graph_object.num_edges) / optimized_graph_object.num_nodes:.2f}\" ) print(f\"Number of training nodes: {optimized_graph_object.train_mask.sum()}\") print(f\"Number of test nodes: {optimized_graph_object.test_mask.sum()}\") print(     f\"Training node label rate: {int(optimized_graph_object.train_mask.sum()) / optimized_graph_object.num_nodes:.2f}\" ) print(     f\"Test node label rate: {int(optimized_graph_object.test_mask.sum()) / optimized_graph_object.num_nodes:.2f}\" ) print(f\"Contains isolated nodes: {optimized_graph_object.has_isolated_nodes()}\") print(f\"Contains self-loops: {optimized_graph_object.has_self_loops()}\") print(f\"Is undirected: {optimized_graph_object.is_undirected()}\") <pre>Data(x=[2708, 200], edge_index=[2, 10858], y=[2708], train_mask=[2708], test_mask=[2708])\n==============================================================\nNumber of nodes: 2708\nNumber of edges: 5429\nAverage node degree: 4.01\nNumber of training nodes: 1895\nNumber of test nodes: 813\nTraining node label rate: 0.70\nTest node label rate: 0.30\nContains isolated nodes: False\nContains self-loops: False\nIs undirected: True\n</pre> In\u00a0[24]: Copied! <pre>class GNNTrain:\n    def __init__(self, graph_object, nn_model):\n        self.graph_object = graph_object\n        self.nn_model = nn_model\n        self.loss_function = torch.nn.CrossEntropyLoss()\n        self.optimizer = torch.optim.Adam(\n            nn_model.parameters(), lr=0.01, weight_decay=5e-4\n        )\n\n    def train(self):\n        self.nn_model.train()\n        self.optimizer.zero_grad()\n        out = self.nn_model(self.graph_object.x, self.graph_object.edge_index)\n        loss = self.loss_function(\n            out[self.graph_object.train_mask],\n            self.graph_object.y[self.graph_object.train_mask],\n        )\n        loss.backward()  # compute loss\n        self.optimizer.step()  # apply grad\n        return loss\n\n    def test(self, mask):\n        self.nn_model.eval()\n        out = self.nn_model(self.graph_object.x, self.graph_object.edge_index)\n        pred = out.argmax(dim=1)\n        correct = pred[mask] == self.graph_object.y[mask]\n        acc = int(correct.sum()) / int(mask.sum())\n        return acc\n\n    def run(self):\n        max_test_acc = 0\n        test_acc_list = []\n        best_model = None\n        for epoch in range(1, 1001):\n            self.train()\n            train_acc = self.test(self.graph_object.train_mask)\n            test_acc = self.test(self.graph_object.test_mask)\n            test_acc_list.append(test_acc)\n            if test_acc &gt; max_test_acc:\n                max_test_acc = test_acc\n                best_model = self.nn_model\n            if epoch % 50 == 0:\n                print(\n                    f\"Epoch: {epoch:03d}, Train acc: {train_acc:.4f}\"\n                    f\", Test acc: {test_acc:.4f}\"\n                )\n        self.test_acc_list = test_acc_list\n        self.max_test_acc = max_test_acc\n        self.best_model = best_model\n</pre> class GNNTrain:     def __init__(self, graph_object, nn_model):         self.graph_object = graph_object         self.nn_model = nn_model         self.loss_function = torch.nn.CrossEntropyLoss()         self.optimizer = torch.optim.Adam(             nn_model.parameters(), lr=0.01, weight_decay=5e-4         )      def train(self):         self.nn_model.train()         self.optimizer.zero_grad()         out = self.nn_model(self.graph_object.x, self.graph_object.edge_index)         loss = self.loss_function(             out[self.graph_object.train_mask],             self.graph_object.y[self.graph_object.train_mask],         )         loss.backward()  # compute loss         self.optimizer.step()  # apply grad         return loss      def test(self, mask):         self.nn_model.eval()         out = self.nn_model(self.graph_object.x, self.graph_object.edge_index)         pred = out.argmax(dim=1)         correct = pred[mask] == self.graph_object.y[mask]         acc = int(correct.sum()) / int(mask.sum())         return acc      def run(self):         max_test_acc = 0         test_acc_list = []         best_model = None         for epoch in range(1, 1001):             self.train()             train_acc = self.test(self.graph_object.train_mask)             test_acc = self.test(self.graph_object.test_mask)             test_acc_list.append(test_acc)             if test_acc &gt; max_test_acc:                 max_test_acc = test_acc                 best_model = self.nn_model             if epoch % 50 == 0:                 print(                     f\"Epoch: {epoch:03d}, Train acc: {train_acc:.4f}\"                     f\", Test acc: {test_acc:.4f}\"                 )         self.test_acc_list = test_acc_list         self.max_test_acc = max_test_acc         self.best_model = best_model In\u00a0[25]: Copied! <pre>nn_model = GCNConv(\n    in_channels=optimized_graph_object.num_features,\n    out_channels=len(optimized_graph_object.y.unique()),\n)\noptimized_gcn = GNNTrain(graph_object=optimized_graph_object, nn_model=nn_model)\noptimized_gcn.run()\nprint(\"Maximum Test Accuracy: \", optimized_gcn.max_test_acc)\n</pre> nn_model = GCNConv(     in_channels=optimized_graph_object.num_features,     out_channels=len(optimized_graph_object.y.unique()), ) optimized_gcn = GNNTrain(graph_object=optimized_graph_object, nn_model=nn_model) optimized_gcn.run() print(\"Maximum Test Accuracy: \", optimized_gcn.max_test_acc) <pre>Epoch: 050, Train acc: 0.9203, Test acc: 0.9127\nEpoch: 100, Train acc: 0.9272, Test acc: 0.9151\nEpoch: 150, Train acc: 0.9303, Test acc: 0.9151\nEpoch: 200, Train acc: 0.9309, Test acc: 0.9176\nEpoch: 250, Train acc: 0.9325, Test acc: 0.9188\nEpoch: 300, Train acc: 0.9356, Test acc: 0.9188\nEpoch: 350, Train acc: 0.9361, Test acc: 0.9200\nEpoch: 400, Train acc: 0.9367, Test acc: 0.9213\nEpoch: 450, Train acc: 0.9372, Test acc: 0.9200\nEpoch: 500, Train acc: 0.9377, Test acc: 0.9188\nEpoch: 550, Train acc: 0.9383, Test acc: 0.9200\nEpoch: 600, Train acc: 0.9393, Test acc: 0.9188\nEpoch: 650, Train acc: 0.9398, Test acc: 0.9188\nEpoch: 700, Train acc: 0.9393, Test acc: 0.9176\nEpoch: 750, Train acc: 0.9393, Test acc: 0.9188\nEpoch: 800, Train acc: 0.9398, Test acc: 0.9176\nEpoch: 850, Train acc: 0.9398, Test acc: 0.9164\nEpoch: 900, Train acc: 0.9398, Test acc: 0.9164\nEpoch: 950, Train acc: 0.9398, Test acc: 0.9176\nEpoch: 1000, Train acc: 0.9398, Test acc: 0.9176\nMaximum Test Accuracy:  0.9212792127921279\n</pre> <p>92% Accuracy! This outcome looks really promising. It compares favourably to the GNN standalone solution (87.6%) and the getML standalone solution (88.3%) of the previous notebook.</p> <p>Let's dig a bit deeper and lift our results on more solid grounds. We ran a large scale experiment with 100 random train test splits, the raw results of which can be found below:</p> In\u00a0[26]: Copied! <pre>getml_accs = [\n    0.89012,\n    0.87812,\n    0.87627,\n    0.89474,\n    0.87442,\n    0.8892,\n    0.89751,\n    0.88273,\n    0.88181,\n    0.88089,\n    0.88366,\n    0.87996,\n    0.89566,\n    0.87627,\n    0.89104,\n    0.87719,\n    0.88366,\n    0.88366,\n    0.87258,\n    0.8892,\n    0.89104,\n    0.8855,\n    0.87812,\n    0.8892,\n    0.88827,\n    0.88273,\n    0.87165,\n    0.86981,\n    0.88273,\n    0.88735,\n    0.8735,\n    0.87627,\n    0.87719,\n    0.8892,\n    0.88366,\n    0.87627,\n    0.88181,\n    0.86981,\n    0.87627,\n    0.89751,\n    0.8855,\n    0.87165,\n    0.87073,\n    0.88827,\n    0.88827,\n    0.88181,\n    0.88827,\n    0.88089,\n    0.87073,\n    0.8892,\n    0.89012,\n    0.87442,\n    0.88735,\n    0.87073,\n    0.89197,\n    0.89751,\n    0.88827,\n    0.89289,\n    0.88089,\n    0.8855,\n    0.88366,\n    0.88458,\n    0.88181,\n    0.87442,\n    0.88735,\n    0.86981,\n    0.87812,\n    0.86888,\n    0.86981,\n    0.88827,\n    0.87627,\n    0.88827,\n    0.89289,\n    0.88181,\n    0.87442,\n    0.88181,\n    0.8855,\n    0.88181,\n    0.87535,\n    0.87812,\n    0.86519,\n    0.89104,\n    0.8892,\n    0.88366,\n    0.87258,\n    0.87719,\n    0.87719,\n    0.88366,\n    0.87535,\n    0.87904,\n    0.88089,\n    0.88643,\n    0.88273,\n    0.89197,\n    0.89381,\n    0.89381,\n    0.87258,\n    0.89197,\n    0.86981,\n    0.8855,\n]\n\noriginal_accs = [\n    0.88284,\n    0.87823,\n    0.86162,\n    0.87085,\n    0.87638,\n    0.87915,\n    0.88192,\n    0.86624,\n    0.88838,\n    0.86531,\n    0.88376,\n    0.87269,\n    0.88284,\n    0.87269,\n    0.87177,\n    0.86439,\n    0.8607,\n    0.87454,\n    0.8607,\n    0.87454,\n    0.87269,\n    0.88469,\n    0.85701,\n    0.89207,\n    0.88838,\n    0.87731,\n    0.86993,\n    0.87269,\n    0.87085,\n    0.87454,\n    0.86439,\n    0.89207,\n    0.87823,\n    0.87546,\n    0.87454,\n    0.86993,\n    0.87362,\n    0.869,\n    0.86347,\n    0.86347,\n    0.87454,\n    0.86531,\n    0.86624,\n    0.87454,\n    0.87454,\n    0.87269,\n    0.87546,\n    0.869,\n    0.86993,\n    0.88469,\n    0.87915,\n    0.86808,\n    0.87085,\n    0.86716,\n    0.87362,\n    0.87454,\n    0.88469,\n    0.88745,\n    0.86255,\n    0.87546,\n    0.87546,\n    0.88284,\n    0.85793,\n    0.86808,\n    0.87731,\n    0.87454,\n    0.88376,\n    0.85978,\n    0.86347,\n    0.881,\n    0.86624,\n    0.88561,\n    0.88653,\n    0.89391,\n    0.87362,\n    0.869,\n    0.86993,\n    0.88376,\n    0.88376,\n    0.87362,\n    0.8524,\n    0.89483,\n    0.86808,\n    0.88284,\n    0.88192,\n    0.87546,\n    0.85886,\n    0.87269,\n    0.87269,\n    0.87915,\n    0.86255,\n    0.869,\n    0.87085,\n    0.87731,\n    0.87915,\n    0.87362,\n    0.869,\n    0.881,\n    0.84963,\n    0.88376,\n]\n\noptimized_accs = [\n    0.92343,\n    0.92251,\n    0.9179,\n    0.93266,\n    0.9262,\n    0.93266,\n    0.94096,\n    0.92712,\n    0.93173,\n    0.91882,\n    0.93358,\n    0.92804,\n    0.92435,\n    0.92989,\n    0.9262,\n    0.92989,\n    0.92528,\n    0.92159,\n    0.9179,\n    0.92343,\n    0.92989,\n    0.93266,\n    0.9179,\n    0.93081,\n    0.92343,\n    0.92066,\n    0.91513,\n    0.91052,\n    0.91882,\n    0.92712,\n    0.92066,\n    0.92343,\n    0.93173,\n    0.92804,\n    0.91974,\n    0.91882,\n    0.92989,\n    0.9262,\n    0.91882,\n    0.92989,\n    0.92712,\n    0.92343,\n    0.92712,\n    0.92804,\n    0.92159,\n    0.92251,\n    0.93173,\n    0.92159,\n    0.90959,\n    0.92251,\n    0.92343,\n    0.91974,\n    0.92528,\n    0.92066,\n    0.92712,\n    0.93358,\n    0.93173,\n    0.9345,\n    0.92804,\n    0.92343,\n    0.92989,\n    0.9345,\n    0.92159,\n    0.92804,\n    0.93173,\n    0.91236,\n    0.91882,\n    0.9179,\n    0.91605,\n    0.92712,\n    0.9262,\n    0.93266,\n    0.93266,\n    0.92712,\n    0.92251,\n    0.92804,\n    0.92712,\n    0.92528,\n    0.92251,\n    0.9262,\n    0.90867,\n    0.93173,\n    0.93266,\n    0.9262,\n    0.92251,\n    0.9262,\n    0.91513,\n    0.91974,\n    0.92343,\n    0.90959,\n    0.92528,\n    0.92712,\n    0.92989,\n    0.9345,\n    0.93081,\n    0.9345,\n    0.92251,\n    0.92159,\n    0.91605,\n    0.9262,\n]\n</pre> getml_accs = [     0.89012,     0.87812,     0.87627,     0.89474,     0.87442,     0.8892,     0.89751,     0.88273,     0.88181,     0.88089,     0.88366,     0.87996,     0.89566,     0.87627,     0.89104,     0.87719,     0.88366,     0.88366,     0.87258,     0.8892,     0.89104,     0.8855,     0.87812,     0.8892,     0.88827,     0.88273,     0.87165,     0.86981,     0.88273,     0.88735,     0.8735,     0.87627,     0.87719,     0.8892,     0.88366,     0.87627,     0.88181,     0.86981,     0.87627,     0.89751,     0.8855,     0.87165,     0.87073,     0.88827,     0.88827,     0.88181,     0.88827,     0.88089,     0.87073,     0.8892,     0.89012,     0.87442,     0.88735,     0.87073,     0.89197,     0.89751,     0.88827,     0.89289,     0.88089,     0.8855,     0.88366,     0.88458,     0.88181,     0.87442,     0.88735,     0.86981,     0.87812,     0.86888,     0.86981,     0.88827,     0.87627,     0.88827,     0.89289,     0.88181,     0.87442,     0.88181,     0.8855,     0.88181,     0.87535,     0.87812,     0.86519,     0.89104,     0.8892,     0.88366,     0.87258,     0.87719,     0.87719,     0.88366,     0.87535,     0.87904,     0.88089,     0.88643,     0.88273,     0.89197,     0.89381,     0.89381,     0.87258,     0.89197,     0.86981,     0.8855, ]  original_accs = [     0.88284,     0.87823,     0.86162,     0.87085,     0.87638,     0.87915,     0.88192,     0.86624,     0.88838,     0.86531,     0.88376,     0.87269,     0.88284,     0.87269,     0.87177,     0.86439,     0.8607,     0.87454,     0.8607,     0.87454,     0.87269,     0.88469,     0.85701,     0.89207,     0.88838,     0.87731,     0.86993,     0.87269,     0.87085,     0.87454,     0.86439,     0.89207,     0.87823,     0.87546,     0.87454,     0.86993,     0.87362,     0.869,     0.86347,     0.86347,     0.87454,     0.86531,     0.86624,     0.87454,     0.87454,     0.87269,     0.87546,     0.869,     0.86993,     0.88469,     0.87915,     0.86808,     0.87085,     0.86716,     0.87362,     0.87454,     0.88469,     0.88745,     0.86255,     0.87546,     0.87546,     0.88284,     0.85793,     0.86808,     0.87731,     0.87454,     0.88376,     0.85978,     0.86347,     0.881,     0.86624,     0.88561,     0.88653,     0.89391,     0.87362,     0.869,     0.86993,     0.88376,     0.88376,     0.87362,     0.8524,     0.89483,     0.86808,     0.88284,     0.88192,     0.87546,     0.85886,     0.87269,     0.87269,     0.87915,     0.86255,     0.869,     0.87085,     0.87731,     0.87915,     0.87362,     0.869,     0.881,     0.84963,     0.88376, ]  optimized_accs = [     0.92343,     0.92251,     0.9179,     0.93266,     0.9262,     0.93266,     0.94096,     0.92712,     0.93173,     0.91882,     0.93358,     0.92804,     0.92435,     0.92989,     0.9262,     0.92989,     0.92528,     0.92159,     0.9179,     0.92343,     0.92989,     0.93266,     0.9179,     0.93081,     0.92343,     0.92066,     0.91513,     0.91052,     0.91882,     0.92712,     0.92066,     0.92343,     0.93173,     0.92804,     0.91974,     0.91882,     0.92989,     0.9262,     0.91882,     0.92989,     0.92712,     0.92343,     0.92712,     0.92804,     0.92159,     0.92251,     0.93173,     0.92159,     0.90959,     0.92251,     0.92343,     0.91974,     0.92528,     0.92066,     0.92712,     0.93358,     0.93173,     0.9345,     0.92804,     0.92343,     0.92989,     0.9345,     0.92159,     0.92804,     0.93173,     0.91236,     0.91882,     0.9179,     0.91605,     0.92712,     0.9262,     0.93266,     0.93266,     0.92712,     0.92251,     0.92804,     0.92712,     0.92528,     0.92251,     0.9262,     0.90867,     0.93173,     0.93266,     0.9262,     0.92251,     0.9262,     0.91513,     0.91974,     0.92343,     0.90959,     0.92528,     0.92712,     0.92989,     0.9345,     0.93081,     0.9345,     0.92251,     0.92159,     0.91605,     0.9262, ] In\u00a0[27]: Copied! <pre>print(\"getML standalone mean accuracy: \", round(np.mean(getml_accs), 4))\nprint(\"GNN standalone mean accuracy: \", round(np.mean(original_accs), 4))\nprint(\"GNN + getML accuracy: \", round(np.mean(optimized_accs), 4))\nprint()\nprint(\"Accuracy gain :\", round(np.mean(optimized_accs) - np.mean(original_accs), 4))\n</pre> print(\"getML standalone mean accuracy: \", round(np.mean(getml_accs), 4)) print(\"GNN standalone mean accuracy: \", round(np.mean(original_accs), 4)) print(\"GNN + getML accuracy: \", round(np.mean(optimized_accs), 4)) print() print(\"Accuracy gain :\", round(np.mean(optimized_accs) - np.mean(original_accs), 4)) <pre>getML standalone mean accuracy:  0.8823\nGNN standalone mean accuracy:  0.8739\nGNN + getML accuracy:  0.925\n\nAccuracy gain : 0.0511\n</pre> <p>There we have it: By simply adding getML, off-the-shelf GNN implementations gain a whopping 5.1% points in accuracy! The following histogram illustrates the difference.</p> In\u00a0[28]: Copied! <pre>value_list = []\nlabel_list = []\n\nfor label, accs in [\n    (\"getML standalone\", getml_accs),\n    (\"GNN standalone\", original_accs),\n    (\"GNN + getML\", optimized_accs),\n]:\n    value_list = value_list + accs\n    label_list = label_list + len(accs) * [label]\nsns_df = pd.DataFrame({\"Accuracy\": value_list, \"Solution\": label_list})\nsns.displot(sns_df, x=\"Accuracy\", hue=\"Solution\", stat=\"density\", bins=35)\n</pre> value_list = [] label_list = []  for label, accs in [     (\"getML standalone\", getml_accs),     (\"GNN standalone\", original_accs),     (\"GNN + getML\", optimized_accs), ]:     value_list = value_list + accs     label_list = label_list + len(accs) * [label] sns_df = pd.DataFrame({\"Accuracy\": value_list, \"Solution\": label_list}) sns.displot(sns_df, x=\"Accuracy\", hue=\"Solution\", stat=\"density\", bins=35) Out[28]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x71edb34c38b0&gt;</pre> <p>Though overlapping, we see that the getML standalone implementation performs slightly better than the GNN standalone solution. Much more eyecatching, however, is the massive boost in accuracy, when both approaches are combined and integrated.</p> <p>When visualizing the learned embeddings of both GNN versions, the contrast between the two becomes also apparent. Let's quickly run a standalone GNN's model to output the node embeddings.</p> In\u00a0[29]: Copied! <pre>original_x = torch.tensor(\n    index_aligned_df.word_vector.values.tolist(), dtype=torch.float32\n)\n\ny = torch.tensor(np.array(index_aligned_df[\"class_label_enc\"].values), dtype=torch.long)\nedge_index = torch.tensor([cited_node, citing_node], dtype=torch.int64)\n\noriginal_graph_object = Data(x=original_x, edge_index=edge_index, y=y)\noriginal_graph_object.train_mask = torch.tensor(train_mask)\noriginal_graph_object.test_mask = torch.tensor(test_mask)\n</pre> original_x = torch.tensor(     index_aligned_df.word_vector.values.tolist(), dtype=torch.float32 )  y = torch.tensor(np.array(index_aligned_df[\"class_label_enc\"].values), dtype=torch.long) edge_index = torch.tensor([cited_node, citing_node], dtype=torch.int64)  original_graph_object = Data(x=original_x, edge_index=edge_index, y=y) original_graph_object.train_mask = torch.tensor(train_mask) original_graph_object.test_mask = torch.tensor(test_mask) In\u00a0[30]: Copied! <pre>nn_model = GCNConv(\n    in_channels=original_graph_object.num_features,\n    out_channels=len(original_graph_object.y.unique()),\n)\noriginal_gcn = GNNTrain(graph_object=original_graph_object, nn_model=nn_model)\noriginal_gcn.run()\nprint(\"Maximum Test Accuracy: \", original_gcn.max_test_acc)\n</pre> nn_model = GCNConv(     in_channels=original_graph_object.num_features,     out_channels=len(original_graph_object.y.unique()), ) original_gcn = GNNTrain(graph_object=original_graph_object, nn_model=nn_model) original_gcn.run() print(\"Maximum Test Accuracy: \", original_gcn.max_test_acc) <pre>Epoch: 050, Train acc: 0.9420, Test acc: 0.8659\nEpoch: 100, Train acc: 0.9567, Test acc: 0.8758\nEpoch: 150, Train acc: 0.9646, Test acc: 0.8758\nEpoch: 200, Train acc: 0.9657, Test acc: 0.8745\nEpoch: 250, Train acc: 0.9662, Test acc: 0.8708\nEpoch: 300, Train acc: 0.9673, Test acc: 0.8745\nEpoch: 350, Train acc: 0.9662, Test acc: 0.8770\nEpoch: 400, Train acc: 0.9673, Test acc: 0.8758\nEpoch: 450, Train acc: 0.9668, Test acc: 0.8758\nEpoch: 500, Train acc: 0.9673, Test acc: 0.8770\nEpoch: 550, Train acc: 0.9673, Test acc: 0.8782\nEpoch: 600, Train acc: 0.9678, Test acc: 0.8782\nEpoch: 650, Train acc: 0.9678, Test acc: 0.8782\nEpoch: 700, Train acc: 0.9678, Test acc: 0.8795\nEpoch: 750, Train acc: 0.9678, Test acc: 0.8795\nEpoch: 800, Train acc: 0.9678, Test acc: 0.8770\nEpoch: 850, Train acc: 0.9678, Test acc: 0.8782\nEpoch: 900, Train acc: 0.9678, Test acc: 0.8782\nEpoch: 950, Train acc: 0.9678, Test acc: 0.8770\nEpoch: 1000, Train acc: 0.9678, Test acc: 0.8770\nMaximum Test Accuracy:  0.8794587945879458\n</pre> In\u00a0[31]: Copied! <pre>def visualize_embeddings(h1, h2, color):\n    z1 = TSNE(n_components=2).fit_transform(h1.detach().cpu().numpy())\n    z2 = TSNE(n_components=2).fit_transform(h2.detach().cpu().numpy())\n\n    z1 = pd.DataFrame(z1)\n    z1[\"label\"] = color\n    z2 = pd.DataFrame(z2)\n    z2[\"label\"] = color\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n\n    sns.scatterplot(z1, x=0, y=1, hue=\"label\", ax=ax1, palette=\"Set1\")\n    sns.scatterplot(z2, x=0, y=1, hue=\"label\", ax=ax2, palette=\"Set1\")\n\n    ax1.set_title(\"GNN with getML features\")\n    ax2.set_title(\"GNN only\")\n\n    plt.show()\n</pre> def visualize_embeddings(h1, h2, color):     z1 = TSNE(n_components=2).fit_transform(h1.detach().cpu().numpy())     z2 = TSNE(n_components=2).fit_transform(h2.detach().cpu().numpy())      z1 = pd.DataFrame(z1)     z1[\"label\"] = color     z2 = pd.DataFrame(z2)     z2[\"label\"] = color      fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))      sns.scatterplot(z1, x=0, y=1, hue=\"label\", ax=ax1, palette=\"Set1\")     sns.scatterplot(z2, x=0, y=1, hue=\"label\", ax=ax2, palette=\"Set1\")      ax1.set_title(\"GNN with getML features\")     ax2.set_title(\"GNN only\")      plt.show() In\u00a0[32]: Copied! <pre>visualize_embeddings(\n    optimized_gcn.best_model(\n        optimized_graph_object.x, optimized_graph_object.edge_index\n    ),\n    original_gcn.best_model(original_graph_object.x, original_graph_object.edge_index),\n    color=original_graph_object.y,\n)\n</pre> visualize_embeddings(     optimized_gcn.best_model(         optimized_graph_object.x, optimized_graph_object.edge_index     ),     original_gcn.best_model(original_graph_object.x, original_graph_object.edge_index),     color=original_graph_object.y, ) <p>In the integrated solution, the learned embeddings are much better fine tuned to distinguish between labels. More coherent and well delineated label patches in the graphic representation above reflect that ability. That is the difference 5 percentage points of accuracy can make!</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/getml-and-gnns-a-natural-symbiosis/#getml-and-graph-neural-networks-a-natural-symbiosis","title":"getML and Graph Neural Networks: A Natural Symbiosis\u00b6","text":"<p>In the previous notebook we compared the predictive performance of getML's relational feature learning approach with off-the-shelf Graph Neural Network (GNN) implementations. We demonstrated a slight performance gain when using the relational feature learner over the GNN approach. Naturally, now the question arises: is it possible to combine both approaches, pick the best of both worlds and boost predictive power?</p> <p>The answer is a resounding Yes!</p> <p>The input to GNN's Neural Network is the one-hot-encoded word matrix of the papers' abstracts. This matrix is very sparse and represents target relevant information inefficently. getML's power lies in aggregating over large amounts of data and distilling relevant target information into few, highly optimized features. That means, both approaches are completely compatible! Features are engineered from word matrix based on the tabular form of the CORA data set. That smaller set of features is given to the GNN to learn the embeddings for every node and predict its class label.</p> <p>And that is exactly what we will do in this notebook!</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/getml-and-gnns-a-natural-symbiosis/#setup-and-download-of-data","title":"Setup and Download of Data \u00b6","text":"<p>With respect to contextual settings, we keep everything as in the former notebook: We use the well entrenched benchmarking data set CORA, which consists of academic papers, their references and the word composition of their respective abstracts. We start out from the tabular format of the data set. Make sure you use Python 3.9.19.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/getml-and-gnns-a-natural-symbiosis/#data-preparation","title":"Data Preparation \u00b6","text":"<p>Data preparation is similar to the \"ordinary\" GNN analysis. We establish the node index and derive from it the edge index. The main difference is, that we do not need to vectorize the word information, because we replace the raw word information with the feature vector, optimized by getML. However, later we use the original GNN implementation for comparison purposes. For this purpose, we vectorize the word information and add it to the dataframe.</p>","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/getml-and-gnns-a-natural-symbiosis/#feature-learning-with-getml","title":"Feature Learning with getML \u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/getml-and-gnns-a-natural-symbiosis/#graph-neural-network-with-getml-engineered-features","title":"Graph Neural Network with getML Engineered Features \u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/getml-and-gnns-a-natural-symbiosis/#comparison-of-approaches","title":"Comparison of Approaches \u00b6","text":"","boost":0.8},{"location":"examples/enterprise-notebooks/kaggle_notebooks/getml-and-gnns-a-natural-symbiosis/#conclusion","title":"Conclusion \u00b6","text":"<p>In this notebook we have gone beyond the vanilla implementations of getML and GNN. Instead, we combined them and harnessed the best of both worlds. And the results, again, speak for themselves. A gain of more than 5% points in accuracy pushes that implementation well ahead of state of the art GNN development, and ranks us #1 at Papers with Code.</p> <p>In the next notebook we will explore the different neural net layers that pytorch provides and we will investigate how the choice of layers affects the performance when combined with getML feature engineering. So, stay tuned!</p>","boost":0.8},{"location":"examples/integrations/fastapi/fastapi/","title":"FastAPI","text":""},{"location":"examples/integrations/fastapi/fastapi/#provide-generic-prediction-endpoint-via-fastapi","title":"Provide generic prediction endpoint via FastAPI","text":"<p>A common way to communicate with resources is via REST-APIs. In Python, FastAPI is a well known web framework package to build web-APIs.</p> <p>The following shows an example how easily, pipelines in a project can be made accessible via endpoints in FastAPI.</p> <p>It is assumed that you have some basic knowledge of FastAPI and the getML framework.</p> <p>Helpful resources to get started:</p> <p>FastAPI get started getML example notebooks getML user guide </p> <p>This integration example requires at least v1.4.0 of the getml package and at least Python 3.8.</p>"},{"location":"examples/integrations/fastapi/fastapi/#example-data","title":"Example Data","text":"<p>As an example project we first run the demo notebook \"Loan default prediction\" which creates a project named \"loans\" in the getML Engine.</p>"},{"location":"examples/integrations/fastapi/fastapi/#code-explained","title":"Code Explained","text":"<p>First, import the necessary packages and create a FastAPI-App <code>app</code>. If the Engine isn't running yet  (<code>getml.engine.is_engine_alive()</code>) launch it  (<code>getml.engine.launch()</code>). The <code>launch_browser=False</code>  option prevents the browser to be opened  when the Engine spins up. Further, direct the Engine to load and set the previously created  <code>project</code> \"loans\". (<code>getml.engine.set_project()</code>)</p> <pre><code>from typing import Dict, List, Optional, Union\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom uvicorn import run\nfrom getml import engine, pipeline, Pipeline, DataFrame\n\napp: FastAPI = FastAPI()\n\nif not engine.is_alive():\n    engine.launch(launch_browser=False)\nengine.set_project(\"loans\")\n</code></pre> <p>Create the first GET endpoint which returns a list with all <code>pipeline</code>s present (<code>list_pipelines()</code>) in the project. The list itself will only contain the names of the pipelines and no additional metainformation. For sake of simplicity of the tutorial pagination is left out.</p> <pre><code>@app.get(\"/pipeline\")\nasync def get_pipeline() -&gt; List[str]:\n    return pipeline.list_pipelines()\n</code></pre> <p>The following is required to start the app with uvicorn. Run your Python code and test the endpoint via localhost:8080/pipeline.</p> <pre><code>if __name__ == \"__main__\":\n    run(app, host=\"localhost\", port=8080)\n</code></pre> <p>To expand the functionality, add another informative GET endpoint for a single pipeline. The <code>pipeline_id</code> can be retrieved from the previously created GET endpoint. The existence of the pipeline can be checked using <code>exists()</code>. After validating its existence, the Engine must be directed to load the pipeline identified with the provided <code>pipeline_id</code>. Information of interest could be the name of the population data frame and peripheral data frames, the applied preprocessors, used feature learners and selectors and target predictors. Those information can be retrieved from the member variable <code>metadata</code> of the pipeline (<code>pipeline_.metadata</code>) and the pipeline itself.  Again this endpoint can be tested by running your code and invoking the endpoint localhost:8080/pipeline/a1b2c3 assuming that the previously created pipeline has the id <code>a1b2c3</code>.</p> <pre><code>@app.get(\"/pipeline/{pipeline_id}\")\nasync def get_pipeline_pipeline_id(pipeline_id: str) -&gt; Dict[str, Union[str, List[str]]]:\n    if not pipeline.exists(pipeline_id):\n        raise HTTPException(status_code=404, detail=f'Pipeline {pipeline_id} not found.')\n\n    pipeline_ = pipeline.load(pipeline_id)\n\n    if pipeline_.metadata is None:\n        raise HTTPException(status_code=409,\n                            detail='The data schema is missing or pipeline is incomplete')\n\n    meta_data = pipeline_.metadata\n    metadata: Dict[str, Union[str, List[str]]] = {}\n    metadata[\"data_model\"] = meta_data.population.name\n    metadata[\"peripheral\"] = [_.name for _ in meta_data.peripheral]\n    metadata[\"preprocessors\"] = [_.type for _ in pipeline_.preprocessors]\n    metadata[\"feature_learners\"] = [_.type for _ in pipeline_.feature_learners]\n    metadata[\"feature_selectors\"] = [_.type for _ in pipeline_.feature_selectors]\n    metadata[\"predictors\"] = [_.type for _ in pipeline_.predictors]\n\n    return metadata\n</code></pre> <p>To create the prediction endpoint the data scheme for the request body needs to be created first. For a prediction the getML Engine requires multiple data sets, the population data set <code>population</code> and any related peripheral data set <code>peripheral</code> based on the Data model of the pipeline. The peripheral data sets can be either a list or a dictionary where the order of the data sets in the list needs to match the order returned by <code>[_.name for _ in getml.pipeline.metadata.peripheral]</code>. This information can also be retrieved by calling the previously created GET endpoint.</p> <pre><code>class PredictionBody(BaseModel):\n    peripheral: Union[List[Dict[str, List]], Dict[str, Dict[str, List]]]\n    population: Dict[str, List]\n</code></pre> <p>Next up, implement the POST endpoint which accepts data to task the Engine to make a prediction. Validate that the pipeline exist, load the pipeline (<code>load()</code>), and validate that the pipeline has been finalized.</p> <pre><code>@app.post(\"/pipeline/{pipeline_id}/predict\")\nasync def post_project_predict(pipeline_id: str, body: PredictionBody) -&gt; Optional[List]:\n    if not pipeline.exists(pipeline_id):\n        raise HTTPException(status_code=404,\n                            detail=f'Pipeline {pipeline_id} not found.')\n\n    pipeline_: Pipeline = pipeline.load(pipeline_id)\n\n    if pipeline_.metadata is None:\n        raise HTTPException(status_code=409,\n                            detail='The data schema is missing or pipeline is incomplete')\n</code></pre> <p>The request body should contain both the population and peripheral data. Check that the population in the request body contains any content. Create a data frame from the dictionary (<code>from_dict()</code>): the name of the data frame must not collide with an existing data frame in the pipeline, the roles of the population can be obtained from the pipeline, using <code>pipeline_.metadata.population.roles</code>.</p> <pre><code>if not body.population:\n    raise HTTPException(status_code=400, detail='Missing population data.')\n\npopulation_data_frame = DataFrame.from_dict(name='future',\n                                            roles=pipeline_.metadata.population.roles,\n                                            data=body.population)\n</code></pre> <p>The peripheral can be submitted in the request body both as list and dictionary. Check that in case the peripheral data sets are received as dictionaries that the names of all required peripheral data sets exist in the dictionary keys, and in case the peripheral data sets are received as a list, check that the length of the list matches the number of peripheral data sets in the pipeline. After, create a list of data frames of the peripheral data. Again, ensure that the names of the created data frames do not collide with existing data frames and use the roles defined in the pipeline for the peripheral data sets (<code>pipeline_.metadata.peripheral[i].roles</code>).</p> <pre><code>peripheral_names = [_.name for _ in pipeline_.peripheral]\n\nif isinstance(body.peripheral, dict):\n    if set(peripheral_names) - set(body.peripheral.keys()):\n        raise HTTPException(\n            status_code=400,\n            detail=f'Missing peripheral data, expected {peripheral_names}')\n    periperal_raw_data = body.peripheral\nelse:\n    if len(peripheral_names) != len(body.peripheral):\n        raise HTTPException(\n            status_code=400,\n            detail=f\"Expected {len(pipeline_.peripheral)} peripheral data frames.\")\n    periperal_raw_data = dict(zip(peripheral_names, body.peripheral))\n\nperipheral_data_frames = [\n    DataFrame.from_dict(name=name + '_predict',\n                        data=periperal_raw_data[name],\n                        roles=pipeline_.metadata.peripheral[i].roles)\n    for i, name in enumerate(peripheral_names)\n]\n</code></pre> <p>This leaves the actual call to the Engine to make a prediction (<code>predict()</code>) using the previously created population data frame and peripheral data frames. The predicted target value is a numpy array and returned transformed to a list as request response. </p> <pre><code>prediction = pipeline_.predict(\n    population_table=population_data_frame,\n    peripheral_tables=peripheral_data_frames\n)\n\nif prediction:\n    return prediction.tolist()\n\nraise HTTPException(status_code=500, detail='getML Engine didn\\'t return a result.')\n</code></pre> <p>This endpoint can be called on localhost:8080/pipeline/a1b2c3/predict, where the body needs the form: </p> <pre><code>{\n    \"peripheral\": [{\n        \"column_1\": [2.4, 3.0, 1.2, 1.4, 2.2],\n        \"column_2\": [\"a\", \"b\", \"a\", \"b\", \"b\"]\n    }],\n    \"population\": {\n        \"column_1\": [0.2, 0.1],\n        \"column_2\": [\"a\", \"b\"],\n        \"time_stamp\": [\"2010-01-01 12:30:00\", \"2010-01-01 23:30:00\"]\n    }\n}\n</code></pre> <p>Example json data can be extracted from the notebook using the following code snippet at the end of the notebook used to create the Example Data.</p> <pre><code>from typing import Union, Any\nfrom datetime import datetime\nfrom json import dumps\n\n\ndef handle_timestamp(x: Union[Any, datetime]):\n    if isinstance(x, datetime):\n        return x.strftime(r'%Y-%m-%d %H:%M:%S')\n\n\npd_population_test = population_test.to_pandas()\naccount_id = pd_population_test.iloc[0][\"account_id\"]\npopulaton_dict = pd_population_test[pd_population_test[\"account_id\"] == account_id].to_dict()\npopulaton_json = dumps({k: list(v.values()) for k, v in populaton_dict.items()}, default=handle_timestamp)\npd_peripherals = {_.name: _.to_pandas() for _ in [order, trans, meta]}\nperipheral_dict = {k: v[v[\"account_id\"] == account_id].to_dict() for k, v in pd_peripherals.items()}\nperipheral_json = dumps(\n    {k: {vk: list(vv.values()) for vk, vv in v.items()} for k, v in peripheral_dict.items()},\n    default=handle_timestamp)\npopulaton_json\nperipheral_json\n</code></pre>"},{"location":"examples/integrations/fastapi/fastapi/#conclusion","title":"Conclusion","text":"<p>With only a few lines it is possible to create a web API to make project pipelines accessible and request target predictions for provided population and peripheral data.</p>"},{"location":"examples/integrations/vertexai/vertexai/","title":"<span class=\"ntitle\">vertexai.ipynb</span> <span class=\"ndesc\">getML on Vertex AI</span>","text":"In\u00a0[\u00a0]: Copied! <pre>import os\n\nIS_WORKBENCH_ENV = \"GOOGLE_VM_CONFIG_LOCK_FILE\" in os.environ\n</pre> import os  IS_WORKBENCH_ENV = \"GOOGLE_VM_CONFIG_LOCK_FILE\" in os.environ <p>For more information on getML, checkout the documentation</p> In\u00a0[\u00a0]: Copied! <pre># type: ignore\n\nif IS_WORKBENCH_ENV:\n    # stip-components=1 is necessary to avoid creating a directory with the name of the repository\n    ! curl -L https://api.github.com/repos/getml/getml-demo/tarball/vertexai | tar --strip-components=1 -xz\n! uv pip install --force-reinstall \".\"\n</pre> # type: ignore  if IS_WORKBENCH_ENV:     # stip-components=1 is necessary to avoid creating a directory with the name of the repository     ! curl -L https://api.github.com/repos/getml/getml-demo/tarball/vertexai | tar --strip-components=1 -xz ! uv pip install --force-reinstall \".\" In\u00a0[\u00a0]: Copied! <pre># type: ignore\n\nif IS_WORKBENCH_ENV:\n    import IPython\n\n    app = IPython.Application.instance()\n    app.kernel.do_shutdown(True)\n</pre> # type: ignore  if IS_WORKBENCH_ENV:     import IPython      app = IPython.Application.instance()     app.kernel.do_shutdown(True) \u26a0\ufe0f On Workbench: The kernel is going to restart. Wait until it's finished before continuing to the next step. \u26a0\ufe0f <p>Redefine <code>IS_WORKBENCH_ENV</code> after kernel restart</p> In\u00a0[\u00a0]: Copied! <pre>import os\n\nIS_WORKBENCH_ENV = \"GOOGLE_VM_CONFIG_LOCK_FILE\" in os.environ\n</pre> import os  IS_WORKBENCH_ENV = \"GOOGLE_VM_CONFIG_LOCK_FILE\" in os.environ In\u00a0[\u00a0]: Copied! <pre>from google.cloud import aiplatform\n</pre> from google.cloud import aiplatform In\u00a0[\u00a0]: Copied! <pre>from getml.vertexai.config import Config\n\ncfg = Config(\n    {\n        \"GCP_PROJECT_NAME\": \"\",  # NOTE: Must be globally(!) unique on GCP\n        \"BUCKET_NAME\": \"\",  # NOTE: Must be globally(!) unique on GCP\n        \"BUCKET_DIR_MODEL\": \"model_artifact\",\n        \"BUCKET_DIR_DATASET\": \"datasets\",\n        \"REGION\": \"europe-west1\",  # NOTE: Adapt to your preferred region\n        \"SERVICE_ACCOUNT_NAME\": \"getml-vertexai-sa\",  # NOTE: Gets replaced, if you run on Vertex AI Workbench\n        \"DOCKER_REPOSITORY\": \"getml-vertexai-docker-repository\",\n        \"GETML_PROJECT_NAME\": \"Loans\",\n    }\n)\n\n# Save configuration for later use in Docker containers\ncfg.save(\"config.yaml\")\n</pre> from getml.vertexai.config import Config  cfg = Config(     {         \"GCP_PROJECT_NAME\": \"\",  # NOTE: Must be globally(!) unique on GCP         \"BUCKET_NAME\": \"\",  # NOTE: Must be globally(!) unique on GCP         \"BUCKET_DIR_MODEL\": \"model_artifact\",         \"BUCKET_DIR_DATASET\": \"datasets\",         \"REGION\": \"europe-west1\",  # NOTE: Adapt to your preferred region         \"SERVICE_ACCOUNT_NAME\": \"getml-vertexai-sa\",  # NOTE: Gets replaced, if you run on Vertex AI Workbench         \"DOCKER_REPOSITORY\": \"getml-vertexai-docker-repository\",         \"GETML_PROJECT_NAME\": \"Loans\",     } )  # Save configuration for later use in Docker containers cfg.save(\"config.yaml\") <p>Print all available configurations</p> In\u00a0[\u00a0]: Copied! <pre>cfg\n</pre> cfg <p>Set the project and region to ensure <code>! gcloud</code> commands are executed accordingly.</p> In\u00a0[\u00a0]: Copied! <pre>! gcloud config set project {cfg.GCP_PROJECT_NAME}\n! gcloud config set ai/region {cfg.REGION}\n</pre> ! gcloud config set project {cfg.GCP_PROJECT_NAME} ! gcloud config set ai/region {cfg.REGION} <p>Initialize the Vertex AI SDK and set the project and location defaults there as well. This ensures all <code>aiplatform</code> related commands/functions execute on the correct project and region.</p> In\u00a0[\u00a0]: Copied! <pre>aiplatform.init(project=cfg.GCP_PROJECT_NAME, location=cfg.REGION)\n</pre> aiplatform.init(project=cfg.GCP_PROJECT_NAME, location=cfg.REGION) In\u00a0[\u00a0]: Copied! <pre>! gcloud services enable \\\n    iam.googleapis.com \\\n    compute.googleapis.com \\\n    containerregistry.googleapis.com \\\n    aiplatform.googleapis.com\n</pre> ! gcloud services enable \\     iam.googleapis.com \\     compute.googleapis.com \\     containerregistry.googleapis.com \\     aiplatform.googleapis.com In\u00a0[\u00a0]: Copied! <pre>from getml.vertexai import open_iam_permissions\n\nif IS_WORKBENCH_ENV:\n    open_iam_permissions(cfg.GCP_PROJECT_NAME)\n\n    cfg.print([\"SERVICE_ACCOUNT_EMAIL\"])\n    cfg.print_links([\"iam_permissions\"])\n</pre> from getml.vertexai import open_iam_permissions  if IS_WORKBENCH_ENV:     open_iam_permissions(cfg.GCP_PROJECT_NAME)      cfg.print([\"SERVICE_ACCOUNT_EMAIL\"])     cfg.print_links([\"iam_permissions\"]) In\u00a0[\u00a0]: Copied! <pre># NOTE: If the service account already exists in the project, the following error can be ignored:\n# ERROR: (gcloud.iam.service-accounts.create) Resource in projects [$PROJECT_ID] is the subject of a conflict..\n\nif not IS_WORKBENCH_ENV:\n    cfg.print([\"SERVICE_ACCOUNT_NAME\"])\n\n    ! gcloud iam service-accounts create {cfg.SERVICE_ACCOUNT_NAME} \\\n        --display-name=\"getML Vertex AI Service Account\"\n</pre> # NOTE: If the service account already exists in the project, the following error can be ignored: # ERROR: (gcloud.iam.service-accounts.create) Resource in projects [$PROJECT_ID] is the subject of a conflict..  if not IS_WORKBENCH_ENV:     cfg.print([\"SERVICE_ACCOUNT_NAME\"])      ! gcloud iam service-accounts create {cfg.SERVICE_ACCOUNT_NAME} \\         --display-name=\"getML Vertex AI Service Account\" In\u00a0[\u00a0]: Copied! <pre>if not IS_WORKBENCH_ENV:\n    cfg.print([\"GCP_PROJECT_NAME\", \"SERVICE_ACCOUNT_EMAIL\"])\n\n    # Assign the Vertex AI User role\n    ! gcloud projects add-iam-policy-binding {cfg.GCP_PROJECT_NAME} \\\n        --member=\"serviceAccount:{cfg.SERVICE_ACCOUNT_EMAIL}\" \\\n        --role=\"roles/aiplatform.user\"\n\n    # Assign the Storage Admin role\n    ! gcloud projects add-iam-policy-binding {cfg.GCP_PROJECT_NAME} \\\n        --member=\"serviceAccount:{cfg.SERVICE_ACCOUNT_EMAIL}\" \\\n        --role=\"roles/storage.admin\"\n</pre> if not IS_WORKBENCH_ENV:     cfg.print([\"GCP_PROJECT_NAME\", \"SERVICE_ACCOUNT_EMAIL\"])      # Assign the Vertex AI User role     ! gcloud projects add-iam-policy-binding {cfg.GCP_PROJECT_NAME} \\         --member=\"serviceAccount:{cfg.SERVICE_ACCOUNT_EMAIL}\" \\         --role=\"roles/aiplatform.user\"      # Assign the Storage Admin role     ! gcloud projects add-iam-policy-binding {cfg.GCP_PROJECT_NAME} \\         --member=\"serviceAccount:{cfg.SERVICE_ACCOUNT_EMAIL}\" \\         --role=\"roles/storage.admin\" In\u00a0[\u00a0]: Copied! <pre># type: ignore\n\nfrom pathlib import Path\n\ncfg.print([\"SERVICE_ACCOUNT_EMAIL\"])\n\nPATH_SERVICE_ACCOUNT_CREDENTIALS = Path(\"service_account.json\")\n\nif not PATH_SERVICE_ACCOUNT_CREDENTIALS.exists():\n    ! gcloud iam service-accounts keys create {PATH_SERVICE_ACCOUNT_CREDENTIALS.name} \\\n        --iam-account={cfg.SERVICE_ACCOUNT_EMAIL}\n</pre> # type: ignore  from pathlib import Path  cfg.print([\"SERVICE_ACCOUNT_EMAIL\"])  PATH_SERVICE_ACCOUNT_CREDENTIALS = Path(\"service_account.json\")  if not PATH_SERVICE_ACCOUNT_CREDENTIALS.exists():     ! gcloud iam service-accounts keys create {PATH_SERVICE_ACCOUNT_CREDENTIALS.name} \\         --iam-account={cfg.SERVICE_ACCOUNT_EMAIL} In\u00a0[\u00a0]: Copied! <pre># NOTE: If BUCKET_URI already exists. The following error can be ignored:\n# \"ServiceException 409 A Cloud Storage bucket named $BUCKET_NAME already exists.\"\n\n# Create the bucket\n! gsutil mb -l {cfg.REGION} -p {cfg.GCP_PROJECT_NAME} {cfg.BUCKET_URI}\n\ncfg.print([\"BUCKET_URI\"])\ncfg.print_links([\"bucket\"])\n</pre> # NOTE: If BUCKET_URI already exists. The following error can be ignored: # \"ServiceException 409 A Cloud Storage bucket named $BUCKET_NAME already exists.\"  # Create the bucket ! gsutil mb -l {cfg.REGION} -p {cfg.GCP_PROJECT_NAME} {cfg.BUCKET_URI}  cfg.print([\"BUCKET_URI\"]) cfg.print_links([\"bucket\"]) In\u00a0[\u00a0]: Copied! <pre># NOTE: If DOCKER_REPOSITORY already exists. The following error can be ignored:\n# ERROR: (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n\n! gcloud artifacts repositories create {cfg.DOCKER_REPOSITORY} \\\n    --repository-format=docker \\\n    --location={cfg.REGION} \\\n    --description=\"Docker repository for getML Vertex AI Images\"\n\ncfg.print([\"DOCKER_REPOSITORY\", \"REGION\"])\ncfg.print_links([\"docker_repository\"])\n</pre> # NOTE: If DOCKER_REPOSITORY already exists. The following error can be ignored: # ERROR: (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists  ! gcloud artifacts repositories create {cfg.DOCKER_REPOSITORY} \\     --repository-format=docker \\     --location={cfg.REGION} \\     --description=\"Docker repository for getML Vertex AI Images\"  cfg.print([\"DOCKER_REPOSITORY\", \"REGION\"]) cfg.print_links([\"docker_repository\"]) In\u00a0[\u00a0]: Copied! <pre>! gcloud auth configure-docker --quiet\n! gcloud auth configure-docker --quiet {cfg.REGION}-docker.pkg.dev\n</pre> ! gcloud auth configure-docker --quiet ! gcloud auth configure-docker --quiet {cfg.REGION}-docker.pkg.dev <p>Set the <code>DOCKER_HOST</code> environment variable to the current docker daemon path.</p> <p>This is necessary for compatibility of rootless Docker setups in combination with Vertex AI SDK.</p> In\u00a0[\u00a0]: Copied! <pre>from getml.vertexai.utils import get_docker_daemon_path\n\nos.environ[\"DOCKER_HOST\"] = get_docker_daemon_path()\n</pre> from getml.vertexai.utils import get_docker_daemon_path  os.environ[\"DOCKER_HOST\"] = get_docker_daemon_path() In\u00a0[\u00a0]: Copied! <pre>import warnings\n\nwarnings.filterwarnings(\"ignore\", message=\"line buffering\")\n</pre> import warnings  warnings.filterwarnings(\"ignore\", message=\"line buffering\") In\u00a0[\u00a0]: Copied! <pre>from getml.vertexai import create_vertex_dataset_tabular\n\ndataset_loans = create_vertex_dataset_tabular(\n    cfg=cfg, filename_csv=\"datasets/loans_population_test.csv\"\n)\n</pre> from getml.vertexai import create_vertex_dataset_tabular  dataset_loans = create_vertex_dataset_tabular(     cfg=cfg, filename_csv=\"datasets/loans_population_test.csv\" ) In\u00a0[\u00a0]: Copied! <pre>print(\"Content of Dockerfile.train:\\n\")\n%cat training/Dockerfile.train\n</pre> print(\"Content of Dockerfile.train:\\n\") %cat training/Dockerfile.train <p>Now let's</p> <ul> <li><code>build</code> the Dockerfile.train image</li> <li>and <code>push</code> it to the Artifact Registry</li> </ul> In\u00a0[\u00a0]: Copied! <pre>cfg.print([\"DOCKER_IMAGE_URI_TRAIN\"])\n\n! docker build -f training/Dockerfile.train -t {cfg.DOCKER_IMAGE_URI_TRAIN} .\n! docker push {cfg.DOCKER_IMAGE_URI_TRAIN}\n</pre> cfg.print([\"DOCKER_IMAGE_URI_TRAIN\"])  ! docker build -f training/Dockerfile.train -t {cfg.DOCKER_IMAGE_URI_TRAIN} . ! docker push {cfg.DOCKER_IMAGE_URI_TRAIN} In\u00a0[\u00a0]: Copied! <pre>cfg.print(\n    [\n        \"GETML_PROJECT_NAME\",\n        \"GCP_PROJECT_NAME\",\n        \"REGION\",\n        \"SERVICE_ACCOUNT_EMAIL\",\n        \"DOCKER_IMAGE_URI_TRAIN\",\n        \"BUCKET_URI_DATASET\",\n    ]\n)\n</pre> cfg.print(     [         \"GETML_PROJECT_NAME\",         \"GCP_PROJECT_NAME\",         \"REGION\",         \"SERVICE_ACCOUNT_EMAIL\",         \"DOCKER_IMAGE_URI_TRAIN\",         \"BUCKET_URI_DATASET\",     ] ) In\u00a0[\u00a0]: Copied! <pre># Define variables for the training job\nTRAIN_DISPLAY_NAME = f\"getml-train-{cfg.GETML_PROJECT_NAME}\"\nTRAIN_LOCAL_PACKAGE_PATH = \"training\"\nTRAIN_SCRIPT = \"train.py\"\nTRAIN_MACHINE_TYPE = \"n1-standard-4\"\nTRAIN_REPLICA_COUNT = 1\n\n# Create and run the custom training job\n! gcloud ai custom-jobs create \\\n  --project={cfg.GCP_PROJECT_NAME} \\\n  --region={cfg.REGION} \\\n  --display-name={TRAIN_DISPLAY_NAME} \\\n  --service-account={cfg.SERVICE_ACCOUNT_EMAIL} \\\n  --worker-pool-spec=machine-type={TRAIN_MACHINE_TYPE},replica-count={TRAIN_REPLICA_COUNT},executor-image-uri={cfg.DOCKER_IMAGE_URI_TRAIN},local-package-path={TRAIN_LOCAL_PACKAGE_PATH},script={TRAIN_SCRIPT}\n</pre> # Define variables for the training job TRAIN_DISPLAY_NAME = f\"getml-train-{cfg.GETML_PROJECT_NAME}\" TRAIN_LOCAL_PACKAGE_PATH = \"training\" TRAIN_SCRIPT = \"train.py\" TRAIN_MACHINE_TYPE = \"n1-standard-4\" TRAIN_REPLICA_COUNT = 1  # Create and run the custom training job ! gcloud ai custom-jobs create \\   --project={cfg.GCP_PROJECT_NAME} \\   --region={cfg.REGION} \\   --display-name={TRAIN_DISPLAY_NAME} \\   --service-account={cfg.SERVICE_ACCOUNT_EMAIL} \\   --worker-pool-spec=machine-type={TRAIN_MACHINE_TYPE},replica-count={TRAIN_REPLICA_COUNT},executor-image-uri={cfg.DOCKER_IMAGE_URI_TRAIN},local-package-path={TRAIN_LOCAL_PACKAGE_PATH},script={TRAIN_SCRIPT} In\u00a0[\u00a0]: Copied! <pre>cfg.print_links([\"training_jobs\", \"model_artifact\", \"experiments\"])\n</pre> cfg.print_links([\"training_jobs\", \"model_artifact\", \"experiments\"]) In\u00a0[\u00a0]: Copied! <pre>print(\"Content of Dockerfile.pred:\\n\")\n%cat prediction/Dockerfile.pred\n</pre> print(\"Content of Dockerfile.pred:\\n\") %cat prediction/Dockerfile.pred <p>Now let's <code>build</code> the Dockerfile.pred image</p> In\u00a0[\u00a0]: Copied! <pre>! docker build -f prediction/Dockerfile.pred \\\n    -t {cfg.DOCKER_IMAGE_URI_PRED} .\n</pre> ! docker build -f prediction/Dockerfile.pred \\     -t {cfg.DOCKER_IMAGE_URI_PRED} . In\u00a0[\u00a0]: Copied! <pre>from google.cloud.aiplatform.prediction import LocalModel\n\nlocal_model = LocalModel(serving_container_image_uri=cfg.DOCKER_IMAGE_URI_PRED)\n\ncfg.print([\"DOCKER_IMAGE_URI_PRED\"])\n</pre> from google.cloud.aiplatform.prediction import LocalModel  local_model = LocalModel(serving_container_image_uri=cfg.DOCKER_IMAGE_URI_PRED)  cfg.print([\"DOCKER_IMAGE_URI_PRED\"]) In\u00a0[\u00a0]: Copied! <pre>from getml.vertexai import load_json_from_file\n\nrequest_json = load_json_from_file(\"./prediction/request_test.json\")\nrequest_json\n</pre> from getml.vertexai import load_json_from_file  request_json = load_json_from_file(\"./prediction/request_test.json\") request_json In\u00a0[\u00a0]: Copied! <pre># from getml.vertexai import create_test_request\n\n# create_test_request()\n</pre> # from getml.vertexai import create_test_request  # create_test_request() \u26a0\ufe0f The model training and artifact creation must be completed before proceeding to the next step \u26a0\ufe0f <p>Verify that the training has successfully finished by checking the following links:</p> In\u00a0[\u00a0]: Copied! <pre>cfg.print_links([\"training_jobs\", \"model_artifact\"])\n</pre> cfg.print_links([\"training_jobs\", \"model_artifact\"]) <p>Waiting for the training job to finish before proceeding to the next step.</p> <p>NOTE: This may take a few minutes.</p> In\u00a0[\u00a0]: Copied! <pre>from getml.vertexai.utils_gcp import wait_for_training_artifact\n\nwait_for_training_artifact(cfg)\n</pre> from getml.vertexai.utils_gcp import wait_for_training_artifact  wait_for_training_artifact(cfg) In\u00a0[\u00a0]: Copied! <pre>with local_model.deploy_to_local_endpoint(\n    credential_path=PATH_SERVICE_ACCOUNT_CREDENTIALS.name,\n    artifact_uri=cfg.ARTIFACT_URI,\n) as local_endpoint:\n    health_check_response = local_endpoint.run_health_check()\n    print(\n        \"Health check response:\", health_check_response, health_check_response.content\n    )\n\n    # Make a prediction\n    predict_response = local_endpoint.predict(\n        request=request_json,\n        headers={\"Content-Type\": \"application/json\"},\n    )\n    print(\"Predict response:\", predict_response, predict_response.content)\n</pre> with local_model.deploy_to_local_endpoint(     credential_path=PATH_SERVICE_ACCOUNT_CREDENTIALS.name,     artifact_uri=cfg.ARTIFACT_URI, ) as local_endpoint:     health_check_response = local_endpoint.run_health_check()     print(         \"Health check response:\", health_check_response, health_check_response.content     )      # Make a prediction     predict_response = local_endpoint.predict(         request=request_json,         headers={\"Content-Type\": \"application/json\"},     )     print(\"Predict response:\", predict_response, predict_response.content) <p>You should see an output similar to:</p> <pre><code>Health check response: &lt;Response [200]&gt; b'{}'\nPredict response: &lt;Response [200]&gt; b'{\"predictions\": [[0.9659892320632935], [0.8711856007575989], [0.882280170917511],... \n</code></pre> <p>If there is an issue you can check the logs of the container build process:</p> In\u00a0[\u00a0]: Copied! <pre>local_endpoint.container.logs().decode(\"utf-8\").strip().split(\"\\n\")\n</pre> local_endpoint.container.logs().decode(\"utf-8\").strip().split(\"\\n\") In\u00a0[\u00a0]: Copied! <pre>from getml.vertexai import cmd_to_run_local_endpoint\n\ncmd_to_run_local_endpoint(cfg)\n</pre> from getml.vertexai import cmd_to_run_local_endpoint  cmd_to_run_local_endpoint(cfg) In\u00a0[\u00a0]: Copied! <pre>! docker build --platform linux/amd64 -f prediction/Dockerfile.pred \\\n    -t {cfg.DOCKER_IMAGE_URI_PRED} .\n</pre> ! docker build --platform linux/amd64 -f prediction/Dockerfile.pred \\     -t {cfg.DOCKER_IMAGE_URI_PRED} . In\u00a0[\u00a0]: Copied! <pre>local_model.push_image()\n\ncfg.print_links([\"image_for_predictions\"])\n</pre> local_model.push_image()  cfg.print_links([\"image_for_predictions\"]) In\u00a0[\u00a0]: Copied! <pre>cfg.print([\"GCP_PROJECT_NAME\", \"REGION\", \"ARTIFACT_URI\"])\n\nmodel = aiplatform.Model.upload(\n    project=cfg.GCP_PROJECT_NAME,\n    location=cfg.REGION,\n    local_model=local_model,\n    display_name=\"getML model (Loans)\",\n    artifact_uri=f\"{cfg.ARTIFACT_URI}\",\n    description=\"getML model trained on the Loans dataset. Generated by demo_binary_classification.ipynb\",\n)\n\ncfg.print_links([\"model_registry\"])\n</pre> cfg.print([\"GCP_PROJECT_NAME\", \"REGION\", \"ARTIFACT_URI\"])  model = aiplatform.Model.upload(     project=cfg.GCP_PROJECT_NAME,     location=cfg.REGION,     local_model=local_model,     display_name=\"getML model (Loans)\",     artifact_uri=f\"{cfg.ARTIFACT_URI}\",     description=\"getML model trained on the Loans dataset. Generated by demo_binary_classification.ipynb\", )  cfg.print_links([\"model_registry\"]) In\u00a0[\u00a0]: Copied! <pre>ENDPOINT_MACHINE_TYPE = \"n1-standard-4\"\n\nendpoint = model.deploy(\n    machine_type=ENDPOINT_MACHINE_TYPE, service_account=cfg.SERVICE_ACCOUNT_EMAIL\n)\n</pre> ENDPOINT_MACHINE_TYPE = \"n1-standard-4\"  endpoint = model.deploy(     machine_type=ENDPOINT_MACHINE_TYPE, service_account=cfg.SERVICE_ACCOUNT_EMAIL ) In\u00a0[\u00a0]: Copied! <pre># model_id is just needed to build the link\nmodel_id = Path(endpoint.gca_resource.deployed_models[0].model).name\ncfg.print_links([\"deployed_model\"], model_id)\n\nprint(\"JSON request:\", request_json)\n</pre> # model_id is just needed to build the link model_id = Path(endpoint.gca_resource.deployed_models[0].model).name cfg.print_links([\"deployed_model\"], model_id)  print(\"JSON request:\", request_json) In\u00a0[\u00a0]: Copied! <pre># PROJECT_ID (int): The numerical project ID.\n# ENDPOINT_ID (int): The numerical endpoint ID.\n# Example URL format: https://europe-west1-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/europe-west1/endpoints/{ENDPOINT_ID}:predict\n\n! curl -X POST \\\n    -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n    -H \"Content-Type: application/json\" \\\n    \"https://{cfg.REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}:predict\" \\\n    -d \"@prediction/request_test.json\"\n</pre> # PROJECT_ID (int): The numerical project ID. # ENDPOINT_ID (int): The numerical endpoint ID. # Example URL format: https://europe-west1-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/europe-west1/endpoints/{ENDPOINT_ID}:predict  ! curl -X POST \\     -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\     -H \"Content-Type: application/json\" \\     \"https://{cfg.REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}:predict\" \\     -d \"@prediction/request_test.json\" <p>The result should looks similar to:</p> <pre><code>{\n  \"predictions\": [\n    [\n      0.96598923206329346\n    ],\n    [\n      0.87118560075759888\n    ],\n    [\n      0.882280170917511\n    ],\n\n    ...\n    \n    ],\n  \"deployedModelId\": \"5059851355955396608\",\n  \"model\": \"projects/956851751872/locations/europe-west1/models/8409526724114513920\",\n  \"modelDisplayName\": \"getML model (Loans)\",\n  \"modelVersionId\": \"1\"\n}\n</code></pre> In\u00a0[\u00a0]: Copied! <pre>endpoint.undeploy_all()\n</pre> endpoint.undeploy_all()"},{"location":"examples/integrations/vertexai/vertexai/#getml-on-vertex-ai","title":"getML on Vertex AI\u00b6","text":""},{"location":"examples/integrations/vertexai/vertexai/#overview","title":"Overview\u00b6","text":"<p>This tutorial demonstrates how to use the <code>Vertex AI SDK</code> and <code>gcloud cli</code> to build and deploy custom containers for training and prediction of getML models.</p>"},{"location":"examples/integrations/vertexai/vertexai/#dataset","title":"Dataset\u00b6","text":"<p>The financial dataset from the CTU Prague Relational Learning Repository. It consists of multiple tables containing various features related to bank customers and their transaction histories. The target variable is whether a customer defaults on a loan.</p> <p>Note: This notebook is based on Predicting the loan default risk of Czech bank customers using getML. Checkout it out first, if you want to know more about the dataset and getML in general.</p>"},{"location":"examples/integrations/vertexai/vertexai/#objective","title":"Objective\u00b6","text":"<p>The goal of this tutorial is to:</p> <ul> <li>Train a getML model using relational data from multiple tables.</li> <li>Save the trained model and its serialized pre-processor.</li> <li>Build a custom getML serving container with custom prediction logic using the Custom Prediction Routine feature in the Vertex AI SDK.</li> <li>Test the built container locally.</li> <li>Upload and deploy the custom container to Vertex AI Predictions.</li> </ul> <p>Note: This tutorial focuses more on deploying getML models with Vertex AI than on the design of the model itself.</p>"},{"location":"examples/integrations/vertexai/vertexai/#costs","title":"Costs\u00b6","text":"<p>This tutorial involves the use of billable components of Google Cloud:</p> <ul> <li>Vertex AI</li> <li>Google Cloud Storage</li> <li>Google Container Registry</li> </ul> <p>TIP: Check out Vertex AI pricing, and use the Pricing Calculator to generate a cost estimate based on your projected usage.</p>"},{"location":"examples/integrations/vertexai/vertexai/#before-you-begin","title":"Before you begin\u00b6","text":"<p>Note: If you are running this notebook on Vertex AI Workbench, your environment already meets most requirements. However, you need to add the <code>storage.admin</code> role to the <code>*compute@developer.gserviceaccount.com</code> service account that is assigned to this notebook by default. Please be aware of step <code>VertexAI Workbench: Adding Role to Service Account</code></p> <p>If you run this notebook locally, please consider the following requirements:</p>"},{"location":"examples/integrations/vertexai/vertexai/#set-up-your-local-development-environment","title":"Set up Your Local Development Environment\u00b6","text":"<p>If you run this notebook on your local machine make sure your environment meets this notebook's requirements:</p> <ul> <li>Docker</li> <li>Google Cloud SDK (gcloud)</li> </ul> <p>Note: If you need to install Docker or the SDK, the links will guide you to the installation steps.</p>"},{"location":"examples/integrations/vertexai/vertexai/#set-up-your-google-cloud-project","title":"Set up Your Google Cloud Project\u00b6","text":"<p>The following steps are required, regardless of your notebook environment.</p> <ol> <li>Select or create a Google Cloud project.</li> </ol> <p>IMPORTANT! If you have not used gcloud CLI before you need to set it up first. On your local shell, run:</p> <pre>gcloud init\n</pre> <p>During the process you will authenticate, get credentials and can set your default project / region.</p> <ol> <li>Make sure that billing is enabled for your project.</li> </ol> <p>Note: All commands prefixed with <code>!</code> are shell commands. The prefix <code>!</code> allows for direct execution within Juypter. However, you can also execute them in a dedicated Terminal.</p>"},{"location":"examples/integrations/vertexai/vertexai/#determine-environment","title":"Determine Environment\u00b6","text":"<p>We need to adapt to the environment this notebook runs in. So if this notebook runs on VertexAI Workbench or Colab <code>IS_GCLOUD_ENV</code> is <code>True</code></p>"},{"location":"examples/integrations/vertexai/vertexai/#install-requirements","title":"Install requirements\u00b6","text":"<p><code>getml.vertexai</code> is located within <code>src</code>. This package contains:</p> <ul> <li><code>Utility functions</code> for accessing GCP resources</li> <li><code>Configurations</code> for this notebook and training/inference containers we will create later.</li> <li><code>Dependencies</code> needed for notebook and docker containers:<ul> <li>getml==1.4.0</li> <li>google-cloud-aiplatform[prediction]==1.56.0</li> <li>pyyaml==6.0.1</li> </ul> </li> </ul> <p>The Python Cloud Client Library google-cloud-aiplatform is needed to interact with services from Google Cloud, including</p> <ul> <li>Vertex AI</li> <li>Cloud Storage.</li> <li>[prediction] option includes FastAPI, that is needed for building the prediction container later on.</li> </ul>"},{"location":"examples/integrations/vertexai/vertexai/#install-getmlvertexai","title":"Install <code>getml.vertexai</code>\u00b6","text":"<p>In the Vertex AI Workbench environment, perform the following steps:</p> <ul> <li>Download the tarball version of the <code>getml-demo</code> repository.</li> <li>Extract the content of the project folder into the current working directory.</li> </ul>"},{"location":"examples/integrations/vertexai/vertexai/#kernel-restart","title":"Kernel restart\u00b6","text":"<p>On Workbench we also need to restart the kernel to apply all changes.</p>"},{"location":"examples/integrations/vertexai/vertexai/#import-vertex-ai-sdk","title":"Import Vertex AI SDK\u00b6","text":"<p><code>aiplatform</code> is part of the <code>google-cloud-aiplatform</code> package. It provides a Python API for interacting with Vertex AI services.</p>"},{"location":"examples/integrations/vertexai/vertexai/#configuration","title":"Configuration\u00b6","text":"<p>Define and save configuration variables using <code>Config</code>, storing them in <code>config.yaml</code>.</p> <p>This method centralizes project variable definitions within the notebook and ensures availability to Docker containers created later.</p> <p>Access the configuration via the <code>cfg</code> instance using dot notation, e.g., <code>cfg.REGION</code>.</p> <p>Note: If you are using Vertex AI Workbench, this notebook is associated with the Compute Engine default service account. <code>SERVICE_ACCOUNT_NAME</code> will be automatically filled with the corresponding *compute@developer.gserviceaccount.com name.</p>"},{"location":"examples/integrations/vertexai/vertexai/#enable-necessary-apis","title":"Enable Necessary APIs\u00b6","text":"<p>If you have just created a new project, some APIs might not be enabled yet. Use the following command to enable all the APIs needed for this tutorial:</p>"},{"location":"examples/integrations/vertexai/vertexai/#setup-service-account","title":"Setup Service Account\u00b6","text":"<p>We need a service account to provide our containers appropriate permissions to access</p> <ul> <li>Storage Buckets (Save and load model artifacts)</li> <li>MetadataStore (Logging metrics/Experiments)</li> </ul>"},{"location":"examples/integrations/vertexai/vertexai/#vertexai-workbench-adding-role-to-service-account","title":"VertexAI Workbench: Adding Role to Service Account\u00b6","text":"<p>If you are running this notebook on Vertex AI Workbench, it is associated with a Service Account (see <code>SERVICE_ACCOUNT_EMAIL</code>). To ensure proper functionality, you need to add the <code>storage.admin</code> role to this account.</p> <p>Perform this step on the GCP Platform. Follow the link below (it should automatically open) and add the <code>storage.admin</code> role to the Service Account associated with this notebook.</p>"},{"location":"examples/integrations/vertexai/vertexai/#local-environment-create-a-service-account","title":"Local Environment: Create a Service Account\u00b6","text":"<p>If this notebook runs on a local environment and you are authenticated to <code>gcloud cli</code> with your personal account, we need to create a service account.</p> <p>NOTE: If you run this notebook on VertexAI Workbench skip this step and continue with <code>Save Service Account to JSON</code></p>"},{"location":"examples/integrations/vertexai/vertexai/#set-permissions-on-service-account","title":"Set Permissions on Service Account\u00b6","text":"<p>Once the service account is created, we need to grant the roles <code>aiplatform.user</code> and <code>storage.admin</code> to it:</p>"},{"location":"examples/integrations/vertexai/vertexai/#save-service-account-to-json","title":"Save Service Account to JSON\u00b6","text":"<p>We will need the <code>service_account.json</code> file later when we create a local endpoint to test our container.</p> <p>NOTE: If too many keys have been created, the following error can occur:</p> <p><code>ERROR: (gcloud.iam.service-accounts.keys.create) FAILED_PRECONDITION: Precondition check failed.</code></p> <p>In this case older keys should be deleted before creating a new one.</p> <p>To prevent this from happening in the first place, we check if a service_account.json is already present before we create it.</p>"},{"location":"examples/integrations/vertexai/vertexai/#create-cloud-storage-bucket","title":"Create Cloud Storage Bucket\u00b6","text":"<p>The bucket will serve as cloud storage for:</p> <ul> <li>Trained model artifacts (The result of the training container)</li> <li>Datasets (Loans dataset)</li> </ul> <p>Both are included in the getML project dump, <code>Loans.getml</code>, which will be stored in the bucket we create now:</p>"},{"location":"examples/integrations/vertexai/vertexai/#create-docker-repository-on-artifact-registry","title":"Create Docker Repository on Artifact Registry\u00b6","text":"<p>The Docker repository on Google Cloud's <code>Artifact Registry</code> will store the Docker images required for our training and prediction containers. These images will be built locally and then pushed to this repository for deployment on Vertex AI.</p>"},{"location":"examples/integrations/vertexai/vertexai/#configure-docker","title":"Configure Docker\u00b6","text":"<p>To be able to upload images to the repository, you need to update your Docker settings:</p>"},{"location":"examples/integrations/vertexai/vertexai/#handling-line-buffering-warnings","title":"Handling \"line buffering\" Warnings\u00b6","text":"<p>In this notebook, you may see warnings related to line buffering when using the subprocess module. These warnings do not impact the accuracy or performance and cannot be resolved within this notebook's context. Therefore, we will ignore them to keep our output clean.</p> <p>Note: You might still see line buffering warnings when running <code>! gcloud</code> commands. As stated, these can be safely ignored.</p>"},{"location":"examples/integrations/vertexai/vertexai/#setup-finished","title":"Setup Finished\u00b6","text":"<p>We have completed all setup and configuration steps and are now ready to start training our model.</p>"},{"location":"examples/integrations/vertexai/vertexai/#training","title":"Training\u00b6","text":"<p>This notebook demonstrates the training of a binary classification model. It is based on the Loans notebook. Check out the link for more details on the dataset and usage of the getML Python API.</p>"},{"location":"examples/integrations/vertexai/vertexai/#main-objectives","title":"Main Objectives\u00b6","text":"<p>The main objectives of the training container are to:</p> <ul> <li>Get and preprocess the <code>Loans dataset</code>.</li> <li><code>Train</code> a getML model (pipeline) on the trainset.</li> <li><code>Score</code> the trained model on the testset.</li> <li><code>Save</code> the project (including data and model) as an <code>artifact</code> on the <code>GCS Bucket</code>.</li> </ul>"},{"location":"examples/integrations/vertexai/vertexai/#create-managed-dataset","title":"Create Managed Dataset\u00b6","text":"<p>To use experiments, a managed dataset is essential as it creates a default MetadataStore. The Experiments/MetadataStore is crucial for logging and tracking experiments, ensuring all data-related activities are properly recorded and managed within the Vertex AI ecosystem.</p> <p>The managed dataset created here is primarily for demonstration purposes and to establish a MetadataStore. The actual data used to train our model is retrieved within the training Docker container. For details, see <code>training/train.py</code>.</p>"},{"location":"examples/integrations/vertexai/vertexai/#build-docker-container-for-training","title":"Build Docker Container for Training\u00b6","text":"<p>For training we just need a simple Docker container that includes:</p> <ul> <li><code>Python runtime</code> (we conveniently use a public python image as base layer)</li> <li><code>Python dependencies</code>:<ul> <li>getml</li> <li>getml-playbooks</li> <li>google-cloud-aiplatform</li> </ul> </li> </ul>"},{"location":"examples/integrations/vertexai/vertexai/#deploy-training-job","title":"Deploy Training Job\u00b6","text":"<p>The <code>gcloud ai custom-jobs create</code> command</p> <ul> <li><code>wraps</code> the train.py script into our Training Docker Container, then</li> <li><code>runs</code> it in the Vertex AI environment on Google Cloud.</li> <li>Finally, the <code>result</code> is an Artifact containing the getML model and dataframes</li> </ul> <p>For more details about the command, checkout https://cloud.google.com/sdk/gcloud/reference/ai/custom-jobs/create</p>"},{"location":"examples/integrations/vertexai/vertexai/#result-of-training-container","title":"Result of Training Container\u00b6","text":"<p>The following links contain the resources we just created, as well as the resulting artifact from the training container:</p>"},{"location":"examples/integrations/vertexai/vertexai/#prediction-inference","title":"Prediction / Inference\u00b6","text":"<p>Now that we have a trained Model Artifact stored on GCS, we can</p> <ul> <li><code>build</code> a prediction routine that loads the Artifact, and</li> <li><code>deploy</code> an HTTP endpoint to run predictions on our model.</li> </ul>"},{"location":"examples/integrations/vertexai/vertexai/#details-of-the-prediction-container","title":"Details of the prediction container\u00b6","text":"<p>Basically, the container provides the HTTP route <code>predict</code> via FastAPI / Uvicorn, Gunicorn</p> <p>To know more about the <code>Predictor</code> class, see the documentation on custom prediction routines.</p> <p>All relevant files you can find within the <code>prediction</code> folder.</p>"},{"location":"examples/integrations/vertexai/vertexai/#deploy-local-model","title":"Deploy Local Model\u00b6","text":"<p>Before deploying the model to the cloud, it is advisable to build and test it locally. Once the model is confirmed to be functioning correctly, you can then proceed with the cloud deployment.</p> <p>See the Google documentation for more details of the <code>LocalModel</code> class.</p>"},{"location":"examples/integrations/vertexai/vertexai/#local-prediction-on-test-data","title":"Local Prediction on Test Data\u00b6","text":"<p>To run a prediction using the local model, we will send a request with test data in JSON format (as string) to the local endpoint.</p> <p>We have prepared some test request data in JSON format, which can be loaded using <code>load_json_from_file()</code>.</p> <p>Note: Refer to <code>[OPTIONAL] Create Test Request Data</code> for details on how this test data was created.</p>"},{"location":"examples/integrations/vertexai/vertexai/#optional-create-test-request-data","title":"[OPTIONAL] Create Test Request Data\u00b6","text":"<p>If you would like to recreate the test data JSON or see how it is generated, uncomment the following code and check its source in <code>src/getml/vertexai/request_data.py</code>.</p>"},{"location":"examples/integrations/vertexai/vertexai/#deploy-local_model-to-a-local_endpoint","title":"Deploy <code>local_model</code> to a <code>local_endpoint</code>\u00b6","text":"<p>Now that we have the prediction container ready, as well as some test data, we can deploy a local endpoint and send test data to the <code>predict</code> endpoint.</p> <p>See the Google documentation for more details and requirements of the <code>LocalModel</code> class and its <code>deploy_to_local_endpoint</code> method.</p>"},{"location":"examples/integrations/vertexai/vertexai/#manually-spin-up-container-and-call-endpoint-with-test-data","title":"Manually Spin-Up Container and Call Endpoint with Test Data\u00b6","text":"<p>Alternatively, you can manually run your Docker container. This way, you have more control over the parameters of <code>docker run</code>, especially the Google environment variables.</p> <p>See more details about them in the Google documentation.</p> <p>NOTE: You should run the <code>docker run</code> command in a separate Terminal, not in this notebook.</p>"},{"location":"examples/integrations/vertexai/vertexai/#push-image-to-gcp-vertex-ai","title":"Push Image to GCP / Vertex AI\u00b6","text":"<p>Before we can deploy the container to the cloud, we need to push the image to the <code>Artifact Registry</code>.</p>"},{"location":"examples/integrations/vertexai/vertexai/#rebuild-prediction-container","title":"Rebuild Prediction Container\u00b6","text":"<p>To ensure compatibility with GCP (x86_64), the container image must be built with the correct architecture. Regardless of your current platform, the <code>docker build</code> command will now enforce the linux/amd64 platform.</p>"},{"location":"examples/integrations/vertexai/vertexai/#upload-to-model-registry","title":"Upload to Model Registry\u00b6","text":"<p>The <code>Model Registry</code> serves as a centralized repository where you can manage and version your machine learning models. By uploading the model, you make it accessible for deployment and further analysis.</p>"},{"location":"examples/integrations/vertexai/vertexai/#online-prediction-endpoint","title":"Online Prediction Endpoint\u00b6","text":"<p>Endpoints are machine learning models made available for online prediction requests. Endpoints are useful for timely predictions from many users (for example, in response to an application request). You can also request batch predictions if you don't need immediate results.</p>"},{"location":"examples/integrations/vertexai/vertexai/#deploy-endpoint","title":"Deploy Endpoint\u00b6","text":"<p>NOTE: If you encounter a \"FailedPrecondition\" error, this is very likely related to an exception thrown within the docker container. You should checkout the logs of the container to find the cause.</p> <p>NOTE: Deployment of endpoint can take a while (30min+)</p>"},{"location":"examples/integrations/vertexai/vertexai/#prediction-on-deployed-endpoint","title":"Prediction on Deployed Endpoint\u00b6","text":"<p>Once the endpoint is deployed, you can also make predictions using the <code>Test your model</code> feature in the Vertex AI console (see link below).</p> <p>As <code>JSON request</code> you can use the content of the <code>request_test.json</code> file:</p>"},{"location":"examples/integrations/vertexai/vertexai/#undeploy-endpoint","title":"Undeploy Endpoint\u00b6","text":"<p>Remember to undeploy your cloud endpoints after testing to avoid unnecessary costs.</p>"},{"location":"examples/integrations/vertexai/vertexai/#conclusion","title":"Conclusion\u00b6","text":"<p>In this notebook, we walked through the complete workflow of training and deploying a machine learning model using Vertex AI. We began by setting up our environment, configuring necessary project variables, and initializing Vertex AI. We then trained a binary classification model using the getML framework, logged and tracked our experiments using the MetadataStore, and saved the model artifact to Google Cloud Storage.</p> <p>Next, we built and tested a custom prediction routine locally before pushing our Docker image to the Artifact Registry. We deployed the trained model to the Vertex AI Model Registry and created an online prediction endpoint to serve real-time predictions. Additionally, we discussed how to manually manage the Docker container and perform batch predictions.</p> <p>By following these steps, you have learned how to leverage Vertex AI for end-to-end machine learning workflows, from data preprocessing and model training to deployment and prediction. This powerful combination of tools and services ensures a scalable, efficient, and well-managed approach to developing and deploying getML models on Google Cloud.</p>"},{"location":"install/","title":"Installation","text":"<p>getML is a software suite for automated feature engineering on relational data and time series. It enables you to complete your data science projects in a fraction of their usual time and with better results.</p> <p>getML is available for Python 3.8 to 3.12 and supported on the following 64-bit systems and architectures:</p> <ul> <li>Linux (amd64/arm) with glibc 2.28 or above (also via docker)</li> <li>macOS (amd64/arm) via docker</li> <li>Windows (amd64/arm) via docker</li> </ul> <p>The getML Suite is comprised of Python API, Engine and Monitor. The Monitor is shipped along with the Engine.</p> <p>Both our Community and Enterprise editions use the same Python API.</p>"},{"location":"install/#linux","title":"Linux","text":"<p>Use Python's <code>pip</code> package manager to install both the Python API and the Engine on Linux natively.</p> <p>Read the Linux install guide</p>"},{"location":"install/#macos-windows-linux","title":"macOS, Windows &amp; Linux","text":"<p>Setup a Docker container to run the Engine. Install the Python API with <code>pip</code>.</p> <p>Read the Docker install guide</p>"},{"location":"install/#from-source","title":"From source","text":"<p>Install the Python API from source.</p> <p>Read the API install guide</p> <p>Build Engine and the API from source.</p> <p>Read the Engine &amp; API build guide</p>"},{"location":"install/#using-versioned-archives","title":"Using versioned archives","text":"<p>In some scenarios, installing getML from an archive might be necessary.</p> <p>Read the archive install guide</p>"},{"location":"install/release-notes/","title":"Release notes","text":""},{"location":"install/release-notes/#release-notes_1","title":"Release Notes","text":""},{"location":"install/release-notes/#for-getml-enterprise-community-editions","title":"For getML Enterprise &amp; Community editions","text":""},{"location":"install/release-notes/#1.4.0","title":"1.4.0   Oct 17, 2023","text":"<ul> <li>Accelerated feature learning through Fastboost</li> <li>Improved modelling on huge datasets through ScaleGBMClassifier and ScaleGBMRegressor</li> <li>Advanced trend aggregations using EWMATrend aggregations</li> <li>Faster JSON parsing using YYJSON</li> </ul>"},{"location":"install/release-notes/#1.3.2","title":"1.3.2   Jan 26, 2023","text":"<ul> <li>Minor bugfixes</li> </ul>"},{"location":"install/release-notes/#1.3.1","title":"1.3.1   Dec 20, 2022","text":"<ul> <li>Implement <code>tqdm</code> for progress bars</li> <li>Minor bugfixes</li> </ul>"},{"location":"install/release-notes/#1.3.0","title":"1.3.0   Aug 28, 2022","text":"<ul> <li>Use websockets instead of polling</li> <li>Size threshold for better visualization of feature code</li> <li>Faster reading of memory-mapped data, relevant for all feature learners and predictors</li> <li>Introduce CategoryTrimmer as preprocessor</li> </ul>"},{"location":"install/release-notes/#1.2.0","title":"1.2.0   May 20, 2022","text":"<ul> <li>Support for SQL transpilation: TSQL, Postgres, MySQL, BigQuery, Spark</li> <li>Support for memory mapping</li> </ul>"},{"location":"install/release-notes/#1.1.0","title":"1.1.0   Nov 21, 2021","text":"<ul> <li>Enhance data processing by introducing Spark (e.g. spark_sql) and Arrow (e.g. from_arrow())</li> <li>Integrate Vcpkg for dependency management</li> <li>Improve code transpilation for seasonal variables</li> <li>Better control of predictor training and hyperparamter optimization through introduction of early stopping (e.g. in ScaleGBMClassifier)</li> <li>Introduce TREND aggregation</li> <li>Better progress logging</li> </ul>"},{"location":"install/release-notes/#1.0.0","title":"1.0.0   Sep 23, 2021","text":"<ul> <li>Introduction of Containers for data storage</li> <li>Complete overhaul of the API including Views, StarSchema, TimeSeries</li> <li>Add subroles for fine grained data control</li> <li>Improved model evaluation through Plots and Scores container</li> <li>Introduce slicing of Views</li> <li>Add datetime() utility</li> </ul>"},{"location":"install/release-notes/#0.16.0","title":"0.16.0 May 25, 2021","text":"<ul> <li>Add the Mapping and TextFieldSplitter preprocessors</li> </ul>"},{"location":"install/release-notes/#0.15.0","title":"0.15.0 Feb 23, 2021","text":"<ul> <li>Add the Fastprop feature learner</li> <li>Overhaul the way RelMT and Relboost generate features, making them more efficient</li> </ul>"},{"location":"install/release-notes/#0.14.0","title":"0.14.0 Jan 18, 2021","text":"<ul> <li>Significant improvement of  project management:<ul> <li>project.restart(), project.suspend(), and project.switch()</li> <li>multiple project support</li> </ul> </li> <li>Add custom <code>__getattr__</code> and <code>__dir__</code> methods to DataFrame, enabling column retrieval through autocomplete</li> </ul>"},{"location":"install/release-notes/#0.13.0","title":"0.13.0 Nov 13, 2020","text":"<ul> <li>Introduce new feature learner: <ul> <li>RelMTModel [now RelMT], </li> <li>RelMTTimeSeries [now integrated in TimeSeries]</li> </ul> </li> </ul>"},{"location":"install/release-notes/#0.12.0","title":"0.12.0 Oct 1, 2020","text":"<ul> <li>Extend dataframe handling: delete(), exists()</li> <li>Data set provisioning: load_air_pollution(), load_atherosclerosis(), load_biodegradability(), load_consumer_expenditures(), load_interstate94(), load_loans(), load_occupancy()</li> <li>High-level hyperopt handlers: tune_feature_learners(), tune_predictors()</li> <li>Improve pipeline functionality: delete(), exists(), Columns </li> <li>Introduce preprocessors: EmailDomain, Imputation, Seasonal, Substring </li> </ul>"},{"location":"install/release-notes/#0.11.1","title":"0.11.1 Jul 13, 2020","text":"<ul> <li>Add pipeline functionality: Pipeline, list_pipelines(), Features, Metrics, SQLCode, Scores</li> <li>Better control of hyperparameter optimization: burn_in, kernels, optimization</li> <li>Handling of time stamps: time</li> <li>Improve database I/O: connect_odbc(), copy_table(), list_connections(), read_s3(), sniff_s3()</li> <li>Enable S3 access: set_s3_access_key_id(), set_s3_secret_access_key()</li> <li>New Feature Learner: MultirelTimeSeries, RelboostTimeSeries [now both integrated in TimeSeries]</li> </ul>"},{"location":"install/release-notes/#0.10.0","title":"0.10.0 Mar 17, 2020","text":"<ul> <li>Add XGBoostClassifier and XGBoostRegressor for improved predictive power</li> <li>Overhaul of documentation <ul> <li>Introduction of \"getML in one minute\" (now Quickstart) and \"How to use this guide\" (now User Guide)</li> <li>Introduction of User Guide (now Concepts) to include data annotation, feature engineering, hyperparameter optimization and more</li> </ul> </li> <li>Integration with additional databases like Greenplum, MariaDB, MySQL, and extended PostgreSQL support</li> </ul>"},{"location":"install/release-notes/#0.9.1","title":"0.9.1   Mar 17, 2020","text":"<ul> <li>Include hotfix for new domain getml.com</li> </ul>"},{"location":"install/release-notes/#0.9","title":"0.9 Dec 9, 2019","text":"<ul> <li>Rework hyperopt design and handling, added load_hyperopt()</li> <li>Improved dataframe handling: add to_placeholder() and nrows()</li> </ul>"},{"location":"install/release-notes/#0.8","title":"0.8 Oct 22, 2019","text":"<ul> <li>Rename Autosql to Multirel</li> <li>Boolean and categorical columns: Add support for boolean columns and operators, along with enhanced categorical column handling.</li> <li>Introduce API improvements: fitting, saving/loading of models, data transformation</li> <li>Add support for various aggregation functions such as MEDIAN, VAR, STDDEV, and COUNT_DISTINCT</li> <li>Move from closed beta to pip</li> <li>Introduce basic hyperopt algorithms: LatinHypercubeSearch, RandomSearch</li> </ul>"},{"location":"install/remote_access/","title":"Remote Access","text":"<p>This guide helps you set up getML on a remote server and access it from your local machine.</p>"},{"location":"install/remote_access/#running-the-getml-suite-remotely","title":"Running the getML Suite Remotely","text":"<p>Note that this section is more of a how-to guide for SSH and bash than something that is specific to getML.</p>"},{"location":"install/remote_access/#prerequisites","title":"Prerequisites","text":"<p>To run the getML software on a remote server, you should ensure the following:</p> <ol> <li>You know the IP of the server and can log into it using a USER account and corresponding password.</li> <li>Linux is running on the server.</li> <li>The server has a working internet connection (required to authenticate your user account) and is accessible via SSH.</li> </ol>"},{"location":"install/remote_access/#remote-installation","title":"Remote Installation","text":"<p>If all conditions are met, download the Linux version of the getML Suite from archives and copy it to the remote server. <pre><code>scp getml-VERSION-linux.tar.gz USER@IP:\n</code></pre> This will copy the entire bundle into the home folder of your USER on the remote host. Then you need to log onto the server. <pre><code>ssh USER@IP\n</code></pre> Follow the installation instructions to install getML on the remote host.</p> <p></p>"},{"location":"install/remote_access/#starting-engine-and-monitor","title":"Starting Engine and Monitor","text":"<p>Start getML using the command-line interface. It is a good idea to <code>disown</code> or <code>nohup</code> the process, so that it keeps running when you close the SSH terminal or if the connection breaks down temporarily.</p> <p><pre><code>./getML &gt; run.log &amp;\ndisown\n</code></pre> or <pre><code>nohup ./getML &amp;\n</code></pre></p> <p>Both methods will pipe the log of the Engine into a file - either run.log or nohup.out.</p>"},{"location":"install/remote_access/#login","title":"Login","text":"<p>Now the getML Engine and Monitor are running. To view the Monitor, use port forwarding via SSH. <pre><code>ssh -L 2222:localhost:1709 USER@IP\n</code></pre> This collects all traffic on port 1709 of the remote host\u2014the HTTP port of the getML Monitor\u2014and binds it to port 2222 of your local computer. By entering localhost:2222 into the navigation bar of your web browser, you can log into the remote instance. Note that this connection is only available as long as the SSH session started with the previous command is still active and running.</p>"},{"location":"install/remote_access/#running-analyses-using-the-python-api","title":"Running Analyses Using the Python API","text":"<p>When you start a Python script, you should also <code>disown</code> or <code>nohup</code> it, as explained in the previous section.</p> <p>If you want to know whether the Python process is still running, use <code>ps -aux</code>. <pre><code>ps -aux | grep python\n</code></pre> It lists all running processes and filters only those containing the letters 'python'. If your scripts appear in the listings, they are still running.</p> <p>Running an interactive session using <code>IPython</code> is also possible but should not be done directly (since you will lose all progress the moment you get disconnected). Instead, we recommend using third-party helper programs, like GNU screen or tmux.</p> <p>Note</p> <p>It is usually NOT a good idea to forward the port of the getML Engine to your local computer and then run the Python API locally. If you decide to do so anyway, make sure to always use absolute paths for data loading.</p>"},{"location":"install/remote_access/#retrieving-results","title":"Retrieving Results","text":"<p>Once your analysis is done, all results are located in the corresponding project folder. You can access them directly on the server or copy them to your local machine. <pre><code>scp USER@IP:~/.getML/getml-&lt;version&gt;/projects/* ~/.getML/getml-&lt;version&gt;/projects\n</code></pre></p>"},{"location":"install/remote_access/#stopping-engine-and-monitor","title":"Stopping Engine and Monitor","text":"<p>If you want to shutdown getML, you can use the appropriate command. <pre><code>./getML -stop\n</code></pre></p>"},{"location":"install/remote_access/#accessing-the-getml-monitor-via-the-internet","title":"Accessing the getML Monitor Via the Internet","text":"<p>Up to now you only have used the HTTP port of the Monitor and required no encryption. Isn't this insecure?</p> <p>Not at all. The getML Monitor is implemented in such a way the HTTP port can only be accessed from a browser located at the same machine the Monitor is running on. No one else will have access to it. In the scenario discussed in the previous section all communication with the remote host had been encrypted using the strong SSH protocol and all queries of the getML Suite to authenticate your login were encrypted too.</p> <p>But allowing access to the Monitor over the internet is not a bad idea  in principle. It allows you to omit the port forwarding step and grants other entities permission to view the results of your analysis in e.g. your company's intranet. This is where the HTTPS port opened by the Monitor comes in.</p>"},{"location":"install/remote_access/#what-is-accessible-and-what-is-not","title":"What is Accessible and What is Not?","text":"<p>Only the getML Monitor is accessible via the HTTPS port. There is no way to connect to the getML Engine via the internet (the Engine will reject any command sent remotely).</p> <p>After having started the Engine and Monitor on your server, connect to the latter by entering <code>https://host-ip:1710</code> into the navigation bar of your web browser. Every user still needs to log into the getML Monitor using a valid getML account and needs to be whitelisted in order to have access to the Monitor.</p>"},{"location":"install/remote_access/#creating-and-using-tls-certificates","title":"Creating and Using TLS Certificates","text":"<p>The encryption via HTTPS requires a valid TLS certificate. The TLS certificate is created when you start getML for the first time. You can discard the current certificate and generate a new one in the configuration tab of the getML Monitor. When doing so, you can choose whether the certificate should be self-signed or not. This is because HTTPS encryption is based on the so-called web of trust. Every certificate has to be checked and validated by a Certificate Authority (CA). If your browser knows and trusts the CA, it will display a closed lock in the left part of its navigation bar. If not, it will warn you and not establish the connection right away. But since a certificate must include the exact hostname including the subdomain it is used for, almost every certificate for every getML Monitor will look different and they all have to be validated by a CA somehow. This is neither cheap nor feasible. That's why the Monitor can act as a CA itself.</p> <p>When accessing the getML Monitor via HTTPS (even locally on https://localhost:1710), your browser will be alarmed, refuse to access the page at first, and tell you it doesn't know the CA. You have to allow an exception manually. Since every Monitor will be a different CA, there is no loss in security either.</p>"},{"location":"install/remote_access/#adding-an-exception-in-browsers","title":"Adding an Exception in Browsers","text":"<p>In Firefox, you first have to click on 'Advanced',</p> <p></p> <p>followed by 'Accept the Risk and Continue'. </p> <p></p> <p>In Chrome, you first have to click on 'Advanced',</p> <p></p> <p>followed by 'Proceed to localhost (unsafe)'.</p> <p></p>"},{"location":"install/remote_access/#opening-the-https-port","title":"Opening the HTTPS Port","text":"<p>Telling the getML Monitor to serve its web frontend via HTTPS on a specific port usually does not make it accessible from the outside yet. Your computer or the server does not allow arbitrary programs to open connections to the outside world. You need to add the corresponding port number to a whitelist in your system's configuration. Since there are far too many combinations of systems and applications used as firewalls, we won't cover them here. If you have questions or need help concerning this step, please feel free to contact us.</p>"},{"location":"install/uninstall/","title":"Uninstall","text":""},{"location":"install/uninstall/#python-api-all-platforms","title":"Python API (all platforms)","text":"<p>Project data might be deleted</p> <p>If you have not installed the Engine separately and have not set the home directory to a custom location on the Engine launch, project data will be deleted when you uninstall the Python API.</p> <p>To uninstall the Python API, execute the following command in a terminal:</p> <pre><code>pip uninstall getml\n</code></pre>"},{"location":"install/uninstall/#linux","title":"Linux","text":"<p>Project data might be deleted</p> <p>If you have not set the home directory to a custom location on the Engine launch, project data will be deleted when you remove the <code>.getML</code> directory.</p> <p>You will have to remove the folder <code>.getML</code> from your home directory. In a terminal, execute:  <pre><code>rm -r $HOME/.getML\n</code></pre></p>"},{"location":"install/uninstall/#docker","title":"Docker","text":"<p>To remove the resources defined in the <code>docker-compose.yml</code> file, you can follow these steps:</p>"},{"location":"install/uninstall/#docker-image","title":"Docker Image","text":"<p>Remove the Docker image <code>getml/getml</code> from your local Docker repository as follows:</p> <pre><code>docker rmi getml/getml\n</code></pre> <p>Note that if there are any containers using this image, you must remove those containers first.</p>"},{"location":"install/uninstall/#named-volumes","title":"Named Volumes","text":"<p>Remove the <code>getml</code> volume as follows:</p> <pre><code>docker volume rm getml\n</code></pre> <p>Ensure that the volume is not in use by any other containers.</p>"},{"location":"install/packages/archive/","title":"Archive","text":""},{"location":"install/packages/archive/#getml-community-edition-archives","title":"getML Community edition archives","text":"<p>The getML Community edition is provided as downloadable, versioned archives for each release.</p>"},{"location":"install/packages/archive/#version-150","title":"Version 1.5.0","text":"<ul> <li>getml-1.5.0-arm64-community-edition-linux.tar.gz - X MB</li> <li>getml-1.5.0-x64-community-edition-linux.tar.gz - X MB</li> </ul>"},{"location":"install/packages/docker/","title":"Docker","text":""},{"location":"install/packages/docker/#docker-for-macos-windows-linux","title":"Docker for macOS, Windows &amp; Linux","text":"<p>On macOS and Windows, you can to run the Engine in a Docker container. We are working on providing native support for them in the near future.</p> <p>Setup the Python API and the Engine of the getML Community edition as follows.</p>"},{"location":"install/packages/docker/#python-api","title":"Python API","text":"<p>Use Python's pip package manager to install the API:</p> <p><pre><code>pip install getml\n</code></pre> </p>"},{"location":"install/packages/docker/#engine","title":"Engine","text":"<p>Make sure that Docker is installed. For Linux, follow these post-installation steps to run Docker without root rights.</p> <p>Enterprise Edition</p> <p>Need the highest models accuracy in commercial prediction applications and enterprise grade support?</p> <p>Choose getML Enterprise</p> <p>Once you have obtained the getML Enterprise edition, you can install it as follows:</p> <ul> <li>Install the Python API: <code>pip install getml</code></li> <li>Install the Engine by following the same instructions as below. Just replace the URL with that of the Enterprise <code>docker-compose.yml</code> file you have been provided with.</li> </ul> <p>Run the following command in your terminal (macOS &amp; Linux) or PowerShell (Windows):</p> <pre><code>curl https://raw.githubusercontent.com/getml/getml-community/1.5.0/runtime/docker-compose.yml | docker-compose up -f -\n</code></pre> <p>This will download <code>docker-compose.yml</code> configuration file and use <code>docker compose</code> to run a getML service.</p> <p>In addition, a docker volume <code>getml</code> will be created and mounted into the container. This volume will contain the files of projects. The ports required for the Python API to communicate with the Engine will be mapped to the host system.</p> <p>To shut down the service after you are done, press <code>Ctrl+c</code>.</p>"},{"location":"install/packages/docker/#where-to-go-next","title":"Where to go next","text":"<p>To get started with getML, you may check out:</p> <ul> <li>User Guide, which provides a comprehensive introduction to getML at varying levels of detail</li> <li>Examples, where we demonstrate practical examples inside Jupyter Notebooks</li> </ul>"},{"location":"install/packages/linux/","title":"Linux","text":"<p> Install getML Community edition with Python's pip package manager.</p> <p><pre><code>pip install getml\n</code></pre> The will install both the Python API and the Engine on your Linux machine. You are done.</p> <p>Enterprise Edition</p> <p>Need the highest models accuracy in commercial prediction applications and enterprise grade support?</p> <p>Choose getML Enterprise</p> <p>Once you have obtained the getML Enterprise edition, you can install it as follows:</p> <ul> <li>Install the Python API: <code>pip install getml</code></li> <li>Install the Engine by following instructions for separate installation of it below. Start from step 2 and use the enterprise <code>tar</code> file you have been provided with.</li> </ul> <p></p>"},{"location":"install/packages/linux/#separate-installation-of-engine_1","title":"Separate installation of Engine","text":"<p>In some cases, it might be preferred to install the Engine separately on Linux using CLI. For example, if you want to use the Enterprise edition of the Engine.</p> <p>Please execute the following commands, replacing <code>ARCH</code> with either <code>x64</code> or <code>arm64</code>, depending on your architecture. If you are unsure, <code>x64</code> is probably the right choice. You can also use <code>uname -m</code> to figure out the architecture. If it says something like <code>aarch64</code> or <code>arm64</code>, you need to use <code>arm64</code>, otherwise go with <code>x64</code>.</p> <pre><code># 1. Download the tar file of the Engine\nwget https://static.getml.com/download/1.5.0/getml-1.5.0-ARCH-community-edition-linux.tar.gz\n\n# 2. Extract the tar file\ntar -xzf getml-1.5.0-ARCH-community-edition-linux.tar.gz\n\n# 3. Change directory \ncd getml-1.5.0-ARCH-community-edition-linux\n\n# 4. Install the Engine using CLI\n./getML install\n</code></pre> <p>The output of the <code>install</code> command will tell you where the Engine has been installed. It will look something like this:</p> <pre><code>getml@laptop src % ./getML install        \nInstalling getML...\nCould not install into '/usr/local': mkdir /usr/local/getML: permission denied\nGlobal installation failed, most likely due to missing root rights. Trying local installation instead.\nInstalling getML...\nSuccessfully installed getML into '/Users/getml/.getML/getml-1.5.0-arm64-community-edition-linux'.\nInstallation successful. To be able to call 'getML' from anywhere, add the following path to PATH:\n/home/user/.getML/getml-1.5.0-arm64-community-edition-linux\n</code></pre>"},{"location":"install/packages/linux/#launching-the-engine","title":"Launching the engine","text":"<p>To run the engine, execute: <pre><code>./getML\n</code></pre></p> <p>If the Engine was installed to the user home directory, you can add the installation directory to your <code>PATH</code> variable if you want to call the getML CLI from anywhere.</p> <pre><code>export PATH=$PATH:/path/to/getml-1.5.0-ARCH-community-edition-linux\n</code></pre> <p>To make the changes permanent, you will have to add the line to your <code>.bashrc</code> or <code>.bash_profile</code> file. </p>"},{"location":"install/packages/linux/#where-to-go-next","title":"Where to go next","text":"<p>To get started with getML, you may check out:</p> <ul> <li>User Guide, which provides a comprehensive introduction to getML at varying levels of detail</li> <li>Examples, where we demonstrate practical examples inside Jupyter Notebooks</li> </ul>"},{"location":"install/source/build/","title":"Build Engine & API","text":"<p>Because getML is complex software, we use Docker for our build environment. If you want to compile our Community edition from source, you can start with cloning its repository:</p> <pre><code>git clone https://github.com/getml/getml-community.git\n</code></pre> <p>We provide a set of wrappers to ease local development. They are located inside the <code>bin</code> directory of the repository. You can use them with the <code>getml</code> command and the <code>build</code> subcommand:</p> <pre><code>./bin/getml\n\nUsage:\n    getml &lt;subcommand&gt; [options]\n\nSubcommands:\n    build   Build utilities\n    help    Show help (this message)\n\nOptions:\n    -h      Show help (this message)\n</code></pre>"},{"location":"install/source/build/#subcommand-build","title":"Subcommand <code>build</code>","text":"<p>The <code>build</code> subcommand is the entry point for building getML from source. For details about the build process, see Directly Interacting with Bake below.</p> <pre><code>getml build wrapper\n\nUsage:\n  build &lt;subcommand&gt; [options]\n\nSubcommands:\n  [a]ll       Build all (whole package, [p]ackage + [py]thon API + tar+gz [ar]chive)\n  [c]li       Build CLI\n  [e]ngine    Build Engine\n  [p]ackage   Export runnable [e]ngine + [c]li package\n  [py]thon    Package Python API\n  [ar]chive   Create tar.gz archive of [p]ackage\n\nOptions:\n  -b &lt;args&gt;   Specify build args (-b KEY=VALUE); passed to docker build\n  -h          Show help (this message)\n  -o &lt;path&gt;   Set output path (default: build); passed to docker build\n</code></pre> <p>Most of the time you probably want to build the (C++) Engine:</p> <pre><code>./bin/getml build engine\n</code></pre> <p>If you are calling <code>getml build package</code>, all build artifacts will be packaged inside the specified output folder. With <code>archive</code>, a compressed tarball (<code>getml-&lt;version&gt;-&lt;arch&gt;-linux.tar.gz</code>) will be created inside the folder.</p>"},{"location":"install/source/build/#build-options","title":"Build options","text":""},{"location":"install/source/build/#-b-args-build-args","title":"<code>-b &lt;args&gt;</code>: Build args","text":"<p>These are build arguments passed to <code>docker build</code>. Build args can be provided as key-value pairs. The following build args are supported:</p> <ul> <li><code>VERSION</code>: the build version (default: the version specified in the <code>VERSION</code> file, present in the root of getml community repository)</li> <li><code>NJOBS</code>: the number of threads to use for each compilation step</li> </ul>"},{"location":"install/source/build/#-h-show-help","title":"<code>-h</code>: Show help","text":"<p>Show the help screen</p>"},{"location":"install/source/build/#-o-path-output-folder","title":"<code>-o &lt;path&gt;</code>: Output folder","text":"<p>The output folder used by Docker's export backend</p>"},{"location":"install/source/build/#directly-interacting-with-bake","title":"Directly Interacting with Bake","text":"<p>The build pipeline is based on multi-stage Docker builds. There are two <code>Dockerfile</code>s:</p> <ul> <li>One for CLI, wheel, and packaging located in the repository's root:   <code>./Dockerfile</code></li> <li>One related to the Engine and its dependencies located in the repository's <code>src/engine</code> subfolder:   <code>./src/engine/Dockerfile</code></li> </ul> <p>As the second <code>Dockerfile</code> is a dependency for the first, we use bake to orchestrate the builds. The bake file (<code>./docker-bake.hcl</code>) holds definitions for all build targets and ensures the appropriate build contexts are set.</p> <p>If you want to interact with Docker directly, you can do so by calling <code>docker buildx bake</code>:</p> <pre><code>VERSION=1.5.0 docker buildx bake engine\n</code></pre> <p>If you want to override build-args, you can do so per build stage via bake's <code>--set</code> overrides:</p> <pre><code>docker buildx bake engine --set engine.args.VERSION=1.5.0\n</code></pre> <p>This way, you can also override some of a target's default attributes:</p> <pre><code>docker buildx bake engine --set engine.output=out\n</code></pre>"},{"location":"install/source/python-api/","title":"Python API from source","text":"<p>If you want to install the Python API from the source, you will have to install the Engine separately unless you install the wheel file provided by following the steps for building the Engine from source.</p> <pre><code># Clone the Community edition\ngit clone https://github.com/getml/getml-community.git\n\n# Change directory \ncd src/python-api\n\n# Install the Python API\npip3 install .\n</code></pre>"},{"location":"reference/","title":"API Reference","text":""},{"location":"reference/#python-api","title":"Python API","text":"<p>Welcome to the API documentation for Python. The Python API is a convenient, easy to use interface to the getML Engine. General information about the interoperation of the different parts of getML can be found in the user guide.</p> <p>If you have never used the Python API, it is probably easiest to start with the walkthrough.</p>"},{"location":"reference/cli/engine/","title":"Engine CLI","text":""},{"location":"reference/cli/engine/#command-line-interface","title":"Command Line Interface","text":"<p>Note</p> <p>You do not need to launch getML Engine with the command line if you installed getML Suite using <code>pip</code> on Linux. You can execute getml.engine.launch() inside Python code to launch the Engine.</p> <p>On Linux, getML Engine can also installed and used via the command line interface (CLI) called <code>getML</code>.</p> <p>Some parameters can be set via command line flags. If you do not explicitly set them, the values from your config.json are taken instead. The config.json is located  in <code>$HOME/.getML/getml-VERSION</code>. For Enterprise edition users,  the most elegant way to edit your config.json is via  the configuration view in the getML Monitor. Community edition users can edit  the file via a text editor.</p> <p>The help menu can also be displayed by passing the flag <code>-help</code> or <code>-h</code>. The default values displayed in the help menu are the values in the config.json (therefore, they are not hard-coded).</p> <pre><code>usage: ./getML &lt;command&gt; [&lt;args&gt;] or ./getML [&lt;args&gt;].\n</code></pre> <pre><code>Possible commands are:\n run        Runs getML. Type \"./getML -h\" or \"./getML run -h\" to display the arguments. \"run\" is executed by default.\n install    Installs getML.\n stop       Stops a running instance of getML. Type \"./getML stop -h\" to display the arguments.\n uninstall  Uninstalls getML\n version    Prints the version of getML (getml-VERSION-ARCH-PLATFORM)\n</code></pre> <p><pre><code>Usage of run:\n  -allow-push-notifications\n        Whether you want to allow the getML Monitor to send push notifications to your desktop. (default true)\n  -allow-remote-ips\n        Whether you want to allow remote IPs to access the http-port.\n  -home-directory string\n        The directory which should be treated as the home directory by getML. getML will create a hidden folder named '.getML' in said directory. This is where the binaries are installed, if the install process does not have root rights. (default \"/root\")\n  -https-port int\n        The local port of the getML Monitor. This port can only be accessed from your local computer, unless you set allow-remote-ips=True. (default 1709)\n  -in-memory\n        Whether you want the Engine to process everything in memory. (default true)\n  -install\n        Installs getML, even if it is already installed. (default true)\n  -launch-browser\n        Whether you want to automatically launch your browser. (default true)\n  -log\n        Whether you want the Engine log to appear in the command line. The Engine log also appears in the 'Log' page of the monitor.\n  -project-directory string\n        The directory in which to store all of your projects. (default \"~/.getML/projects\")\n  -proxy-url string\n        The URL of any proxy server that that redirects to the getML Monitor.\n  -tcp-port int\n        Local TCP port which serves as the communication point for the Engine. This port can only be accessed from your local computer. (default 1711)\n  -token string\n        The token used for authentication. Authentication is required when remote IPs are allowed to access the monitor. If authentication is required and no token is passed, a random hexcode will be generated as the token.\n</code></pre> <pre><code>Usage of stop:\n  -tcp-port int\n      The TCP port of the getML instance you would like to stop. (default 1711)\n</code></pre></p>"},{"location":"reference/data/","title":"Data","text":""},{"location":"reference/data/#getml.data","title":"getml.data","text":"<p>Contains functionalities for importing, handling, and retrieving data from the getML Engine.</p> <p>All data relevant for the getML Suite has to be present in the getML Engine. Its Python API itself does not store any of the data used for training or prediction. Instead, it provides a handler class for the data frame objects in the getML Engine, the <code>DataFrame</code>. Either using this overall handler for the underlying data set or the individual <code>columns</code> it is composed of, one can both import and retrieve data from the Engine as well as performing operations on them. In addition to the data frame objects, the Engine also uses an abstract and lightweight version of the underlying data model, which is represented by the <code>Placeholder</code>.</p> <p>In general, working with data within the getML Suite is organized in three different steps.</p> <ul> <li>Importing the data into the getML Engine .</li> <li>Annotating the data by assigning   <code>roles</code> to the individual <code>columns</code></li> <li>Constructing the data model by deriving   <code>Placeholder</code> from the data and joining them to   represent the data schema.</li> </ul> Example <p>Creating a new data frame object in the getML Engine and importing data is done by one of the class methods <code>from_csv</code>, <code>from_db</code>, <code>from_json</code>, or <code>from_pandas</code>.</p> <p>In this example we chose to directly load data from a public database in the internet. But, firstly, we have to connect the getML Engine to the database (see MySQL interface in the user guide for further details).</p> <pre><code>getml.database.connect_mysql(\n    host=\"db.relational-data.org\",\n    dbname=\"financial\",\n    port=3306,\n    user=\"guest\",\n    password=\"relational\",\n    time_formats=['%Y/%m/%d']\n)\n</code></pre> <p>Using the established connection, we can tell the Engine to construct a new data frame object called <code>df_loan</code>, fill it with the data of <code>loan</code> table contained in the MySQL database, and return a <code>DataFrame</code> handler associated with it.</p> <p><pre><code>loan = getml.DataFrame.from_db('loan', 'df_loan')\n\nprint(loan)\n</code></pre> <pre><code>| loan_id      | account_id   | amount       | duration     | date          | payments      | status        |\n| unused float | unused float | unused float | unused float | unused string | unused string | unused string |\n-------------------------------------------------------------------------------------------------------------\n| 4959         | 2            | 80952        | 24           | 1994-01-05    | 3373.00       | A             |\n| 4961         | 19           | 30276        | 12           | 1996-04-29    | 2523.00       | B             |\n| 4962         | 25           | 30276        | 12           | 1997-12-08    | 2523.00       | A             |\n| 4967         | 37           | 318480       | 60           | 1998-10-14    | 5308.00       | D             |\n| 4968         | 38           | 110736       | 48           | 1998-04-19    | 2307.00       | C             |\n</code></pre> In order to construct the data model and for the feature learning algorithm to get the most out of your data, you have to assign roles to columns using the <code>set_role</code> method (see Annotating data for details).</p> <p>(For demonstration purposes, we assign <code>payments</code> the <code>target role</code>. In reality, you would want to forecast the defaulting behaviour, which is encoded in the <code>status</code> column. See the loans notebook.)</p> <p><pre><code>loan.set_role([\"duration\", \"amount\"], getml.data.roles.numerical)\nloan.set_role([\"loan_id\", \"account_id\"], getml.data.roles.join_key)\nloan.set_role(\"date\", getml.data.roles.time_stamp)\nloan.set_role([\"payments\"], getml.data.roles.target)\n\nprint(loan)\n</code></pre> <pre><code>| date                        | loan_id  | account_id | payments  | duration  | amount    | status        |\n| time stamp                  | join key | join key   | target    | numerical | numerical | unused string |\n-----------------------------------------------------------------------------------------------------------\n| 1994-01-05T00:00:00.000000Z | 4959     | 2          | 3373      | 24        | 80952     | A             |\n| 1996-04-29T00:00:00.000000Z | 4961     | 19         | 2523      | 12        | 30276     | B             |\n| 1997-12-08T00:00:00.000000Z | 4962     | 25         | 2523      | 12        | 30276     | A             |\n| 1998-10-14T00:00:00.000000Z | 4967     | 37         | 5308      | 60        | 318480    | D             |\n| 1998-04-19T00:00:00.000000Z | 4968     | 38         | 2307      | 48        | 110736    | C             |\n</code></pre> Finally, we are able to construct the data model by deriving <code>Placeholder</code> from each <code>DataFrame</code> and establishing relations between them using the <code>join</code> method.</p> <pre><code># But, first, we need a second data set to build a data model.\ntrans = getml.DataFrame.from_db(\n    'trans', 'df_trans',\n    roles = {getml.data.roles.numerical: [\"amount\", \"balance\"],\n             getml.data.roles.categorical: [\"type\", \"bank\", \"k_symbol\",\n                                            \"account\", \"operation\"],\n             getml.data.roles.join_key: [\"account_id\"],\n             getml.data.roles.time_stamp: [\"date\"]\n    }\n)\n\nph_loan = loan.to_placeholder()\nph_trans = trans.to_placeholder()\n\nph_loan.join(ph_trans, on=\"account_id\",\n            time_stamps=\"date\")\n</code></pre> <p>The data model contained in <code>ph_loan</code> can now be used to construct a <code>Pipeline</code>.</p>"},{"location":"reference/data/#getml.data.arange","title":"arange","text":"<pre><code>arange(\n    start: Union[Real, float] = 0.0,\n    stop: Optional[Union[Real, float]] = None,\n    step: Union[Real, float] = 1.0,\n)\n</code></pre> <p>Returns evenly spaced variables, within a given interval.</p> PARAMETER DESCRIPTION <code>start</code> <p>The beginning of the interval. Defaults to 0.</p> <p> TYPE: <code>Union[Real, float]</code> DEFAULT: <code>0.0</code> </p> <code>stop</code> <p>The end of the interval.</p> <p> TYPE: <code>Optional[Union[Real, float]]</code> DEFAULT: <code>None</code> </p> <code>step</code> <p>The step taken. Defaults to 1.</p> <p> TYPE: <code>Union[Real, float]</code> DEFAULT: <code>1.0</code> </p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def arange(\n    start: Union[numbers.Real, float] = 0.0,\n    stop: Optional[Union[numbers.Real, float]] = None,\n    step: Union[numbers.Real, float] = 1.0,\n):\n    \"\"\"\n    Returns evenly spaced variables, within a given interval.\n\n    Args:\n        start:\n            The beginning of the interval. Defaults to 0.\n\n        stop:\n            The end of the interval.\n\n        step:\n            The step taken. Defaults to 1.\n    \"\"\"\n    if stop is None:\n        stop = start\n        start = 0.0\n\n    if step is None:\n        step = 1.0\n\n    if not isinstance(start, numbers.Real):\n        raise TypeError(\"'start' must be a real number\")\n\n    if not isinstance(stop, numbers.Real):\n        raise TypeError(\"'stop' must be a real number\")\n\n    if not isinstance(step, numbers.Real):\n        raise TypeError(\"'step' must be a real number\")\n\n    col = FloatColumnView(\n        operator=\"arange\",\n        operand1=None,\n        operand2=None,\n    )\n\n    col.cmd[\"start_\"] = float(start)\n    col.cmd[\"stop_\"] = float(stop)\n    col.cmd[\"step_\"] = float(step)\n\n    return col\n</code></pre>"},{"location":"reference/data/#getml.data.rowid","title":"rowid","text":"<pre><code>rowid() -&gt; FloatColumnView\n</code></pre> <p>Get the row numbers of the table.</p> RETURNS DESCRIPTION <code>FloatColumnView</code> <p>(numerical) column containing the row id, starting with 0</p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def rowid() -&gt; FloatColumnView:\n    \"\"\"\n    Get the row numbers of the table.\n\n    Returns:\n            (numerical) column containing the row id, starting with 0\n    \"\"\"\n    return FloatColumnView(operator=\"rowid\", operand1=None, operand2=None)\n</code></pre>"},{"location":"reference/data/#getml.data.list_data_frames","title":"list_data_frames","text":"<pre><code>list_data_frames() -&gt; Dict[str, List[str]]\n</code></pre> <p>Lists all available data frames of the project.</p> RETURNS DESCRIPTION <code>dict</code> <p>Dict containing lists of strings representing the names of the data frames objects</p> <ul> <li>'in_memory'     held in memory (RAM).</li> <li>'on_disk'     stored on disk.</li> </ul> <p> TYPE: <code>Dict[str, List[str]]</code> </p> Example <pre><code>d, _ = getml.datasets.make_numerical()\ngetml.data.list_data_frames()\nd.save()\ngetml.data.list_data_frames()\n</code></pre> Source code in <code>getml/data/helpers.py</code> <pre><code>def list_data_frames() -&gt; Dict[str, List[str]]:\n    \"\"\"Lists all available data frames of the project.\n\n    Returns:\n        dict:\n            Dict containing lists of strings representing the names of\n            the data frames objects\n\n            - 'in_memory'\n                held in memory (RAM).\n            - 'on_disk'\n                stored on disk.\n\n    ??? example\n        ```python\n        d, _ = getml.datasets.make_numerical()\n        getml.data.list_data_frames()\n        d.save()\n        getml.data.list_data_frames()\n        ```\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"list_data_frames\"\n    cmd[\"name_\"] = \"\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.handle_engine_exception(msg)\n        json_str = comm.recv_string(sock)\n\n    return json.loads(json_str)\n</code></pre>"},{"location":"reference/data/#getml.data.delete","title":"delete","text":"<pre><code>delete(name: str)\n</code></pre> <p>If a data frame named 'name' exists, it is deleted.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the data frame.</p> <p> TYPE: <code>str</code> </p> Source code in <code>getml/data/helpers2.py</code> <pre><code>def delete(name: str):\n    \"\"\"\n    If a data frame named 'name' exists, it is deleted.\n\n    Args:\n        name:\n            Name of the data frame.\n    \"\"\"\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    if exists(name):\n        DataFrame(name).delete()\n</code></pre>"},{"location":"reference/data/#getml.data.exists","title":"exists","text":"<pre><code>exists(name: str)\n</code></pre> <p>Returns true if a data frame named 'name' exists.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the data frame.</p> <p> TYPE: <code>str</code> </p> Source code in <code>getml/data/helpers2.py</code> <pre><code>def exists(name: str):\n    \"\"\"\n    Returns true if a data frame named 'name' exists.\n\n    Args:\n        name:\n            Name of the data frame.\n    \"\"\"\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    all_df = list_data_frames()\n\n    return name in (all_df[\"in_memory\"] + all_df[\"on_disk\"])\n</code></pre>"},{"location":"reference/data/#getml.data.load_data_frame","title":"load_data_frame","text":"<pre><code>load_data_frame(name: str) -&gt; DataFrame\n</code></pre> <p>Retrieves a <code>DataFrame</code> handler of data in the getML Engine.</p> <p>A data frame object can be loaded regardless if it is held in memory or not. It only has to be present in the current project and thus listed in the output of <code>list_data_frames</code>.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the data frame.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handle the underlying data frame in the getML Engine.</p> Example <pre><code>d, _ = getml.datasets.make_numerical(population_name = 'test')\nd2 = getml.data.load_data_frame('test')\n</code></pre> Source code in <code>getml/data/helpers2.py</code> <pre><code>def load_data_frame(name: str) -&gt; DataFrame:\n    \"\"\"Retrieves a [`DataFrame`][getml.DataFrame] handler of data in the\n    getML Engine.\n\n    A data frame object can be loaded regardless if it is held in\n    memory or not. It only has to be present in the current project\n    and thus listed in the output of\n    [`list_data_frames`][getml.data.list_data_frames].\n\n    Args:\n        name:\n            Name of the data frame.\n\n    Returns:\n            Handle the underlying data frame in the getML Engine.\n\n    ??? example\n        ```python\n        d, _ = getml.datasets.make_numerical(population_name = 'test')\n        d2 = getml.data.load_data_frame('test')\n        ```\n    \"\"\"\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    data_frames_available = list_data_frames()\n\n    if name in data_frames_available[\"in_memory\"]:\n        return DataFrame(name).refresh()\n\n    if name in data_frames_available[\"on_disk\"]:\n        return DataFrame(name).load()\n\n    raise ValueError(\n        \"No data frame holding the name '\" + name + \"' present on the getML Engine.\"\n    )\n</code></pre>"},{"location":"reference/data/#getml.data.make_target_columns","title":"make_target_columns","text":"<pre><code>make_target_columns(\n    base: Union[DataFrame, View], colname: str\n) -&gt; View\n</code></pre> <p>Returns a view containing binary target columns.</p> <p>getML expects binary target columns for classification problems. This helper function allows you to split up a column into such binary target columns.</p> PARAMETER DESCRIPTION <code>base</code> <p>The original view or data frame. <code>base</code> will remain unaffected by this function, instead you will get a view with the appropriate changes.</p> <p> TYPE: <code>Union[DataFrame, View]</code> </p> <code>colname</code> <p>The column you would like to split. A column named <code>colname</code> should appear on <code>base</code>.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>View</code> <p>A view containing binary target columns.</p> Source code in <code>getml/data/helpers2.py</code> <pre><code>def make_target_columns(base: Union[DataFrame, View], colname: str) -&gt; View:\n    \"\"\"\n    Returns a view containing binary target columns.\n\n    getML expects binary target columns for classification problems. This\n    helper function allows you to split up a column into such binary\n    target columns.\n\n    Args:\n        base:\n            The original view or data frame. `base` will remain unaffected\n            by this function, instead you will get a view with the appropriate\n            changes.\n\n        colname: The column you would like to split. A column named\n            `colname` should appear on `base`.\n\n    Returns:\n        A view containing binary target columns.\n    \"\"\"\n    if not isinstance(\n        base[colname], (FloatColumn, FloatColumnView, StringColumn, StringColumnView)\n    ):\n        raise TypeError(\n            \"'\"\n            + colname\n            + \"' must be a FloatColumn, a FloatColumnView, \"\n            + \"a StringColumn or a StringColumnView.\"\n        )\n\n    unique_values = base[colname].unique()\n\n    if len(unique_values) &gt; 10:\n        logger.warning(\n            \"You are splitting the column into more than 10 target \"\n            + \"columns. This might take a long time to fit.\"\n        )\n\n    view = base\n\n    for label in unique_values:\n        col = (base[colname] == label).as_num()\n        name = colname + \"=\" + label\n        view = view.with_column(col=col, name=name, role=target)\n\n    return view.drop(colname)\n</code></pre>"},{"location":"reference/data/#getml.data.to_placeholder","title":"to_placeholder","text":"<pre><code>to_placeholder(\n    *args: Union[\n        DataFrame, View, List[Union[DataFrame, View]]\n    ],\n    **kwargs: Union[\n        DataFrame, View, List[Union[DataFrame, View]]\n    ]\n) -&gt; List[Placeholder]\n</code></pre> <p>Factory function for extracting placeholders from a <code>DataFrame</code> or <code>View</code>.</p> PARAMETER DESCRIPTION <code>args</code> <p>The data frames or views you would like to convert to placeholders.</p> <p> TYPE: <code>Union[DataFrame, View, List[Union[DataFrame, View]]]</code> DEFAULT: <code>()</code> </p> <code>kwargs</code> <p>The data frames or views you would like to convert to placeholders.</p> <p> TYPE: <code>Union[DataFrame, View, List[Union[DataFrame, View]]]</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>List[Placeholder]</code> <p>A list of placeholders.</p> Example <p>Suppose we wanted to create a <code>DataModel</code>:</p> <pre><code>dm = getml.data.DataModel(\n    population_train.to_placeholder(\"population\")\n)\n\n# Add placeholders for the peripheral tables.\ndm.add(meta.to_placeholder(\"meta\"))\ndm.add(order.to_placeholder(\"order\"))\ndm.add(trans.to_placeholder(\"trans\"))\n</code></pre> <p>But this is a bit repetitive. So instead, we can do the following: <pre><code>dm = getml.data.DataModel(\n    population_train.to_placeholder(\"population\")\n)\n\n# Add placeholders for the peripheral tables.\ndm.add(getml.data.to_placeholder(\n    meta=meta, order=order, trans=trans))\n</code></pre></p> Source code in <code>getml/data/helpers2.py</code> <pre><code>def to_placeholder(\n    *args: Union[DataFrame, View, List[Union[DataFrame, View]]],\n    **kwargs: Union[DataFrame, View, List[Union[DataFrame, View]]],\n) -&gt; List[Placeholder]:\n    \"\"\"\n    Factory function for extracting placeholders from a\n    [`DataFrame`][getml.DataFrame] or [`View`][getml.data.View].\n\n    Args:\n        args:\n            The data frames or views you would like to convert to placeholders.\n\n        kwargs:\n            The data frames or views you would like to convert to placeholders.\n\n    Returns:\n        A list of placeholders.\n\n    ??? example\n        Suppose we wanted to create a [`DataModel`][getml.data.DataModel]:\n\n\n\n            dm = getml.data.DataModel(\n                population_train.to_placeholder(\"population\")\n            )\n\n            # Add placeholders for the peripheral tables.\n            dm.add(meta.to_placeholder(\"meta\"))\n            dm.add(order.to_placeholder(\"order\"))\n            dm.add(trans.to_placeholder(\"trans\"))\n\n        But this is a bit repetitive. So instead, we can do\n        the following:\n        ```python\n        dm = getml.data.DataModel(\n            population_train.to_placeholder(\"population\")\n        )\n\n        # Add placeholders for the peripheral tables.\n        dm.add(getml.data.to_placeholder(\n            meta=meta, order=order, trans=trans))\n        ```\n    \"\"\"\n\n    def to_ph_list(list_or_elem, key=None):\n        as_list = list_or_elem if isinstance(list_or_elem, list) else [list_or_elem]\n        return [elem.to_placeholder(key) for elem in as_list]\n\n    return [elem for item in args for elem in to_ph_list(item)] + [\n        elem for (k, v) in kwargs.items() for elem in to_ph_list(v, k)\n    ]\n</code></pre>"},{"location":"reference/data/#getml.data.load_container.load_container","title":"load_container","text":"<pre><code>load_container(container_id: str) -&gt; Container\n</code></pre> <p>Loads a container and all associated data frames from disk.</p> PARAMETER DESCRIPTION <code>container_id</code> <p>The id of the container you would like to load.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Container</code> <p>The container with the given id.</p> Source code in <code>getml/data/load_container.py</code> <pre><code>def load_container(container_id: str) -&gt; Container:\n    \"\"\"\n    Loads a container and all associated data frames from disk.\n\n    Args:\n        container_id:\n            The id of the container you would like to load.\n\n    Returns:\n        The container with the given id.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataContainer.load\"\n    cmd[\"name_\"] = container_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.handle_engine_exception(msg)\n        json_str = comm.recv_string(sock)\n\n    cmd = json.loads(json_str)\n\n    population = _load_view(cmd[\"population_\"]) if \"population_\" in cmd else None\n\n    peripheral = {k: _load_view(v) for (k, v) in cmd[\"peripheral_\"].items()}\n\n    subsets = {k: _load_view(v) for (k, v) in cmd[\"subsets_\"].items()}\n\n    split = _parse(cmd[\"split_\"]) if \"split_\" in cmd else None\n\n    deep_copy = cmd[\"deep_copy_\"]\n    frozen_time = cmd[\"frozen_time_\"] if \"frozen_time_\" in cmd else None\n    last_change = cmd[\"last_change_\"]\n\n    container = Container(\n        population=population, peripheral=peripheral, deep_copy=deep_copy, **subsets\n    )\n\n    container._id = container_id\n    container._frozen_time = frozen_time\n    container._split = split\n    container._last_change = last_change\n\n    return container\n</code></pre>"},{"location":"reference/data/#getml.data.concat.concat","title":"concat","text":"<pre><code>concat(\n    name: str, data_frames: List[Union[DataFrame, View]]\n)\n</code></pre> <p>Creates a new data frame by concatenating a list of existing ones.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the new column.</p> <p> TYPE: <code>str</code> </p> <code>data_frames</code> <p>The data frames to concatenate. Must be non-empty. However, it can contain only one data frame. Column names and roles must match. Columns will be appended by name, not order.</p> <p> TYPE: <code>List[Union[DataFrame, View]]</code> </p> <p>Examples:</p> <pre><code>new_df = data.concat(\"NEW_DF_NAME\", [df1, df2])\n</code></pre> Source code in <code>getml/data/concat.py</code> <pre><code>def concat(name: str, data_frames: List[Union[DataFrame, View]]):\n    \"\"\"\n    Creates a new data frame by concatenating a list of existing ones.\n\n    Args:\n        name:\n            Name of the new column.\n\n        data_frames:\n            The data frames to concatenate.\n            Must be non-empty. However, it can contain only one data frame.\n            Column names and roles must match.\n            Columns will be appended by name, not order.\n\n    Examples:\n        ```python\n        new_df = data.concat(\"NEW_DF_NAME\", [df1, df2])\n        ```\n    \"\"\"\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be a string.\")\n\n    if not _is_non_empty_typed_list(data_frames, (View, DataFrame)):\n        raise TypeError(\n            \"'data_frames' must be a non-empty list of getml.data.Views \"\n            + \"or getml.DataFrames.\"\n        )\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.concat\"\n    cmd[\"name_\"] = name\n\n    cmd[\"data_frames_\"] = [df._getml_deserialize() for df in data_frames]\n\n    comm.send(cmd)\n\n    return DataFrame(name=name).refresh()\n</code></pre>"},{"location":"reference/data/#getml.data.random.random","title":"random","text":"<pre><code>random(seed: int = 5849) -&gt; FloatColumnView\n</code></pre> <p>Create random column.</p> <p>The numbers will be uniformly distributed from 0.0 to 1.0. This can be used to randomly split a population table into a training and a test set</p> PARAMETER DESCRIPTION <code>seed</code> <p>Seed used for the random number generator.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5849</code> </p> RETURNS DESCRIPTION <code>FloatColumnView</code> <p>FloatColumn containing random numbers</p> Example <pre><code>population = getml.DataFrame('population')\npopulation.add(numpy.zeros(100), 'column_01')\n\nidx = random(seed=42)\npopulation_train = population[idx &gt; 0.7]\npopulation_test = population[idx &lt;= 0.7]\n</code></pre> Source code in <code>getml/data/columns/random.py</code> <pre><code>def random(seed: int = 5849) -&gt; FloatColumnView:\n    \"\"\"\n    Create random column.\n\n    The numbers will be uniformly distributed from 0.0 to 1.0. This can be\n    used to randomly split a population table into a training and a test\n    set\n\n    Args:\n        seed:\n            Seed used for the random number generator.\n\n    Returns:\n            FloatColumn containing random numbers\n\n    ??? example\n        ```python\n        population = getml.DataFrame('population')\n        population.add(numpy.zeros(100), 'column_01')\n\n        idx = random(seed=42)\n        population_train = population[idx &gt; 0.7]\n        population_test = population[idx &lt;= 0.7]\n        ```\n    \"\"\"\n\n    if not isinstance(seed, numbers.Real):\n        raise TypeError(\"'seed' must be a real number\")\n\n    col = FloatColumnView(operator=\"random\", operand1=None, operand2=None)\n    col.cmd[\"seed_\"] = seed\n    return col\n</code></pre>"},{"location":"reference/data/#getml.data.OnType","title":"OnType  <code>module-attribute</code>","text":"<pre><code>OnType = Optional[\n    Union[\n        str,\n        Tuple[str, str],\n        List[Union[str, Tuple[str, str]]],\n    ]\n]\n</code></pre> <p>Types that can be passed to the 'on' argument of the 'join' method.</p>"},{"location":"reference/data/#getml.data.TimeStampsType","title":"TimeStampsType  <code>module-attribute</code>","text":"<pre><code>TimeStampsType = Optional[Union[str, Tuple[str, str]]]\n</code></pre> <p>Types of time stamps used in joins.</p>"},{"location":"reference/data/access/","title":"access","text":""},{"location":"reference/data/access/#getml.data.access","title":"getml.data.access","text":"<p>Manages the access to various data sources.</p>"},{"location":"reference/data/access/#getml.data.access.set_s3_access_key_id","title":"set_s3_access_key_id","text":"<pre><code>set_s3_access_key_id(value: str)\n</code></pre> <p>Sets the Access Key ID to S3.</p> Notes <p>Note that S3 is not supported on Windows.</p> <p>In order to retrieve data from S3, you need to set the Access Key ID and the Secret Access Key. You can either set them as environment variables before you start the getML Engine, or you can set them from this module.</p> PARAMETER DESCRIPTION <code>value</code> <p>The value to which you want to set the Access Key ID.</p> <p> TYPE: <code>str</code> </p> Source code in <code>getml/data/access.py</code> <pre><code>def set_s3_access_key_id(value: str):\n    \"\"\"Sets the Access Key ID to S3.\n\n    Notes:\n        Note that S3 is not supported on Windows.\n\n    In order to retrieve data from S3, you need to set the Access Key ID\n    and the Secret Access Key. You can either set them as environment\n    variables before you start the getML Engine, or you can set them from\n    this module.\n\n    Args:\n        value:\n            The value to which you want to set the Access Key ID.\n    \"\"\"\n\n    if not isinstance(value, str):\n        raise TypeError(\"'value' must be of type str\")\n\n    if not _is_alive():\n        raise ConnectionRefusedError(\n            \"\"\"\n        Cannot connect to getML Engine.\n        Make sure the Engine is running on port '\"\"\"\n            + str(comm.port)\n            + \"\"\"' and you are logged in.\n        See `help(getml.engine)`.\"\"\"\n        )\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"set_s3_access_key_id\"\n    cmd[\"name_\"] = \"\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        comm.send_string(sock, value)\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.handle_engine_exception(msg)\n</code></pre>"},{"location":"reference/data/access/#getml.data.access.set_s3_secret_access_key","title":"set_s3_secret_access_key","text":"<pre><code>set_s3_secret_access_key(value: str)\n</code></pre> <p>Sets the Secret Access Key to S3.</p> Notes <p>Note that S3 is not supported on Windows.</p> <p>In order to retrieve data from S3, you need to set the Access Key ID and the Secret Access Key. You can either set them as environment variables before you start the getML Engine, or you can set them from this module.</p> PARAMETER DESCRIPTION <code>value</code> <p>The value to which you want to set the Secret Access Key.</p> <p> TYPE: <code>str</code> </p> Source code in <code>getml/data/access.py</code> <pre><code>def set_s3_secret_access_key(value: str):\n    \"\"\"Sets the Secret Access Key to S3.\n\n    Notes:\n        Note that S3 is not supported on Windows.\n\n    In order to retrieve data from S3, you need to set the Access Key ID\n    and the Secret Access Key. You can either set them as environment\n    variables before you start the getML Engine, or you can set them from\n    this module.\n\n    Args:\n        value:\n            The value to which you want to set the Secret Access Key.\n    \"\"\"\n\n    if not isinstance(value, str):\n        raise TypeError(\"'value' must be of type str\")\n\n    if not _is_alive():\n        raise ConnectionRefusedError(\n            \"\"\"\n        Cannot connect to getML Engine.\n        Make sure the Engine is running on port '\"\"\"\n            + str(comm.port)\n            + \"\"\"' and you are logged in.\n        See `help(getml.engine)`.\"\"\"\n        )\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"set_s3_secret_access_key\"\n    cmd[\"name_\"] = \"\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        comm.send_string(sock, value)\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.handle_engine_exception(msg)\n</code></pre>"},{"location":"reference/data/columns/","title":"columns","text":""},{"location":"reference/data/columns/#getml.data.columns","title":"getml.data.columns","text":"<p>Handlers for 1-d arrays storing the data of an individual variable.</p> <p>Like the <code>DataFrame</code>, the <code>columns</code> do not contain any actual data themselves but are only handlers to objects within the getML Engine. These containers store data of a single variable in a one-dimensional array of a uniform type.</p> <p>Columns are immutable and lazily evaluated.</p> <ul> <li> <p>Immutable means that there are no in-place   operation on the columns. Any change to the column   will return a new, changed column.</p> </li> <li> <p>Lazy evaluation means that operations won't be   executed until results are required. This is reflected   in the column views: Column views do not exist   until they are required.</p> </li> </ul> Example <p>This is what some column operations might look like:</p> <pre><code>import numpy as np\n\nimport getml.data as data\nimport getml.engine as engine\nimport getml.data.roles as roles\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n# Create a data frame from a JSON string\n\njson_str = \"\"\"{\n    \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n    \"column_01\": [2.4, 3.0, 1.2, 1.4],\n    \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n    \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n}\"\"\"\n\nmy_df = data.DataFrame(\n    \"MY DF\",\n    roles={\n        \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n        \"unused_float\": [\"column_01\"]}\n).read_json(\n    json_str\n)\n\n# ----------------\n\ncol1 = my_df[\"column_01\"]\n\n# ----------------\n\n# col2 is a column view.\n# The operation is not executed yet.\ncol2 = 2.0 - col1\n\n# This is when '2.0 - col1' is actually\n# executed.\nmy_df[\"column_02\"] = col2\nmy_df.set_role(\"column_02\", roles.numerical)\n\n# If you want to update column_01,\n# you can't do that in-place.\n# You need to replace it with a new column\ncol1 = col1 + col2\nmy_df[\"column_01\"] = col1\nmy_df.set_role(\"column_01\", roles.numerical)\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.BooleanColumnView","title":"BooleanColumnView","text":"<pre><code>BooleanColumnView(\n    operator: str,\n    operand1: Optional[OperandType],\n    operand2: Optional[OperandType],\n)\n</code></pre> <p>               Bases: <code>_View</code></p> <p>Handle for a lazily evaluated boolean column view.</p> <p>Column views do not actually exist - they will be lazily evaluated when necessary.</p> <p>They can be used to take subselection of the data frame or to update other columns.</p> Example <pre><code>import numpy as np\n\nimport getml.data as data\nimport getml.engine as engine\nimport getml.data.roles as roles\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n# Create a data frame from a JSON string\n\njson_str = \"\"\"{\n    \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n    \"column_01\": [2.4, 3.0, 1.2, 1.4],\n    \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n    \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n}\"\"\"\n\nmy_df = data.DataFrame(\n    \"MY DF\",\n    roles={\n        \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n        \"unused_float\": [\"column_01\"]}\n).read_json(\n    json_str\n)\n\n# ----------------\n\nnames = my_df[\"names\"]\n\n# This is a virtual boolean column.\na_or_p_in_names = names.contains(\"p\") | names.contains(\"a\")\n\n# Creates a view containing\n# only those entries, where \"names\" contains a or p.\nmy_view = my_df[a_or_p_in_names]\n\n# ----------------\n\n# Returns a new column, where all names\n# containing \"rick\" are replaced by \"Patrick\".\n# Again, columns are immutable - this returns an updated\n# version, but leaves the original column unchanged.\nnew_names = names.update(names.contains(\"rick\"), \"Patrick\")\n\nmy_df[\"new_names\"] = new_names\n\n# ----------------\n\n# Boolean columns can also be used to\n# create binary target variables.\ntarget = (names == \"phil\")\n\nmy_df[\"target\"] = target\nmy_df.set_role(target, roles.target)\n\n# By the way, instead of using the\n# __setitem__ operator and .set_role(...)\n# you can just use .add(...).\nmy_df.add(target, \"target\", roles.target)\n</code></pre> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def __init__(\n    self,\n    operator: str,\n    operand1: Optional[OperandType],\n    operand2: Optional[OperandType],\n):\n    self.cmd: Dict[str, Any] = {}\n\n    self.cmd[\"type_\"] = BOOLEAN_COLUMN_VIEW\n\n    self.cmd[\"operator_\"] = operator\n\n    if operand1 is not None:\n        self.cmd[\"operand1_\"] = self._parse_operand(operand1)\n\n    if operand2 is not None:\n        self.cmd[\"operand2_\"] = self._parse_operand(operand2)\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.BooleanColumnView.is_false","title":"is_false","text":"<pre><code>is_false()\n</code></pre> <p>Whether an entry is False - effectively inverts the Boolean column.</p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def is_false(self):\n    \"\"\"Whether an entry is False - effectively inverts the Boolean column.\"\"\"\n    return BooleanColumnView(\n        operator=\"not\",\n        operand1=self,\n        operand2=None,\n    )\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.BooleanColumnView.as_num","title":"as_num","text":"<pre><code>as_num()\n</code></pre> <p>Transforms the boolean column into a numerical column</p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def as_num(self):\n    \"\"\"Transforms the boolean column into a numerical column\"\"\"\n    return FloatColumnView(\n        operator=\"boolean_as_num\",\n        operand1=self,\n        operand2=None,\n    )\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.FloatColumn","title":"FloatColumn","text":"<pre><code>FloatColumn(\n    name: str = \"\",\n    role: str = \"numerical\",\n    df_name: str = \"\",\n)\n</code></pre> <p>               Bases: <code>_Column</code></p> <p>Handle for numerical data in the Engine.</p> <p>This is a handler for all numerical data in the getML Engine, including time stamps.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>Name of the categorical column.</p> <p> </p> <code>role</code> <p>Role that the column plays.</p> <p> </p> <code>df_name</code> <p><code>name</code> instance variable of the <code>DataFrame</code>  containing this column.</p> <p> </p> Example <pre><code>import numpy as np\n\nimport getml.data as data\nimport getml.engine as engine\nimport getml.data.roles as roles\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n# Create a data frame from a JSON string\n\njson_str = \"\"\"{\n    \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n    \"column_01\": [2.4, 3.0, 1.2, 1.4],\n    \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n    \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n}\"\"\"\n\nmy_df = data.DataFrame(\n    \"MY DF\",\n    roles={\n        \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n        \"unused_float\": [\"column_01\"]}\n).read_json(\n    json_str\n)\n\n# ----------------\n\ncol1 = my_df[\"column_01\"]\n\n# ----------------\n\ncol2 = 2.0 - col1\n\nmy_df.add(col2, \"name\", roles.numerical)\n\n# ----------------\n# If you do not explicitly set a role,\n# the assigned role will either be\n# roles.unused_float.\n\ncol3 = (col1 + 2.0*col2) / 3.0\n\nmy_df[\"column_03\"] = col3\nmy_df.set_role(\"column_03\", roles.numerical)\n</code></pre> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def __init__(self, name: str = \"\", role: str = \"numerical\", df_name: str = \"\"):\n    super().__init__()\n\n    FloatColumn._num_columns += 1\n    if name == \"\":\n        name = FLOAT_COLUMN + \" \" + str(FloatColumn._num_columns)\n\n    self.cmd: Dict[str, Any] = {}\n\n    self.cmd[\"operator_\"] = FLOAT_COLUMN\n\n    self.cmd[\"df_name_\"] = df_name\n\n    self.cmd[\"name_\"] = name\n\n    self.cmd[\"role_\"] = role\n\n    self.cmd[\"type_\"] = FLOAT_COLUMN\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.FloatColumnView","title":"FloatColumnView","text":"<pre><code>FloatColumnView(\n    operator: str,\n    operand1: Optional[FloatOperandType],\n    operand2: Optional[FloatOperandType],\n)\n</code></pre> <p>               Bases: <code>_View</code></p> <p>Lazily evaluated view on a <code>FloatColumn</code>.</p> <p>Column views do not actually exist - they will be lazily evaluated when necessary.</p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def __init__(\n    self,\n    operator: str,\n    operand1: Optional[FloatOperandType],\n    operand2: Optional[FloatOperandType],\n):\n    self.cmd: Dict[str, Any] = {}\n\n    self.cmd[\"type_\"] = FLOAT_COLUMN_VIEW\n\n    self.cmd[\"operator_\"] = operator\n\n    if operand1 is not None:\n        self.cmd[\"operand1_\"] = self._parse_operand(operand1)\n\n    if operand2 is not None:\n        self.cmd[\"operand2_\"] = self._parse_operand(operand2)\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.StringColumn","title":"StringColumn","text":"<pre><code>StringColumn(\n    name: str = \"\",\n    role: str = \"categorical\",\n    df_name: str = \"\",\n)\n</code></pre> <p>               Bases: <code>_Column</code></p> <p>Handle for categorical data that is kept in the getML Engine</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>Name of the categorical column.</p> <p> </p> <code>role</code> <p>Role that the column plays.</p> <p> </p> <code>df_name</code> <p><code>name</code> instance variable of the <code>DataFrame</code> containing this column.</p> <p> </p> Example <pre><code>import numpy as np\n\nimport getml.data as data\nimport getml.engine as engine\nimport getml.data.roles as roles\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n# Create a data frame from a JSON string\n\njson_str = \"\"\"{\n    \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n    \"column_01\": [2.4, 3.0, 1.2, 1.4],\n    \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n    \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n}\"\"\"\n\nmy_df = data.DataFrame(\n    \"MY DF\",\n    roles={\n        \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n        \"unused_float\": [\"column_01\"]}\n).read_json(\n    json_str\n)\n\n# ----------------\n\ncol1 = my_df[\"names\"]\n\n# ----------------\n\ncol2 = col1.substr(4, 3)\n\nmy_df.add(col2, \"short_names\", roles.categorical)\n\n# ----------------\n# If you do not explicitly set a role,\n# the assigned role will either be\n# roles.unused_string.\n\ncol3 = \"user-\" + col1 + \"-\" + col2\n\nmy_df[\"new_names\"] = col3\nmy_df.set_role(\"new_names\", roles.categorical)\n</code></pre> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def __init__(self, name: str = \"\", role: str = \"categorical\", df_name: str = \"\"):\n    super().__init__()\n\n    StringColumn._num_columns += 1\n    if name == \"\":\n        name = STRING_COLUMN + \" \" + str(StringColumn._num_columns)\n\n    self.cmd: Dict[str, Any] = {}\n\n    self.cmd[\"operator_\"] = STRING_COLUMN\n    self.cmd[\"df_name_\"] = df_name\n    self.cmd[\"name_\"] = name\n    self.cmd[\"role_\"] = role\n    self.cmd[\"type_\"] = STRING_COLUMN\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.StringColumnView","title":"StringColumnView","text":"<pre><code>StringColumnView(\n    operator: str,\n    operand1: Optional[Union[str, _Column, _View]],\n    operand2: Optional[Union[str, _Column, _View]],\n)\n</code></pre> <p>               Bases: <code>_View</code></p> <p>Lazily evaluated view on a <code>StringColumn</code>.</p> <p>Columns views do not actually exist - they will be lazily evaluated when necessary.</p> Example <pre><code>import numpy as np\n\nimport getml.data as data\nimport getml.engine as engine\nimport getml.data.roles as roles\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n# Create a data frame from a JSON string\n\njson_str = \"\"\"{\n    \"names\": [\"patrick\", \"alex\", \"phil\", \"ulrike\"],\n    \"column_01\": [2.4, 3.0, 1.2, 1.4],\n    \"join_key\": [\"0\", \"1\", \"2\", \"3\"],\n    \"time_stamp\": [\"2019-01-01\", \"2019-01-02\", \"2019-01-03\", \"2019-01-04\"]\n}\"\"\"\n\nmy_df = data.DataFrame(\n    \"MY DF\",\n    roles={\n        \"unused_string\": [\"names\", \"join_key\", \"time_stamp\"],\n        \"unused_float\": [\"column_01\"]}\n).read_json(\n    json_str\n)\n\n# ----------------\n\ncol1 = my_df[\"names\"]\n\n# ----------------\n\n# col2 is a virtual column.\n# The substring operation is not\n# executed yet.\ncol2 = col1.substr(4, 3)\n\n# This is where the Engine executes\n# the substring operation.\nmy_df.add(col2, \"short_names\", roles.categorical)\n\n# ----------------\n# If you do not explicitly set a role,\n# the assigned role will either be\n# roles.unused_string.\n\n# col3 is a virtual column.\n# The operation is not\n# executed yet.\ncol3 = \"user-\" + col1 + \"-\" + col2\n\n# This is where the operation is\n# is executed.\nmy_df[\"new_names\"] = col3\nmy_df.set_role(\"new_names\", roles.categorical)\n</code></pre> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def __init__(\n    self,\n    operator: str,\n    operand1: Optional[Union[str, _Column, _View]],\n    operand2: Optional[Union[str, _Column, _View]],\n):\n    self.cmd: Dict[str, Any] = {}\n\n    self.cmd[\"type_\"] = STRING_COLUMN_VIEW\n    self.cmd[\"operator_\"] = operator\n    if operand1 is not None:\n        self.cmd[\"operand1_\"] = self._parse_operand(operand1)\n    if operand2 is not None:\n        self.cmd[\"operand2_\"] = self._parse_operand(operand2)\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.arange","title":"arange","text":"<pre><code>arange(\n    start: Union[Real, float] = 0.0,\n    stop: Optional[Union[Real, float]] = None,\n    step: Union[Real, float] = 1.0,\n)\n</code></pre> <p>Returns evenly spaced variables, within a given interval.</p> PARAMETER DESCRIPTION <code>start</code> <p>The beginning of the interval. Defaults to 0.</p> <p> TYPE: <code>Union[Real, float]</code> DEFAULT: <code>0.0</code> </p> <code>stop</code> <p>The end of the interval.</p> <p> TYPE: <code>Optional[Union[Real, float]]</code> DEFAULT: <code>None</code> </p> <code>step</code> <p>The step taken. Defaults to 1.</p> <p> TYPE: <code>Union[Real, float]</code> DEFAULT: <code>1.0</code> </p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def arange(\n    start: Union[numbers.Real, float] = 0.0,\n    stop: Optional[Union[numbers.Real, float]] = None,\n    step: Union[numbers.Real, float] = 1.0,\n):\n    \"\"\"\n    Returns evenly spaced variables, within a given interval.\n\n    Args:\n        start:\n            The beginning of the interval. Defaults to 0.\n\n        stop:\n            The end of the interval.\n\n        step:\n            The step taken. Defaults to 1.\n    \"\"\"\n    if stop is None:\n        stop = start\n        start = 0.0\n\n    if step is None:\n        step = 1.0\n\n    if not isinstance(start, numbers.Real):\n        raise TypeError(\"'start' must be a real number\")\n\n    if not isinstance(stop, numbers.Real):\n        raise TypeError(\"'stop' must be a real number\")\n\n    if not isinstance(step, numbers.Real):\n        raise TypeError(\"'step' must be a real number\")\n\n    col = FloatColumnView(\n        operator=\"arange\",\n        operand1=None,\n        operand2=None,\n    )\n\n    col.cmd[\"start_\"] = float(start)\n    col.cmd[\"stop_\"] = float(stop)\n    col.cmd[\"step_\"] = float(step)\n\n    return col\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.rowid","title":"rowid","text":"<pre><code>rowid() -&gt; FloatColumnView\n</code></pre> <p>Get the row numbers of the table.</p> RETURNS DESCRIPTION <code>FloatColumnView</code> <p>(numerical) column containing the row id, starting with 0</p> Source code in <code>getml/data/columns/columns.py</code> <pre><code>def rowid() -&gt; FloatColumnView:\n    \"\"\"\n    Get the row numbers of the table.\n\n    Returns:\n            (numerical) column containing the row id, starting with 0\n    \"\"\"\n    return FloatColumnView(operator=\"rowid\", operand1=None, operand2=None)\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.collect_footer_data","title":"collect_footer_data","text":"<p>Collects the data necessary for displaying the column footer.</p>"},{"location":"reference/data/columns/#getml.data.columns.collect_footer_data.Footer","title":"Footer","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Contains the data to be shown in the footer of the data frame or column.</p>"},{"location":"reference/data/columns/#getml.data.columns.aggregation","title":"aggregation","text":"<p>Lazily evaluated aggregation over a column.</p>"},{"location":"reference/data/columns/#getml.data.columns.aggregation.Aggregation","title":"Aggregation","text":"<pre><code>Aggregation(alias, col, agg_type)\n</code></pre> <p>Lazily evaluated aggregation over a column.</p> Example <pre><code>my_data_frame[\"my_column\"].avg()\n3.0\n</code></pre> Source code in <code>getml/data/columns/aggregation.py</code> <pre><code>def __init__(self, alias, col, agg_type):\n    self.cmd: Dict[str, Any] = {}\n    self.cmd[\"as_\"] = alias\n    self.cmd[\"col_\"] = col.cmd\n    self.cmd[\"type_\"] = agg_type\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.aggregation.Aggregation.get","title":"get","text":"<pre><code>get()\n</code></pre> <p>Receives the value of the aggregation over the column.</p> Source code in <code>getml/data/columns/aggregation.py</code> <pre><code>def get(self):\n    \"\"\"\n    Receives the value of the aggregation over the column.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"FloatColumn.aggregate\"\n\n    cmd[\"aggregation_\"] = self.cmd\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.handle_engine_exception(msg)\n        mat = comm.recv_float_matrix(sock)\n\n    return mat.ravel()[0]\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.format","title":"format","text":"<p>Format the column</p>"},{"location":"reference/data/columns/#getml.data.columns.last_change","title":"last_change","text":"<p>Returns the last time a data frame has been changed.</p>"},{"location":"reference/data/columns/#getml.data.columns.last_change_from_col","title":"last_change_from_col","text":"<p>The last time any of the underlying data frames has been changed.</p>"},{"location":"reference/data/columns/#getml.data.columns.length","title":"length","text":"<p>Returns the length of the column</p>"},{"location":"reference/data/columns/#getml.data.columns.length_property","title":"length_property","text":"<p>The length of the column (number of rows in the data frame).</p>"},{"location":"reference/data/columns/#getml.data.columns.make_iter","title":"make_iter","text":"<p>Factory function for a function that can be used to iterate through a column.</p>"},{"location":"reference/data/columns/#getml.data.columns.parse","title":"parse","text":"<p>Parses the columns from a cmd</p>"},{"location":"reference/data/columns/#getml.data.columns.repr","title":"repr","text":"<p>ASCII representation of the column.</p>"},{"location":"reference/data/columns/#getml.data.columns.repr_html","title":"repr_html","text":"<p>HTML representation of the column.</p>"},{"location":"reference/data/columns/#getml.data.columns.subroles","title":"subroles","text":"<p>The subroles of this column.</p>"},{"location":"reference/data/columns/#getml.data.columns.to_arrow","title":"to_arrow","text":"<p>Transform column to a pyarrow.ChunkedArray</p>"},{"location":"reference/data/columns/#getml.data.columns.to_numpy","title":"to_numpy","text":"<p>Transform column to a numpy array.</p>"},{"location":"reference/data/columns/#getml.data.columns.unique","title":"unique","text":"<p>Transform column to numpy array containing unique values</p>"},{"location":"reference/data/columns/#getml.data.columns.unit","title":"unit","text":"<p>The unit of this column.</p>"},{"location":"reference/data/columns/#getml.data.columns.from_value.from_value","title":"from_value","text":"<pre><code>from_value(\n    val: Union[bool, str, int, float, datetime64]\n) -&gt; ReturnType\n</code></pre> <p>Creates an infinite column that contains the same value in all of its elements.</p> PARAMETER DESCRIPTION <code>val</code> <p>The value you want to insert into your column.</p> <p> TYPE: <code>Union[bool, str, int, float, datetime64]</code> </p> RETURNS DESCRIPTION <code>ReturnType</code> <p>The column view containing the value.</p> Source code in <code>getml/data/columns/from_value.py</code> <pre><code>def from_value(val: Union[bool, str, int, float, np.datetime64]) -&gt; ReturnType:\n    \"\"\"\n    Creates an infinite column that contains the same\n    value in all of its elements.\n\n    Args:\n        val:\n            The value you want to insert into your column.\n\n    Returns:\n        The column view containing the value.\n    \"\"\"\n    cmd = _value_to_cmd(val)\n\n    if isinstance(val, bool):\n        col: ReturnType = BooleanColumnView(\n            operator=\"const\",\n            operand1=None,\n            operand2=None,\n        )\n        col.cmd = cmd\n        return col\n\n    if isinstance(val, str):\n        col = StringColumnView(\n            operator=\"const\",\n            operand1=val,\n            operand2=None,\n        )\n        col.cmd = cmd\n        return col\n\n    if isinstance(val, (int, float, numbers.Number)):\n        col = FloatColumnView(\n            operator=\"const\",\n            operand1=val,\n            operand2=None,\n        )\n        col.cmd = cmd\n        return col\n\n    if isinstance(val, np.datetime64):\n        col = FloatColumnView(\n            operator=\"const\",\n            operand1=np.datetime64(val, \"s\").astype(float),\n            operand2=None,\n        )\n        col.cmd = cmd\n        return col\n\n    raise TypeError(\"val must be bool, str or a number.\")\n</code></pre>"},{"location":"reference/data/columns/#getml.data.columns.random.random","title":"random","text":"<pre><code>random(seed: int = 5849) -&gt; FloatColumnView\n</code></pre> <p>Create random column.</p> <p>The numbers will be uniformly distributed from 0.0 to 1.0. This can be used to randomly split a population table into a training and a test set</p> PARAMETER DESCRIPTION <code>seed</code> <p>Seed used for the random number generator.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5849</code> </p> RETURNS DESCRIPTION <code>FloatColumnView</code> <p>FloatColumn containing random numbers</p> Example <pre><code>population = getml.DataFrame('population')\npopulation.add(numpy.zeros(100), 'column_01')\n\nidx = random(seed=42)\npopulation_train = population[idx &gt; 0.7]\npopulation_test = population[idx &lt;= 0.7]\n</code></pre> Source code in <code>getml/data/columns/random.py</code> <pre><code>def random(seed: int = 5849) -&gt; FloatColumnView:\n    \"\"\"\n    Create random column.\n\n    The numbers will be uniformly distributed from 0.0 to 1.0. This can be\n    used to randomly split a population table into a training and a test\n    set\n\n    Args:\n        seed:\n            Seed used for the random number generator.\n\n    Returns:\n            FloatColumn containing random numbers\n\n    ??? example\n        ```python\n        population = getml.DataFrame('population')\n        population.add(numpy.zeros(100), 'column_01')\n\n        idx = random(seed=42)\n        population_train = population[idx &gt; 0.7]\n        population_test = population[idx &lt;= 0.7]\n        ```\n    \"\"\"\n\n    if not isinstance(seed, numbers.Real):\n        raise TypeError(\"'seed' must be a real number\")\n\n    col = FloatColumnView(operator=\"random\", operand1=None, operand2=None)\n    col.cmd[\"seed_\"] = seed\n    return col\n</code></pre>"},{"location":"reference/data/container/","title":"Container","text":""},{"location":"reference/data/container/#getml.data.Container","title":"getml.data.Container","text":"<pre><code>Container(\n    population: Optional[Union[DataFrame, View]] = None,\n    peripheral: Optional[\n        Dict[str, Union[DataFrame, View]]\n    ] = None,\n    split: Optional[\n        Union[StringColumn, StringColumnView]\n    ] = None,\n    deep_copy: Optional[bool] = False,\n    train: Optional[Union[DataFrame, View]] = None,\n    validation: Optional[Union[DataFrame, View]] = None,\n    test: Optional[Union[DataFrame, View]] = None,\n    **kwargs: Optional[Union[DataFrame, View]]\n)\n</code></pre> <p>A container holds the actual data in the form of a <code>DataFrame</code> or a <code>View</code>.</p> <p>The purpose of a container is twofold:</p> <ul> <li> <p>Assigning concrete data to an abstract <code>DataModel</code>.</p> </li> <li> <p>Storing data and allowing you to reproduce previous results.</p> </li> </ul> ATTRIBUTE DESCRIPTION <code>population</code> <p>The population table defines the statistical population  of the machine learning problem and contains the target variables.</p> <p> </p> <code>peripheral</code> <p>The peripheral tables are joined onto population or other peripheral tables. Note that you can also pass them using <code>add</code>.</p> <p> </p> <code>split</code> <p>Contains information on how you want to split population into different <code>Subset</code>s. Also refer to <code>split</code>.</p> <p> </p> <code>deep_copy</code> <p>Whether you want to create deep copies or your tables.</p> <p> </p> <code>train</code> <p>The population table used in the train <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <p> </p> <code>validation</code> <p>The population table used in the validation <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <p> </p> <code>test</code> <p>The population table used in the test <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <p> </p> <code>kwargs</code> <p>The population table used in <code>Subset</code>s other than the predefined train, validation and test subsets. You can call these subsets anything you want to, and you can access them just like train, validation and test. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> Example <pre><code># Pass the subset.\ncontainer = getml.data.Container(my_subset=my_data_frame)\n\n# You can access the subset just like train,\n# validation or test\nmy_pipeline.fit(container.my_subset)\n</code></pre> <p> </p> Example <p>A <code>DataModel</code> only contains abstract data. When we fit a pipeline, we need to assign concrete data.</p> <p>This example is taken from the loans notebook . Note that in the notebook the high level <code>StarSchema</code> implementation is used. For demonstration purposes we are proceeding now with the low level implementation.</p> <p><pre><code># The abstract data model is constructed\n# using the DataModel class. A data model\n# does not contain any actual data. It just\n# defines the abstract relational structure.\ndm = getml.data.DataModel(\n    population_train.to_placeholder(\"population\")\n)\n\ndm.add(getml.data.to_placeholder(\n    meta=meta,\n    order=order,\n    trans=trans)\n)\n\ndm.population.join(\n    dm.trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\ndm.population.join(\n    dm.order,\n    on=\"account_id\",\n)\n\ndm.population.join(\n    dm.meta,\n    on=\"account_id\",\n)\n\n# We now have abstract placeholders on something\n# called \"population\", \"meta\", \"order\" and \"trans\".\n# But how do we assign concrete data? By using\n# a container.\ncontainer = getml.data.Container(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views. Their aliases need\n# to match the names of the placeholders in the\n# data model.\ncontainer.add(\n    meta=meta,\n    order=order,\n    trans=trans\n)\n\n# Freezing makes the container immutable.\n# This is not required, but often a good idea.\ncontainer.freeze()\n\n# When we call 'train', the container\n# will return the train set and the\n# peripheral tables.\nmy_pipeline.fit(container.train)\n\n# Same for 'test'\nmy_pipeline.score(container.test)\n</code></pre> If you don't already have a train and test set, you can use a function from the <code>split</code> module.</p> <pre><code>split = getml.data.split.random(\n    train=0.8, test=0.2)\n\ncontainer = getml.data.Container(\n    population=population_all,\n    split=split,\n)\n\n# The remaining code is the same as in\n# the example above. In particular,\n# container.train and container.test\n# work just like above.\n</code></pre> <p>Containers can also be used for storage and reproducing your results. A recommended pattern is to assign 'baseline roles' to your data frames and then using a <code>View</code> to tweak them:</p> <pre><code># Assign baseline roles\ndata_frame.set_role([\"jk\"], getml.data.roles.join_key)\ndata_frame.set_role([\"col1\", \"col2\"], getml.data.roles.categorical)\ndata_frame.set_role([\"col3\", \"col4\"], getml.data.roles.numerical)\ndata_frame.set_role([\"col5\"], getml.data.roles.target)\n\n# Make the data frame immutable, so in-place operations are\n# no longer possible.\ndata_frame.freeze()\n\n# Save the data frame.\ndata_frame.save()\n\n# I suspect that col1 leads to overfitting, so I will drop it.\nview = data_frame.drop([\"col1\"])\n\n# Insert the view into a container.\ncontainer = getml.data.Container(...)\ncontainer.add(some_alias=view)\ncontainer.save()\n</code></pre> <p>The advantage of using such a pattern is that it enables you to always completely retrace your entire pipeline without creating deep copies of the data frames whenever you have made a small change like the one in our example. Note that the pipeline will record which container you have used.</p> Source code in <code>getml/data/container.py</code> <pre><code>def __init__(\n    self,\n    population: Optional[Union[DataFrame, View]] = None,\n    peripheral: Optional[Dict[str, Union[DataFrame, View]]] = None,\n    split: Optional[Union[StringColumn, StringColumnView]] = None,\n    deep_copy: Optional[bool] = False,\n    train: Optional[Union[DataFrame, View]] = None,\n    validation: Optional[Union[DataFrame, View]] = None,\n    test: Optional[Union[DataFrame, View]] = None,\n    **kwargs: Optional[Union[DataFrame, View]],\n):\n    if population is not None and not isinstance(population, (DataFrame, View)):\n        raise TypeError(\n            \"'population' must be a getml.DataFrame or a getml.data.View, got \"\n            + type(population).__name__\n            + \".\"\n        )\n\n    if peripheral is not None and not _is_typed_dict(\n        peripheral, str, [DataFrame, View]\n    ):\n        raise TypeError(\n            \"'peripheral' must be a dict \"\n            + \"of getml.DataFrames or getml.data.Views.\"\n        )\n\n    if split is not None and not isinstance(\n        split, (StringColumn, StringColumnView)\n    ):\n        raise TypeError(\n            \"'split' must be StringColumn or a StringColumnView, got \"\n            + type(split).__name__\n            + \".\"\n        )\n\n    if not isinstance(deep_copy, bool):\n        raise TypeError(\n            \"'deep_copy' must be a bool, got \" + type(split).__name__ + \".\"\n        )\n\n    exclusive = (population is not None) ^ (\n        len(_make_subsets_from_kwargs(train, validation, test, **kwargs)) != 0\n    )\n\n    if not exclusive:\n        raise ValueError(\n            \"'population' and 'train', 'validation', 'test' as well as \"\n            + \"other subsets signified by kwargs are mutually exclusive. \"\n            + \"You have to pass \"\n            + \"either 'population' or some subsets, but you cannot pass both.\"\n        )\n\n    if population is None and split is not None:\n        raise ValueError(\n            \"'split's are used for splitting population DataFrames.\"\n            \"Hence, if you supply 'split', you also have to supply \"\n            \"a population.\"\n        )\n\n    if population is not None and split is None:\n        logger.warning(\n            \"You have passed a population table without passing 'split'. \"\n            \"You can access the entire set to pass to your pipeline \"\n            \"using the .full attribute.\"\n        )\n        split = from_value(\"full\")\n\n    self._id = _make_id()\n\n    self._population = population\n    self._peripheral = peripheral or {}\n    self._split = split\n    self._deep_copy = deep_copy\n\n    self._subsets = (\n        _make_subsets_from_split(population, split)\n        if split is not None\n        else _make_subsets_from_kwargs(train, validation, test, **kwargs)\n    )\n\n    if split is None and not _is_typed_dict(self._subsets, str, [DataFrame, View]):\n        raise TypeError(\n            \"'train', 'validation', 'test' and all other subsets must be either a \"\n            \"getml.DataFrame or a getml.data.View.\"\n        )\n\n    if deep_copy:\n        self._population = _deep_copy(self._population, self._id)\n        self._peripheral = {\n            k: _deep_copy(v, self._id) for (k, v) in self._peripheral.items()\n        }\n        self._subsets = {\n            k: _deep_copy(v, self._id) for (k, v) in self._subsets.items()\n        }\n\n    self._last_change = _get_last_change(\n        self._population, self._peripheral, self._subsets\n    )\n\n    self._frozen_time = None\n</code></pre>"},{"location":"reference/data/container/#getml.data.Container.add","title":"add","text":"<pre><code>add(*args, **kwargs)\n</code></pre> <p>Adds new peripheral data frames or views.</p> Source code in <code>getml/data/container.py</code> <pre><code>def add(self, *args, **kwargs):\n    \"\"\"\n    Adds new peripheral data frames or views.\n    \"\"\"\n    wrong_type = [item for item in args if not isinstance(item, (DataFrame, View))]\n\n    if wrong_type:\n        raise TypeError(\n            \"All unnamed arguments must be getml.DataFrames or getml.data.Views.\"\n        )\n\n    wrong_type = [\n        k for (k, v) in kwargs.items() if not isinstance(v, (DataFrame, View))\n    ]\n\n    if wrong_type:\n        raise TypeError(\n            \"You must pass getml.DataFrames or getml.data.Views, \"\n            f\"but the following arguments were neither: {wrong_type!r}.\"\n        )\n\n    kwargs = {**{item.name: item for item in args}, **kwargs}\n\n    if self._frozen_time is not None:\n        raise ValueError(\n            f\"You cannot add data frames after the {type(self).__name__} has been frozen.\"\n        )\n\n    if self._deep_copy:\n        kwargs = {k: _deep_copy(v, self._id) for (k, v) in kwargs.items()}\n\n    self._peripheral = {**self._peripheral, **kwargs}\n\n    self._last_change = _get_last_change(\n        self._population, self._peripheral, self._subsets\n    )\n</code></pre>"},{"location":"reference/data/container/#getml.data.Container.freeze","title":"freeze","text":"<pre><code>freeze()\n</code></pre> <p>Freezes the container, so that changes are no longer possible.</p> <p>This is required before you can extract data when <code>deep_copy=True</code>. The idea of <code>deep_copy</code> is to ensure that you can always retrace and reproduce your results. That is why the container needs to be immutable before it can be used.</p> Source code in <code>getml/data/container.py</code> <pre><code>def freeze(self):\n    \"\"\"\n    Freezes the container, so that changes are no longer possible.\n\n    This is required before you can extract data when `deep_copy=True`. The idea of\n    `deep_copy` is to ensure that you can always retrace and reproduce your results.\n    That is why the container needs to be immutable before it can be\n    used.\n    \"\"\"\n    self.sync()\n    self._frozen_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"reference/data/container/#getml.data.Container.save","title":"save","text":"<pre><code>save()\n</code></pre> <p>Saves the Container to disk.</p> Source code in <code>getml/data/container.py</code> <pre><code>def save(self):\n    \"\"\"\n    Saves the Container to disk.\n    \"\"\"\n\n    cmd = dict()\n    cmd[\"type_\"] = \"DataContainer.save\"\n    cmd[\"name_\"] = self._id\n\n    cmd[\"container_\"] = self._getml_deserialize()\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/container/#getml.data.Container.sync","title":"sync","text":"<pre><code>sync()\n</code></pre> <p>Synchronizes the last change with the data to avoid warnings that the data has been changed.</p> <p>This is only a problem when <code>deep_copy=False</code>.</p> Source code in <code>getml/data/container.py</code> <pre><code>def sync(self):\n    \"\"\"\n    Synchronizes the last change with the data to avoid warnings that the data\n    has been changed.\n\n    This is only a problem when `deep_copy=False`.\n    \"\"\"\n    if self._frozen_time is not None:\n        raise ValueError(f\"{type(self).__name__} has already been frozen.\")\n    self._last_change = _get_last_change(\n        self._population, self._peripheral, self._subsets\n    )\n</code></pre>"},{"location":"reference/data/container/#getml.data.Container.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas() -&gt; Dict[str, DataFrame]\n</code></pre> <p>Returns a <code>Container</code>'s contents as a dictionary of <code>pandas.DataFrame</code>s. <code>key</code> holds the data frame's <code>name</code>, value the data converted to a <code>pandas.DataFrame</code>.</p> Source code in <code>getml/data/container.py</code> <pre><code>def to_pandas(self) -&gt; Dict[str, pd.DataFrame]:\n    \"\"\"\n    Returns a `Container`'s contents as a dictionary of `pandas.DataFrame`s.\n    `key` holds the data frame's `name`, value the data converted to a `pandas.DataFrame`.\n    \"\"\"\n    subsets = (\n        {name: df.to_pandas() for name, df in self._subsets.items()}\n        if self._subsets\n        else {}\n    )\n    peripherals = (\n        {name: df.to_pandas() for name, df in self.peripheral.items()}\n        if self.peripheral\n        else {}\n    )\n    if subsets or peripherals:\n        return {**subsets, **peripherals}\n\n    raise ValueError(\"Container is empty.\")\n</code></pre>"},{"location":"reference/data/data_frame/","title":"DataFrame","text":""},{"location":"reference/data/data_frame/#getml.data.DataFrame","title":"getml.data.DataFrame","text":"<pre><code>DataFrame(\n    name: str,\n    roles: Optional[\n        Union[Dict[Union[Role, str], Iterable[str]], Roles]\n    ] = None,\n)\n</code></pre> <p>Handler for the data stored in the getML Engine.</p> <p>The <code>DataFrame</code> class represents a data frame object in the getML Engine but does not contain any actual data itself. To create such a data frame object, fill it with data via the Python API, and to retrieve a handler for it, you can use one of the <code>from_csv</code>, <code>from_db</code>, <code>from_json</code>, or <code>from_pandas</code> class methods. The Importing Data section in the user guide explains the particularities of each of those flavors of the unified import interface.</p> <p>If the data frame object is already present in the Engine - either in memory as a temporary object or on disk when <code>save</code> was called earlier -, the <code>load_data_frame</code> function will create a new handler without altering the underlying data. For more information about the lifecycle of the data in the getML Engine and its synchronization with the Python API please see the corresponding User Guide.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>Unique identifier used to link the handler with the underlying data frame object in the Engine.</p> <p> </p> <code>roles</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <p> </p> Example <p>Creating a new data frame object in the getML Engine and importing data is done by one the class functions <code>from_csv</code>, <code>from_db</code>, <code>from_json</code>, or <code>from_pandas</code>.</p> <p><pre><code>random = numpy.random.RandomState(7263)\n\ntable = pandas.DataFrame()\ntable['column_01'] = random.randint(0, 10, 1000).astype(numpy.str)\ntable['join_key'] = numpy.arange(1000)\ntable['time_stamp'] = random.rand(1000)\ntable['target'] = random.rand(1000)\n\ndf_table = getml.DataFrame.from_pandas(table, name = 'table')\n</code></pre> In addition to creating a new data frame object in the getML Engine and filling it with all the content of <code>table</code>, the <code>from_pandas</code> function also returns a <code>DataFrame</code> handler to the underlying data.</p> <p>You don't have to create the data frame objects anew for each session. You can use their <code>save</code> method to write them to disk, the <code>list_data_frames</code> function to list all available objects in the Engine, and <code>load_data_frame</code> to create a <code>DataFrame</code> handler for a data set already present in the getML Engine (see User Guide for details).</p> <pre><code>df_table.save()\n\ngetml.data.list_data_frames()\n\ndf_table_reloaded = getml.data.load_data_frame('table')\n</code></pre> Note <p>Although the Python API does not store the actual data itself, you can use the <code>to_csv</code>, <code>to_db</code>, <code>to_json</code>, and <code>to_pandas</code> methods to retrieve them.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    roles: Optional[Union[Dict[Union[Role, str], Iterable[str]], Roles]] = None,\n):\n    # ------------------------------------------------------------\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    vars(self)[\"name\"] = name\n\n    if roles is None:\n        roles = {}\n\n    if isinstance(roles, dict):\n        roles = Roles.from_dict(roles)\n\n    # ------------------------------------------------------------\n\n    vars(self)[\"_categorical_columns\"] = [\n        StringColumn(name=cname, role=roles_.categorical, df_name=self.name)\n        for cname in roles.categorical\n    ]\n\n    vars(self)[\"_join_key_columns\"] = [\n        StringColumn(name=cname, role=roles_.join_key, df_name=self.name)\n        for cname in roles.join_key\n    ]\n\n    vars(self)[\"_numerical_columns\"] = [\n        FloatColumn(name=cname, role=roles_.numerical, df_name=self.name)\n        for cname in roles.numerical\n    ]\n\n    vars(self)[\"_target_columns\"] = [\n        FloatColumn(name=cname, role=roles_.target, df_name=self.name)\n        for cname in roles.target\n    ]\n\n    vars(self)[\"_text_columns\"] = [\n        StringColumn(name=cname, role=roles_.text, df_name=self.name)\n        for cname in roles.text\n    ]\n\n    vars(self)[\"_time_stamp_columns\"] = [\n        FloatColumn(name=cname, role=roles_.time_stamp, df_name=self.name)\n        for cname in roles.time_stamp\n    ]\n\n    vars(self)[\"_unused_float_columns\"] = [\n        FloatColumn(name=cname, role=roles_.unused_float, df_name=self.name)\n        for cname in roles.unused_float\n    ]\n\n    vars(self)[\"_unused_string_columns\"] = [\n        StringColumn(name=cname, role=roles_.unused_string, df_name=self.name)\n        for cname in roles.unused_string\n    ]\n\n    # ------------------------------------------------------------\n\n    self._check_duplicates()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.colnames","title":"colnames  <code>property</code>","text":"<pre><code>colnames: List[str]\n</code></pre> <p>List of the names of all columns.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>List of the names of all columns.</p>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.columns","title":"columns  <code>property</code>","text":"<pre><code>columns: List[str]\n</code></pre> <p>Alias for <code>colnames</code>.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>List of the names of all columns.</p>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.last_change","title":"last_change  <code>property</code>","text":"<pre><code>last_change: str\n</code></pre> <p>A string describing the last time this data frame has been changed.</p>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.memory_usage","title":"memory_usage  <code>property</code>","text":"<pre><code>memory_usage\n</code></pre> <p>Convenience wrapper that returns the memory usage in MB.</p>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.roles","title":"roles  <code>property</code>","text":"<pre><code>roles\n</code></pre> <p>The roles of the columns included in this DataFrame.</p>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.rowid","title":"rowid  <code>property</code>","text":"<pre><code>rowid\n</code></pre> <p>The rowids for this data frame.</p>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.shape","title":"shape  <code>property</code>","text":"<pre><code>shape\n</code></pre> <p>A tuple containing the number of rows and columns of the DataFrame.</p>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.add","title":"add","text":"<pre><code>add(\n    col: Union[StringColumn, FloatColumn, ndarray],\n    name: str,\n    role: Optional[Role] = None,\n    subroles: Optional[Union[Role, Iterable[str]]] = None,\n    unit: str = \"\",\n    time_formats: Optional[Iterable[str]] = None,\n)\n</code></pre> <p>Adds a column to the current <code>DataFrame</code>.</p> PARAMETER DESCRIPTION <code>col</code> <p>The column or numpy.ndarray to be added.</p> <p> TYPE: <code>Union[StringColumn, FloatColumn, ndarray]</code> </p> <code>name</code> <p>Name of the new column.</p> <p> TYPE: <code>str</code> </p> <code>role</code> <p>Role of the new column. Must be from <code>roles</code>.</p> <p> TYPE: <code>Optional[Role]</code> DEFAULT: <code>None</code> </p> <code>subroles</code> <p>Subroles of the new column. Must be from <code>subroles</code>.</p> <p> TYPE: <code>Optional[Union[Role, Iterable[str]]]</code> DEFAULT: <code>None</code> </p> <code>unit</code> <p>Unit of the column.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>time_formats</code> <p>Formats to be used to parse the time stamps.</p> <p>This is only necessary, if an implicit conversion from a <code>StringColumn</code> to a time stamp is taking place.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[Iterable[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def add(\n    self,\n    col: Union[StringColumn, FloatColumn, np.ndarray],\n    name: str,\n    role: Optional[Role] = None,\n    subroles: Optional[Union[Role, Iterable[str]]] = None,\n    unit: str = \"\",\n    time_formats: Optional[Iterable[str]] = None,\n):\n    \"\"\"Adds a column to the current [`DataFrame`][getml.DataFrame].\n\n    Args:\n        col:\n            The column or numpy.ndarray to be added.\n\n        name:\n            Name of the new column.\n\n        role:\n            Role of the new column. Must be from [`roles`][getml.data.roles].\n\n        subroles:\n            Subroles of the new column. Must be from [`subroles`][getml.data.subroles].\n\n        unit:\n            Unit of the column.\n\n        time_formats:\n            Formats to be used to parse the time stamps.\n\n            This is only necessary, if an implicit conversion from\n            a [`StringColumn`][getml.data.columns.StringColumn] to a time\n            stamp is taking place.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n    \"\"\"\n\n    if isinstance(col, np.ndarray):\n        self._add_numpy_array(col, name, role, subroles, unit)\n        return\n\n    col, role, subroles = _with_column(\n        col, name, role, subroles, unit, time_formats\n    )\n\n    is_string = isinstance(col, (StringColumnView, StringColumn))\n\n    if is_string:\n        self._add_categorical_column(col, name, role, subroles, unit)\n    else:\n        self._add_column(col, name, role, subroles, unit)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.copy","title":"copy","text":"<pre><code>copy(name: str) -&gt; DataFrame\n</code></pre> <p>Creates a deep copy of the data frame under a new name.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the new data frame.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A handle to the deep copy.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def copy(self, name: str) -&gt; DataFrame:\n    \"\"\"\n    Creates a deep copy of the data frame under a new name.\n\n    Args:\n        name:\n            The name of the new data frame.\n\n    Returns:\n            A handle to the deep copy.\n    \"\"\"\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be a string.\")\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.concat\"\n    cmd[\"name_\"] = name\n\n    cmd[\"data_frames_\"] = [self._getml_deserialize()]\n\n    comm.send(cmd)\n\n    return DataFrame(name=name).refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.delete","title":"delete","text":"<pre><code>delete()\n</code></pre> <p>Permanently deletes the data frame. <code>delete</code> first unloads the data frame from memory and then deletes it from disk.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def delete(self):\n    \"\"\"\n    Permanently deletes the data frame. `delete` first unloads the data frame\n    from memory and then deletes it from disk.\n    \"\"\"\n    # ------------------------------------------------------------\n\n    self._delete()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.drop","title":"drop","text":"<pre><code>drop(\n    cols: Union[\n        FloatColumn,\n        StringColumn,\n        str,\n        Union[\n            Iterable[FloatColumn],\n            Iterable[StringColumn],\n            Iterable[str],\n        ],\n    ]\n) -&gt; View\n</code></pre> <p>Returns a new <code>View</code> that has one or several columns removed.</p> PARAMETER DESCRIPTION <code>cols</code> <p>The columns or the names thereof.</p> <p> TYPE: <code>Union[FloatColumn, StringColumn, str, Union[Iterable[FloatColumn], Iterable[StringColumn], Iterable[str]]]</code> </p> RETURNS DESCRIPTION <code>View</code> <p>A new <code>View</code> object with the specified columns removed.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def drop(\n    self,\n    cols: Union[\n        FloatColumn,\n        StringColumn,\n        str,\n        Union[Iterable[FloatColumn], Iterable[StringColumn], Iterable[str]],\n    ],\n) -&gt; View:\n    \"\"\"Returns a new [`View`][getml.data.View] that has one or several columns removed.\n\n    Args:\n        cols:\n            The columns or the names thereof.\n\n    Returns:\n        A new [`View`][getml.data.View] object with the specified columns removed.\n    \"\"\"\n\n    names = _handle_cols(cols)\n\n    return View(base=self, dropped=names)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.freeze","title":"freeze","text":"<pre><code>freeze()\n</code></pre> <p>Freezes the data frame.</p> <p>After you have frozen the data frame, the data frame is immutable and in-place operations are no longer possible. However, you can still create views. In other words, operations like <code>set_role</code> are no longer possible, but operations like <code>with_role</code> are.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def freeze(self):\n    \"\"\"Freezes the data frame.\n\n    After you have frozen the data frame, the data frame is immutable\n    and in-place operations are no longer possible. However, you can\n    still create views. In other words, operations like\n    [`set_role`][getml.DataFrame.set_role] are no longer possible,\n    but operations like [`with_role`][getml.DataFrame.with_role] are.\n    \"\"\"\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.freeze\"\n    cmd[\"name_\"] = self.name\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.from_arrow","title":"from_arrow  <code>classmethod</code>","text":"<pre><code>from_arrow(\n    table: Table,\n    name: str,\n    roles: Optional[\n        Union[Dict[Union[Role, str], Iterable[str]], Roles]\n    ] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; Union[DataFrame, Roles]\n</code></pre> <p>Create a DataFrame from an Arrow Table.</p> <p>This is one of the fastest way to get data into the getML Engine.</p> PARAMETER DESCRIPTION <code>table</code> <p>The arrow tablelike to be read.</p> <p> TYPE: <code>Table</code> </p> <code>name</code> <p>Name of the data frame to be created.</p> <p> TYPE: <code>str</code> </p> <code>roles</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <p> TYPE: <code>Optional[Union[Dict[Union[Role, str], Iterable[str]], Roles]]</code> DEFAULT: <code>None</code> </p> <code>ignore</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dry</code> <p>If set to True, the data will not be read. Instead, the method will return the inffered roles.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[DataFrame, Roles]</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_arrow(\n    cls,\n    table: pa.Table,\n    name: str,\n    roles: Optional[Union[Dict[Union[Role, str], Iterable[str]], Roles]] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; Union[DataFrame, Roles]:\n    \"\"\"Create a DataFrame from an Arrow Table.\n\n    This is one of the fastest way to get data into the\n    getML Engine.\n\n    Args:\n        table:\n            The arrow tablelike to be read.\n\n        name:\n            Name of the data frame to be created.\n\n        roles:\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore:\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry:\n            If set to True, the data will not be read. Instead, the method\n            will return the inffered roles.\n\n    Returns:\n            Handler of the underlying data.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    # The content of roles is checked in the class constructor called below.\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    sniffed_roles = sniff_arrow(table)\n\n    roles = _prepare_roles(roles, sniffed_roles, ignore_sniffed_roles=ignore)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_arrow(table=table, append=False)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.from_csv","title":"from_csv  <code>classmethod</code>","text":"<pre><code>from_csv(\n    fnames: Union[str, Iterable[str]],\n    name: str,\n    num_lines_sniffed: None = None,\n    num_lines_read: int = 0,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Iterable[str] = (),\n    roles: Optional[\n        Union[Dict[Union[Role, str], Iterable[str]], Roles]\n    ] = None,\n    ignore: bool = False,\n    dry: bool = False,\n    verbose: bool = True,\n    block_size: int = DEFAULT_CSV_READ_BLOCK_SIZE,\n    in_batches: bool = False,\n) -&gt; Union[DataFrame, Roles]\n</code></pre> <p>Create a DataFrame from CSV files.</p> <p>The getML Engine will construct a data frame object in the Engine, fill it with the data read from the CSV file(s), and return a corresponding <code>DataFrame</code> handle.</p> PARAMETER DESCRIPTION <code>fnames</code> <p>CSV file paths to be read.</p> <p> TYPE: <code>Union[str, Iterable[str]]</code> </p> <code>name</code> <p>Name of the data frame to be created.</p> <p> TYPE: <code>str</code> </p> <code>num_lines_sniffed</code> <p>Number of lines analyzed by the sniffer.</p> <p> TYPE: <code>None</code> DEFAULT: <code>None</code> </p> <code>num_lines_read</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>quotechar</code> <p>The character used to wrap strings.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'\"'</code> </p> <code>sep</code> <p>The separator used for separating fields.</p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>skip</code> <p>Number of lines to skip at the beginning of each file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>colnames</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <p> TYPE: <code>Iterable[str]</code> DEFAULT: <code>()</code> </p> <code>roles</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <p> TYPE: <code>Optional[Union[Dict[Union[Role, str], Iterable[str]], Roles]]</code> DEFAULT: <code>None</code> </p> <code>ignore</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dry</code> <p>If set to True, the data will not be read. Instead, the method will return the inffered roles.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>verbose</code> <p>If True, when fnames are urls, the filenames are printed to stdout during the download.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>block_size</code> <p>The number of bytes read with each batch. Passed down to pyarrow.</p> <p> TYPE: <code>int</code> DEFAULT: <code>DEFAULT_CSV_READ_BLOCK_SIZE</code> </p> <code>in_batches</code> <p>If True, read blocks streamwise manner and send those batches to the engine. Blocks are read and sent to the engine sequentially. While more memory efficient, streaming in batches is slower as it is inherently single-threaded. If False (default) the data is read with multiple threads into arrow first and sent to the engine afterwards.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[DataFrame, Roles]</code> <p>Handler of the underlying data.</p> Deprecated <p>1.5: The <code>num_lines_sniffed</code> parameter is deprecated.</p> Note <p>It is assumed that the first line of each CSV file contains a header with the column names.</p> Example <p>Let's assume you have two CSV files - file1.csv and file2.csv - in the current working directory. You can import their data into the getML Engine using. <pre><code>df_expd = data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"'\n    )\n\n# However, the CSV format lacks type safety. If you want to\n# build a reliable pipeline, it is a good idea\n# to hard-code the roles:\n\nroles = {\"categorical\": [\"col1\", \"col2\"], \"target\": [\"col3\"]}\n\ndf_expd = data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"',\n    roles=roles\n    )\n\n# If you think that typing out all the roles by hand is too\n# cumbersome, you can use a dry run:\n\nroles = data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"',\n    dry=True\n)\n</code></pre></p> <p>This will return the roles dictionary it would have used. You can now hard-code this.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_csv(\n    cls,\n    fnames: Union[str, Iterable[str]],\n    name: str,\n    num_lines_sniffed: None = None,\n    num_lines_read: int = 0,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Iterable[str] = (),\n    roles: Optional[Union[Dict[Union[Role, str], Iterable[str]], Roles]] = None,\n    ignore: bool = False,\n    dry: bool = False,\n    verbose: bool = True,\n    block_size: int = DEFAULT_CSV_READ_BLOCK_SIZE,\n    in_batches: bool = False,\n) -&gt; Union[DataFrame, Roles]:\n    \"\"\"Create a DataFrame from CSV files.\n\n    The getML Engine will construct a data\n    frame object in the Engine, fill it with the data read from\n    the CSV file(s), and return a corresponding\n    [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        fnames:\n            CSV file paths to be read.\n\n        name:\n            Name of the data frame to be created.\n\n        num_lines_sniffed:\n            Number of lines analyzed by the sniffer.\n\n        num_lines_read:\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        quotechar:\n            The character used to wrap strings.\n\n        sep:\n            The separator used for separating fields.\n\n        skip:\n            Number of lines to skip at the beginning of each file.\n\n        colnames: The first line of a CSV file\n            usually contains the column names. When this is not the case,\n            you need to explicitly pass them.\n\n        roles:\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore:\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry:\n            If set to True, the data will not be read. Instead, the method\n            will return the inffered roles.\n\n        verbose:\n            If True, when fnames are urls, the filenames are\n            printed to stdout during the download.\n\n        block_size:\n            The number of bytes read with each batch. Passed down to\n            pyarrow.\n\n        in_batches:\n            If True, read blocks streamwise manner and send those batches to\n            the engine. Blocks are read and sent to the engine sequentially.\n            While more memory efficient, streaming in batches is slower as\n            it is inherently single-threaded. If False (default) the data is\n            read with multiple threads into arrow first and sent to the engine\n            afterwards.\n\n    Returns:\n            Handler of the underlying data.\n\n\n    Deprecated:\n        1.5: The `num_lines_sniffed` parameter is deprecated.\n\n    Note:\n        It is assumed that the first line of each CSV file\n        contains a header with the column names.\n\n    ??? example\n        Let's assume you have two CSV files - *file1.csv* and\n        *file2.csv* - in the current working directory. You can\n        import their data into the getML Engine using.\n        ```python\n        df_expd = data.DataFrame.from_csv(\n            fnames=[\"file1.csv\", \"file2.csv\"],\n            name=\"MY DATA FRAME\",\n            sep=';',\n            quotechar='\"'\n            )\n\n        # However, the CSV format lacks type safety. If you want to\n        # build a reliable pipeline, it is a good idea\n        # to hard-code the roles:\n\n        roles = {\"categorical\": [\"col1\", \"col2\"], \"target\": [\"col3\"]}\n\n        df_expd = data.DataFrame.from_csv(\n            fnames=[\"file1.csv\", \"file2.csv\"],\n            name=\"MY DATA FRAME\",\n            sep=';',\n            quotechar='\"',\n            roles=roles\n            )\n\n        # If you think that typing out all the roles by hand is too\n        # cumbersome, you can use a dry run:\n\n        roles = data.DataFrame.from_csv(\n            fnames=[\"file1.csv\", \"file2.csv\"],\n            name=\"MY DATA FRAME\",\n            sep=';',\n            quotechar='\"',\n            dry=True\n        )\n        ```\n\n        This will return the roles dictionary it would have used. You\n        can now hard-code this.\n\n    \"\"\"\n\n    if num_lines_sniffed is not None:\n        warnings.warn(\n            \"The 'num_lines_sniffed' parameter is deprecated and will be ignored.\",\n            DeprecationWarning,\n        )\n\n    if isinstance(fnames, str):\n        fnames = [fnames]\n\n    if not _is_non_empty_typed_list(fnames, str):\n        raise TypeError(\"'fnames' must be either a str or a list of str.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    if not isinstance(num_lines_read, numbers.Real):\n        raise TypeError(\"'num_lines_read' must be a real number\")\n\n    if not isinstance(quotechar, str):\n        raise TypeError(\"'quotechar' must be str.\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be str.\")\n\n    if not isinstance(skip, numbers.Real):\n        raise TypeError(\"'skip' must be a real number\")\n\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    if colnames:\n        if not _is_iterable_not_str_of_type(colnames, str):\n            raise TypeError(\"'colnames' must be an iterable of str\")\n\n    fnames = _retrieve_urls(fnames, verbose=verbose)\n\n    sniffed_roles = sniff_csv(\n        fnames=fnames,\n        quotechar=quotechar,\n        sep=sep,\n        skip=int(skip),\n        colnames=colnames,\n    )\n\n    roles = _prepare_roles(roles, sniffed_roles, ignore_sniffed_roles=ignore)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_csv(\n        fnames=fnames,\n        append=False,\n        quotechar=quotechar,\n        sep=sep,\n        num_lines_read=num_lines_read,\n        skip=skip,\n        colnames=colnames,\n        block_size=block_size,\n        in_batches=in_batches,\n    )\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.from_db","title":"from_db  <code>classmethod</code>","text":"<pre><code>from_db(\n    table_name: str,\n    name: Optional[str] = None,\n    roles: Optional[\n        Union[Dict[Union[Role, str], Iterable[str]], Roles]\n    ] = None,\n    ignore: bool = False,\n    dry: bool = False,\n    conn: Optional[Connection] = None,\n) -&gt; Union[DataFrame, Roles]\n</code></pre> <p>Create a DataFrame from a table in a database.</p> <p>It will construct a data frame object in the Engine, fill it with the data read from table <code>table_name</code> in the connected database (see <code>database</code>), and return a corresponding <code>DataFrame</code> handle.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to be read.</p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>Name of the data frame to be created. If not passed, then the table_name will be used.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>roles</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <p> TYPE: <code>Optional[Union[Dict[Union[Role, str], Iterable[str]], Roles]]</code> DEFAULT: <code>None</code> </p> <code>ignore</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dry</code> <p>If set to True, the data will not be read. Instead, the method will return the inffered roles.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the Engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Union[DataFrame, Roles]</code> <p>Handler of the underlying data.</p> Example <pre><code>getml.database.connect_mysql(\n    host=\"db.relational-data.org\",\n    port=3306,\n    dbname=\"financial\",\n    user=\"guest\",\n    password=\"relational\"\n)\n\nloan = getml.DataFrame.from_db(\n    table_name='loan', name='data_frame_loan')\n</code></pre> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_db(\n    cls,\n    table_name: str,\n    name: Optional[str] = None,\n    roles: Optional[Union[Dict[Union[Role, str], Iterable[str]], Roles]] = None,\n    ignore: bool = False,\n    dry: bool = False,\n    conn: Optional[Connection] = None,\n) -&gt; Union[DataFrame, Roles]:\n    \"\"\"Create a DataFrame from a table in a database.\n\n    It will construct a data frame object in the Engine, fill it\n    with the data read from table `table_name` in the connected\n    database (see [`database`][getml.database]), and return a\n    corresponding [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        table_name:\n            Name of the table to be read.\n\n        name:\n            Name of the data frame to be created. If not passed,\n            then the *table_name* will be used.\n\n        roles:\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore:\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry:\n            If set to True, the data will not be read. Instead, the method\n            will return the inffered roles.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection, the Engine\n            will use the default connection.\n\n    Returns:\n            Handler of the underlying data.\n\n    ??? example\n        ```python\n        getml.database.connect_mysql(\n            host=\"db.relational-data.org\",\n            port=3306,\n            dbname=\"financial\",\n            user=\"guest\",\n            password=\"relational\"\n        )\n\n        loan = getml.DataFrame.from_db(\n            table_name='loan', name='data_frame_loan')\n        ```\n    \"\"\"\n\n    # -------------------------------------------\n\n    name = name or table_name\n\n    # -------------------------------------------\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be str.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    # The content of roles is checked in the class constructor called below.\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\n            \"'roles' must be a getml.data.Roles object, a dict or None.\"\n        )\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # -------------------------------------------\n\n    conn = conn or database.Connection()\n\n    # ------------------------------------------------------------\n\n    sniffed_roles = _sniff_db(table_name, conn)\n\n    roles = _prepare_roles(roles, sniffed_roles, ignore_sniffed_roles=ignore)\n\n    if dry:\n        return roles\n\n    # ------------------------------------------------------------\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_db(table_name=table_name, append=False, conn=conn)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(\n    data: Dict[Hashable, Iterable[Any]],\n    name: str,\n    roles: Optional[\n        Union[Dict[Union[Role, str], Iterable[str]], Roles]\n    ] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; Union[DataFrame, Roles]\n</code></pre> <p>Create a new DataFrame from a dict</p> PARAMETER DESCRIPTION <code>data</code> <p>The dict containing the data. The data should be in the following format: <pre><code>data = {'col1': [1.0, 2.0, 1.0], 'col2': ['A', 'B', 'C']}\n</code></pre></p> <p> TYPE: <code>Dict[Hashable, Iterable[Any]]</code> </p> <code>name</code> <p>Name of the data frame to be created.</p> <p> TYPE: <code>str</code> </p> <code>roles</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <p> TYPE: <code>Optional[Union[Dict[Union[Role, str], Iterable[str]], Roles]]</code> DEFAULT: <code>None</code> </p> <code>ignore</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dry</code> <p>If set to True, the data will not be read. Instead, the method will return the inffered roles.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:         Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_dict(\n    cls,\n    data: Dict[Hashable, Iterable[Any]],\n    name: str,\n    roles: Optional[Union[Dict[Union[Role, str], Iterable[str]], Roles]] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; Union[DataFrame, Roles]:\n    \"\"\"Create a new DataFrame from a dict\n\n    Args:\n        data:\n            The dict containing the data.\n            The data should be in the following format:\n            ```python\n            data = {'col1': [1.0, 2.0, 1.0], 'col2': ['A', 'B', 'C']}\n            ```\n        name:\n            Name of the data frame to be created.\n\n        roles:\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore:\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry:\n            If set to True, the data will not be read. Instead, the method\n            will return the inffered roles.\n    Returns:\n            Handler of the underlying data.\n    \"\"\"\n\n    if not isinstance(data, dict):\n        raise TypeError(\"'data' must be dict.\")\n\n    return cls.from_arrow(\n        table=pa.Table.from_pydict(data),\n        name=name,\n        roles=roles,\n        ignore=ignore,\n        dry=dry,\n    )\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(\n    json_str: str,\n    name: str,\n    roles: Optional[\n        Union[Dict[Union[Role, str], Iterable[str]], Roles]\n    ] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; Union[DataFrame, Roles]\n</code></pre> <p>Create a new DataFrame from a JSON string.</p> <p>It will construct a data frame object in the Engine, fill it with the data read from the JSON string, and return a corresponding <code>DataFrame</code> handle.</p> PARAMETER DESCRIPTION <code>json_str</code> <p>The JSON string containing the data. The json_str should be in the following format: <pre><code>json_str = \"{'col1': [1.0, 2.0, 1.0], 'col2': ['A', 'B', 'C']}\"\n</code></pre></p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>Name of the data frame to be created.</p> <p> TYPE: <code>str</code> </p> <code>roles</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <p> TYPE: <code>Optional[Union[Dict[Union[Role, str], Iterable[str]], Roles]]</code> DEFAULT: <code>None</code> </p> <code>ignore</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dry</code> <p>If set to True, the data will not be read. Instead, the method will return the inffered roles.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[DataFrame, Roles]</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_json(\n    cls,\n    json_str: str,\n    name: str,\n    roles: Optional[Union[Dict[Union[Role, str], Iterable[str]], Roles]] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; Union[DataFrame, Roles]:\n    \"\"\"Create a new DataFrame from a JSON string.\n\n    It will construct a data frame object in the Engine, fill it\n    with the data read from the JSON string, and return a\n    corresponding [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        json_str:\n            The JSON string containing the data.\n            The json_str should be in the following format:\n            ```python\n            json_str = \"{'col1': [1.0, 2.0, 1.0], 'col2': ['A', 'B', 'C']}\"\n            ```\n        name:\n            Name of the data frame to be created.\n\n        roles:\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore:\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry:\n            If set to True, the data will not be read. Instead, the method\n            will return the inffered roles.\n\n    Returns:\n        Handler of the underlying data.\n\n    \"\"\"\n\n    if not isinstance(json_str, str):\n        raise TypeError(\"'json_str' must be str.\")\n\n    return cls.from_dict(\n        data=json.loads(json_str),\n        name=name,\n        roles=roles,\n        ignore=ignore,\n        dry=dry,\n    )\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.from_pandas","title":"from_pandas  <code>classmethod</code>","text":"<pre><code>from_pandas(\n    pandas_df: DataFrame,\n    name: str,\n    roles: Optional[\n        Union[Dict[Union[Role, str], Iterable[str]], Roles]\n    ] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; Union[DataFrame, Roles]\n</code></pre> <p>Create a DataFrame from a <code>pandas.DataFrame</code>.</p> <p>It will construct a data frame object in the Engine, fill it with the data read from the <code>pandas.DataFrame</code>, and return a corresponding <code>DataFrame</code> handle.</p> PARAMETER DESCRIPTION <code>pandas_df</code> <p>The table to be read.</p> <p> TYPE: <code>DataFrame</code> </p> <code>name</code> <p>Name of the data frame to be created.</p> <p> TYPE: <code>str</code> </p> <code>roles</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code> roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n          getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <p> TYPE: <code>Optional[Union[Dict[Union[Role, str], Iterable[str]], Roles]]</code> DEFAULT: <code>None</code> </p> <code>ignore</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dry</code> <p>If set to True, the data will not be read. Instead, the method will return the inffered roles.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[DataFrame, Roles]</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_pandas(\n    cls,\n    pandas_df: pd.DataFrame,\n    name: str,\n    roles: Optional[Union[Dict[Union[Role, str], Iterable[str]], Roles]] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; Union[DataFrame, Roles]:\n    \"\"\"Create a DataFrame from a `pandas.DataFrame`.\n\n    It will construct a data frame object in the Engine, fill it\n    with the data read from the `pandas.DataFrame`, and\n    return a corresponding [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        pandas_df:\n            The table to be read.\n\n        name:\n            Name of the data frame to be created.\n\n        roles:\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n             roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                      getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore:\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry:\n            If set to True, the data will not be read. Instead, the method\n            will return the inffered roles.\n\n    Returns:\n        Handler of the underlying data.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(pandas_df, pd.DataFrame):\n        raise TypeError(\"'pandas_df' must be of type pandas.DataFrame.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    # The content of roles is checked in the class constructor called below.\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    sniffed_roles = _sniff_pandas(pandas_df)\n\n    roles = _prepare_roles(roles, sniffed_roles, ignore_sniffed_roles=ignore)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_pandas(pandas_df=pandas_df, append=False)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.from_parquet","title":"from_parquet  <code>classmethod</code>","text":"<pre><code>from_parquet(\n    fnames: Union[str, Iterable[str]],\n    name: str,\n    roles: Optional[\n        Union[Dict[Union[Role, str], Iterable[str]], Roles]\n    ] = None,\n    ignore: bool = False,\n    dry: bool = False,\n    colnames: Iterable[str] = (),\n) -&gt; Union[DataFrame, Roles]\n</code></pre> <p>Create a DataFrame from parquet files.</p> <p>This is one of the fastest way to get data into the getML Engine.</p> PARAMETER DESCRIPTION <code>fnames</code> <p>The path of the parquet file(s) to be read.</p> <p> TYPE: <code>Union[str, Iterable[str]]</code> </p> <code>name</code> <p>Name of the data frame to be created.</p> <p> TYPE: <code>str</code> </p> <code>roles</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <p> TYPE: <code>Optional[Union[Dict[Union[Role, str], Iterable[str]], Roles]]</code> DEFAULT: <code>None</code> </p> <code>ignore</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dry</code> <p>If set to True, the data will not be read. Instead, the method will return the inffered roles.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[DataFrame, Roles]</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_parquet(\n    cls,\n    fnames: Union[str, Iterable[str]],\n    name: str,\n    roles: Optional[Union[Dict[Union[Role, str], Iterable[str]], Roles]] = None,\n    ignore: bool = False,\n    dry: bool = False,\n    colnames: Iterable[str] = (),\n) -&gt; Union[DataFrame, Roles]:\n    \"\"\"Create a DataFrame from parquet files.\n\n    This is one of the fastest way to get data into the\n    getML Engine.\n\n    Args:\n        fnames:\n            The path of the parquet file(s) to be read.\n\n        name:\n            Name of the data frame to be created.\n\n        roles:\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore:\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry:\n            If set to True, the data will not be read. Instead, the method\n            will return the inffered roles.\n\n    Returns:\n        Handler of the underlying data.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if isinstance(fnames, str):\n        fnames = [fnames]\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    # The content of roles is checked in the class constructor called below.\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    sniffed_roles = sniff_parquet(fnames, colnames)\n\n    roles = _prepare_roles(roles, sniffed_roles, ignore_sniffed_roles=ignore)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_parquet(fnames=fnames, append=False, colnames=colnames)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.from_pyspark","title":"from_pyspark  <code>classmethod</code>","text":"<pre><code>from_pyspark(\n    spark_df: DataFrame,\n    name: str,\n    roles: Optional[\n        Union[Dict[Union[Role, str], Iterable[str]], Roles]\n    ] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; Union[DataFrame, Roles]\n</code></pre> <p>Create a DataFrame from a <code>pyspark.sql.DataFrame</code>.</p> <p>It will construct a data frame object in the Engine, fill it with the data read from the <code>pyspark.sql.DataFrame</code>, and return a corresponding <code>DataFrame</code> handle.</p> PARAMETER DESCRIPTION <code>spark_df</code> <p>The table to be read.</p> <p> TYPE: <code>DataFrame</code> </p> <code>name</code> <p>Name of the data frame to be created.</p> <p> TYPE: <code>str</code> </p> <code>roles</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre></p> <p>Otherwise, you can use the <code>Roles</code> class.</p> <p> TYPE: <code>Optional[Union[Dict[Union[Role, str], Iterable[str]], Roles]]</code> DEFAULT: <code>None</code> </p> <code>ignore</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dry</code> <p>If set to True, the data will not be read. Instead, the method will return the inffered roles.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[DataFrame, Roles]</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_pyspark(\n    cls,\n    spark_df: pyspark.sql.DataFrame,\n    name: str,\n    roles: Optional[Union[Dict[Union[Role, str], Iterable[str]], Roles]] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; Union[DataFrame, Roles]:\n    \"\"\"Create a DataFrame from a `pyspark.sql.DataFrame`.\n\n    It will construct a data frame object in the Engine, fill it\n    with the data read from the `pyspark.sql.DataFrame`, and\n    return a corresponding [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        spark_df:\n            The table to be read.\n\n        name:\n            Name of the data frame to be created.\n\n        roles:\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore:\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry:\n            If set to True, the data will not be read. Instead, the method\n            will return the inffered roles.\n\n    Returns:\n            Handler of the underlying data.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    # The content of roles is checked in the class constructor called below.\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    head = spark_df.limit(2).toPandas()\n\n    sniffed_roles = _sniff_pandas(head)\n\n    roles = _prepare_roles(roles, sniffed_roles, ignore_sniffed_roles=ignore)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_pyspark(spark_df=spark_df, append=False)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.from_s3","title":"from_s3  <code>classmethod</code>","text":"<pre><code>from_s3(\n    bucket: str,\n    keys: Iterable[str],\n    region: str,\n    name: str,\n    num_lines_sniffed: int = 1000,\n    num_lines_read: int = 0,\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[Iterable[str]] = None,\n    roles: Optional[\n        Union[Dict[Union[Role, str], Iterable[str]], Roles]\n    ] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; Union[DataFrame, Roles]\n</code></pre> <p>Create a DataFrame from CSV files located in an S3 bucket.</p> <p>This classmethod will construct a data frame object in the Engine, fill it with the data read from the CSV file(s), and return a corresponding <code>DataFrame</code> handle.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> Note <p>Note that S3 is not supported on Windows.</p> PARAMETER DESCRIPTION <code>bucket</code> <p>The bucket from which to read the files.</p> <p> TYPE: <code>str</code> </p> <code>keys</code> <p>The list of keys (files in the bucket) to be read.</p> <p> TYPE: <code>Iterable[str]</code> </p> <code>region</code> <p>The region in which the bucket is located.</p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>Name of the data frame to be created.</p> <p> TYPE: <code>str</code> </p> <code>num_lines_sniffed</code> <p>Number of lines analyzed by the sniffer.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>num_lines_read</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>sep</code> <p>The separator used for separating fields.</p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>skip</code> <p>Number of lines to skip at the beginning of each file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>colnames</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <p> TYPE: <code>Optional[Iterable[str]]</code> DEFAULT: <code>None</code> </p> <code>roles</code> <p>Maps the <code>roles</code> to the column names (see <code>colnames</code>).</p> <p>The <code>roles</code> dictionary is expected to have the following format: <pre><code>roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n         getml.data.role.target: [\"colname3\"]}\n</code></pre> Otherwise, you can use the <code>Roles</code> class.</p> <p> TYPE: <code>Optional[Union[Dict[Union[Role, str], Iterable[str]], Roles]]</code> DEFAULT: <code>None</code> </p> <code>ignore</code> <p>Only relevant when roles is not None. Determines what you want to do with any colnames not mentioned in roles. Do you want to ignore them (True) or read them in as unused columns (False)?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dry</code> <p>If set to True, the data will not be read. Instead, the method will return the inffered roles.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[DataFrame, Roles]</code> <p>Handler of the underlying data.</p> Example <p>Let's assume you have two CSV files - file1.csv and file2.csv - in the bucket. You can import their data into the getML Engine using the following commands: <pre><code>getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\ngetml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\ndata_frame_expd = data.DataFrame.from_s3(\n    bucket=\"your-bucket-name\",\n    keys=[\"file1.csv\", \"file2.csv\"],\n    region=\"us-east-2\",\n    name=\"MY DATA FRAME\",\n    sep=';'\n)\n</code></pre></p> <p>You can also set the access credential as environment variables before you launch the getML Engine.</p> <p>Also refer to the documentation on <code>from_csv</code> for further information on overriding the CSV sniffer for greater type safety.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_s3(\n    cls,\n    bucket: str,\n    keys: Iterable[str],\n    region: str,\n    name: str,\n    num_lines_sniffed: int = 1000,\n    num_lines_read: int = 0,\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[Iterable[str]] = None,\n    roles: Optional[Union[Dict[Union[Role, str], Iterable[str]], Roles]] = None,\n    ignore: bool = False,\n    dry: bool = False,\n) -&gt; Union[DataFrame, Roles]:\n    \"\"\"Create a DataFrame from CSV files located in an S3 bucket.\n\n    This classmethod will construct a data\n    frame object in the Engine, fill it with the data read from\n    the CSV file(s), and return a corresponding\n    [`DataFrame`][getml.DataFrame] handle.\n\n    enterprise-adm: Enterprise edition\n        This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the [benefits of the Enterprise edition][enterprise-benefits] and [compare their features][enterprise-feature-list].\n\n        For licensing information and technical support, please [contact us][contact-page].\n\n    Note:\n        Note that S3 is not supported on Windows.\n\n    Args:\n        bucket:\n            The bucket from which to read the files.\n\n        keys:\n            The list of keys (files in the bucket) to be read.\n\n        region:\n            The region in which the bucket is located.\n\n        name:\n            Name of the data frame to be created.\n\n        num_lines_sniffed:\n            Number of lines analyzed by the sniffer.\n\n        num_lines_read:\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        sep:\n            The separator used for separating fields.\n\n        skip:\n            Number of lines to skip at the beginning of each file.\n\n        colnames:\n            The first line of a CSV file\n            usually contains the column names. When this is not the case,\n            you need to explicitly pass them.\n\n        roles:\n            Maps the [`roles`][getml.data.roles] to the\n            column names (see [`colnames`][getml.DataFrame.colnames]).\n\n            The `roles` dictionary is expected to have the following format:\n            ```python\n            roles = {getml.data.role.numeric: [\"colname1\", \"colname2\"],\n                     getml.data.role.target: [\"colname3\"]}\n            ```\n            Otherwise, you can use the [`Roles`][getml.data.Roles] class.\n\n        ignore:\n            Only relevant when roles is not None.\n            Determines what you want to do with any colnames not\n            mentioned in roles. Do you want to ignore them (True)\n            or read them in as unused columns (False)?\n\n        dry:\n            If set to True, the data will not be read. Instead, the method\n            will return the inffered roles.\n\n    Returns:\n            Handler of the underlying data.\n\n    ??? example\n        Let's assume you have two CSV files - *file1.csv* and\n        *file2.csv* - in the bucket. You can\n        import their data into the getML Engine using the following\n        commands:\n        ```python\n        getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n        getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n        data_frame_expd = data.DataFrame.from_s3(\n            bucket=\"your-bucket-name\",\n            keys=[\"file1.csv\", \"file2.csv\"],\n            region=\"us-east-2\",\n            name=\"MY DATA FRAME\",\n            sep=';'\n        )\n        ```\n\n        You can also set the access credential as environment variables\n        before you launch the getML Engine.\n\n        Also refer to the documentation on [`from_csv`][getml.DataFrame.from_csv]\n        for further information on overriding the CSV sniffer for greater\n        type safety.\n\n    \"\"\"\n\n    if isinstance(keys, str):\n        keys = [keys]\n\n    if not isinstance(bucket, str):\n        raise TypeError(\"'bucket' must be str.\")\n\n    if not _is_non_empty_typed_list(keys, str):\n        raise TypeError(\"'keys' must be either a string or a list of str\")\n\n    if not isinstance(region, str):\n        raise TypeError(\"'region' must be str.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    if not isinstance(num_lines_sniffed, numbers.Real):\n        raise TypeError(\"'num_lines_sniffed' must be a real number\")\n\n    if not isinstance(num_lines_read, numbers.Real):\n        raise TypeError(\"'num_lines_read' must be a real number\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be str.\")\n\n    if not isinstance(skip, numbers.Real):\n        raise TypeError(\"'skip' must be a real number\")\n\n    if roles is not None and not isinstance(roles, (dict, Roles)):\n        raise TypeError(\"'roles' must be a geml.data.Roles object, a dict or None.\")\n\n    if not isinstance(ignore, bool):\n        raise TypeError(\"'ignore' must be bool.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n        raise TypeError(\n            \"'colnames' must be either be None or a non-empty list of str.\"\n        )\n\n    sniffed_roles = _sniff_s3(\n        bucket=bucket,\n        keys=keys,\n        region=region,\n        num_lines_sniffed=int(num_lines_sniffed),\n        sep=sep,\n        skip=int(skip),\n        colnames=colnames,\n    )\n\n    roles = _prepare_roles(roles, sniffed_roles, ignore)\n\n    if dry:\n        return roles\n\n    data_frame = cls(name, roles)\n\n    return data_frame.read_s3(\n        bucket=bucket,\n        keys=keys,\n        region=region,\n        append=False,\n        sep=sep,\n        num_lines_read=int(num_lines_read),\n        skip=int(skip),\n        colnames=colnames,\n    )\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.from_view","title":"from_view  <code>classmethod</code>","text":"<pre><code>from_view(\n    view: View, name: str, dry: bool = False\n) -&gt; Union[DataFrame, Roles]\n</code></pre> <p>Create a DataFrame from a <code>View</code>.</p> <p>This classmethod will construct a data frame object in the Engine, fill it with the data read from the <code>View</code>, and return a corresponding <code>DataFrame</code> handle.</p> PARAMETER DESCRIPTION <code>view</code> <p>The view from which we want to read the data.</p> <p> TYPE: <code>View</code> </p> <code>name</code> <p>Name of the data frame to be created.</p> <p> TYPE: <code>str</code> </p> <code>dry</code> <p>If set to True, the data will not be read. Instead, the method will return an empty data frame with the roles set as inferred.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[DataFrame, Roles]</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>@classmethod\ndef from_view(\n    cls,\n    view: View,\n    name: str,\n    dry: bool = False,\n) -&gt; Union[DataFrame, Roles]:\n    \"\"\"Create a DataFrame from a [`View`][getml.data.View].\n\n    This classmethod will construct a data\n    frame object in the Engine, fill it with the data read from\n    the [`View`][getml.data.View], and return a corresponding\n    [`DataFrame`][getml.DataFrame] handle.\n\n    Args:\n        view:\n            The view from which we want to read the data.\n\n        name:\n            Name of the data frame to be created.\n\n        dry:\n            If set to True, the data will not be read. Instead, the method\n            will return an empty data frame with the roles set as inferred.\n\n    Returns:\n            Handler of the underlying data.\n\n\n    \"\"\"\n    # ------------------------------------------------------------\n\n    if not isinstance(view, View):\n        raise TypeError(\"'view' must be getml.data.View.\")\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be str.\")\n\n    if not isinstance(dry, bool):\n        raise TypeError(\"'dry' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    if dry:\n        return view.roles\n\n    data_frame = cls(name, view.roles)\n\n    # ------------------------------------------------------------\n\n    return data_frame.read_view(view=view, append=False)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.load","title":"load","text":"<pre><code>load() -&gt; DataFrame\n</code></pre> <p>Loads saved data from disk.</p> <p>The data frame object holding the same name as the current <code>DataFrame</code> instance will be loaded from disk into the getML Engine and updates the current handler using <code>refresh</code>.</p> Example <p>First, we have to create and import data sets. <pre><code>d, _ = getml.datasets.make_numerical(population_name = 'test')\ngetml.data.list_data_frames()\n</code></pre></p> <p>In the output of <code>list_data_frames</code> we can find our underlying data frame object 'test' listed under the 'in_memory' key (it was created and imported by <code>make_numerical</code>). This means the getML Engine does only hold it in memory (RAM) yet, and we still have to <code>save</code> it to disk in order to <code>load</code> it again or to prevent any loss of information between different sessions. <pre><code>d.save()\ngetml.data.list_data_frames()\nd2 = getml.DataFrame(name = 'test').load()\n</code></pre></p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Updated handle the underlying data frame in the getML Engine.</p> Note <p>When invoking <code>load</code> all changes of the underlying data frame object that took place after the last call to the <code>save</code> method will be lost. Thus, this method  enables you to undo changes applied to the <code>DataFrame</code>. <pre><code>d, _ = getml.datasets.make_numerical()\nd.save()\n\n# Accidental change we want to undo\nd.rm('column_01')\n\nd.load()\n</code></pre> If <code>save</code> hasn't been called on the current instance yet, or it wasn't stored to disk in a previous session, <code>load</code> will throw an exception</p> <pre><code>File or directory '../projects/X/data/Y/' not found!\n</code></pre> <p>Alternatively, <code>load_data_frame</code> offers an easier way of creating <code>DataFrame</code> handlers to data in the getML Engine.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def load(self) -&gt; DataFrame:\n    \"\"\"Loads saved data from disk.\n\n    The data frame object holding the same name as the current\n    [`DataFrame`][getml.DataFrame] instance will be loaded from\n    disk into the getML Engine and updates the current handler\n    using [`refresh`][getml.DataFrame.refresh].\n\n    ??? example\n        First, we have to create and import data sets.\n        ```python\n        d, _ = getml.datasets.make_numerical(population_name = 'test')\n        getml.data.list_data_frames()\n        ```\n\n        In the output of [`list_data_frames`][getml.data.list_data_frames] we\n        can find our underlying data frame object 'test' listed\n        under the 'in_memory' key (it was created and imported by\n        [`make_numerical`][getml.datasets.make_numerical]). This means the\n        getML Engine does only hold it in memory (RAM) yet, and we\n        still have to [`save`][getml.DataFrame.save] it to\n        disk in order to [`load`][getml.DataFrame.load] it\n        again or to prevent any loss of information between\n        different sessions.\n        ```python\n        d.save()\n        getml.data.list_data_frames()\n        d2 = getml.DataFrame(name = 'test').load()\n        ```\n\n    Returns:\n            Updated handle the underlying data frame in the getML\n                Engine.\n\n    Note:\n        When invoking [`load`][getml.DataFrame.load] all\n        changes of the underlying data frame object that took\n        place after the last call to the\n        [`save`][getml.DataFrame.save] method will be\n        lost. Thus, this method  enables you to undo changes\n        applied to the [`DataFrame`][getml.DataFrame].\n        ```python\n        d, _ = getml.datasets.make_numerical()\n        d.save()\n\n        # Accidental change we want to undo\n        d.rm('column_01')\n\n        d.load()\n        ```\n        If [`save`][getml.DataFrame.save] hasn't been called\n        on the current instance yet, or it wasn't stored to disk in\n        a previous session, [`load`][getml.DataFrame.load]\n        will throw an exception\n\n            File or directory '../projects/X/data/Y/' not found!\n\n        Alternatively, [`load_data_frame`][getml.data.load_data_frame]\n        offers an easier way of creating\n        [`DataFrame`][getml.DataFrame] handlers to data in the\n        getML Engine.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.load\"\n    cmd[\"name_\"] = self.name\n    comm.send(cmd)\n    return self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.nbytes","title":"nbytes","text":"<pre><code>nbytes() -&gt; uint64\n</code></pre> <p>Size of the data stored in the underlying data frame in the getML Engine.</p> RETURNS DESCRIPTION <code>uint64</code> <p>Size of the underlying object in bytes.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def nbytes(self) -&gt; np.uint64:\n    \"\"\"Size of the data stored in the underlying data frame in the getML\n    Engine.\n\n    Returns:\n            Size of the underlying object in bytes.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.nbytes\"\n    cmd[\"name_\"] = self.name\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            sock.close()\n            comm.handle_engine_exception(msg)\n        nbytes = comm.recv_string(sock)\n\n    return np.uint64(nbytes)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.ncols","title":"ncols","text":"<pre><code>ncols() -&gt; int\n</code></pre> <p>Number of columns in the current instance.</p> RETURNS DESCRIPTION <code>int</code> <p>Overall number of columns</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def ncols(self) -&gt; int:\n    \"\"\"\n    Number of columns in the current instance.\n\n    Returns:\n            Overall number of columns\n    \"\"\"\n    return len(self.colnames)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.nrows","title":"nrows","text":"<pre><code>nrows() -&gt; int\n</code></pre> <p>Number of rows in the current instance.</p> RETURNS DESCRIPTION <code>int</code> <p>Overall number of rows</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def nrows(self) -&gt; int:\n    \"\"\"\n    Number of rows in the current instance.\n\n    Returns:\n            Overall number of rows\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.nrows\"\n    cmd[\"name_\"] = self.name\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            sock.close()\n            comm.handle_engine_exception(msg)\n        nrows = comm.recv_string(sock)\n\n    return int(nrows)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.read_arrow","title":"read_arrow","text":"<pre><code>read_arrow(\n    table: Union[RecordBatch, Table, Iterable[RecordBatch]],\n    append: bool = False,\n) -&gt; DataFrame\n</code></pre> <p>Uploads a <code>pyarrow.Table</code> or <code>pyarrow.RecordBatch</code> to the getML Engine.</p> <p>Replaces the actual content of the underlying data frame in the getML Engine with <code>table</code>.</p> PARAMETER DESCRIPTION <code>table</code> <p>The arrow tablelike to be read as a <code>DataFrame</code>.</p> <p> TYPE: <code>Union[RecordBatch, Table, Iterable[RecordBatch]]</code> </p> <code>append</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML Engine, should the content in <code>query</code> be appended or replace the existing data?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Current instance.</p> Note <p>For columns containing <code>pandas.Timestamp</code> there can be small inconsistencies in the order of microseconds when sending the data to the getML Engine. This is due to the way the underlying information is stored.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_arrow(\n    self,\n    table: Union[pa.RecordBatch, pa.Table, Iterable[pa.RecordBatch]],\n    append: bool = False,\n) -&gt; DataFrame:\n    \"\"\"Uploads a `pyarrow.Table` or `pyarrow.RecordBatch` to the getML Engine.\n\n    Replaces the actual content of the underlying data frame in\n    the getML Engine with `table`.\n\n    Args:\n        table:\n            The arrow tablelike to be read as a `DataFrame`.\n\n        append:\n            If a data frame object holding the same `name` is\n            already present in the getML Engine, should the content in\n            `query` be appended or replace the existing data?\n\n    Returns:\n            Current instance.\n\n    Note:\n        For columns containing `pandas.Timestamp` there can\n        be small inconsistencies in the order of microseconds\n        when sending the data to the getML Engine. This is due to\n        the way the underlying information is stored.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    inferred_schema, batches = to_arrow_batches(table)\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    # ------------------------------------------------------------\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_pandas(...).\"\"\"\n        )\n\n    # ------------------------------------------------------------\n\n    preprocessed_schema = preprocess_arrow_schema(inferred_schema, self.roles)\n\n    read_arrow_batches(batches, preprocessed_schema, self, append)\n\n    return self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.read_csv","title":"read_csv","text":"<pre><code>read_csv(\n    fnames: Iterable[str],\n    append: bool = False,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    num_lines_read: int = 0,\n    skip: int = 0,\n    colnames: Optional[Iterable[str]] = None,\n    time_formats: Optional[Iterable[str]] = None,\n    verbose: bool = True,\n    block_size: int = DEFAULT_CSV_READ_BLOCK_SIZE,\n    in_batches: bool = False,\n) -&gt; DataFrame\n</code></pre> <p>Read CSV files.</p> <p>It is assumed that the first line of each CSV file contains a header with the column names.</p> PARAMETER DESCRIPTION <code>fnames</code> <p>CSV file paths to be read.</p> <p> TYPE: <code>Iterable[str]</code> </p> <code>append</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML, should the content of the CSV files in <code>fnames</code> be appended or replace the existing data?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>quotechar</code> <p>The character used to wrap strings.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'\"'</code> </p> <code>sep</code> <p>The separator used for separating fields.</p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>num_lines_read</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>skip</code> <p>Number of lines to skip at the beginning of each file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>colnames</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <p> TYPE: <code>Optional[Iterable[str]]</code> DEFAULT: <code>None</code> </p> <code>time_formats</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[Iterable[str]]</code> DEFAULT: <code>None</code> </p> <code>verbose</code> <p>If True, when <code>fnames</code> are urls, the filenames are printed to stdout during the download.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>block_size</code> <p>The number of bytes read with each batch. Passed down to pyarrow.</p> <p> TYPE: <code>int</code> DEFAULT: <code>DEFAULT_CSV_READ_BLOCK_SIZE</code> </p> <code>in_batches</code> <p>If True, read blocks streamwise manner and send those batches to the engine. Blocks are read and sent to the engine sequentially. While more memory efficient, streaming in batches is slower as it is inherently single-threaded. If False (default) the data is read with multiple threads into arrow first and sent to the engine afterwards.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_csv(\n    self,\n    fnames: Iterable[str],\n    append: bool = False,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    num_lines_read: int = 0,\n    skip: int = 0,\n    colnames: Optional[Iterable[str]] = None,\n    time_formats: Optional[Iterable[str]] = None,\n    verbose: bool = True,\n    block_size: int = DEFAULT_CSV_READ_BLOCK_SIZE,\n    in_batches: bool = False,\n) -&gt; DataFrame:\n    \"\"\"Read CSV files.\n\n    It is assumed that the first line of each CSV file contains a\n    header with the column names.\n\n    Args:\n        fnames:\n            CSV file paths to be read.\n\n        append:\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            the CSV files in `fnames` be appended or replace the\n            existing data?\n\n        quotechar:\n            The character used to wrap strings.\n\n        sep:\n            The separator used for separating fields.\n\n        num_lines_read:\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        skip:\n            Number of lines to skip at the beginning of each file.\n\n        colnames:\n            The first line of a CSV file\n            usually contains the column names.\n            When this is not the case, you need to explicitly pass them.\n\n        time_formats:\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        verbose:\n            If True, when `fnames` are urls, the filenames are printed to\n            stdout during the download.\n\n        block_size:\n            The number of bytes read with each batch. Passed down to\n            pyarrow.\n\n        in_batches:\n            If True, read blocks streamwise manner and send those batches to\n            the engine. Blocks are read and sent to the engine sequentially.\n            While more memory efficient, streaming in batches is slower as\n            it is inherently single-threaded. If False (default) the data is\n            read with multiple threads into arrow first and sent to the engine\n            afterwards.\n\n    Returns:\n            Handler of the underlying data.\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    if isinstance(fnames, str):\n        fnames = [fnames]\n\n    if not _is_non_empty_typed_list(fnames, str):\n        raise TypeError(\"'fnames' must be either a string or a list of str\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    if not isinstance(quotechar, str):\n        raise TypeError(\"'quotechar' must be str.\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be str.\")\n\n    if not isinstance(num_lines_read, numbers.Real):\n        raise TypeError(\"'num_lines_read' must be a real number\")\n\n    if not isinstance(skip, numbers.Real):\n        raise TypeError(\"'skip' must be a real number\")\n\n    if not _is_non_empty_typed_list(time_formats, str):\n        raise TypeError(\"'time_formats' must be a non-empty list of str\")\n\n    if colnames:\n        if not _is_iterable_not_str_of_type(colnames, str):\n            raise TypeError(\"'colnames' must be an iterable of str\")\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_csv(...).\"\"\"\n        )\n\n    if not _is_non_empty_typed_list(fnames, str):\n        raise TypeError(\n            \"\"\"'fnames' must be a list containing at\n            least one path to a CSV file\"\"\"\n        )\n\n    fnames_ = _retrieve_urls(fnames, verbose)\n\n    if colnames is None:\n        colnames = ()\n\n    stream_read_csv = stream_csv if in_batches else read_csv\n\n    readers = (\n        stream_read_csv(\n            Path(fname),\n            roles=self.roles,\n            skip_rows=skip,\n            column_names=colnames,\n            delimiter=sep,\n            quote_char=quotechar,\n            block_size=block_size,\n        )\n        for fname in fnames_\n    )\n\n    for batches in readers:\n        first_batch = next(batches)\n        schema = first_batch.schema\n        read_arrow_batches(iter((first_batch, *batches)), schema, self, append)\n        if not append:\n            append = True\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.read_json","title":"read_json","text":"<pre><code>read_json(\n    json_str: str,\n    append: bool = False,\n    time_formats: Optional[Iterable[str]] = None,\n) -&gt; DataFrame\n</code></pre> <p>Fill from JSON</p> <p>Fills the data frame with data from a JSON string.</p> <p>Args:</p> <pre><code>json_str:\n    The JSON string containing the data.\n\nappend:\n    If a data frame object holding the same ``name`` is\n    already present in the getML, should the content of\n    `json_str` be appended or replace the existing data?\n\ntime_formats:\n    The list of formats tried when parsing time stamps.\n    The formats are allowed to contain the following\n    special characters:\n\n    * %w - abbreviated weekday (Mon, Tue, ...)\n    * %W - full weekday (Monday, Tuesday, ...)\n    * %b - abbreviated month (Jan, Feb, ...)\n    * %B - full month (January, February, ...)\n    * %d - zero-padded day of month (01 .. 31)\n    * %e - day of month (1 .. 31)\n    * %f - space-padded day of month ( 1 .. 31)\n    * %m - zero-padded month (01 .. 12)\n    * %n - month (1 .. 12)\n    * %o - space-padded month ( 1 .. 12)\n    * %y - year without century (70)\n    * %Y - year with century (1970)\n    * %H - hour (00 .. 23)\n    * %h - hour (00 .. 12)\n    * %a - am/pm\n    * %A - AM/PM\n    * %M - minute (00 .. 59)\n    * %S - second (00 .. 59)\n    * %s - seconds and microseconds (equivalent to %S.%F)\n    * %i - millisecond (000 .. 999)\n    * %c - centisecond (0 .. 9)\n    * %F - fractional seconds/microseconds (000000 - 999999)\n    * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n    * %Z - time zone differential in RFC format (GMT or +NNNN)\n    * %% - percent sign\n</code></pre> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Note <p>This does not support NaN values. If you want support for NaN, use <code>from_json</code> instead.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_json(\n    self,\n    json_str: str,\n    append: bool = False,\n    time_formats: Optional[Iterable[str]] = None,\n) -&gt; DataFrame:\n    \"\"\"Fill from JSON\n\n    Fills the data frame with data from a JSON string.\n\n    Args:\n\n        json_str:\n            The JSON string containing the data.\n\n        append:\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            `json_str` be appended or replace the existing data?\n\n        time_formats:\n            The list of formats tried when parsing time stamps.\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n    Returns:\n            Handler of the underlying data.\n\n    Note:\n        This does not support NaN values. If you want support for NaN,\n        use [`from_json`][getml.DataFrame.from_json] instead.\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_json(...).\"\"\"\n        )\n\n    if not isinstance(json_str, str):\n        raise TypeError(\"'json_str' must be of type str\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be of type bool\")\n\n    if not _is_non_empty_typed_list(time_formats, str):\n        raise TypeError(\n            \"\"\"'time_formats' must be a list of strings\n            containing at least one time format\"\"\"\n        )\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.from_json\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    cmd[\"append_\"] = append\n    cmd[\"time_formats_\"] = time_formats\n\n    with comm.send_and_get_socket(cmd) as sock:\n        comm.send_string(sock, json_str)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.handle_engine_exception(msg)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.read_parquet","title":"read_parquet","text":"<pre><code>read_parquet(\n    fnames: Union[str, Iterable[str]],\n    append: bool = False,\n    verbose: bool = False,\n    colnames: Iterable[str] = (),\n) -&gt; DataFrame\n</code></pre> <p>Read a parquet file.</p> PARAMETER DESCRIPTION <code>fnames</code> <p>The filepath of the parquet file(s) to be read.</p> <p> TYPE: <code>Union[str, Iterable[str]]</code> </p> <code>append</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML, should the content of the CSV files in <code>fnames</code> be appended or replace the existing data?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>verbose</code> <p>If True, when <code>fnames</code> are urls, the filenames are printed to stdout during the download.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_parquet(\n    self,\n    fnames: Union[str, Iterable[str]],\n    append: bool = False,\n    verbose: bool = False,\n    colnames: Iterable[str] = (),\n) -&gt; DataFrame:\n    \"\"\"Read a parquet file.\n\n    Args:\n        fnames:\n            The filepath of the parquet file(s) to be read.\n\n        append:\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            the CSV files in `fnames` be appended or replace the\n            existing data?\n\n        verbose:\n            If True, when `fnames` are urls, the filenames are printed to\n            stdout during the download.\n\n    Returns:\n        Handler of the underlying data.\n    \"\"\"\n\n    if isinstance(fnames, str):\n        fnames = [fnames]\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    if not colnames:\n        colnames = self.colnames\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than\n            zero columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_parquet(...).\"\"\"\n        )\n\n    fnames = _retrieve_urls(fnames, verbose)\n\n    readers = (pq.ParquetFile(fname) for fname in fnames)\n\n    for reader in readers:\n        inferred_schema = reader.schema_arrow\n        preprocessed_schema = preprocess_arrow_schema(inferred_schema, self.roles)\n        read_arrow_batches(\n            reader.iter_batches(columns=colnames),\n            preprocessed_schema,\n            self,\n            append,\n        )\n        if not append:\n            append = True\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.read_s3","title":"read_s3","text":"<pre><code>read_s3(\n    bucket: str,\n    keys: Iterable[str],\n    region: str,\n    append: bool = False,\n    sep: str = \",\",\n    num_lines_read: int = 0,\n    skip: int = 0,\n    colnames: Optional[Iterable[str]] = None,\n    time_formats: Optional[Iterable[str]] = None,\n) -&gt; DataFrame\n</code></pre> <p>Read CSV files from an S3 bucket.</p> <p>It is assumed that the first line of each CSV file contains a header with the column names.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> Note <p>Note that S3 is not supported on Windows.</p> PARAMETER DESCRIPTION <code>bucket</code> <p>The bucket from which to read the files.</p> <p> TYPE: <code>str</code> </p> <code>keys</code> <p>The list of keys (files in the bucket) to be read.</p> <p> TYPE: <code>Iterable[str]</code> </p> <code>region</code> <p>The region in which the bucket is located.</p> <p> TYPE: <code>str</code> </p> <code>append</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML, should the content of the CSV files in <code>fnames</code> be appended or replace the existing data?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sep</code> <p>The separator used for separating fields.</p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>num_lines_read</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>skip</code> <p>Number of lines to skip at the beginning of each file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>colnames</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <p> TYPE: <code>Optional[Iterable[str]]</code> DEFAULT: <code>None</code> </p> <code>time_formats</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[Iterable[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_s3(\n    self,\n    bucket: str,\n    keys: Iterable[str],\n    region: str,\n    append: bool = False,\n    sep: str = \",\",\n    num_lines_read: int = 0,\n    skip: int = 0,\n    colnames: Optional[Iterable[str]] = None,\n    time_formats: Optional[Iterable[str]] = None,\n) -&gt; DataFrame:\n    \"\"\"Read CSV files from an S3 bucket.\n\n    It is assumed that the first line of each CSV file contains a\n    header with the column names.\n\n    enterprise-adm: Enterprise edition\n        This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the [benefits of the Enterprise edition][enterprise-benefits] and [compare their features][enterprise-feature-list].\n\n        For licensing information and technical support, please [contact us][contact-page].\n\n    Note:\n        Note that S3 is not supported on Windows.\n\n    Args:\n        bucket:\n            The bucket from which to read the files.\n\n        keys:\n            The list of keys (files in the bucket) to be read.\n\n        region:\n            The region in which the bucket is located.\n\n        append:\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            the CSV files in `fnames` be appended or replace the\n            existing data?\n\n        sep:\n            The separator used for separating fields.\n\n        num_lines_read:\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        skip:\n            Number of lines to skip at the beginning of each file.\n\n        colnames:\n            The first line of a CSV file\n            usually contains the column names.\n            When this is not the case, you need to explicitly pass them.\n\n        time_formats:\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n    Returns:\n            Handler of the underlying data.\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    if isinstance(keys, str):\n        keys = [keys]\n\n    if not isinstance(bucket, str):\n        raise TypeError(\"'bucket' must be str.\")\n\n    if not _is_non_empty_typed_list(keys, str):\n        raise TypeError(\"'keys' must be either a string or a list of str\")\n\n    if not isinstance(region, str):\n        raise TypeError(\"'region' must be str.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be str.\")\n\n    if not isinstance(num_lines_read, numbers.Real):\n        raise TypeError(\"'num_lines_read' must be a real number\")\n\n    if not isinstance(skip, numbers.Real):\n        raise TypeError(\"'skip' must be a real number\")\n\n    if not _is_non_empty_typed_list(time_formats, str):\n        raise TypeError(\"'time_formats' must be a non-empty list of str\")\n\n    if colnames is not None and not _is_non_empty_typed_list(colnames, str):\n        raise TypeError(\n            \"'colnames' must be either be None or a non-empty list of str.\"\n        )\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_s3(...).\"\"\"\n        )\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.read_s3\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"append_\"] = append\n    cmd[\"bucket_\"] = bucket\n    cmd[\"keys_\"] = keys\n    cmd[\"region_\"] = region\n    cmd[\"sep_\"] = sep\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"num_lines_read_\"] = num_lines_read\n    cmd[\"skip_\"] = skip\n\n    if colnames is not None:\n        cmd[\"colnames_\"] = colnames\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.read_view","title":"read_view","text":"<pre><code>read_view(view: View, append: bool = False) -&gt; DataFrame\n</code></pre> <p>Read the data from a <code>View</code>.</p> PARAMETER DESCRIPTION <code>view</code> <p>The view to read.</p> <p> TYPE: <code>View</code> </p> <code>append</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML, should the content of the CSV files in <code>fnames</code> be appended or replace the existing data?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_view(\n    self,\n    view: View,\n    append: bool = False,\n) -&gt; DataFrame:\n    \"\"\"Read the data from a [`View`][getml.data.View].\n\n    Args:\n        view:\n            The view to read.\n\n        append:\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            the CSV files in `fnames` be appended or replace the\n            existing data?\n\n    Returns:\n            Handler of the underlying data.\n\n    \"\"\"\n\n    if not isinstance(view, View):\n        raise TypeError(\"'view' must be getml.data.View.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    view.check()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.from_view\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"view_\"] = view._getml_deserialize()\n\n    cmd[\"append_\"] = append\n\n    comm.send(cmd)\n\n    return self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.read_db","title":"read_db","text":"<pre><code>read_db(\n    table_name: str,\n    append: bool = False,\n    conn: Optional[Connection] = None,\n) -&gt; DataFrame\n</code></pre> <p>Fill from Database.</p> <p>The DataFrame will be filled from a table in the database.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Table from which we want to retrieve the data.</p> <p> TYPE: <code>str</code> </p> <code>append</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML, should the content of <code>table_name</code> be appended or replace the existing data?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the Engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_db(\n    self, table_name: str, append: bool = False, conn: Optional[Connection] = None\n) -&gt; DataFrame:\n    \"\"\"\n    Fill from Database.\n\n    The DataFrame will be filled from a table in the database.\n\n    Args:\n        table_name:\n            Table from which we want to retrieve the data.\n\n        append:\n            If a data frame object holding the same ``name`` is\n            already present in the getML, should the content of\n            `table_name` be appended or replace the existing data?\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the Engine will use the default connection.\n\n    Returns:\n            Handler of the underlying data.\n    \"\"\"\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be str.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_db(...).\"\"\"\n        )\n\n    conn = conn or database.Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.from_db\"\n    cmd[\"name_\"] = self.name\n    cmd[\"table_name_\"] = table_name\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    cmd[\"append_\"] = append\n\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.read_pandas","title":"read_pandas","text":"<pre><code>read_pandas(\n    pandas_df: DataFrame, append: bool = False\n) -&gt; DataFrame\n</code></pre> <p>Uploads a <code>pandas.DataFrame</code>.</p> <p>Replaces the actual content of the underlying data frame in the getML Engine with <code>pandas_df</code>.</p> PARAMETER DESCRIPTION <code>pandas_df</code> <p>Data the underlying data frame object in the getML Engine should obtain.</p> <p> TYPE: <code>DataFrame</code> </p> <code>append</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML Engine, should the content in <code>query</code> be appended or replace the existing data?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> <p>Note:     For columns containing <code>pandas.Timestamp</code> there can     occur small inconsistencies in the order of microseconds     when sending the data to the getML Engine. This is due to     the way the underlying information is stored.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_pandas(self, pandas_df: pd.DataFrame, append: bool = False) -&gt; DataFrame:\n    \"\"\"Uploads a `pandas.DataFrame`.\n\n    Replaces the actual content of the underlying data frame in\n    the getML Engine with `pandas_df`.\n\n    Args:\n        pandas_df:\n            Data the underlying data frame object in the getML\n            Engine should obtain.\n\n        append:\n            If a data frame object holding the same ``name`` is\n            already present in the getML Engine, should the content in\n            `query` be appended or replace the existing data?\n\n    Returns:\n            Handler of the underlying data.\n    Note:\n        For columns containing `pandas.Timestamp` there can\n        occur small inconsistencies in the order of microseconds\n        when sending the data to the getML Engine. This is due to\n        the way the underlying information is stored.\n    \"\"\"\n\n    if not isinstance(pandas_df, pd.DataFrame):\n        raise TypeError(\"'pandas_df' must be of type pandas.DataFrame.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_pandas(...).\"\"\"\n        )\n\n    inferred_schema = pa.Schema.from_pandas(pandas_df[self.columns])\n    preprocessed_schema = preprocess_arrow_schema(inferred_schema, self.roles)\n\n    table = pa.Table.from_pandas(pandas_df[self.columns], preserve_index=False)\n    table.cast(preprocessed_schema)\n\n    return self.read_arrow(table, append=append)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.read_pyspark","title":"read_pyspark","text":"<pre><code>read_pyspark(\n    spark_df: DataFrame, append: bool = False\n) -&gt; DataFrame\n</code></pre> <p>Uploads a <code>pyspark.sql.DataFrame</code>.</p> <p>Replaces the actual content of the underlying data frame in the getML Engine with <code>pandas_df</code>.</p> PARAMETER DESCRIPTION <code>spark_df</code> <p>Data the underlying data frame object in the getML Engine should obtain.</p> <p> TYPE: <code>DataFrame</code> </p> <code>append</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML Engine, should the content in <code>query</code> be appended or replace the existing data?</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_pyspark(\n    self, spark_df: pyspark.sql.DataFrame, append: bool = False\n) -&gt; DataFrame:\n    \"\"\"Uploads a `pyspark.sql.DataFrame`.\n\n    Replaces the actual content of the underlying data frame in\n    the getML Engine with `pandas_df`.\n\n    Args:\n        spark_df:\n            Data the underlying data frame object in the getML\n            Engine should obtain.\n\n        append:\n            If a data frame object holding the same ``name`` is\n            already present in the getML Engine, should the content in\n            `query` be appended or replace the existing data?\n\n    Returns:\n            Handler of the underlying data.\n    \"\"\"\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be bool.\")\n\n    temp_dir = _retrieve_temp_dir()\n    path = temp_dir / str(self.name)\n    spark_df.write.mode(\"overwrite\").parquet(str(path))\n\n    filepaths = [\n        os.path.join(path, filepath)\n        for filepath in os.listdir(path)\n        if filepath[-8:] == \".parquet\"\n    ]\n\n    for i, filepath in enumerate(filepaths):\n        self.read_parquet(filepath, append or i &gt; 0)\n\n    shutil.rmtree(path)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.read_query","title":"read_query","text":"<pre><code>read_query(\n    query: str,\n    append: Optional[bool] = False,\n    conn: Optional[Connection] = None,\n) -&gt; DataFrame\n</code></pre> <p>Fill from query</p> <p>Fills the data frame with data from a table in the database.</p> PARAMETER DESCRIPTION <code>query</code> <p>The query used to retrieve the data.</p> <p> TYPE: <code>str</code> </p> <code>append</code> <p>If a data frame object holding the same <code>name</code> is already present in the getML Engine, should the content in <code>query</code> be appended or replace the existing data?</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>False</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the Engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Handler of the underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def read_query(\n    self,\n    query: str,\n    append: Optional[bool] = False,\n    conn: Optional[Connection] = None,\n) -&gt; DataFrame:\n    \"\"\"Fill from query\n\n    Fills the data frame with data from a table in the database.\n\n    Args:\n        query:\n            The query used to retrieve the data.\n\n        append:\n            If a data frame object holding the same ``name`` is\n            already present in the getML Engine, should the content in\n            `query` be appended or replace the existing data?\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the Engine will use the default connection.\n\n    Returns:\n            Handler of the underlying data.\n    \"\"\"\n\n    if self.ncols() == 0:\n        raise Exception(\n            \"\"\"Reading data is only possible in a DataFrame with more than zero\n            columns. You can pre-define columns during\n            initialization of the DataFrame or use the classmethod\n            from_db(...).\"\"\"\n        )\n\n    if not isinstance(query, str):\n        raise TypeError(\"'query' must be of type str\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be of type bool\")\n\n    conn = conn or database.Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.from_query\"\n    cmd[\"name_\"] = self.name\n    cmd[\"query_\"] = query\n\n    cmd[\"categorical_\"] = self._categorical_names\n    cmd[\"join_keys_\"] = self._join_key_names\n    cmd[\"numerical_\"] = self._numerical_names\n    cmd[\"targets_\"] = self._target_names\n    cmd[\"text_\"] = self._text_names\n    cmd[\"time_stamps_\"] = self._time_stamp_names\n    cmd[\"unused_floats_\"] = self._unused_float_names\n    cmd[\"unused_strings_\"] = self._unused_string_names\n\n    cmd[\"append_\"] = append\n\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.refresh","title":"refresh","text":"<pre><code>refresh() -&gt; DataFrame\n</code></pre> <p>Aligns meta-information of the current instance with the corresponding data frame in the getML Engine.</p> <p>This method can be used to avoid encoding conflicts. Note that <code>load</code> as well as several other methods automatically call <code>refresh</code>.</p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Updated handle the underlying data frame in the getML Engine.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def refresh(self) -&gt; DataFrame:\n    \"\"\"Aligns meta-information of the current instance with the\n    corresponding data frame in the getML Engine.\n\n    This method can be used to avoid encoding conflicts. Note that\n    [`load`][getml.DataFrame.load] as well as several other\n    methods automatically call [`refresh`][getml.DataFrame.refresh].\n\n    Returns:\n            Updated handle the underlying data frame in the getML\n                Engine.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"DataFrame.refresh\"\n    cmd[\"name_\"] = self.name\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n\n    if msg[0] != \"{\":\n        comm.handle_engine_exception(msg)\n\n    roles = json.loads(msg)\n\n    self.__init__(name=cast(str, self.name), roles=roles)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.remove_subroles","title":"remove_subroles","text":"<pre><code>remove_subroles(\n    cols: Union[\n        str,\n        FloatColumn,\n        StringColumn,\n        List[Union[str, FloatColumn, StringColumn]],\n    ]\n) -&gt; None\n</code></pre> <p>Removes all <code>subroles</code> from one or more columns.</p> PARAMETER DESCRIPTION <code>cols</code> <p>The columns or the names thereof.</p> <p> TYPE: <code>Union[str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]]</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def remove_subroles(\n    self,\n    cols: Union[\n        str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]\n    ],\n) -&gt; None:\n    \"\"\"Removes all [`subroles`][getml.data.subroles] from one or more columns.\n\n    Args:\n        cols:\n            The columns or the names thereof.\n    \"\"\"\n\n    names = _handle_cols(cols)\n\n    for name in names:\n        self._set_subroles(name, append=False, subroles=[])\n\n    self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.remove_unit","title":"remove_unit","text":"<pre><code>remove_unit(\n    cols: Union[\n        str,\n        FloatColumn,\n        StringColumn,\n        List[Union[str, FloatColumn, StringColumn]],\n    ]\n)\n</code></pre> <p>Removes the unit from one or more columns.</p> PARAMETER DESCRIPTION <code>cols</code> <p>The columns or the names thereof.</p> <p> TYPE: <code>Union[str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]]</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def remove_unit(\n    self,\n    cols: Union[\n        str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]\n    ],\n):\n    \"\"\"Removes the unit from one or more columns.\n\n    Args:\n        cols:\n            The columns or the names thereof.\n    \"\"\"\n\n    names = _handle_cols(cols)\n\n    for name in names:\n        self._set_unit(name, \"\")\n\n    self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.save","title":"save","text":"<pre><code>save() -&gt; DataFrame\n</code></pre> <p>Writes the underlying data in the getML Engine to disk.</p> RETURNS DESCRIPTION <code>DataFrame</code> <p>The current instance.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def save(self) -&gt; DataFrame:\n    \"\"\"Writes the underlying data in the getML Engine to disk.\n\n    Returns:\n            The current instance.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.save\"\n    cmd[\"name_\"] = self.name\n\n    comm.send(cmd)\n\n    return self\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.set_role","title":"set_role","text":"<pre><code>set_role(\n    cols: Union[\n        str,\n        FloatColumn,\n        StringColumn,\n        List[Union[str, FloatColumn, StringColumn]],\n    ],\n    role: str,\n    time_formats: Optional[Iterable[str]] = None,\n)\n</code></pre> <p>Assigns a new role to one or more columns.</p> <p>When switching from a role based on type float to a role based on type string or vice verse, an implicit type conversion will be conducted. The <code>time_formats</code> argument is used to interpret Time Stamps. For more information on roles, please refer to the User Guide.</p> PARAMETER DESCRIPTION <code>cols</code> <p>The columns or the names of the columns.</p> <p> TYPE: <code>Union[str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]]</code> </p> <code>role</code> <p>The role to be assigned.</p> <p> TYPE: <code>str</code> </p> <code>time_formats</code> <p>Formats to be used to parse the time stamps. This is only necessary, if an implicit conversion from a StringColumn to a time stamp is taking place.</p> <p> TYPE: <code>Optional[Iterable[str]]</code> DEFAULT: <code>None</code> </p> Example <p><pre><code>data_df = dict(\n    animal=[\"hawk\", \"parrot\", \"goose\"],\n    votes=[12341, 5127, 65311],\n    date=[\"04/06/2019\", \"01/03/2019\", \"24/12/2018\"])\ndf = getml.DataFrame.from_dict(data_df, \"animal_elections\")\ndf.set_role(['animal'], getml.data.roles.categorical)\ndf.set_role(['votes'], getml.data.roles.numerical)\ndf.set_role(\n    ['date'], getml.data.roles.time_stamp, time_formats=['%d/%m/%Y'])\n\ndf\n</code></pre> <pre><code>| date                        | animal      | votes     |\n| time stamp                  | categorical | numerical |\n---------------------------------------------------------\n| 2019-06-04T00:00:00.000000Z | hawk        | 12341     |\n| 2019-03-01T00:00:00.000000Z | parrot      | 5127      |\n| 2018-12-24T00:00:00.000000Z | goose       | 65311     |\n</code></pre></p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def set_role(\n    self,\n    cols: Union[\n        str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]\n    ],\n    role: str,\n    time_formats: Optional[Iterable[str]] = None,\n):\n    \"\"\"Assigns a new role to one or more columns.\n\n    When switching from a role based on type float to a role based on type\n    string or vice verse, an implicit type conversion will be conducted.\n    The `time_formats` argument is used to interpret [Time Stamps][annotating-data-time-stamp]. For more information on\n    roles, please refer to the [User Guide][annotating-data].\n\n    Args:\n        cols:\n            The columns or the names of the columns.\n\n        role:\n            The role to be assigned.\n\n        time_formats:\n            Formats to be used to parse the time stamps.\n            This is only necessary, if an implicit conversion from a StringColumn to\n            a time stamp is taking place.\n\n    ??? example\n        ```python\n        data_df = dict(\n            animal=[\"hawk\", \"parrot\", \"goose\"],\n            votes=[12341, 5127, 65311],\n            date=[\"04/06/2019\", \"01/03/2019\", \"24/12/2018\"])\n        df = getml.DataFrame.from_dict(data_df, \"animal_elections\")\n        df.set_role(['animal'], getml.data.roles.categorical)\n        df.set_role(['votes'], getml.data.roles.numerical)\n        df.set_role(\n            ['date'], getml.data.roles.time_stamp, time_formats=['%d/%m/%Y'])\n\n        df\n        ```\n        ```\n        | date                        | animal      | votes     |\n        | time stamp                  | categorical | numerical |\n        ---------------------------------------------------------\n        | 2019-06-04T00:00:00.000000Z | hawk        | 12341     |\n        | 2019-03-01T00:00:00.000000Z | parrot      | 5127      |\n        | 2018-12-24T00:00:00.000000Z | goose       | 65311     |\n        ```\n    \"\"\"\n    # ------------------------------------------------------------\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    # ------------------------------------------------------------\n\n    names = _handle_cols(cols)\n\n    if not isinstance(role, str):\n        raise TypeError(\"'role' must be str.\")\n\n    if not _is_non_empty_typed_list(time_formats, str):\n        raise TypeError(\"'time_formats' must be a non-empty list of str\")\n\n    # ------------------------------------------------------------\n\n    for nname in names:\n        if nname not in self.colnames:\n            raise ValueError(\"No column called '\" + nname + \"' found.\")\n\n    if role not in self._all_roles:\n        raise ValueError(\n            \"'role' must be one of the following values: \" + str(self._all_roles)\n        )\n\n    # ------------------------------------------------------------\n\n    for name in names:\n        if self[name].role != role:\n            self._set_role(name, role, time_formats)\n\n    # ------------------------------------------------------------\n\n    self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.set_subroles","title":"set_subroles","text":"<pre><code>set_subroles(\n    cols: Union[\n        str,\n        FloatColumn,\n        StringColumn,\n        List[Union[str, FloatColumn, StringColumn]],\n    ],\n    subroles: Optional[\n        Union[Subrole, Iterable[str]]\n    ] = None,\n    append: Optional[bool] = True,\n)\n</code></pre> <p>Assigns one or several new <code>subroles</code> to one or more columns.</p> PARAMETER DESCRIPTION <code>cols</code> <p>The columns or the names thereof.</p> <p> TYPE: <code>Union[str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]]</code> </p> <code>subroles</code> <p>The subroles to be assigned. Must be from <code>subroles</code>.</p> <p> TYPE: <code>Optional[Union[Subrole, Iterable[str]]]</code> DEFAULT: <code>None</code> </p> <code>append</code> <p>Whether you want to append the new subroles to the existing subroles.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>True</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def set_subroles(\n    self,\n    cols: Union[\n        str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]\n    ],\n    subroles: Optional[Union[Subrole, Iterable[str]]] = None,\n    append: Optional[bool] = True,\n):\n    \"\"\"Assigns one or several new [`subroles`][getml.data.subroles] to one or more columns.\n\n    Args:\n        cols:\n            The columns or the names thereof.\n\n        subroles:\n            The subroles to be assigned.\n            Must be from [`subroles`][getml.data.subroles].\n\n        append:\n            Whether you want to append the\n            new subroles to the existing subroles.\n    \"\"\"\n\n    names = _handle_cols(cols)\n\n    if isinstance(subroles, str):\n        subroles = [subroles]\n\n    if not _is_non_empty_typed_list(subroles, str):\n        raise TypeError(\"'subroles' must be either a string or a list of strings.\")\n\n    if not isinstance(append, bool):\n        raise TypeError(\"'append' must be a bool.\")\n\n    for name in names:\n        self._set_subroles(name, append, subroles)\n\n    self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.set_unit","title":"set_unit","text":"<pre><code>set_unit(\n    cols: Union[\n        str,\n        FloatColumn,\n        StringColumn,\n        List[Union[str, FloatColumn, StringColumn]],\n    ],\n    unit: str,\n    comparison_only: bool = False,\n)\n</code></pre> <p>Assigns a new unit to one or more columns.</p> PARAMETER DESCRIPTION <code>cols</code> <p>The columns or the names thereof.</p> <p> TYPE: <code>Union[str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]]</code> </p> <code>unit</code> <p>The unit to be assigned.</p> <p> TYPE: <code>str</code> </p> <code>comparison_only</code> <p>Whether you want the column to be used for comparison only. This means that the column can only be used in comparison to other columns of the same unit.</p> <p>An example might be a bank account number: The number in itself is hardly interesting, but it might be useful to know how often we have seen that same bank account number in another table.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def set_unit(\n    self,\n    cols: Union[\n        str, FloatColumn, StringColumn, List[Union[str, FloatColumn, StringColumn]]\n    ],\n    unit: str,\n    comparison_only: bool = False,\n):\n    \"\"\"Assigns a new unit to one or more columns.\n\n    Args:\n        cols:\n            The columns or the names thereof.\n\n        unit:\n            The unit to be assigned.\n\n        comparison_only:\n            Whether you want the column to\n            be used for comparison only. This means that the column can\n            only be used in comparison to other columns of the same unit.\n\n            An example might be a bank account number: The number in itself\n            is hardly interesting, but it might be useful to know how often\n            we have seen that same bank account number in another table.\n    \"\"\"\n\n    names = _handle_cols(cols)\n\n    if not isinstance(unit, str):\n        raise TypeError(\"Parameter 'unit' must be a str.\")\n\n    if comparison_only:\n        unit += COMPARISON_ONLY\n\n    for name in names:\n        self._set_unit(name, unit)\n\n    self.refresh()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.to_arrow","title":"to_arrow","text":"<pre><code>to_arrow() -&gt; Table\n</code></pre> <p>Creates a <code>pyarrow.Table</code> from the current instance.</p> <p>Loads the underlying data from the getML Engine and constructs a <code>pyarrow.Table</code>.</p> RETURNS DESCRIPTION <code>Table</code> <p>Pyarrow equivalent of the current instance including its underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_arrow(self) -&gt; pa.Table:\n    \"\"\"Creates a `pyarrow.Table` from the current instance.\n\n    Loads the underlying data from the getML Engine and constructs\n    a `pyarrow.Table`.\n\n    Returns:\n            Pyarrow equivalent of the current instance including its underlying data.\n    \"\"\"\n    return to_arrow(self)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.to_csv","title":"to_csv","text":"<pre><code>to_csv(\n    fname: str,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    batch_size: int = DEFAULT_BATCH_SIZE,\n    quoting_style: str = \"needed\",\n)\n</code></pre> <p>Writes the underlying data into a newly created CSV file.</p> PARAMETER DESCRIPTION <code>fname</code> <p>The name of the CSV file. The ending \".csv\" and an optional batch number will be added automatically.</p> <p> TYPE: <code>str</code> </p> <code>quotechar</code> <p>The character used to wrap strings.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'\"'</code> </p> <code>sep</code> <p>The character used for separating fields.</p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>batch_size</code> <p>Maximum number of lines per file. Set to 0 to read the entire data frame into a single file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>DEFAULT_BATCH_SIZE</code> </p> <code>quoting_style</code> <p>The quoting style to use. Delegated to pyarrow.</p> <p>The following values are accepted: - <code>\"needed\"</code> (default): only enclose values in quotes when needed. - <code>\"all_valid\"</code>: enclose all valid values in quotes; nulls are not   quoted. - <code>\"none\"</code>: do not enclose any values in quotes; values containing   special characters (such as quotes, cell delimiters or line   endings) will raise an error.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'needed'</code> </p> Deprecated <p>1.5: The <code>quotechar</code> parameter is deprecated.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_csv(\n    self,\n    fname: str,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    batch_size: int = DEFAULT_BATCH_SIZE,\n    quoting_style: str = \"needed\",\n):\n    \"\"\"\n    Writes the underlying data into a newly created CSV file.\n\n    Args:\n        fname:\n            The name of the CSV file.\n            The ending \".csv\" and an optional batch number will\n            be added automatically.\n\n        quotechar:\n            The character used to wrap strings.\n\n        sep:\n            The character used for separating fields.\n\n        batch_size:\n            Maximum number of lines per file. Set to 0 to read\n            the entire data frame into a single file.\n\n        quoting_style (str):\n            The quoting style to use. Delegated to pyarrow.\n\n            The following values are accepted:\n            - `\"needed\"` (default): only enclose values in quotes when needed.\n            - `\"all_valid\"`: enclose all valid values in quotes; nulls are not\n              quoted.\n            - `\"none\"`: do not enclose any values in quotes; values containing\n              special characters (such as quotes, cell delimiters or line\n              endings) will raise an error.\n\n    Deprecated:\n       1.5: The `quotechar` parameter is deprecated.\n    \"\"\"\n\n    if quotechar != '\"':\n        warnings.warn(\n            \"'quotechar' is deprecated, use 'quoting_style' instead.\",\n            DeprecationWarning,\n        )\n\n    to_csv(self, fname, sep, batch_size, quoting_style)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.to_db","title":"to_db","text":"<pre><code>to_db(table_name: str, conn: Optional[Connection] = None)\n</code></pre> <p>Writes the underlying data into a newly created table in the database.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to be created.</p> <p>If a table of that name already exists, it will be replaced.</p> <p> TYPE: <code>str</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the Engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_db(self, table_name: str, conn: Optional[Connection] = None):\n    \"\"\"Writes the underlying data into a newly created table in the\n    database.\n\n    Args:\n        table_name:\n            Name of the table to be created.\n\n            If a table of that name already exists, it will be\n            replaced.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the Engine will use the default connection.\n    \"\"\"\n\n    conn = conn or database.Connection()\n\n    self.refresh()\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be of type str\")\n\n    cmd = {}\n\n    cmd[\"type_\"] = \"DataFrame.to_db\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"table_name_\"] = table_name\n\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.to_html","title":"to_html","text":"<pre><code>to_html(max_rows: int = 10)\n</code></pre> <p>Represents the data frame in HTML format, optimized for an iPython notebook.</p> PARAMETER DESCRIPTION <code>max_rows</code> <p>The maximum number of rows to be displayed.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_html(self, max_rows: int = 10):\n    \"\"\"\n    Represents the data frame in HTML format, optimized for an\n    iPython notebook.\n\n    Args:\n        max_rows:\n            The maximum number of rows to be displayed.\n    \"\"\"\n\n    if not _exists_in_memory(self.name):\n        return _empty_data_frame().replace(\"\\n\", \"&lt;br&gt;\")\n\n    formatted = self._format()\n    formatted.max_rows = max_rows\n\n    footer = self._collect_footer_data()\n\n    return formatted._render_html(footer=footer)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.to_json","title":"to_json","text":"<pre><code>to_json()\n</code></pre> <p>Creates a JSON string from the current instance.</p> <p>Loads the underlying data from the getML Engine and constructs a JSON string.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_json(self):\n    \"\"\"Creates a JSON string from the current instance.\n\n    Loads the underlying data from the getML Engine and constructs\n    a JSON string.\n    \"\"\"\n    return self.to_pandas().to_json()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas() -&gt; DataFrame\n</code></pre> <p>Creates a <code>pandas.DataFrame</code> from the current instance.</p> <p>Loads the underlying data from the getML Engine and constructs <code>pandas.DataFrame</code>.</p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Pandas equivalent of the current instance including its underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"Creates a `pandas.DataFrame` from the current instance.\n\n    Loads the underlying data from the getML Engine and constructs\n    `pandas.DataFrame`.\n\n    Returns:\n            Pandas equivalent of the current instance including\n                its underlying data.\n\n    \"\"\"\n    return to_arrow(self).to_pandas()\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.to_parquet","title":"to_parquet","text":"<pre><code>to_parquet(\n    fname: str,\n    compression: Literal[\n        \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"\n    ] = \"snappy\",\n)\n</code></pre> <p>Writes the underlying data into a newly created parquet file.</p> PARAMETER DESCRIPTION <code>fname</code> <p>The name of the parquet file. The ending \".parquet\" will be added automatically.</p> <p> TYPE: <code>str</code> </p> <code>compression</code> <p>The compression format to use. Supported values are \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"</p> <p> TYPE: <code>Literal['brotli', 'gzip', 'lz4', 'snappy', 'zstd']</code> DEFAULT: <code>'snappy'</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_parquet(\n    self,\n    fname: str,\n    compression: Literal[\"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"] = \"snappy\",\n):\n    \"\"\"\n    Writes the underlying data into a newly created parquet file.\n\n    Args:\n        fname:\n            The name of the parquet file.\n            The ending \".parquet\" will be added automatically.\n\n        compression:\n            The compression format to use.\n            Supported values are \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"\n    \"\"\"\n    to_parquet(self, fname, compression)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.to_placeholder","title":"to_placeholder","text":"<pre><code>to_placeholder(name: Optional[str] = None) -&gt; Placeholder\n</code></pre> <p>Generates a <code>Placeholder</code> from the current <code>DataFrame</code>.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the placeholder. If no name is passed, then the name of the placeholder will be identical to the name of the current data frame.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Placeholder</code> <p>A placeholder with the same name as this data frame.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_placeholder(self, name: Optional[str] = None) -&gt; Placeholder:\n    \"\"\"Generates a [`Placeholder`][getml.data.Placeholder] from the\n    current [`DataFrame`][getml.DataFrame].\n\n    Args:\n        name:\n            The name of the placeholder. If no\n            name is passed, then the name of the placeholder will\n            be identical to the name of the current data frame.\n\n    Returns:\n            A placeholder with the same name as this data frame.\n\n\n    \"\"\"\n    self.refresh()\n    return Placeholder(name=name or self.name, roles=self.roles)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.to_pyspark","title":"to_pyspark","text":"<pre><code>to_pyspark(\n    spark: SparkSession, name: Optional[str] = None\n) -&gt; DataFrame\n</code></pre> <p>Creates a <code>pyspark.sql.DataFrame</code> from the current instance.</p> <p>Loads the underlying data from the getML Engine and constructs a <code>pyspark.sql.DataFrame</code>.</p> PARAMETER DESCRIPTION <code>spark</code> <p>The pyspark session in which you want to create the data frame.</p> <p> TYPE: <code>SparkSession</code> </p> <code>name</code> <p>The name of the temporary view to be created on top of the <code>pyspark.sql.DataFrame</code>, with which it can be referred to in Spark SQL (refer to <code>pyspark.sql.DataFrame.createOrReplaceTempView</code>). If None is passed, then the name of this <code>DataFrame</code> will be used.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Pyspark equivalent of the current instance including its underlying data.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_pyspark(\n    self, spark: pyspark.sql.SparkSession, name: Optional[str] = None\n) -&gt; pyspark.sql.DataFrame:\n    \"\"\"Creates a `pyspark.sql.DataFrame` from the current instance.\n\n    Loads the underlying data from the getML Engine and constructs\n    a `pyspark.sql.DataFrame`.\n\n    Args:\n        spark:\n            The pyspark session in which you want to\n            create the data frame.\n\n        name:\n            The name of the temporary view to be created on top\n            of the `pyspark.sql.DataFrame`,\n            with which it can be referred to\n            in Spark SQL (refer to\n            `pyspark.sql.DataFrame.createOrReplaceTempView`).\n            If None is passed, then the name of this\n            [`DataFrame`][getml.DataFrame] will be used.\n\n    Returns:\n            Pyspark equivalent of the current instance including its underlying data.\n\n    \"\"\"\n    return _to_pyspark(self, name, spark)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.to_s3","title":"to_s3","text":"<pre><code>to_s3(\n    bucket: str,\n    key: str,\n    region: str,\n    sep: Optional[str] = \",\",\n    batch_size: Optional[int] = 50000,\n)\n</code></pre> <p>Writes the underlying data into a newly created CSV file located in an S3 bucket.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> Note <p>Note that S3 is not supported on Windows.</p> PARAMETER DESCRIPTION <code>bucket</code> <p>The bucket from which to read the files.</p> <p> TYPE: <code>str</code> </p> <code>key</code> <p>The key in the S3 bucket in which you want to write the output. The ending \".csv\" and an optional batch number will be added automatically.</p> <p> TYPE: <code>str</code> </p> <code>region</code> <p>The region in which the bucket is located.</p> <p> TYPE: <code>str</code> </p> <code>sep</code> <p>The character used for separating fields.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>','</code> </p> <code>batch_size</code> <p>Maximum number of lines per file. Set to 0 to read the entire data frame into a single file.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>50000</code> </p> Example <pre><code>getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\ngetml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\nyour_df.to_s3(\n    bucket=\"your-bucket-name\",\n    key=\"filename-on-s3\",\n    region=\"us-east-2\",\n    sep=';'\n)\n</code></pre> Source code in <code>getml/data/data_frame.py</code> <pre><code>def to_s3(\n    self,\n    bucket: str,\n    key: str,\n    region: str,\n    sep: Optional[str] = \",\",\n    batch_size: Optional[int] = 50000,\n):\n    \"\"\"\n    Writes the underlying data into a newly created CSV file\n    located in an S3 bucket.\n\n    enterprise-adm: Enterprise edition\n        This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the [benefits of the Enterprise edition][enterprise-benefits] and [compare their features][enterprise-feature-list].\n\n        For licensing information and technical support, please [contact us][contact-page].\n\n    Note:\n        Note that S3 is not supported on Windows.\n\n    Args:\n        bucket:\n            The bucket from which to read the files.\n\n        key:\n            The key in the S3 bucket in which you want to\n            write the output. The ending \".csv\" and an optional\n            batch number will be added automatically.\n\n        region:\n            The region in which the bucket is located.\n\n        sep:\n            The character used for separating fields.\n\n        batch_size:\n            Maximum number of lines per file. Set to 0 to read\n            the entire data frame into a single file.\n\n    ??? example\n        ```python\n        getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n        getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n        your_df.to_s3(\n            bucket=\"your-bucket-name\",\n            key=\"filename-on-s3\",\n            region=\"us-east-2\",\n            sep=';'\n        )\n        ```\n\n    \"\"\"\n\n    self.refresh()\n\n    if not isinstance(bucket, str):\n        raise TypeError(\"'bucket' must be of type str\")\n\n    if not isinstance(key, str):\n        raise TypeError(\"'fname' must be of type str\")\n\n    if not isinstance(region, str):\n        raise TypeError(\"'region' must be of type str\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be of type str\")\n\n    if not isinstance(batch_size, numbers.Real):\n        raise TypeError(\"'batch_size' must be a real number\")\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"DataFrame.to_s3\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"bucket_\"] = bucket\n    cmd[\"key_\"] = key\n    cmd[\"region_\"] = region\n    cmd[\"sep_\"] = sep\n    cmd[\"batch_size_\"] = batch_size\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.unload","title":"unload","text":"<pre><code>unload()\n</code></pre> <p>Unloads the data frame from memory.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def unload(self):\n    \"\"\"\n    Unloads the data frame from memory.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    self._delete(mem_only=True)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.where","title":"where","text":"<pre><code>where(\n    index: Union[\n        Integral,\n        slice,\n        BooleanColumnView,\n        FloatColumnView,\n        FloatColumn,\n    ]\n) -&gt; View\n</code></pre> <p>Extract a subset of rows.</p> <p>Creates a new <code>View</code> as a subselection of the current instance.</p> PARAMETER DESCRIPTION <code>index</code> <p>Indicates the rows you want to select.</p> <p> TYPE: <code>Union[Integral, slice, BooleanColumnView, FloatColumnView, FloatColumn]</code> </p> RETURNS DESCRIPTION <code>View</code> <p>A new <code>View</code> containing the selected rows.</p> Example <p>Generate example data: <pre><code>data = dict(\n    fruit=[\"banana\", \"apple\", \"cherry\", \"cherry\", \"melon\", \"pineapple\"],\n    price=[2.4, 3.0, 1.2, 1.4, 3.4, 3.4],\n    join_key=[\"0\", \"1\", \"2\", \"2\", \"3\", \"3\"])\n\nfruits = getml.DataFrame.from_dict(data, name=\"fruits\",\nroles={\"categorical\": [\"fruit\"], \"join_key\": [\"join_key\"], \"numerical\": [\"price\"]})\n\nfruits\n</code></pre> <pre><code>| join_key | fruit       | price     |\n| join key | categorical | numerical |\n--------------------------------------\n| 0        | banana      | 2.4       |\n| 1        | apple       | 3         |\n| 2        | cherry      | 1.2       |\n| 2        | cherry      | 1.4       |\n| 3        | melon       | 3.4       |\n| 3        | pineapple   | 3.4       |\n</code></pre> Apply where condition. This creates a new DataFrame called \"cherries\":</p> <p><pre><code>cherries = fruits.where(\n    fruits[\"fruit\"] == \"cherry\")\n\ncherries\n</code></pre> <pre><code>| join_key | fruit       | price     |\n| join key | categorical | numerical |\n--------------------------------------\n| 2        | cherry      | 1.2       |\n| 2        | cherry      | 1.4       |\n</code></pre></p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def where(\n    self,\n    index: Union[\n        numbers.Integral, slice, BooleanColumnView, FloatColumnView, FloatColumn\n    ],\n) -&gt; View:\n    \"\"\"Extract a subset of rows.\n\n    Creates a new [`View`][getml.data.View] as a\n    subselection of the current instance.\n\n    Args:\n        index:\n            Indicates the rows you want to select.\n\n    Returns:\n            A new [`View`][getml.data.View] containing the selected rows.\n\n    ??? example\n        Generate example data:\n        ```python\n        data = dict(\n            fruit=[\"banana\", \"apple\", \"cherry\", \"cherry\", \"melon\", \"pineapple\"],\n            price=[2.4, 3.0, 1.2, 1.4, 3.4, 3.4],\n            join_key=[\"0\", \"1\", \"2\", \"2\", \"3\", \"3\"])\n\n        fruits = getml.DataFrame.from_dict(data, name=\"fruits\",\n        roles={\"categorical\": [\"fruit\"], \"join_key\": [\"join_key\"], \"numerical\": [\"price\"]})\n\n        fruits\n        ```\n        ```\n        | join_key | fruit       | price     |\n        | join key | categorical | numerical |\n        --------------------------------------\n        | 0        | banana      | 2.4       |\n        | 1        | apple       | 3         |\n        | 2        | cherry      | 1.2       |\n        | 2        | cherry      | 1.4       |\n        | 3        | melon       | 3.4       |\n        | 3        | pineapple   | 3.4       |\n        ```\n        Apply where condition. This creates a new DataFrame called \"cherries\":\n\n        ```python\n        cherries = fruits.where(\n            fruits[\"fruit\"] == \"cherry\")\n\n        cherries\n        ```\n        ```\n        | join_key | fruit       | price     |\n        | join key | categorical | numerical |\n        --------------------------------------\n        | 2        | cherry      | 1.2       |\n        | 2        | cherry      | 1.4       |\n        ```\n\n    \"\"\"\n\n    return _where(self, index)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.with_column","title":"with_column","text":"<pre><code>with_column(\n    col: Union[\n        bool,\n        str,\n        float,\n        int,\n        datetime64,\n        FloatColumn,\n        FloatColumnView,\n        StringColumn,\n        StringColumnView,\n        BooleanColumnView,\n    ],\n    name: str,\n    role: Optional[Role] = None,\n    subroles: Optional[\n        Union[Subrole, Iterable[str]]\n    ] = None,\n    unit: str = \"\",\n    time_formats: Optional[Iterable[str]] = None,\n)\n</code></pre> <p>Returns a new <code>View</code> that contains an additional column.</p> PARAMETER DESCRIPTION <code>col</code> <p>The column to be added.</p> <p> TYPE: <code>Union[bool, str, float, int, datetime64, FloatColumn, FloatColumnView, StringColumn, StringColumnView, BooleanColumnView]</code> </p> <code>name</code> <p>Name of the new column.</p> <p> TYPE: <code>str</code> </p> <code>role</code> <p>Role of the new column. Must be from <code>roles</code>.</p> <p> TYPE: <code>Optional[Role]</code> DEFAULT: <code>None</code> </p> <code>subroles</code> <p>Subroles of the new column. Must be from <code>subroles</code>.</p> <p> TYPE: <code>Optional[Union[Subrole, Iterable[str]]]</code> DEFAULT: <code>None</code> </p> <code>unit</code> <p>Unit of the column.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>time_formats</code> <p>Formats to be used to parse the time stamps.</p> <p>This is only necessary, if an implicit conversion from a <code>StringColumn</code> to a time stamp is taking place.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[Iterable[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def with_column(\n    self,\n    col: Union[\n        bool,\n        str,\n        float,\n        int,\n        np.datetime64,\n        FloatColumn,\n        FloatColumnView,\n        StringColumn,\n        StringColumnView,\n        BooleanColumnView,\n    ],\n    name: str,\n    role: Optional[Role] = None,\n    subroles: Optional[Union[Subrole, Iterable[str]]] = None,\n    unit: str = \"\",\n    time_formats: Optional[Iterable[str]] = None,\n):\n    \"\"\"Returns a new [`View`][getml.data.View] that contains an additional column.\n\n    Args:\n        col:\n            The column to be added.\n\n        name:\n            Name of the new column.\n\n        role:\n            Role of the new column. Must be from [`roles`][getml.data.roles].\n\n        subroles:\n            Subroles of the new column. Must be from [`subroles`][getml.data.subroles].\n\n        unit:\n            Unit of the column.\n\n        time_formats:\n            Formats to be used to parse the time stamps.\n\n            This is only necessary, if an implicit conversion from\n            a [`StringColumn`][getml.data.columns.StringColumn] to a time\n            stamp is taking place.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n    \"\"\"\n    col, role, subroles = _with_column(\n        col, name, role, subroles, unit, time_formats\n    )\n    return View(\n        base=self,\n        added={\n            \"col_\": col,\n            \"name_\": name,\n            \"role_\": role,\n            \"subroles_\": subroles,\n            \"unit_\": unit,\n        },\n    )\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.with_name","title":"with_name","text":"<pre><code>with_name(name: str) -&gt; View\n</code></pre> <p>Returns a new <code>View</code> with a new name.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the new view.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>View</code> <p>A new view with the new name.</p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def with_name(self, name: str) -&gt; View:\n    \"\"\"Returns a new [`View`][getml.data.View] with a new name.\n\n    Args:\n        name:\n            The name of the new view.\n\n    Returns:\n        A new view with the new name.\n    \"\"\"\n    return View(base=self, name=name)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.with_role","title":"with_role","text":"<pre><code>with_role(\n    cols: Union[\n        str,\n        FloatColumn,\n        StringColumn,\n        Union[\n            Iterable[str],\n            List[FloatColumn],\n            List[StringColumn],\n        ],\n    ],\n    role: Role,\n    time_formats: Optional[Iterable[str]] = None,\n)\n</code></pre> <p>Returns a new <code>View</code> with modified roles.</p> <p>The difference between <code>with_role</code> and <code>set_role</code> is that <code>with_role</code> returns a view that is lazily evaluated when needed whereas <code>set_role</code> is an in-place operation. From a memory perspective, in-place operations like <code>set_role</code> are preferable.</p> <p>When switching from a role based on type float to a role based on type string or vice verse, an implicit type conversion will be conducted. The <code>time_formats</code> argument is used to interpret time format string: <code>annotating_roles_time_stamp</code>. For more information on roles, please refer to the User Guide.</p> PARAMETER DESCRIPTION <code>cols</code> <p>The columns or the names thereof.</p> <p> TYPE: <code>Union[str, FloatColumn, StringColumn, Union[Iterable[str], List[FloatColumn], List[StringColumn]]]</code> </p> <code>role</code> <p>The role to be assigned.</p> <p> TYPE: <code>Role</code> </p> <code>time_formats</code> <p>Formats to be used to parse the time stamps. This is only necessary, if an implicit conversion from a StringColumn to a time stamp is taking place.</p> <p> TYPE: <code>Optional[Iterable[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def with_role(\n    self,\n    cols: Union[\n        str,\n        FloatColumn,\n        StringColumn,\n        Union[Iterable[str], List[FloatColumn], List[StringColumn]],\n    ],\n    role: Role,\n    time_formats: Optional[Iterable[str]] = None,\n):\n    \"\"\"Returns a new [`View`][getml.data.View] with modified roles.\n\n    The difference between [`with_role`][getml.DataFrame.with_role] and\n    [`set_role`][getml.DataFrame.set_role] is that\n    [`with_role`][getml.DataFrame.with_role] returns a view that is lazily\n    evaluated when needed whereas [`set_role`][getml.DataFrame.set_role]\n    is an in-place operation. From a memory perspective, in-place operations\n    like [`set_role`][getml.DataFrame.set_role] are preferable.\n\n    When switching from a role based on type float to a role based on type\n    string or vice verse, an implicit type conversion will be conducted.\n    The `time_formats` argument is used to interpret time\n    format string: `annotating_roles_time_stamp`. For more information on\n    roles, please refer to the [User Guide][annotating-data].\n\n    Args:\n        cols:\n            The columns or the names thereof.\n\n        role:\n            The role to be assigned.\n\n        time_formats:\n            Formats to be used to\n            parse the time stamps.\n            This is only necessary, if an implicit conversion from a StringColumn to\n            a time stamp is taking place.\n    \"\"\"\n    return _with_role(self, cols, role, time_formats)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.with_subroles","title":"with_subroles","text":"<pre><code>with_subroles(\n    cols: Union[\n        str,\n        FloatColumn,\n        StringColumn,\n        Union[\n            Iterable[str],\n            List[FloatColumn],\n            List[StringColumn],\n        ],\n    ],\n    subroles: Union[Subrole, Iterable[str]],\n    append: bool = True,\n)\n</code></pre> <p>Returns a new view with one or several new subroles on one or more columns.</p> <p>The difference between <code>with_subroles</code> and <code>set_subroles</code> is that <code>with_subroles</code> returns a view that is lazily evaluated when needed whereas <code>set_subroles</code> is an in-place operation. From a memory perspective, in-place operations like <code>set_subroles</code> are preferable.</p> PARAMETER DESCRIPTION <code>cols</code> <p>The columns or the names thereof.</p> <p> TYPE: <code>Union[str, FloatColumn, StringColumn, Union[Iterable[str], List[FloatColumn], List[StringColumn]]]</code> </p> <code>subroles</code> <p>The subroles to be assigned.</p> <p> TYPE: <code>Union[Subrole, Iterable[str]]</code> </p> <code>append</code> <p>Whether you want to append the new subroles to the existing subroles.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def with_subroles(\n    self,\n    cols: Union[\n        str,\n        FloatColumn,\n        StringColumn,\n        Union[Iterable[str], List[FloatColumn], List[StringColumn]],\n    ],\n    subroles: Union[Subrole, Iterable[str]],\n    append: bool = True,\n):\n    \"\"\"Returns a new view with one or several new subroles on one or more columns.\n\n    The difference between [`with_subroles`][getml.DataFrame.with_subroles] and\n    [`set_subroles`][getml.DataFrame.set_subroles] is that\n    [`with_subroles`][getml.DataFrame.with_subroles] returns a view that is lazily\n    evaluated when needed whereas [`set_subroles`][getml.DataFrame.set_subroles]\n    is an in-place operation. From a memory perspective, in-place operations\n    like [`set_subroles`][getml.DataFrame.set_subroles] are preferable.\n\n    Args:\n        cols:\n            The columns or the names thereof.\n\n        subroles:\n            The subroles to be assigned.\n\n        append:\n            Whether you want to append the\n            new subroles to the existing subroles.\n    \"\"\"\n    return _with_subroles(self, cols, subroles, append)\n</code></pre>"},{"location":"reference/data/data_frame/#getml.data.DataFrame.with_unit","title":"with_unit","text":"<pre><code>with_unit(\n    cols: Union[\n        str,\n        FloatColumn,\n        StringColumn,\n        Union[\n            Iterable[str],\n            List[FloatColumn],\n            List[StringColumn],\n        ],\n    ],\n    unit: str,\n    comparison_only: bool = False,\n)\n</code></pre> <p>Returns a view that contains a new unit on one or more columns.</p> <p>The difference between <code>with_unit</code> and <code>set_unit</code> is that <code>with_unit</code> returns a view that is lazily evaluated when needed whereas <code>set_unit</code> is an in-place operation. From a memory perspective, in-place operations like <code>set_unit</code> are preferable.</p> PARAMETER DESCRIPTION <code>cols</code> <p>The columns or the names thereof.</p> <p> TYPE: <code>Union[str, FloatColumn, StringColumn, Union[Iterable[str], List[FloatColumn], List[StringColumn]]]</code> </p> <code>unit</code> <p>The unit to be assigned.</p> <p> TYPE: <code>str</code> </p> <code>comparison_only</code> <p>Whether you want the column to be used for comparison only. This means that the column can only be used in comparison to other columns of the same unit.</p> <p>An example might be a bank account number: The number in itself is hardly interesting, but it might be useful to know how often we have seen that same bank account number in another table.</p> <p>For more information on units, please refer to the User Guide.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>getml/data/data_frame.py</code> <pre><code>def with_unit(\n    self,\n    cols: Union[\n        str,\n        FloatColumn,\n        StringColumn,\n        Union[Iterable[str], List[FloatColumn], List[StringColumn]],\n    ],\n    unit: str,\n    comparison_only: bool = False,\n):\n    \"\"\"Returns a view that contains a new unit on one or more columns.\n\n    The difference between [`with_unit`][getml.DataFrame.with_unit] and\n    [`set_unit`][getml.DataFrame.set_unit] is that\n    [`with_unit`][getml.DataFrame.with_unit] returns a view that is lazily\n    evaluated when needed whereas [`set_unit`][getml.DataFrame.set_unit]\n    is an in-place operation. From a memory perspective, in-place operations\n    like [`set_unit`][getml.DataFrame.set_unit] are preferable.\n\n    Args:\n        cols:\n            The columns or the names thereof.\n\n        unit:\n            The unit to be assigned.\n\n        comparison_only:\n            Whether you want the column to\n            be used for comparison only. This means that the column can\n            only be used in comparison to other columns of the same unit.\n\n            An example might be a bank account number: The number in itself\n            is hardly interesting, but it might be useful to know how often\n            we have seen that same bank account number in another table.\n\n            For more information on units, please refer to the\n            [User Guide][annotating-data-units].\n    \"\"\"\n    return _with_unit(self, cols, unit, comparison_only)\n</code></pre>"},{"location":"reference/data/data_model/","title":"DataModel","text":""},{"location":"reference/data/data_model/#getml.data.DataModel","title":"getml.data.DataModel","text":"<pre><code>DataModel(population: Union[Placeholder, str])\n</code></pre> <p>Abstract representation of the relationship between tables.</p> <p>You might also want to refer to <code>Placeholder</code>.</p> ATTRIBUTE DESCRIPTION <code>population</code> <p>The placeholder representing the population table, which defines the statistical population and contains the targets.</p> <p> </p> Example <p>This example will construct a data model in which the 'population_table' depends on the 'peripheral_table' via the 'join_key' column. In addition, only those rows in 'peripheral_table' for which 'time_stamp' is smaller or equal to the 'time_stamp' in 'population_table' are considered: <pre><code>dm = getml.data.DataModel(\n    population_table.to_placeholder(\"POPULATION\")\n)\n\ndm.add(peripheral_table.to_placeholder(\"PERIPHERAL\"))\n\ndm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\"\n)\n</code></pre> If you want to add more than one peripheral table, you can use <code>to_placeholder</code>: <pre><code>dm = getml.data.DataModel(\n    population_table.to_placeholder(\"POPULATION\")\n)\n\ndm.add(\n    getml.data.to_placeholder(\n        PERIPHERAL1=peripheral_table_1,\n        PERIPHERAL2=peripheral_table_2,\n    )\n)\n</code></pre> If the relationship between two tables is many-to-one or one-to-one you should clearly say so: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.many_to_one,\n)\n</code></pre> Please also refer to <code>relationship</code>.</p> <p>If the join keys or time stamps are named differently in the two different tables, use a tuple: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=(\"join_key\", \"other_join_key\"),\n    time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n)\n</code></pre> You can join over more than one join key: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n    time_stamps=\"time_stamp\",\n)\n</code></pre> You can also limit the scope of your joins using memory. This can significantly speed up training time. For instance, if you only want to consider data from the last seven days, you could do something like this: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    memory=getml.data.time.days(7),\n)\n</code></pre> In some use cases, particularly those involving time series, it might be a good idea to use targets from the past. You can activate this using lagged_targets. But if you do that, you must also define a prediction horizon. For instance, if you want to predict data for the next hour, using data from the last seven days, you could do this: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    lagged_targets=True,\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.days(7),\n)\n</code></pre> Please also refer to <code>time</code>.</p> <p>If the join involves many matches, it might be a good idea to set the relationship to <code>propositionalization</code>. This forces the pipeline to always use a propositionalization algorithm for this join, which can significantly speed things up. <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.propositionalization,\n    )\n</code></pre></p> <p>Please also refer to <code>relationship</code>.</p> <p>In some cases, it is necessary to have more than one placeholder on the same table. This is necessary to create more complicated data models. In this case, you can do something like this: <pre><code>dm.add(\n    getml.data.to_placeholder(\n        PERIPHERAL=[peripheral_table]*2,\n    )\n)\n\n# We can now access our two placeholders like this:\nplaceholder1 = dm.PERIPHERAL[0]\nplaceholder2 = dm.PERIPHERAL[1]\n</code></pre>     If you want to check out a real-world example where this     is necessary, refer to the     CORA notebook.</p> Source code in <code>getml/data/data_model.py</code> <pre><code>def __init__(self, population: Union[Placeholder, str]):\n    if isinstance(population, str):\n        population = Placeholder(population)\n\n    if not isinstance(population, Placeholder):\n        raise TypeError(\n            \"'population' must be a getml.data.Placeholder or a str, got \"\n            + type(population).__name__\n            + \".\"\n        )\n\n    self.population = population\n\n    self.peripheral = {}\n</code></pre>"},{"location":"reference/data/data_model/#getml.data.DataModel.names","title":"names  <code>property</code>","text":"<pre><code>names: List[str]\n</code></pre> <p>A list of the names of all tables contained in the DataModel.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of names.</p>"},{"location":"reference/data/data_model/#getml.data.DataModel.add","title":"add","text":"<pre><code>add(*placeholders: Placeholder)\n</code></pre> <p>Adds peripheral placeholders to the data model.</p> PARAMETER DESCRIPTION <code>placeholders</code> <p>The placeholder or placeholders you would like to add.</p> <p> TYPE: <code>Placeholder</code> DEFAULT: <code>()</code> </p> Source code in <code>getml/data/data_model.py</code> <pre><code>def add(self, *placeholders: Placeholder):\n    \"\"\"\n    Adds peripheral placeholders to the data model.\n\n    Args:\n        placeholders:\n            The placeholder or placeholders you would like to add.\n    \"\"\"\n\n    def to_list(elem):\n        return elem if isinstance(elem, list) else [elem]\n\n    # We want to be 100% sure that all handles are unique,\n    # so we need deepcopy.\n    placeholders_dc = [\n        deepcopy(ph) for elem in placeholders for ph in to_list(elem)\n    ]\n\n    if not _is_typed_list(placeholders_dc, Placeholder):\n        raise TypeError(\n            \"'placeholders' must consist of getml.data.Placeholders \"\n            + \"or lists thereof.\"\n        )\n\n    for placeholder in placeholders_dc:\n        self._add(placeholder)\n</code></pre>"},{"location":"reference/data/placeholder/","title":"Placeholder","text":""},{"location":"reference/data/placeholder/#getml.data.Placeholder","title":"getml.data.Placeholder","text":"<pre><code>Placeholder(\n    name: str,\n    roles: Optional[\n        Union[Roles, Dict[str, List[str]]]\n    ] = None,\n)\n</code></pre> <p>Abstract representation of tables and their relations.</p> <p>This class is an abstract representation of the <code>DataFrame</code> or <code>View</code>. However, it does not contain any actual data.</p> <p>You might also want to refer to <code>DataModel</code>.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>The name used for this placeholder. This name will appear in the generated SQL code.</p> <p> TYPE: <code>str</code> </p> <code>roles</code> <p>The roles of the columns in this placeholder. If you pass a dictionary, the keys must be the column names and the values must be lists of roles. If you pass a <code>Roles</code> object, it will be used as is.</p> <p> </p> Example <p>This example will construct a data model in which the 'population_table' depends on the 'peripheral_table' via the 'join_key' column. In addition, only those rows in 'peripheral_table' for which 'time_stamp' is smaller or equal to the 'time_stamp' in 'population_table' are considered: <pre><code>dm = getml.data.DataModel(\n    population_table.to_placeholder(\"POPULATION\")\n)\n\ndm.add(peripheral_table.to_placeholder(\"PERIPHERAL\"))\n\ndm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\"\n)\n</code></pre> If you want to add more than one peripheral table, you can use <code>to_placeholder</code>: <pre><code>dm = getml.data.DataModel(\n    population_table.to_placeholder(\"POPULATION\")\n)\n\ndm.add(\n    getml.data.to_placeholder(\n        PERIPHERAL1=peripheral_table_1,\n        PERIPHERAL2=peripheral_table_2,\n    )\n)\n</code></pre> If the relationship between two tables is many-to-one or one-to-one you should clearly say so: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.many_to_one,\n)\n</code></pre> Please also refer to <code>relationship</code>.</p> <p>If the join keys or time stamps are named differently in the two different tables, use a tuple: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=(\"join_key\", \"other_join_key\"),\n    time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n)\n</code></pre> You can join over more than one join key: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n    time_stamps=\"time_stamp\",\n)\n</code></pre> You can also limit the scope of your joins using memory. This can significantly speed up training time. For instance, if you only want to consider data from the last seven days, you could do something like this: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    memory=getml.data.time.days(7),\n)\n</code></pre> In some use cases, particularly those involving time series, it might be a good idea to use targets from the past. You can activate this using lagged_targets. But if you do that, you must also define a prediction horizon. For instance, if you want to predict data for the next hour, using data from the last seven days, you could do this: <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    lagged_targets=True,\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.days(7),\n)\n</code></pre> Please also refer to <code>time</code>.</p> <p>If the join involves many matches, it might be a good idea to set the relationship to <code>propositionalization</code>. This forces the pipeline to always use a propositionalization algorithm for this join, which can significantly speed things up. <pre><code>dm.POPULATION.join(\n    dm.PERIPHERAL,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.propositionalization,\n)\n</code></pre> Please also refer to <code>relationship</code>.</p> <p>In some cases, it is necessary to have more than one placeholder on the same table. This is necessary to create more complicated data models. In this case, you can do something like this: <pre><code>dm.add(\n    getml.data.to_placeholder(\n        PERIPHERAL=[peripheral_table]*2,\n    )\n)\n\n# We can now access our two placeholders like this:\nplaceholder1 = dm.PERIPHERAL[0]\nplaceholder2 = dm.PERIPHERAL[1]\n</code></pre> If you want to check out a real-world example where this is necessary, refer to the CORA notebook .</p> Source code in <code>getml/data/placeholder.py</code> <pre><code>def __init__(\n    self, name: str, roles: Optional[Union[Roles, Dict[str, List[str]]]] = None\n):\n    self._name = name\n\n    if roles is None:\n        self._roles: Roles = Roles()\n    elif isinstance(roles, dict):\n        self._roles = Roles(**roles)\n    else:\n        self._roles = roles\n\n    self.joins: List[Join] = []\n    self.parent = None\n</code></pre>"},{"location":"reference/data/placeholder/#getml.data.Placeholder.join","title":"join","text":"<pre><code>join(\n    right: Placeholder,\n    on: OnType = None,\n    time_stamps: TimeStampsType = None,\n    relationship: str = many_to_many,\n    memory: Optional[float] = None,\n    horizon: Optional[float] = None,\n    lagged_targets: bool = False,\n    upper_time_stamp: Optional[str] = None,\n)\n</code></pre> <p>Joins another to placeholder to this placeholder.</p> PARAMETER DESCRIPTION <code>right</code> <p>The placeholder you would like to join.</p> <p> TYPE: <code>Placeholder</code> </p> <code>on</code> <p>The join keys to use. If none is passed, then everything will be joined to everything else.</p> <p> TYPE: <code>OnType</code> DEFAULT: <code>None</code> </p> <code>time_stamps</code> <p>The time stamps used to limit the join.</p> <p> TYPE: <code>TimeStampsType</code> DEFAULT: <code>None</code> </p> <code>relationship</code> <p>The relationship between the two tables. Must be from <code>relationship</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>many_to_many</code> </p> <code>memory</code> <p>The difference between the time stamps until data is 'forgotten'. Limiting your joins using memory can significantly speed up training time. Also refer to <code>time</code>.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>horizon</code> <p>The prediction horizon to apply to this join. Also refer to <code>time</code>.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>lagged_targets</code> <p>Whether you want to allow lagged targets. If this is set to True, you must also pass a positive, non-zero horizon.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>upper_time_stamp</code> <p>Name of a time stamp in right that serves as an upper limit on the join.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/data/placeholder.py</code> <pre><code>def join(\n    self,\n    right: \"Placeholder\",\n    on: OnType = None,\n    time_stamps: TimeStampsType = None,\n    relationship: str = many_to_many,\n    memory: Optional[float] = None,\n    horizon: Optional[float] = None,\n    lagged_targets: bool = False,\n    upper_time_stamp: Optional[str] = None,\n):\n    \"\"\"\n    Joins another to placeholder to this placeholder.\n\n    Args:\n        right:\n            The placeholder you would like to join.\n\n        on:\n            The join keys to use. If none is passed, then everything\n            will be joined to everything else.\n\n        time_stamps:\n            The time stamps used to limit the join.\n\n        relationship:\n            The relationship between the two tables. Must be from\n            [`relationship`][getml.data.relationship].\n\n        memory:\n            The difference between the time stamps until data is 'forgotten'.\n            Limiting your joins using memory can significantly speed up\n            training time. Also refer to [`time`][getml.data.time].\n\n        horizon:\n            The prediction horizon to apply to this join.\n            Also refer to [`time`][getml.data.time].\n\n        lagged_targets:\n            Whether you want to allow lagged targets. If this is set to True,\n            you must also pass a positive, non-zero *horizon*.\n\n        upper_time_stamp:\n            Name of a time stamp in *right* that serves as an upper limit\n            on the join.\n    \"\"\"\n\n    if not isinstance(right, type(self)):\n        msg = (\n            \"'right' must be a getml.data.Placeholder. \"\n            + \"You can create a placeholder by calling .to_placeholder() \"\n            + \"on DataFrames or Views.\"\n        )\n        raise TypeError(msg)\n\n    if self in right.to_list():\n        raise ValueError(\n            \"Cicular references to other placeholders are not allowed.\"\n        )\n\n    if isinstance(on, str):\n        on = (on, on)\n\n    if isinstance(time_stamps, str):\n        time_stamps = (time_stamps, time_stamps)\n\n    keys = (\n        list(zip(*on))\n        if isinstance(on, list) and all(isinstance(key, tuple) for key in on)\n        else on\n    )\n\n    for i, ph in enumerate([self, right]):\n        if ph.roles.join_key and keys:\n            not_a_join_key = _check_join_key(keys[i], ph.roles.join_key)  # type: ignore\n            if not_a_join_key:\n                raise ValueError(f\"Not a join key: {not_a_join_key}.\")\n\n        if ph.roles.time_stamp and time_stamps:\n            if time_stamps[i] not in ph.roles.time_stamp:\n                raise ValueError(f\"Not a time stamp: {time_stamps[i]}.\")\n\n    if lagged_targets and horizon in (0.0, None):\n        raise ValueError(\n            \"If you allow lagged targets, then you must also set a \"\n            + \"horizon &gt; 0.0. This is to avoid 'easter eggs'.\"\n        )\n\n    if horizon not in (0.0, None) and time_stamps is None:\n        raise ValueError(\n            \"Setting 'horizon' (i.e. a relative look-back window) \"\n            + \"requires a 'time_stamp'.\"\n        )\n\n    if memory not in (0.0, None) and time_stamps is None:\n        raise ValueError(\n            \"Setting 'memory' (i.e. a relative look-back window) \"\n            + \"requires a 'time_stamp'.\"\n        )\n\n    join = Join(\n        right=right,\n        on=on,\n        time_stamps=time_stamps,\n        relationship=relationship,\n        memory=memory,\n        horizon=horizon,\n        lagged_targets=lagged_targets,\n        upper_time_stamp=upper_time_stamp,\n    )\n\n    if any(join == existing for existing in self.joins):\n        raise ValueError(\n            \"A join with the following set of parameters already exists on \"\n            f\"the placeholder {self.name!r}:\"\n            f\"\\n\\n{join}\\n\\n\"\n            \"Redundant joins are not allowed.\"\n        )\n\n    self.joins.append(join)\n    right.parent = self  # type: ignore\n</code></pre>"},{"location":"reference/data/placeholder/#getml.data.Placeholder.to_list","title":"to_list","text":"<pre><code>to_list()\n</code></pre> <p>Returns a list of this placeholder and all of its descendants.</p> Source code in <code>getml/data/placeholder.py</code> <pre><code>def to_list(self):\n    \"\"\"\n    Returns a list of this placeholder and all of its descendants.\n    \"\"\"\n    return [self] + [ph for join in self.joins for ph in join.right.to_list()]\n</code></pre>"},{"location":"reference/data/placeholder/#getml.data.Placeholder.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Expresses this placeholder and all of its descendants as a dictionary.</p> Source code in <code>getml/data/placeholder.py</code> <pre><code>def to_dict(self):\n    \"\"\"\n    Expresses this placeholder and all of its descendants as a dictionary.\n    \"\"\"\n    phs = {}\n    for ph in self.to_list():\n        key = ph.name\n        if ph.children:\n            i = 2\n            while key in phs:\n                key = f\"{ph.name}{i}\"\n                i += 1\n        phs[key] = ph\n    return phs\n</code></pre>"},{"location":"reference/data/relationship/","title":"relationship","text":""},{"location":"reference/data/relationship/#getml.data.relationship","title":"getml.data.relationship","text":"<p>Marks the relationship between joins in <code>Placeholder</code></p>"},{"location":"reference/data/relationship/#getml.data.relationship.many_to_many","title":"many_to_many  <code>module-attribute</code>","text":"<pre><code>many_to_many: ManyToMany = 'many-to-many'\n</code></pre> <p>Used for one-to-many or many-to-many relationships.</p> <p>When there is such a relationship, feature learning is necessary and meaningful. If you mark a join as a default relationship, but that assumption is violated for the training data, the pipeline will raise a warning.</p>"},{"location":"reference/data/relationship/#getml.data.relationship.many_to_one","title":"many_to_one  <code>module-attribute</code>","text":"<pre><code>many_to_one: ManyToONE = 'many-to-one'\n</code></pre> <p>Used for many-to-one relationships.</p> <p>If two tables are guaranteed to be in a many-to-one relationship, then feature learning is not necessary as they can simply be joined. If a relationship is marked many-to-one, but the assumption is violated, the pipeline will raise an exception.</p>"},{"location":"reference/data/relationship/#getml.data.relationship.one_to_many","title":"one_to_many  <code>module-attribute</code>","text":"<pre><code>one_to_many: OneToMany = 'one-to-many'\n</code></pre> <p>Used for one-to-many or many-to-many relationships.</p> <p>When there is such a relationship, feature learning is necessary and meaningful. If you mark a join as a default relationship, but that assumption is violated for the training data, the pipeline will raise a warning.</p>"},{"location":"reference/data/relationship/#getml.data.relationship.one_to_one","title":"one_to_one  <code>module-attribute</code>","text":"<pre><code>one_to_one: OneToOne = 'one-to-one'\n</code></pre> <p>Used for one-to-one relationships.</p> <p>If two tables are guaranteed to be in a one-to-one relationship, then feature learning is not necessary as they can simply be joined. If a relationship is marked one-to-one, but the assumption is violated, the pipeline will raise an exception. If you are unsure whether you want to use many_to_one or one_to_one, user many_to_one.</p>"},{"location":"reference/data/relationship/#getml.data.relationship.propositionalization","title":"propositionalization  <code>module-attribute</code>","text":"<pre><code>propositionalization: Propositionalization = (\n    \"propositionalization\"\n)\n</code></pre> <p>Used for one-to-many or many-to-many relationships.</p> <p>The flag means that you want a propositionalization algorithm to be used for this particular join. This is recommended when there are very many matches within the join and normal algorithms would take too long.</p>"},{"location":"reference/data/roles/","title":"roles","text":""},{"location":"reference/data/roles/#getml.data.roles","title":"getml.data.roles","text":""},{"location":"reference/data/roles/#getml.data.roles.categorical","title":"categorical  <code>module-attribute</code>","text":"<pre><code>categorical: Final[Categorical] = 'categorical'\n</code></pre> <p>Marks categorical columns.</p> <p>This role tells the getML Engine to include the associated <code>StringColumn</code> during feature learning.</p> <p>It should be used for all data with no inherent ordering, even if the categories are encoded as integer instead of strings in your provided data set.</p>"},{"location":"reference/data/roles/#getml.data.roles.join_key","title":"join_key  <code>module-attribute</code>","text":"<pre><code>join_key: Final[JoinKey] = 'join_key'\n</code></pre> <p>Marks join keys.</p> <p>Role required to establish a relation between two <code>Placeholder</code>, the abstract representation of the <code>DataFrame</code>, by using the <code>join</code> method. Please refer to the chapter Data Model for details.</p> <p>The content of this column is allowed to contain NULL values. But beware, columns with NULL in their join keys won't be matched to anything, not even to NULL in other join keys.</p> <p><code>columns</code> of this role will not be handled by the feature learning algorithm.</p>"},{"location":"reference/data/roles/#getml.data.roles.numerical","title":"numerical  <code>module-attribute</code>","text":"<pre><code>numerical: Final[Numerical] = 'numerical'\n</code></pre> <p>Marks numerical columns.</p> <p>This role tells the getML Engine to include the associated <code>FloatColumn</code> during feature learning.</p> <p>It should be used for all data with an inherent ordering, regardless of whether it is sampled from a continuous quantity, like passed time or the total amount of rainfall, or a discrete one, like the number of sugary mulberries one has eaten since lunch.</p>"},{"location":"reference/data/roles/#getml.data.roles.target","title":"target  <code>module-attribute</code>","text":"<pre><code>target: Final[Target] = 'target'\n</code></pre> <p>Marks the column(s) we would like to predict.</p> <p>The associated <code>columns</code> contain the variables we want to predict. They are not used by the feature learning algorithm unless we explicitly tell it to do so (refer to <code>lagged_target</code> in <code>join</code>). But they are such an important part of the analysis that the population table is required to contain at least one of them (refer to Data Model Tables).</p> <p>The content of the target columns needs to be numerical. For classification problems, target variables can only assume the values 0 or 1. Target variables can never be <code>NULL</code>.</p>"},{"location":"reference/data/roles/#getml.data.roles.text","title":"text  <code>module-attribute</code>","text":"<pre><code>text: Final[Text] = 'text'\n</code></pre> <p>Marks text columns.</p> <p>This role tells the getML Engine to include the associated <code>StringColumn</code> during feature learning.</p> <p>It should be used for all data with no inherent ordering. Unlike categorical columns, text columns can not be used as a whole. Instead, the feature learners have to apply basic text mining techniques before they are able to use them.</p>"},{"location":"reference/data/roles/#getml.data.roles.time_stamp","title":"time_stamp  <code>module-attribute</code>","text":"<pre><code>time_stamp: Final[TimeStamp] = 'time_stamp'\n</code></pre> <p>Marks time stamps.</p> <p>This role is used to prevent data leaks. When you join one table onto another, you usually want to make sure that no data from the future is used. Time stamps can be used to limit your joins.</p> <p>In addition, the feature learning algorithm can aggregate time stamps or use them for conditions. However, they will not be compared to fixed values unless you explicitly change their units. This means that conditions like this are not possible by default:</p> <p><pre><code>WHERE time_stamp &gt; some_fixed_date\n</code></pre> Instead, time stamps will always be compared to other time stamps: <pre><code>WHERE time_stamp1 - time_stamp2 &gt; some_value\n</code></pre></p> <p>This is because it is unlikely that comparing time stamps to a fixed date performs well out-of-sample.</p> <p>When assigning the role time stamp to a column that is currently a <code>StringColumn</code>, you need to specify the format of this string. You can do so by using the <code>time_formats</code> argument of <code>set_role</code>. You can pass a list of time formats that is used to try to interpret the input strings. Possible format options are</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p>If none of the formats works, the getML Engine will try to interpret the time stamps as numerical values. If this fails, the time stamp will be set to NULL.</p> Example <p><pre><code>data_df = dict(\n        date1=[getml.data.time.days(365), getml.data.time.days(366), getml.data.time.days(367)],\n        date2=['1971-01-01', '1971-01-02', '1971-01-03'],\n        date3=['1|1|71', '1|2|71', '1|3|71'],\n    )\ndf = getml.DataFrame.from_dict(data_df, name='dates')\ndf.set_role(['date1', 'date2', 'date3'], getml.data.roles.time_stamp, time_formats=['%Y-%m-%d', '%n|%e|%y'])\ndf\n</code></pre> <pre><code>| date1                       | date2                       | date3                       |\n| time stamp                  | time stamp                  | time stamp                  |\n-------------------------------------------------------------------------------------------\n| 1971-01-01T00:00:00.000000Z | 1971-01-01T00:00:00.000000Z | 1971-01-01T00:00:00.000000Z |\n| 1971-01-02T00:00:00.000000Z | 1971-01-02T00:00:00.000000Z | 1971-01-02T00:00:00.000000Z |\n| 1971-01-03T00:00:00.000000Z | 1971-01-03T00:00:00.000000Z | 1971-01-03T00:00:00.000000Z |\n</code></pre></p> Note <p>getML time stamps are actually floats expressing the number of seconds since UNIX time (1970-01-01T00:00:00).</p>"},{"location":"reference/data/roles/#getml.data.roles.unused_float","title":"unused_float  <code>module-attribute</code>","text":"<pre><code>unused_float: Final[UnusedFloat] = 'unused_float'\n</code></pre> <p>Marks a <code>FloatColumn</code> as unused.</p> <p>The associated <code>column</code> will be neither used in the data model nor during feature learning or prediction.</p>"},{"location":"reference/data/roles/#getml.data.roles.unused_string","title":"unused_string  <code>module-attribute</code>","text":"<pre><code>unused_string: Final[UnusedString] = 'unused_string'\n</code></pre> <p>Marks a <code>StringColumn</code> as unused.</p> <p>The associated <code>column</code> will be neither used in the data model nor during feature learning or prediction.</p>"},{"location":"reference/data/roles/#getml.data.roles.types","title":"types","text":""},{"location":"reference/data/roles/#getml.data.roles.types.Categorical","title":"Categorical  <code>module-attribute</code>","text":"<pre><code>Categorical = Literal['categorical']\n</code></pre>"},{"location":"reference/data/roles/#getml.data.roles.types.JoinKey","title":"JoinKey  <code>module-attribute</code>","text":"<pre><code>JoinKey = Literal['join_key']\n</code></pre>"},{"location":"reference/data/roles/#getml.data.roles.types.Numerical","title":"Numerical  <code>module-attribute</code>","text":"<pre><code>Numerical = Literal['numerical']\n</code></pre>"},{"location":"reference/data/roles/#getml.data.roles.types.Target","title":"Target  <code>module-attribute</code>","text":"<pre><code>Target = Literal['target']\n</code></pre>"},{"location":"reference/data/roles/#getml.data.roles.types.Text","title":"Text  <code>module-attribute</code>","text":"<pre><code>Text = Literal['text']\n</code></pre>"},{"location":"reference/data/roles/#getml.data.roles.types.TimeStamp","title":"TimeStamp  <code>module-attribute</code>","text":"<pre><code>TimeStamp = Literal['time_stamp']\n</code></pre>"},{"location":"reference/data/roles/#getml.data.roles.types.UnusedFloat","title":"UnusedFloat  <code>module-attribute</code>","text":"<pre><code>UnusedFloat = Literal['unused_float']\n</code></pre>"},{"location":"reference/data/roles/#getml.data.roles.types.UnusedString","title":"UnusedString  <code>module-attribute</code>","text":"<pre><code>UnusedString = Literal['unused_string']\n</code></pre>"},{"location":"reference/data/roles/#getml.data.roles.types.CategoricalLike","title":"CategoricalLike  <code>module-attribute</code>","text":"<pre><code>CategoricalLike = Literal[\n    Categorical, JoinKey, Text, UnusedString\n]\n</code></pre>"},{"location":"reference/data/roles/#getml.data.roles.types.NumericalLike","title":"NumericalLike  <code>module-attribute</code>","text":"<pre><code>NumericalLike = Literal[\n    Numerical, Target, TimeStamp, UnusedFloat\n]\n</code></pre>"},{"location":"reference/data/roles/#getml.data.roles.types.Role","title":"Role  <code>module-attribute</code>","text":"<pre><code>Role = Literal[\n    Categorical,\n    JoinKey,\n    Numerical,\n    Target,\n    Text,\n    TimeStamp,\n    UnusedFloat,\n    UnusedString,\n]\n</code></pre>"},{"location":"reference/data/roles/#getml.data.roles.sets","title":"sets","text":""},{"location":"reference/data/roles/#getml.data.roles.sets.categorical","title":"categorical  <code>module-attribute</code>","text":"<pre><code>categorical: FrozenSet[CategoricalLike] = frozenset(\n    {categorical, join_key, text, unused_string}\n)\n</code></pre> <p>Set of roles that are interpreted as categorical.</p>"},{"location":"reference/data/roles/#getml.data.roles.sets.numerical","title":"numerical  <code>module-attribute</code>","text":"<pre><code>numerical: FrozenSet[NumericalLike] = frozenset(\n    {numerical, target, time_stamp, unused_float}\n)\n</code></pre> <p>Set of roles that are interpreted as numerical.</p>"},{"location":"reference/data/roles/#getml.data.roles.sets.all_","title":"all_  <code>module-attribute</code>","text":"<pre><code>all_: FrozenSet[Role] = categorical | numerical\n</code></pre> <p>Set of all possible roles.</p>"},{"location":"reference/data/roles_obj/","title":"Roles","text":"getml.data.Roles"},{"location":"reference/data/roles_obj/#getml.data.roles.container.Roles","title":"Roles  <code>dataclass</code>","text":"<pre><code>Roles(\n    categorical: Iterable[str] = tuple(),\n    join_key: Iterable[str] = tuple(),\n    numerical: Iterable[str] = tuple(),\n    target: Iterable[str] = tuple(),\n    text: Iterable[str] = tuple(),\n    time_stamp: Iterable[str] = tuple(),\n    unused_float: Iterable[str] = tuple(),\n    unused_string: Iterable[str] = tuple(),\n)\n</code></pre> <p>Roles can be passed to <code>DataFrame</code> to predefine the roles assigned to certain columns.</p> ATTRIBUTE DESCRIPTION <code>categorical</code> <p>Names of the categorical columns.</p> <p> TYPE: <code>Iterable[str]</code> </p> <code>join_key</code> <p>Names of the join key columns.</p> <p> TYPE: <code>Iterable[str]</code> </p> <code>numerical</code> <p>Names of the numerical columns.</p> <p> TYPE: <code>Iterable[str]</code> </p> <code>target</code> <p>Names of the target columns.</p> <p> TYPE: <code>Iterable[str]</code> </p> <code>text</code> <p>Names of the text columns.</p> <p> TYPE: <code>Iterable[str]</code> </p> <code>time_stamp</code> <p>Names of the time stamp columns.</p> <p> TYPE: <code>Iterable[str]</code> </p> <code>unused_float</code> <p>Names of the unused float columns.</p> <p> TYPE: <code>Iterable[str]</code> </p> <code>unused_string</code> <p>Names of the unused string columns.</p> <p> TYPE: <code>Iterable[str]</code> </p> Example <pre><code>roles = getml.data.Roles(\n    categorical=[\"col1\", \"col2\"], target=[\"col3\"]\n)\n\ndf_expd = data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"',\n    roles=roles\n)\n</code></pre>"},{"location":"reference/data/roles_obj/#getml.data.roles.container.Roles.columns","title":"columns  <code>property</code>","text":"<pre><code>columns: Tuple[str, ...]\n</code></pre> <p>The name of all columns contained in the roles object.</p> RETURNS DESCRIPTION <code>Tuple[str, ...]</code> <p>The names of all columns.</p>"},{"location":"reference/data/roles_obj/#getml.data.roles.container.Roles.unused","title":"unused  <code>property</code>","text":"<pre><code>unused: List[Role]\n</code></pre> <p>Names of all unused columns (unused_float + unused_string).</p> RETURNS DESCRIPTION <code>List[Role]</code> <p>A list of column names that are categorized as unused, combining both float and string types.</p>"},{"location":"reference/data/roles_obj/#getml.data.roles.container.Roles.column","title":"column","text":"<pre><code>column(colname: str) -&gt; Role\n</code></pre> <p>Gets the role of a column by its column name.</p> PARAMETER DESCRIPTION <code>colname</code> <p>The name of the column.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Role</code> <p>The role of the column as a string.</p> Source code in <code>getml/data/roles/container.py</code> <pre><code>def column(self, colname: str) -&gt; Role:\n    \"\"\"\n    Gets the role of a column by its column name.\n\n    Args:\n        colname:\n            The name of the column.\n\n    Returns:\n        The role of the column as a string.\n    \"\"\"\n    for role in self:\n        if colname in self[role]:\n            return role\n    raise ValueError(\"Column named '\" + colname + \"' not found.\")\n</code></pre>"},{"location":"reference/data/roles_obj/#getml.data.roles.container.Roles.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(\n    roles_dict: Mapping[Union[str, Role], List[str]]\n) -&gt; Roles\n</code></pre> <p>Creates a roles object from a dictionary.</p> PARAMETER DESCRIPTION <code>roles_dict</code> <p>A dictionary where keys are role names and values are lists of column names.</p> <p> TYPE: <code>Mapping[Union[str, Role], List[str]]</code> </p> RETURNS DESCRIPTION <code>Roles</code> <p>A roles object.</p> Source code in <code>getml/data/roles/container.py</code> <pre><code>@classmethod\ndef from_dict(cls, roles_dict: Mapping[Union[str, Role], List[str]]) -&gt; Roles:\n    \"\"\"\n    Creates a roles object from a dictionary.\n\n    Args:\n        roles_dict:\n            A dictionary where keys are role names and values are lists of column names.\n\n    Returns:\n        A roles object.\n    \"\"\"\n    roles: Dict[Role, List[str]] = {}\n    for role in roles_dict:\n        if role not in roles_sets.all_:\n            raise ValueError(\n                INVALID_ROLE_ERROR_MESSAGE_TEMPLATE.format(candidate_role=role)\n            )\n        roles[role] = list(roles_dict[role])\n\n    return cls(**roles)\n</code></pre>"},{"location":"reference/data/roles_obj/#getml.data.roles.container.Roles.from_mapping","title":"from_mapping  <code>classmethod</code>","text":"<pre><code>from_mapping(\n    roles_mapping: Mapping[Union[str, Role], Role]\n) -&gt; Roles\n</code></pre> <p>Creates a roles object from a mapping of column names to roles.</p> PARAMETER DESCRIPTION <code>roles_mapping</code> <p>A dictionary where keys are column names and values are role names.</p> <p> TYPE: <code>Mapping[Union[str, Role], Role]</code> </p> RETURNS DESCRIPTION <code>Roles</code> <p>A roles object.</p> Source code in <code>getml/data/roles/container.py</code> <pre><code>@classmethod\ndef from_mapping(cls, roles_mapping: Mapping[Union[str, Role], Role]) -&gt; Roles:\n    \"\"\"\n    Creates a roles object from a mapping of column names to roles.\n\n    Args:\n        roles_mapping:\n            A dictionary where keys are column names and values are role names.\n\n    Returns:\n        A roles object.\n    \"\"\"\n    roles: Dict[Role, List[str]] = {\n        cast(Role, field.name): [] for field in fields(cls)\n    }\n    for column, role in roles_mapping.items():\n        roles[role].append(column)\n    return cls.from_dict(roles)\n</code></pre>"},{"location":"reference/data/roles_obj/#getml.data.roles.container.Roles.infer","title":"infer","text":"<pre><code>infer(colname: str) -&gt; Role\n</code></pre> <p>Infers the role of a column by its name.</p> PARAMETER DESCRIPTION <code>colname</code> <p>The name of the column to be inferred.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Role</code> <p>The role of the column as a string.</p> Source code in <code>getml/data/roles/container.py</code> <pre><code>def infer(self, colname: str) -&gt; Role:\n    \"\"\"\n    Infers the role of a column by its name.\n\n    Args:\n        colname:\n            The name of the column to be inferred.\n\n    Returns:\n        The role of the column as a string.\n    \"\"\"\n    warnings.warn(\n        \"The 'infer' method is deprecated and will be removed in a future \"\n        \"release. To get a specific column's role, use 'column' instead.\",\n        DeprecationWarning,\n    )\n    return self.column(colname)\n</code></pre>"},{"location":"reference/data/roles_obj/#getml.data.roles.container.Roles.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[Role, List[str]]\n</code></pre> <p>Expresses the roles object as a dictionary.</p> RETURNS DESCRIPTION <code>Dict[Role, List[str]]</code> <p>A dictionary where keys are role names and values are lists of column names.</p> Source code in <code>getml/data/roles/container.py</code> <pre><code>def to_dict(self) -&gt; Dict[Role, List[str]]:\n    \"\"\"\n    Expresses the roles object as a dictionary.\n\n    Returns:\n        A dictionary where keys are role names and values are lists of column names.\n    \"\"\"\n    return {role: list(self[role]) for role in self}\n</code></pre>"},{"location":"reference/data/roles_obj/#getml.data.roles.container.Roles.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[Role]\n</code></pre> <p>Returns a list containing the roles, without the corresponding columns names.</p> RETURNS DESCRIPTION <code>List[Role]</code> <p>A list where each element is a role name, repeated by the number of columns in that role.</p> Source code in <code>getml/data/roles/container.py</code> <pre><code>def to_list(self) -&gt; List[Role]:\n    \"\"\"\n    Returns a list containing the roles, without the corresponding\n    columns names.\n\n    Returns:\n        A list where each element is a role name, repeated by the number of columns in that role.\n    \"\"\"\n    return [role for role in self for _ in self[role]]\n</code></pre>"},{"location":"reference/data/roles_obj/#getml.data.roles.container.Roles.to_mapping","title":"to_mapping","text":"<pre><code>to_mapping() -&gt; Dict[str, Role]\n</code></pre> <p>Maps column names to their roles.</p> RETURNS DESCRIPTION <code>Dict[str, Role]</code> <p>A dictionary where keys are column names and values are role names.</p> Source code in <code>getml/data/roles/container.py</code> <pre><code>def to_mapping(self) -&gt; Dict[str, Role]:\n    \"\"\"\n    Maps column names to their roles.\n\n    Returns:\n        A dictionary where keys are column names and values are role names.\n    \"\"\"\n    return {column: role for role in self for column in self[role]}\n</code></pre>"},{"location":"reference/data/roles_obj/#getml.data.roles.container.Roles.update","title":"update","text":"<pre><code>update(other: Roles) -&gt; Roles\n</code></pre> <p>Merges the roles of two roles objects.</p> PARAMETER DESCRIPTION <code>other</code> <p>The roles object to be merged with the current one.</p> <p> TYPE: <code>Roles</code> </p> RETURNS DESCRIPTION <code>Roles</code> <p>A new roles object containing the merged roles.</p> Source code in <code>getml/data/roles/container.py</code> <pre><code>def update(self, other: Roles) -&gt; Roles:\n    \"\"\"\n    Merges the roles of two roles objects.\n\n    Args:\n        other:\n            The roles object to be merged with the current one.\n\n    Returns:\n        A new roles object containing the merged roles.\n    \"\"\"\n\n    current = self.to_mapping()\n    new = other.to_mapping()\n\n    updated: dict[str, Role] = {**current, **new}\n\n    return Roles.from_mapping(updated)\n</code></pre>"},{"location":"reference/data/roles_obj/#getml.data.roles.container.Roles.validate","title":"validate","text":"<pre><code>validate() -&gt; None\n</code></pre> <p>Checks if the roles are consistent.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the roles are inconsistent.</p> Source code in <code>getml/data/roles/container.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"\n    Checks if the roles are consistent.\n\n    Raises:\n        ValueError:\n            If the roles are inconsistent.\n    \"\"\"\n\n    seen = dict()\n\n    for role in self:\n        if not _is_iterable_not_str_of_type(self[role], type=str):\n            raise TypeError(\n                f\"Argument for '{role}' must be an iterable of column names \"\n                \"(strings): Iterable[str].\"\n            )\n\n        for column in self[role]:\n            if (already_defined_role := seen.get(column)) is not None:\n                raise ValueError(\n                    f\"Column names must be unique across all roles. Found \"\n                    f\"duplicate roles set for column '{column}': '{role}' and \"\n                    f\"'{already_defined_role}'.\"\n                )\n            else:\n                seen.update({column: role})\n</code></pre>"},{"location":"reference/data/split/","title":"split","text":""},{"location":"reference/data/split/#getml.data.split","title":"getml.data.split","text":"<p>Splits data into a training, testing, validation or other sets.</p>"},{"location":"reference/data/split/#getml.data.split.concat.concat","title":"concat","text":"<pre><code>concat(\n    name: str, **kwargs: DataFrame\n) -&gt; Tuple[DataFrame, StringColumnView]\n</code></pre> <p>Concatenates several data frames into and produces a split column that keeps track of their origin.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the data frame you would like to create.</p> <p> TYPE: <code>str</code> </p> <code>kwargs</code> <p>The data frames you would like to concat with the name in which they should appear in the split column.</p> <p> TYPE: <code>DataFrame</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Tuple[DataFrame, StringColumnView]</code> <p>A tuple containing the concatenated data frame and the split column.</p> Example <p>A common use case for this functionality are <code>TimeSeries</code>: <pre><code>data_train = getml.DataFrame.from_pandas(\n    datatraining_pandas, name='data_train')\n\ndata_validate = getml.DataFrame.from_pandas(\n    datatest_pandas, name='data_validate')\n\ndata_test = getml.DataFrame.from_pandas(\n    datatest2_pandas, name='data_test')\n\npopulation, split = getml.data.split.concat(\n    \"population\", train=data_train, validate=data_validate, test=data_test)\n\n...\n\ntime_series = getml.data.TimeSeries(\n    population=population, split=split)\n\nmy_pipeline.fit(time_series.train)\n</code></pre></p> Source code in <code>getml/data/split/concat.py</code> <pre><code>def concat(name: str, **kwargs: DataFrame) -&gt; Tuple[DataFrame, StringColumnView]:\n    \"\"\"\n    Concatenates several data frames into and produces a split\n    column that keeps track of their origin.\n\n    Args:\n        name:\n            The name of the data frame you would like to create.\n\n        kwargs:\n            The data frames you would like\n            to concat with the name in which they should appear\n            in the split column.\n\n    Returns:\n        A tuple containing the concatenated data frame and the split column.\n\n    ??? example\n        A common use case for this functionality are [`TimeSeries`][getml.data.TimeSeries]:\n        ```python\n        data_train = getml.DataFrame.from_pandas(\n            datatraining_pandas, name='data_train')\n\n        data_validate = getml.DataFrame.from_pandas(\n            datatest_pandas, name='data_validate')\n\n        data_test = getml.DataFrame.from_pandas(\n            datatest2_pandas, name='data_test')\n\n        population, split = getml.data.split.concat(\n            \"population\", train=data_train, validate=data_validate, test=data_test)\n\n        ...\n\n        time_series = getml.data.TimeSeries(\n            population=population, split=split)\n\n        my_pipeline.fit(time_series.train)\n        ```\n    \"\"\"\n\n    if not _is_non_empty_typed_list(list(kwargs.values()), [DataFrame, View]):\n        raise ValueError(\n            \"'kwargs' must be non-empty and contain getml.DataFrames \"\n            + \"or getml.data.Views.\"\n        )\n\n    names = list(kwargs.keys())\n\n    first = kwargs[names[0]]\n\n    population = first.copy(name) if isinstance(first, DataFrame) else first.to_df(name)\n\n    split = from_value(names[0])\n\n    assert isinstance(split, StringColumnView), \"Should be a StringColumnView\"\n\n    for new_df_name in names[1:]:\n        split = split.update(rowid() &gt; population.nrows(), new_df_name)  # type: ignore\n        population = _concat(name, [population, kwargs[new_df_name]])\n\n    return population, split[: population.nrows()]  # type: ignore\n</code></pre>"},{"location":"reference/data/split/#getml.data.split.random.random","title":"random","text":"<pre><code>random(\n    seed: int = 5849,\n    train: float = 0.8,\n    test: float = 0.2,\n    validation: float = 0,\n    **kwargs: float\n) -&gt; StringColumnView\n</code></pre> <p>Returns a <code>StringColumnView</code> that can be used to randomly divide data into training, testing, validation or other sets.</p> PARAMETER DESCRIPTION <code>seed</code> <p>Seed used for the random number generator.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5849</code> </p> <code>train</code> <p>The share of random samples assigned to the training set.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.8</code> </p> <code>validation</code> <p>The share of random samples assigned to the validation set.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0</code> </p> <code>test</code> <p>The share of random samples assigned to the test set.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> <code>kwargs</code> <p>Any other sets you would like to assign. You can name these sets whatever you want to (in our example, we called it 'other').</p> <p> TYPE: <code>float</code> DEFAULT: <code>{}</code> </p> Example <pre><code>split = getml.data.split.random(\n    train=0.8, test=0.1, validation=0.05, other=0.05\n)\n\ntrain_set = data_frame[split=='train']\nvalidation_set = data_frame[split=='validation']\ntest_set = data_frame[split=='test']\nother_set = data_frame[split=='other']\n</code></pre> Source code in <code>getml/data/split/random.py</code> <pre><code>def random(\n    seed: int = 5849,\n    train: float = 0.8,\n    test: float = 0.2,\n    validation: float = 0,\n    **kwargs: float,\n) -&gt; StringColumnView:\n    \"\"\"\n    Returns a [`StringColumnView`][getml.data.columns.StringColumnView] that\n    can be used to randomly divide data into training, testing,\n    validation or other sets.\n\n    Args:\n        seed:\n            Seed used for the random number generator.\n\n        train:\n            The share of random samples assigned to\n            the training set.\n\n        validation:\n            The share of random samples assigned to\n            the validation set.\n\n        test:\n            The share of random samples assigned to\n            the test set.\n\n        kwargs:\n            Any other sets you would like to assign.\n            You can name these sets whatever you want to (in our example,\n            we called it 'other').\n\n    ??? example\n        ```python\n        split = getml.data.split.random(\n            train=0.8, test=0.1, validation=0.05, other=0.05\n        )\n\n        train_set = data_frame[split=='train']\n        validation_set = data_frame[split=='validation']\n        test_set = data_frame[split=='test']\n        other_set = data_frame[split=='other']\n        ```\n\n    \"\"\"\n\n    values = np.asarray([train, validation, test] + list(kwargs.values()))\n\n    if not _is_typed_list(values.tolist(), numbers.Real):\n        raise ValueError(\"All values must be real numbers.\")\n\n    if np.abs(np.sum(values) - 1.0) &gt; 0.0001:\n        raise ValueError(\n            \"'train', 'validation', 'test' and all other sets must add up to 1, \"\n            + \"but add up to \"\n            + str(np.sum(values))\n            + \".\"\n        )\n\n    upper_bounds = np.cumsum(values)\n    lower_bounds = upper_bounds - values\n\n    names = [\"train\", \"validation\", \"test\"] + list(kwargs.keys())\n\n    col: StringColumnView = from_value(\"train\")  # type: ignore\n\n    assert isinstance(col, StringColumnView), \"Should be a StringColumnView\"\n\n    for i in range(len(names)):\n        col = col.update(  # type: ignore\n            (random_col(seed=seed) &gt;= lower_bounds[i])  # type: ignore\n            &amp; (random_col(seed=seed) &lt; upper_bounds[i]),\n            names[i],\n        )\n\n    return col\n</code></pre>"},{"location":"reference/data/split/#getml.data.split.time.time","title":"time","text":"<pre><code>time(\n    population: DataFrame,\n    time_stamp: Union[str, FloatColumn, FloatColumnView],\n    validation: Optional[\n        Union[float, int, datetime64]\n    ] = None,\n    test: Optional[Union[float, int, datetime64]] = None,\n    **kwargs: Union[float, int, datetime64]\n) -&gt; StringColumnView\n</code></pre> <p>Returns a <code>StringColumnView</code> that can be used to divide data into training, testing, validation or other sets.</p> <p>The arguments are <code>key=value</code> pairs of names (<code>key</code>) and starting points (<code>value</code>). The starting point defines the left endpoint of the subset. Intervals are left closed and right open, such that \\([value, next value)\\).  The (unnamed) subset left from the first named starting point, i.e.  \\([0, first value)\\), is always considered to be the training set.</p> PARAMETER DESCRIPTION <code>population</code> <p>The population table you would like to split.</p> <p> TYPE: <code>DataFrame</code> </p> <code>time_stamp</code> <p>The name of the time stamp column in the population table you want to use. Ideally, the role of said column would be <code>time_stamp</code>. If you want to split on the rowid, then pass \"rowid\" to <code>time_stamp</code>.</p> <p> TYPE: <code>Union[str, FloatColumn, FloatColumnView]</code> </p> <code>validation</code> <p>The start date of the validation set.</p> <p> TYPE: <code>Optional[Union[float, int, datetime64]]</code> DEFAULT: <code>None</code> </p> <code>test</code> <p>The start date of the test set.</p> <p> TYPE: <code>Optional[Union[float, int, datetime64]]</code> DEFAULT: <code>None</code> </p> <code>kwargs</code> <p>Any other sets you would like to assign. You can name these sets whatever you want to (in our example, we called it 'other').</p> <p> TYPE: <code>Union[float, int, datetime64]</code> DEFAULT: <code>{}</code> </p> Example <pre><code>validation_begin = getml.data.time.datetime(2010, 1, 1)\ntest_begin = getml.data.time.datetime(2011, 1, 1)\nother_begin = getml.data.time.datetime(2012, 1, 1)\n\nsplit = getml.data.split.time(\n    population=data_frame,\n    time_stamp=\"ds\",\n    test=test_begin,\n    validation=validation_begin,\n    other=other_begin\n)\n\n# Contains all data before 2010-01-01 (not included)\ntrain_set = data_frame[split=='train']\n\n# Contains all data between 2010-01-01 (included) and 2011-01-01 (not included)\nvalidation_set = data_frame[split=='validation']\n\n# Contains all data between 2011-01-01 (included) and 2012-01-01 (not included)\ntest_set = data_frame[split=='test']\n\n# Contains all data after 2012-01-01 (included)\nother_set = data_frame[split=='other']\n</code></pre> Source code in <code>getml/data/split/time.py</code> <pre><code>def time(\n    population: DataFrame,\n    time_stamp: Union[str, FloatColumn, FloatColumnView],\n    validation: Optional[Union[float, int, np.datetime64]] = None,\n    test: Optional[Union[float, int, np.datetime64]] = None,\n    **kwargs: Union[float, int, np.datetime64],\n) -&gt; StringColumnView:\n    \"\"\"\n    Returns a [`StringColumnView`][getml.data.columns.StringColumnView] that can be used to divide\n    data into training, testing, validation or other sets.\n\n    The arguments are\n    `key=value` pairs of names (`key`) and starting points (`value`).\n    The starting point defines the left endpoint of the subset. Intervals are left\n    closed and right open, such that $[value, next value)$.  The (unnamed) subset\n    left from the first named starting point, i.e.  $[0, first value)$, is always\n    considered to be the training set.\n\n    Args:\n        population:\n            The population table you would like to split.\n\n        time_stamp:\n            The name of the time stamp column in the population table\n            you want to use. Ideally, the role of said column would be\n            [`time_stamp`][getml.data.roles.time_stamp]. If you want to split on the rowid,\n            then pass \"rowid\" to `time_stamp`.\n\n        validation:\n            The start date of the validation set.\n\n        test:\n            The start date of the test set.\n\n        kwargs:\n            Any other sets you would like to assign.\n            You can name these sets whatever you want to (in our example,\n            we called it 'other').\n\n    ??? example\n        ```python\n        validation_begin = getml.data.time.datetime(2010, 1, 1)\n        test_begin = getml.data.time.datetime(2011, 1, 1)\n        other_begin = getml.data.time.datetime(2012, 1, 1)\n\n        split = getml.data.split.time(\n            population=data_frame,\n            time_stamp=\"ds\",\n            test=test_begin,\n            validation=validation_begin,\n            other=other_begin\n        )\n\n        # Contains all data before 2010-01-01 (not included)\n        train_set = data_frame[split=='train']\n\n        # Contains all data between 2010-01-01 (included) and 2011-01-01 (not included)\n        validation_set = data_frame[split=='validation']\n\n        # Contains all data between 2011-01-01 (included) and 2012-01-01 (not included)\n        test_set = data_frame[split=='test']\n\n        # Contains all data after 2012-01-01 (included)\n        other_set = data_frame[split=='other']\n        ```\n    \"\"\"\n    if not isinstance(population, (DataFrame, View)):\n        raise ValueError(\"'population' must be a DataFrame or a View.\")\n\n    if not isinstance(time_stamp, (str, FloatColumn, FloatColumnView)):\n        raise ValueError(\n            \"'time_stamp' must be a string, a FloatColumn, or a FloatColumnView.\"\n        )\n\n    if not test and not validation and not kwargs:\n        raise ValueError(\"You have to supply at least one starting point.\")\n\n    defaults: Dict[str, Optional[Union[float, int, np.datetime64]]] = {\n        \"test\": test,\n        \"validation\": validation,\n    }\n\n    sets = {name: value for name, value in defaults.items() if value is not None}\n\n    sets.update({**kwargs})\n\n    values = np.asarray(list(sets.values()))\n    index = np.argsort(values)\n    values = values[index]\n\n    if not _is_typed_list(values.tolist(), numbers.Real):\n        raise ValueError(\"All values must be real numbers.\")\n\n    names = np.asarray(list(sets.keys()))\n    names = names[index]\n\n    if isinstance(time_stamp, str):\n        time_stamp_col = (\n            population[time_stamp] if time_stamp != \"rowid\" else population.rowid\n        )\n    else:\n        time_stamp_col = time_stamp\n\n    col: StringColumnView = from_value(\"train\")  # type: ignore\n\n    assert isinstance(col, StringColumnView), \"Should be a StringColumnView\"\n\n    for i in range(len(names)):\n        col = col.update(  # type: ignore\n            time_stamp_col &gt;= values[i],\n            names[i],\n        )\n\n    return col\n</code></pre>"},{"location":"reference/data/star_schema/","title":"StarSchema","text":""},{"location":"reference/data/star_schema/#getml.data.StarSchema","title":"getml.data.StarSchema","text":"<pre><code>StarSchema(\n    population: Optional[Union[DataFrame, View]] = None,\n    alias: Optional[str] = None,\n    peripheral: Optional[\n        Dict[str, Union[DataFrame, View]]\n    ] = None,\n    split: Optional[\n        Union[StringColumn, StringColumnView]\n    ] = None,\n    deep_copy: Optional[bool] = False,\n    train: Optional[Union[DataFrame, View]] = None,\n    validation: Optional[Union[DataFrame, View]] = None,\n    test: Optional[Union[DataFrame, View]] = None,\n    **kwargs: Optional[Union[DataFrame, View]]\n)\n</code></pre> <p>A StarSchema is a simplifying abstraction that can be used for machine learning problems that can be organized in a simple star schema.</p> <p>It unifies <code>Container</code> and <code>DataModel</code> thus abstracting away the need to differentiate between the concrete data and the abstract data model.</p> <p>The class is designed using composition  - it is neither <code>Container</code> nor <code>DataModel</code>, but has both of them.</p> <p>This means that you can always fall back to the more flexible methods using <code>Container</code> and <code>DataModel</code> by directly accessing the attributes <code>container</code> and <code>data_model</code>.</p> ATTRIBUTE DESCRIPTION <code>population</code> <p>The population table defines the statistical population  of the machine learning problem and contains the target variables.</p> <p> </p> <code>alias</code> <p>The alias to be used for the population table. This is required, if population is a <code>View</code>.</p> <p> </p> <code>peripheral</code> <p>The peripheral tables are joined onto population or other peripheral tables. Note that you can also pass them using <code>join</code>.</p> <p> </p> <code>split</code> <p>Contains information on how you want to split population into different <code>Subset</code> s. Also refer to <code>split</code>.</p> <p> </p> <code>deep_copy</code> <p>Whether you want to create deep copies or your tables.</p> <p> </p> <code>train</code> <p>The population table used in the train <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <p> </p> <code>validation</code> <p>The population table used in the validation <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <p> </p> <code>test</code> <p>The population table used in the test <code>Subset</code>. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> <p> </p> <code>kwargs</code> <p>The population table used in <code>Subset</code> s other than the predefined train, validation and test subsets. You can call these subsets anything you want to and can access them just like train, validation and test. You can either pass population and split or you can pass the subsets separately using train, validation, test and kwargs.</p> Example <pre><code># Pass the subset.\nstar_schema = getml.data.StarSchema(\n    my_subset=my_data_frame)\n\n# You can access the subset just like train,\n# validation or test\nmy_pipeline.fit(star_schema.my_subset)\n</code></pre> <p> </p> Example <p>Note that this example is taken from the loans notebook.</p> <p>You might also want to refer to <code>DataFrame</code>, <code>View</code> and <code>Pipeline</code>.</p> <p><pre><code># First, we insert our data.\n# population_train and population_test are either\n# DataFrames or Views. The population table\n# defines the statistical population of your\n# machine learning problem and contains the\n# target variables.\nstar_schema = getml.data.StarSchema(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views.\n# Because this is a star schema,\n# all joins take place on the population\n# table.\nstar_schema.join(\n    trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\nstar_schema.join(\n    order,\n    on=\"account_id\",\n)\n\nstar_schema.join(\n    meta,\n    on=\"account_id\",\n)\n\n# Now you can insert your data model,\n# your preprocessors, feature learners,\n# feature selectors and predictors\n# into the pipeline.\n# Note that the pipeline only knows\n# the abstract data model, but hasn't\n# seen the actual data yet.\npipe = getml.Pipeline(\n    data_model=star_schema.data_model,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=predictor,\n)\n\n# Now, we pass the actual data.\n# This passes 'population_train' and the\n# peripheral tables (meta, order and trans)\n# to the pipeline.\npipe.check(star_schema.train)\n\npipe.fit(star_schema.train)\n\npipe.score(star_schema.test)\n\n# To generate predictions on new data,\n# it is sufficient to use a Container.\n# You don't have to recreate the entire\n# StarSchema, because the abstract data model\n# is stored in the pipeline.\ncontainer = getml.data.Container(\n    population=population_new)\n\ncontainer.add(\n    trans=trans_new,\n    order=order_new,\n    meta=meta_new)\n\npredictions = pipe.predict(container.full)\n</code></pre> If you don't already have a train and test set, you can use a function from the <code>split</code> module.</p> <pre><code>split = getml.data.split.random(\n    train=0.8, test=0.2)\n\nstar_schema = getml.data.StarSchema(\n    population=population_all,\n    split=split,\n)\n\n# The remaining code is the same as in\n# the example above. In particular,\n# star_schema.train and star_schema.test\n# work just like above.\n</code></pre> Source code in <code>getml/data/star_schema.py</code> <pre><code>def __init__(\n    self,\n    population: Optional[Union[DataFrame, View]] = None,\n    alias: Optional[str] = None,\n    peripheral: Optional[Dict[str, Union[DataFrame, View]]] = None,\n    split: Optional[Union[StringColumn, StringColumnView]] = None,\n    deep_copy: Optional[bool] = False,\n    train: Optional[Union[DataFrame, View]] = None,\n    validation: Optional[Union[DataFrame, View]] = None,\n    test: Optional[Union[DataFrame, View]] = None,\n    **kwargs: Optional[Union[DataFrame, View]],\n):\n    if (population is None or isinstance(population, View)) and alias is None:\n        raise ValueError(\n            \"If 'population' is None or a getml.data.View, you must set an alias.\"\n        )\n\n    self._alias = alias or population.name\n\n    self._container = Container(\n        population=population,\n        peripheral=peripheral,\n        split=split,\n        deep_copy=deep_copy,\n        train=train,\n        validation=validation,\n        test=test,\n        **kwargs,\n    )\n\n    def get_placeholder():\n        if population is not None:\n            return population.to_placeholder(alias)\n        if train is not None:\n            return train.to_placeholder(alias)\n        if validation is not None:\n            return validation.to_placeholder(alias)\n        if test is not None:\n            return test.to_placeholder(alias)\n        assert (\n            len(kwargs) &gt; 0\n        ), \"This should have been checked by Container.__init__.\"\n        return kwargs[list(kwargs.keys())[0]].to_placeholder(alias)\n\n    self._data_model = DataModel(get_placeholder())\n</code></pre>"},{"location":"reference/data/star_schema/#getml.data.StarSchema.container","title":"container  <code>property</code>","text":"<pre><code>container: Container\n</code></pre> <p>The underlying <code>Container</code>.</p> RETURNS DESCRIPTION <code>Container</code> <p>The underlying container.</p>"},{"location":"reference/data/star_schema/#getml.data.StarSchema.data_model","title":"data_model  <code>property</code>","text":"<pre><code>data_model: DataModel\n</code></pre> <p>The underlying <code>DataModel</code>.</p> RETURNS DESCRIPTION <code>DataModel</code> <p>The underlying data model.</p>"},{"location":"reference/data/star_schema/#getml.data.StarSchema.join","title":"join","text":"<pre><code>join(\n    right_df: Union[DataFrame, View],\n    alias: Optional[str] = None,\n    on: OnType = None,\n    time_stamps: TimeStampsType = None,\n    relationship: str = many_to_many,\n    memory: Optional[float] = None,\n    horizon: Optional[float] = None,\n    lagged_targets: bool = False,\n    upper_time_stamp: Optional[str] = None,\n)\n</code></pre> <p>Joins a <code>DataFrame</code> or <code>View</code> to the population table.</p> <p>In a <code>StarSchema</code> or <code>TimeSeries</code>, all joins take place on the population table. If you want to create more complex data models, use <code>DataModel</code> instead.</p> Example <p>This example will construct a data model in which the 'population_table' depends on the 'peripheral_table' via the 'join_key' column. In addition, only those rows in 'peripheral_table' for which 'time_stamp' is smaller or equal to the 'time_stamp' in 'population_table' are considered:</p> <pre><code>star_schema = getml.data.StarSchema(\n    population=population_table, split=split)\n\nstar_schema.join(\n    peripheral_table,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\"\n)\n</code></pre> <p>If the relationship between two tables is many-to-one or one-to-one you should clearly say so: <pre><code>star_schema.join(\n    peripheral_table,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.many_to_one,\n)\n</code></pre> Please also refer to <code>relationship</code>.</p> <p>If the join keys or time stamps are named differently in the two different tables, use a tuple:</p> <pre><code>star_schema.join(\n    peripheral_table,\n    on=(\"join_key\", \"other_join_key\"),\n    time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n)\n</code></pre> <p>You can join over more than one join key:</p> <pre><code>star_schema.join(\n    peripheral_table,\n    on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n    time_stamps=\"time_stamp\",\n    )\n</code></pre> <p>You can also limit the scope of your joins using memory. This can significantly speed up training time. For instance, if you only want to consider data from the last seven days, you could do something like this:</p> <pre><code>star_schema.join(\n    peripheral_table,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    memory=getml.data.time.days(7),\n)\n</code></pre> <p>In some use cases, particularly those involving time series, it might be a good idea to use targets from the past. You can activate this using lagged_targets. But if you do that, you must also define a prediction horizon. For instance, if you want to predict data for the next hour, using data from the last seven days, you could do this:</p> <pre><code>star_schema.join(\n    peripheral_table,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    lagged_targets=True,\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.days(7),\n)\n</code></pre> <p>Please also refer to <code>time</code>.</p> <p>If the join involves many matches, it might be a good idea to set the relationship to <code>propositionalization</code>. This forces the pipeline to always use a propositionalization algorithm for this join, which can significantly speed things up.</p> <pre><code>star_schema.join(\n    peripheral_table,\n    on=\"join_key\",\n    time_stamps=\"time_stamp\",\n    relationship=getml.data.relationship.propositionalization,\n)\n</code></pre> <p>Please also refer to <code>relationship</code>.</p> PARAMETER DESCRIPTION <code>right_df</code> <p>The data frame or view you would like to join.</p> <p> TYPE: <code>Union[DataFrame, View]</code> </p> <code>alias</code> <p>The name as which you want right_df to be referred to in the generated SQL code.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>on</code> <p>The join keys to use. If none is passed, then everything will be joined to everything else.</p> <p> TYPE: <code>OnType</code> DEFAULT: <code>None</code> </p> <code>time_stamps</code> <p>The time stamps used to limit the join.</p> <p> TYPE: <code>TimeStampsType</code> DEFAULT: <code>None</code> </p> <code>relationship</code> <p>The relationship between the two tables. Must be from <code>relationship</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>many_to_many</code> </p> <code>memory</code> <p>The difference between the time stamps until data is 'forgotten'. Limiting your joins using memory can significantly speed up training time. Also refer to <code>time</code>.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>horizon</code> <p>The prediction horizon to apply to this join. Also refer to <code>time</code>.</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>None</code> </p> <code>lagged_targets</code> <p>Whether you want to allow lagged targets. If this is set to True, you must also pass a positive, non-zero horizon.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>upper_time_stamp</code> <p>Name of a time stamp in right_df that serves as an upper limit on the join.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/data/star_schema.py</code> <pre><code>def join(\n    self,\n    right_df: Union[DataFrame, View],\n    alias: Optional[str] = None,\n    on: OnType = None,\n    time_stamps: TimeStampsType = None,\n    relationship: str = many_to_many,\n    memory: Optional[float] = None,\n    horizon: Optional[float] = None,\n    lagged_targets: bool = False,\n    upper_time_stamp: Optional[str] = None,\n):\n    \"\"\"\n    Joins a [`DataFrame`][getml.DataFrame] or [`View`][getml.data.View]\n    to the population table.\n\n    In a [`StarSchema`][getml.data.StarSchema] or [`TimeSeries`][getml.data.TimeSeries],\n    all joins take place on the population table. If you want to create more\n    complex data models, use [`DataModel`][getml.data.DataModel] instead.\n\n    ??? example\n        This example will construct a data model in which the\n        'population_table' depends on the 'peripheral_table' via\n        the 'join_key' column. In addition, only those rows in\n        'peripheral_table' for which 'time_stamp' is smaller or\n        equal to the 'time_stamp' in 'population_table' are considered:\n\n        ```python\n        star_schema = getml.data.StarSchema(\n            population=population_table, split=split)\n\n        star_schema.join(\n            peripheral_table,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\"\n        )\n        ```\n\n        If the relationship between two tables is many-to-one or one-to-one\n        you should clearly say so:\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            relationship=getml.data.relationship.many_to_one,\n        )\n        ```\n        Please also refer to [`relationship`][getml.data.relationship].\n\n        If the join keys or time stamps are named differently in the two\n        different tables, use a tuple:\n\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=(\"join_key\", \"other_join_key\"),\n            time_stamps=(\"time_stamp\", \"other_time_stamp\"),\n        )\n        ```\n\n        You can join over more than one join key:\n\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=[\"join_key1\", \"join_key2\", (\"join_key3\", \"other_join_key3\")],\n            time_stamps=\"time_stamp\",\n            )\n        ```\n\n        You can also limit the scope of your joins using *memory*. This\n        can significantly speed up training time. For instance, if you\n        only want to consider data from the last seven days, you could\n        do something like this:\n\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            memory=getml.data.time.days(7),\n        )\n        ```\n\n        In some use cases, particularly those involving time series, it\n        might be a good idea to use targets from the past. You can activate\n        this using *lagged_targets*. But if you do that, you must\n        also define a prediction *horizon*. For instance, if you want to\n        predict data for the next hour, using data from the last seven days,\n        you could do this:\n\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            lagged_targets=True,\n            horizon=getml.data.time.hours(1),\n            memory=getml.data.time.days(7),\n        )\n        ```\n\n        Please also refer to [`time`][getml.data.time].\n\n        If the join involves many matches, it might be a good idea to set the\n        relationship to [`propositionalization`][getml.data.relationship.propositionalization].\n        This forces the pipeline to always use a propositionalization\n        algorithm for this join, which can significantly speed things up.\n\n        ```python\n        star_schema.join(\n            peripheral_table,\n            on=\"join_key\",\n            time_stamps=\"time_stamp\",\n            relationship=getml.data.relationship.propositionalization,\n        )\n        ```\n\n        Please also refer to [`relationship`][getml.data.relationship].\n\n    Args:\n        right_df:\n            The data frame or view you would like to join.\n\n        alias:\n            The name as which you want *right_df* to be referred to in\n            the generated SQL code.\n\n        on:\n            The join keys to use. If none is passed, then everything\n            will be joined to everything else.\n\n        time_stamps:\n            The time stamps used to limit the join.\n\n        relationship:\n            The relationship between the two tables. Must be from\n            [`relationship`][getml.data.relationship].\n\n        memory:\n            The difference between the time stamps until data is 'forgotten'.\n            Limiting your joins using memory can significantly speed up\n            training time. Also refer to [`time`][getml.data.time].\n\n        horizon:\n            The prediction horizon to apply to this join.\n            Also refer to [`time`][getml.data.time].\n\n        lagged_targets:\n            Whether you want to allow lagged targets. If this is set to True,\n            you must also pass a positive, non-zero *horizon*.\n\n        upper_time_stamp:\n            Name of a time stamp in *right_df* that serves as an upper limit\n            on the join.\n    \"\"\"\n\n    if not isinstance(right_df, (DataFrame, View)):\n        raise TypeError(\n            f\"Expected a {DataFrame} as 'right_df', got: {type(right_df)}.\"\n        )\n\n    if isinstance(right_df, View):\n        if alias is None:\n            raise ValueError(\n                \"Setting an 'alias' is required if a getml.data.View is supplied \"\n                \"as a peripheral table.\"\n            )\n\n    def modify_join_keys(on):\n        if isinstance(on, list):\n            return [modify_join_keys(jk) for jk in on]\n\n        if isinstance(on, (str, StringColumn)):\n            on = (on, on)\n\n        if on is not None and on:\n            on = tuple(\n                jkey.name if isinstance(jkey, StringColumn) else jkey for jkey in on\n            )\n\n        return on\n\n    def modify_time_stamps(time_stamps):\n        if isinstance(time_stamps, (str, FloatColumn)):\n            time_stamps = (time_stamps, time_stamps)\n\n        if time_stamps is not None:\n            time_stamps = tuple(\n                time_stamp.name\n                if isinstance(time_stamp, FloatColumn)\n                else time_stamp\n                for time_stamp in time_stamps\n            )\n\n        return time_stamps\n\n    on = modify_join_keys(on)\n\n    time_stamps = modify_time_stamps(time_stamps)\n\n    upper_time_stamp = (\n        upper_time_stamp.name\n        if isinstance(upper_time_stamp, FloatColumn)\n        else upper_time_stamp\n    )\n\n    right = right_df.to_placeholder(alias)\n\n    self.data_model.population.join(\n        right=right,\n        on=on,\n        time_stamps=time_stamps,\n        relationship=relationship,\n        memory=memory,\n        horizon=horizon,\n        lagged_targets=lagged_targets,\n        upper_time_stamp=upper_time_stamp,\n    )\n\n    alias = alias or right_df.name\n\n    self.container.add(**{alias: right_df})\n</code></pre>"},{"location":"reference/data/star_schema/#getml.data.StarSchema.sync","title":"sync","text":"<pre><code>sync()\n</code></pre> <p>Synchronizes the last change with the data to avoid warnings that the data has been changed.</p> <p>This is only a problem when <code>deep_copy=False</code>.</p> Source code in <code>getml/data/star_schema.py</code> <pre><code>def sync(self):\n    \"\"\"\n    Synchronizes the last change with the data to avoid warnings that the data\n    has been changed.\n\n    This is only a problem when `deep_copy=False`.\n    \"\"\"\n    self.container.sync()\n</code></pre>"},{"location":"reference/data/subroles/","title":"subroles","text":""},{"location":"reference/data/subroles/#getml.data.subroles","title":"getml.data.subroles","text":"<p>Subroles allow for more fine-granular control of how certain columns will be used by the pipeline.</p> <p>A column can have no subrole, one subrole or several subroles.</p> Example <pre><code># The Relboost feature learning algorithm will\n# ignore this column.\nmy_data_frame.set_subroles(\n    \"my_column\", getml.data.subroles.exclude.relboost)\n\n# The Substring preprocessor will be applied to this column.\n# But other preprocessors, feature learners or predictors\n# are not excluded from using it as well.\nmy_data_frame.set_subroles(\n    \"ucc\", getml.data.subroles.include.substring)\n\n# Only the EmailDomain preprocessor will be applied\n# to \"emails\". All other preprocessors, feature learners,\n# feature selectors and predictors will ignore this column.\nmy_data_frame.set_subroles(\"emails\", getml.data.subroles.only.email)\n</code></pre>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude","title":"exclude","text":""},{"location":"reference/data/subroles/#getml.data.subroles.exclude.category_trimmer","title":"category_trimmer  <code>module-attribute</code>","text":"<pre><code>category_trimmer: Final[ExcludeCategoryTrimmer] = (\n    \"exclude category trimmer\"\n)\n</code></pre> <p>The <code>CategoryTrimmer</code> preprocessor will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.fastprop","title":"fastprop  <code>module-attribute</code>","text":"<pre><code>fastprop: Final[ExcludeFastProp] = 'exclude fastprop'\n</code></pre> <p><code>FastProp</code> will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.feature_learners","title":"feature_learners  <code>module-attribute</code>","text":"<pre><code>feature_learners: Final[ExcludeFeatureLearners] = (\n    \"exclude feature learners\"\n)\n</code></pre> <p>All feature learners (<code>feature_learning</code>) will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.imputation","title":"imputation  <code>module-attribute</code>","text":"<pre><code>imputation: Final[ExcludeImputation] = 'exclude imputation'\n</code></pre> <p>The <code>Imputation</code> preprocessor will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.mapping","title":"mapping  <code>module-attribute</code>","text":"<pre><code>mapping: Final[ExcludeMapping] = 'exclude mapping'\n</code></pre> <p>The <code>Mapping</code> preprocessor will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.multirel","title":"multirel  <code>module-attribute</code>","text":"<pre><code>multirel: Final[ExcludeMultirel] = 'exclude multirel'\n</code></pre> <p><code>Multirel</code> will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.predictors","title":"predictors  <code>module-attribute</code>","text":"<pre><code>predictors: Final[ExcludePredictors] = 'exclude predictors'\n</code></pre> <p>All <code>predictors</code> will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.preprocessors","title":"preprocessors  <code>module-attribute</code>","text":"<pre><code>preprocessors: Final[ExcludePreprocessors] = (\n    \"exclude preprocessors\"\n)\n</code></pre> <p>All <code>preprocessors</code> will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.relboost","title":"relboost  <code>module-attribute</code>","text":"<pre><code>relboost: Final[ExcludeRelboost] = 'exclude relboost'\n</code></pre> <p><code>Relboost</code> will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.relmt","title":"relmt  <code>module-attribute</code>","text":"<pre><code>relmt: Final[ExcludeRelMT] = 'exclude relmt'\n</code></pre> <p><code>RelMT</code> will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.seasonal","title":"seasonal  <code>module-attribute</code>","text":"<pre><code>seasonal: Final[ExcludeSeasonal] = 'exclude seasonal'\n</code></pre> <p>The <code>Seasonal</code> preprocessor will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.exclude.text_field_splitter","title":"text_field_splitter  <code>module-attribute</code>","text":"<pre><code>text_field_splitter: Final[ExcludeTextFieldSplitter] = (\n    \"exclude text field splitter\"\n)\n</code></pre> <p>The <code>TextFieldSplitter</code> will ignore this column.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.include","title":"include","text":""},{"location":"reference/data/subroles/#getml.data.subroles.include.email","title":"email  <code>module-attribute</code>","text":"<pre><code>email: Final[IncludeEmail] = 'include email'\n</code></pre> <p>A column with this subrole will be used for the <code>EmailDomain</code> preprocessor.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.include.substring","title":"substring  <code>module-attribute</code>","text":"<pre><code>substring: Final[IncludeSubstring] = 'include substring'\n</code></pre> <p>A column with this subrole will be used for the <code>Substring</code> preprocessor.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.only","title":"only","text":""},{"location":"reference/data/subroles/#getml.data.subroles.only.email","title":"email  <code>module-attribute</code>","text":"<pre><code>email: Final[OnlyEmail] = 'only email'\n</code></pre> <p>A column with this subrole will only be used for the <code>EmailDomain</code> preprocessor and nothing else. It will be ignored by all other preprocessors, feature learners and predictors.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.only.substring","title":"substring  <code>module-attribute</code>","text":"<pre><code>substring: Final[OnlySubstring] = 'only substring'\n</code></pre> <p>A column with this subrole will only be used for the <code>Substring</code> preprocessor and nothing else. It will be ignored by all other preprocessors, feature learners and predictors.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.sets","title":"sets","text":""},{"location":"reference/data/subroles/#getml.data.subroles.sets.exclude","title":"exclude  <code>module-attribute</code>","text":"<pre><code>exclude: FrozenSet[ExcludeLike] = frozenset(\n    {\n        category_trimmer,\n        fastprop,\n        feature_learners,\n        imputation,\n        mapping,\n        multirel,\n        predictors,\n        preprocessors,\n        relboost,\n        relmt,\n        seasonal,\n        text_field_splitter,\n    }\n)\n</code></pre> <p>Set of subroles that exclude columns from certain operations.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.sets.include","title":"include  <code>module-attribute</code>","text":"<pre><code>include: FrozenSet[IncludeLike] = frozenset(\n    {email, substring}\n)\n</code></pre> <p>Set of subroles that explicitly include columns for certain operations.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.sets.only","title":"only  <code>module-attribute</code>","text":"<pre><code>only: FrozenSet[OnlyLike] = frozenset({email, substring})\n</code></pre> <p>Set of subroles that restrict the operations that can be performed on columns.</p>"},{"location":"reference/data/subroles/#getml.data.subroles.sets.all_","title":"all_  <code>module-attribute</code>","text":"<pre><code>all_: FrozenSet[Subrole] = exclude | include | only\n</code></pre> <p>Set of all possible subroles.</p>"},{"location":"reference/data/subset/","title":"Subset","text":""},{"location":"reference/data/subset/#getml.data.Subset","title":"getml.data.Subset  <code>dataclass</code>","text":"<pre><code>Subset(\n    container_id: str,\n    peripheral: Dict[str, Union[DataFrame, View]],\n    population: Union[DataFrame, View],\n)\n</code></pre> <p>A Subset consists of a population table and one or several peripheral tables.</p> <p>It is passed by a <code>Container</code>, <code>StarSchema</code> and <code>TimeSeries</code> to the <code>Pipeline</code>.</p> ATTRIBUTE DESCRIPTION <code>container_id</code> <p>The ID of the container the subset belongs to.</p> <p> TYPE: <code>str</code> </p> <code>peripheral</code> <p>A dictionary containing the peripheral tables.</p> <p> TYPE: <code>Dict[str, Union[DataFrame, View]]</code> </p> <code>population</code> <p>The population table.</p> <p> TYPE: <code>Union[DataFrame, View]</code> </p> Example <pre><code>container = getml.data.Container(\n    train=population_train,\n    test=population_test\n)\n\ncontainer.add(\n    meta=meta,\n    order=order,\n    trans=trans\n)\n\n# train and test are Subsets.\n# They contain population_train\n# and population_test respectively,\n# as well as their peripheral tables\n# meta, order and trans.\nmy_pipeline.fit(container.train)\n\nmy_pipeline.score(container.test)\n</code></pre>"},{"location":"reference/data/time/","title":"time","text":""},{"location":"reference/data/time/#getml.data.time","title":"getml.data.time","text":"<p>Convenience functions for the handling of time stamps.</p> <p>In getML, time stamps are always expressed as a floating point value. This float measures the number of seconds since UNIX time (January 1, 1970, 00:00:00). Smaller units of time are expressed as fractions of a second.</p> <p>To make this a bit easier to handle, this module contains simple convenience functions that express other time units in terms of seconds.</p>"},{"location":"reference/data/time/#getml.data.time.seconds","title":"seconds","text":"<pre><code>seconds(num: float) -&gt; float\n</code></pre> <p>Returns the number of seconds.</p> PARAMETER DESCRIPTION <code>num</code> <p>The number of seconds.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>float</code> <p>num</p> Source code in <code>getml/data/time.py</code> <pre><code>def seconds(num: float) -&gt; float:\n    \"\"\"\n    Returns the number of seconds.\n\n    Args:\n        num:\n            The number of seconds.\n\n    Returns:\n        *num*\n    \"\"\"\n    return num\n</code></pre>"},{"location":"reference/data/time/#getml.data.time.minutes","title":"minutes","text":"<pre><code>minutes(num: float) -&gt; float\n</code></pre> <p>Expresses num minutes in terms of seconds.</p> PARAMETER DESCRIPTION <code>num</code> <p>The number of minutes.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>float</code> <p>num minutes expressed in terms of seconds.</p> Source code in <code>getml/data/time.py</code> <pre><code>def minutes(num: float) -&gt; float:\n    \"\"\"\n    Expresses *num* minutes in terms of seconds.\n\n    Args:\n        num:\n            The number of minutes.\n\n    Returns:\n        *num* minutes expressed in terms of seconds.\n    \"\"\"\n    return seconds(num) * 60.0\n</code></pre>"},{"location":"reference/data/time/#getml.data.time.hours","title":"hours","text":"<pre><code>hours(num: float) -&gt; float\n</code></pre> <p>Expresses num hours in terms of seconds.</p> PARAMETER DESCRIPTION <code>num</code> <p>The number of hours.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>float</code> <p>num hours expressed in terms of seconds.</p> Source code in <code>getml/data/time.py</code> <pre><code>def hours(num: float) -&gt; float:\n    \"\"\"\n    Expresses *num* hours in terms of seconds.\n\n    Args:\n        num:\n            The number of hours.\n\n    Returns:\n        *num* hours expressed in terms of seconds.\n    \"\"\"\n    return minutes(num) * 60.0\n</code></pre>"},{"location":"reference/data/time/#getml.data.time.days","title":"days","text":"<pre><code>days(num: float) -&gt; float\n</code></pre> <p>Expresses num days in terms of seconds.</p> PARAMETER DESCRIPTION <code>num</code> <p>The number of days.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>float</code> <p>num days expressed in terms of seconds.</p> Source code in <code>getml/data/time.py</code> <pre><code>def days(num: float) -&gt; float:\n    \"\"\"\n    Expresses *num* days in terms of seconds.\n\n    Args:\n        num:\n            The number of days.\n\n    Returns:\n        *num* days expressed in terms of seconds.\n    \"\"\"\n    return hours(num) * 24.0\n</code></pre>"},{"location":"reference/data/time/#getml.data.time.weeks","title":"weeks","text":"<pre><code>weeks(num: float) -&gt; float\n</code></pre> <p>Expresses num weeks in terms of seconds.</p> PARAMETER DESCRIPTION <code>num</code> <p>The number of weeks.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>float</code> <p>num weeks expressed in terms of seconds.</p> Source code in <code>getml/data/time.py</code> <pre><code>def weeks(num: float) -&gt; float:\n    \"\"\"\n    Expresses *num* weeks in terms of seconds.\n\n    Args:\n        num:\n            The number of weeks.\n\n    Returns:\n        *num* weeks expressed in terms of seconds.\n    \"\"\"\n    return days(num) * 7.0\n</code></pre>"},{"location":"reference/data/time/#getml.data.time.milliseconds","title":"milliseconds","text":"<pre><code>milliseconds(num: float) -&gt; float\n</code></pre> <p>Expresses num milliseconds in terms of fractions of a second.</p> PARAMETER DESCRIPTION <code>num</code> <p>The number of milliseconds.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>float</code> <p>num milliseconds expressed in terms of seconds.</p> Source code in <code>getml/data/time.py</code> <pre><code>def milliseconds(num: float) -&gt; float:\n    \"\"\"\n    Expresses *num* milliseconds in terms of fractions of a second.\n\n    Args:\n        num:\n            The number of milliseconds.\n\n    Returns:\n        *num* milliseconds expressed in terms of seconds.\n    \"\"\"\n    return seconds(num) / 1000.0\n</code></pre>"},{"location":"reference/data/time/#getml.data.time.microseconds","title":"microseconds","text":"<pre><code>microseconds(num: float) -&gt; float\n</code></pre> <p>Expresses num microseconds in terms of fractions of a second.</p> PARAMETER DESCRIPTION <code>num</code> <p>The number of microseconds.</p> <p> TYPE: <code>float</code> </p> RETURNS DESCRIPTION <code>float</code> <p>num microseconds expressed in terms of seconds.</p> Source code in <code>getml/data/time.py</code> <pre><code>def microseconds(num: float) -&gt; float:\n    \"\"\"\n    Expresses *num* microseconds in terms of fractions of a second.\n\n    Args:\n        num:\n            The number of microseconds.\n\n    Returns:\n        *num* microseconds expressed in terms of seconds.\n    \"\"\"\n    return milliseconds(num) / 1000.0\n</code></pre>"},{"location":"reference/data/time/#getml.data.time.datetime","title":"datetime","text":"<pre><code>datetime(\n    year: int,\n    month: int,\n    day: int,\n    hour: int = 0,\n    minute: int = 0,\n    second: int = 0,\n    microsecond: int = 0,\n) -&gt; float\n</code></pre> <p>Returns the number of seconds since UNIX time (January 1, 1970, 00:00:00).</p> PARAMETER DESCRIPTION <code>year</code> <p>Year component of the date.</p> <p> TYPE: <code>int</code> </p> <code>month</code> <p>Month component of the date.</p> <p> TYPE: <code>int</code> </p> <code>day</code> <p>Day component of the date.</p> <p> TYPE: <code>int</code> </p> <code>hour</code> <p>Hour component of the date.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>minute</code> <p>Minute component of the date.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>second</code> <p>Second component of the date.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>microsecond</code> <p>Microsecond component of the date.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>float</code> <p>The number of seconds since UNIX time (January 1, 1970, 00:00:00).</p> Source code in <code>getml/data/time.py</code> <pre><code>def datetime(\n    year: int,\n    month: int,\n    day: int,\n    hour: int = 0,\n    minute: int = 0,\n    second: int = 0,\n    microsecond: int = 0,\n) -&gt; float:\n    \"\"\"\n    Returns the number of seconds since UNIX time (January 1, 1970, 00:00:00).\n\n    Args:\n        year:\n            Year component of the date.\n\n        month:\n            Month component of the date.\n\n        day:\n            Day component of the date.\n\n        hour:\n            Hour component of the date.\n\n        minute:\n            Minute component of the date.\n\n        second:\n            Second component of the date.\n\n        microsecond:\n            Microsecond component of the date.\n\n    Returns:\n        The number of seconds since UNIX time (January 1, 1970, 00:00:00).\n    \"\"\"\n    return dt.datetime(\n        year,\n        month,\n        day,\n        hour,\n        minute,\n        second,\n        microsecond,\n        tzinfo=dt.timezone.utc,\n    ).timestamp()\n</code></pre>"},{"location":"reference/data/time_series/","title":"TimeSeries","text":""},{"location":"reference/data/time_series/#getml.data.TimeSeries","title":"getml.data.TimeSeries","text":"<pre><code>TimeSeries(\n    population: Union[DataFrame, View],\n    time_stamps: str,\n    alias: Optional[str] = None,\n    peripheral: Optional[\n        Dict[str, Union[DataFrame, View]]\n    ] = None,\n    split: Optional[\n        Union[StringColumn, StringColumnView]\n    ] = None,\n    deep_copy: Optional[bool] = False,\n    on: OnType = None,\n    memory: Optional[float] = None,\n    horizon: Optional[float] = None,\n    lagged_targets: bool = False,\n    upper_time_stamp: Optional[str] = None,\n)\n</code></pre> <p>               Bases: <code>StarSchema</code></p> <p>A TimeSeries is a simplifying abstraction that can be used for machine learning problems on time series data.</p> <p>It unifies <code>Container</code> and <code>DataModel</code> thus abstracting away the need to differentiate between the concrete data and the abstract data model. It also abstracts away the need for self joins.</p> ATTRIBUTE DESCRIPTION <code>time_stamps</code> <p>The time stamps used to limit the self-join.</p> <p> </p> <code>population</code> <p>The population table defines the statistical population  of the machine learning problem and contains the target variables.</p> <p> </p> <code>alias</code> <p>The alias to be used for the population table. If it isn't set, the 'population' will be used as the alias. To explicitly set an alias for the peripheral table, use <code>with_name</code>.</p> <p> </p> <code>peripheral</code> <p>The peripheral tables are joined onto population or other peripheral tables. Note that you can also pass them using <code>add</code>.</p> <p> </p> <code>split</code> <p>Contains information on how you want to split population into different <code>Subset</code> s. Also refer to <code>split</code>.</p> <p> </p> <code>deep_copy</code> <p>Whether you want to create deep copies or your tables.</p> <p> </p> <code>on</code> <p>The join keys to use. If none is passed, then everything will be joined to everything else.</p> <p> </p> <code>memory</code> <p>The difference between the time stamps until data is 'forgotten'. Limiting your joins using memory can significantly speed up training time. Provide the value in seconds, alternatively use  the convenience functions from <code>time</code>.</p> <p> </p> <code>horizon</code> <p>The prediction horizon to apply to this join. Provide the value in seconds, alternatively use the convenience functions from <code>time</code>.</p> <p> </p> <code>lagged_targets</code> <p>Whether you want to allow lagged targets. If this is set to True, you must also pass a positive, non-zero horizon.</p> <p> </p> <code>upper_time_stamp</code> <p>Name of a time stamp in right_df that serves as an upper limit on the join.</p> <p> </p> Example <pre><code># All rows before row 10500 will be used for training.\nsplit = getml.data.split.time(data_all, \"rowid\", test=10500)\n\ntime_series = getml.data.TimeSeries(\n    population=data_all,\n    time_stamps=\"rowid\",\n    split=split,\n    lagged_targets=False,\n    memory=30,\n)\n\npipe = getml.Pipeline(\n    data_model=time_series.data_model,\n    feature_learners=[...],\n    predictors=...\n)\n\npipe.check(time_series.train)\n\npipe.fit(time_series.train)\n\npipe.score(time_series.test)\n\n# To generate predictions on new data,\n# it is sufficient to use a Container.\n# You don't have to recreate the entire\n# TimeSeries, because the abstract data model\n# is stored in the pipeline.\ncontainer = getml.data.Container(\n    population=population_new,\n)\n\n# Add the data as a peripheral table, for the\n# self-join.\ncontainer.add(population=population_new)\n\npredictions = pipe.predict(container.full)\n</code></pre> Source code in <code>getml/data/time_series.py</code> <pre><code>def __init__(\n    self,\n    population: Union[DataFrame, View],\n    time_stamps: str,\n    alias: Optional[str] = None,\n    peripheral: Optional[Dict[str, Union[DataFrame, View]]] = None,\n    split: Optional[Union[StringColumn, StringColumnView]] = None,\n    deep_copy: Optional[bool] = False,\n    on: OnType = None,\n    memory: Optional[float] = None,\n    horizon: Optional[float] = None,\n    lagged_targets: bool = False,\n    upper_time_stamp: Optional[str] = None,\n):\n    if not isinstance(population, (DataFrame, View)):\n        raise TypeError(\n            \"'population' must be a getml.DataFrame or a getml.data.View\"\n        )\n\n    if isinstance(time_stamps, FloatColumn):\n        time_stamps = time_stamps.name\n\n    if isinstance(time_stamps, FloatColumnView):\n        if \"rowid\" in _finditems(\"operator_\", time_stamps.cmd):\n            time_stamps = \"rowid\"\n\n    population = (\n        population.with_column(\n            population.rowid, name=\"rowid\", role=time_stamp\n        ).with_unit(names=\"rowid\", unit=\"rowid\", comparison_only=True)\n        if time_stamps == \"rowid\"\n        else population\n    )\n\n    alias = \"population\" if alias is None else alias\n\n    super().__init__(\n        population=population,\n        alias=alias,\n        peripheral=peripheral,\n        split=split,\n        deep_copy=deep_copy,\n    )\n\n    self.on = on\n    self.time_stamps = time_stamps\n    self.memory = memory\n    self.horizon = horizon\n    self.lagged_targets = lagged_targets\n    self.upper_time_stamp = upper_time_stamp\n\n    if not isinstance(on, list):\n        on = [on]\n\n    for o in on:\n        self._add_joins(o)\n</code></pre>"},{"location":"reference/data/time_series/#getml.data.TimeSeries.on","title":"on  <code>instance-attribute</code>","text":"<pre><code>on = on\n</code></pre>"},{"location":"reference/data/time_series/#getml.data.TimeSeries.time_stamps","title":"time_stamps  <code>instance-attribute</code>","text":"<pre><code>time_stamps = time_stamps\n</code></pre>"},{"location":"reference/data/time_series/#getml.data.TimeSeries.memory","title":"memory  <code>instance-attribute</code>","text":"<pre><code>memory = memory\n</code></pre>"},{"location":"reference/data/time_series/#getml.data.TimeSeries.horizon","title":"horizon  <code>instance-attribute</code>","text":"<pre><code>horizon = horizon\n</code></pre>"},{"location":"reference/data/time_series/#getml.data.TimeSeries.lagged_targets","title":"lagged_targets  <code>instance-attribute</code>","text":"<pre><code>lagged_targets = lagged_targets\n</code></pre>"},{"location":"reference/data/time_series/#getml.data.TimeSeries.upper_time_stamp","title":"upper_time_stamp  <code>instance-attribute</code>","text":"<pre><code>upper_time_stamp = upper_time_stamp\n</code></pre>"},{"location":"reference/data/view/","title":"View","text":""},{"location":"reference/data/view/#getml.data.View","title":"getml.data.View","text":"<pre><code>View(\n    base: Union[DataFrame, View],\n    name: Optional[str] = None,\n    subselection: Optional[\n        Union[\n            BooleanColumnView, FloatColumn, FloatColumnView\n        ]\n    ] = None,\n    added: Optional[Dict] = None,\n    dropped: Optional[List[str]] = None,\n)\n</code></pre> <p>A view is a lazily evaluated, immutable representation of a <code>DataFrame</code>.</p> <p>There are important differences between a <code>DataFrame</code> and a view:</p> <ul> <li> <p>Views are lazily evaluated. That means that views do not   contain any data themselves. Instead, they just refer to   an underlying data frame. If the underlying data frame changes,   so will the view (but such behavior will result in a warning).</p> </li> <li> <p>Views are immutable. In-place operations on a view are not   possible. Any operation on a view will result in a new view.</p> </li> <li> <p>Views have no direct representation on the getML Engine, and   therefore they do not need to have an identifying name.</p> </li> </ul> ATTRIBUTE DESCRIPTION <code>base</code> <p>A data frame or view used as the basis for this view.</p> <p> TYPE: <code>Union[DataFrame, View]</code> </p> <code>name</code> <p>The name assigned to this view.</p> <p> TYPE: <code>str</code> </p> <code>subselection</code> <p>Indicates which rows we would like to keep.</p> <p> TYPE: <code>Union[BooleanColumnView, FloatColumn, FloatColumnView]</code> </p> <code>added</code> <p>A dictionary that describes a new column that has been added to the view.</p> <p> TYPE: <code>Dict</code> </p> <code>dropped</code> <p>A list of columns that have been dropped.</p> <p> TYPE: <code>List[str]</code> </p> Example <p>You hardly ever directly create views. Instead, it is more likely that you will encounter them as a result of some operation on a <code>DataFrame</code>:</p> <p><pre><code># Creates a view on the first 100 lines\nview1 = data_frame[:100]\n\n# Creates a view without some columns.\nview2 = data_frame.drop([\"col1\", \"col2\"])\n\n# Creates a view in which some roles are reassigned.\nview3 = data_frame.with_role([\"col1\", \"col2\"], getml.data.roles.categorical)\n</code></pre> A recommended pattern is to assign 'baseline roles' to your data frames and then using views to tweak them:</p> <pre><code># Assign baseline roles\ndata_frame.set_role([\"jk\"], getml.data.roles.join_key)\ndata_frame.set_role([\"col1\", \"col2\"], getml.data.roles.categorical)\ndata_frame.set_role([\"col3\", \"col4\"], getml.data.roles.numerical)\ndata_frame.set_role([\"col5\"], getml.data.roles.target)\n\n# Make the data frame immutable, so in-place operations are\n# no longer possible.\ndata_frame.freeze()\n\n# Save the data frame.\ndata_frame.save()\n\n# I suspect that col1 leads to overfitting, so I will drop it.\nview = data_frame.drop([\"col1\"])\n\n# Insert the view into a container.\ncontainer = getml.data.Container(...)\ncontainer.add(some_alias=view)\ncontainer.save()\n</code></pre> <p>The advantage of using such a pattern is that it enables you to always completely retrace your entire pipeline without creating deep copies of the data frames whenever you have made a small change like the one in our example. Note that the pipeline will record which <code>Container</code> you have used.</p> Source code in <code>getml/data/view.py</code> <pre><code>def __init__(\n    self,\n    base: Union[DataFrame, View],\n    name: Optional[str] = None,\n    subselection: Optional[\n        Union[BooleanColumnView, FloatColumn, FloatColumnView]\n    ] = None,\n    added: Optional[Dict] = None,\n    dropped: Optional[List[str]] = None,\n):\n    self._added = added\n    self._base = deepcopy(base)\n    self._dropped = dropped or []\n    self._name = name\n    self._subselection = subselection\n\n    self._initial_timestamp: str = (\n        self._base._initial_timestamp\n        if isinstance(self._base, View)\n        else self._base.last_change\n    )\n\n    self._base.refresh()\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.added","title":"added  <code>property</code>","text":"<pre><code>added: Dict\n</code></pre> <p>The column that has been added to the view.</p> RETURNS DESCRIPTION <code>Dict</code> <p>The column that has been added to the view.</p>"},{"location":"reference/data/view/#getml.data.View.base","title":"base  <code>property</code>","text":"<pre><code>base: Union[DataFrame, View]\n</code></pre> <p>The basis on which the view is created. Must be a <code>DataFrame</code> or a <code>View</code>.</p> RETURNS DESCRIPTION <code>Union[DataFrame, View]</code> <p>The basis on which the view is created.</p>"},{"location":"reference/data/view/#getml.data.View.colnames","title":"colnames  <code>property</code>","text":"<pre><code>colnames: List[str]\n</code></pre> <p>List of the names of all columns.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>List of the names of all columns.</p>"},{"location":"reference/data/view/#getml.data.View.columns","title":"columns  <code>property</code>","text":"<pre><code>columns: List[str]\n</code></pre> <p>Alias for <code>colnames</code>.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>List of the names of all columns.</p>"},{"location":"reference/data/view/#getml.data.View.dropped","title":"dropped  <code>property</code>","text":"<pre><code>dropped: List[str]\n</code></pre> <p>The names of the columns that has been dropped.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>The names of the columns that has been dropped.</p>"},{"location":"reference/data/view/#getml.data.View.last_change","title":"last_change  <code>property</code>","text":"<pre><code>last_change: str\n</code></pre> <p>A string describing the last time this data frame has been changed.</p> RETURNS DESCRIPTION <code>str</code> <p>A string describing the last time this data frame has been changed.</p>"},{"location":"reference/data/view/#getml.data.View.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the view. If no name is explicitly set, the name will be identical to the name of the base.</p> RETURNS DESCRIPTION <code>str</code> <p>The name of the view.</p>"},{"location":"reference/data/view/#getml.data.View.roles","title":"roles  <code>property</code>","text":"<pre><code>roles: Roles\n</code></pre> <p>The roles of the columns included in this View.</p> RETURNS DESCRIPTION <code>Roles</code> <p>The roles of the columns included in this View.</p>"},{"location":"reference/data/view/#getml.data.View.rowid","title":"rowid  <code>property</code>","text":"<pre><code>rowid: List[int]\n</code></pre> <p>The rowids for this view.</p> RETURNS DESCRIPTION <code>List[int]</code> <p>The rowids for this view.</p>"},{"location":"reference/data/view/#getml.data.View.subselection","title":"subselection  <code>property</code>","text":"<pre><code>subselection: Union[\n    BooleanColumnView, FloatColumn, FloatColumnView\n]\n</code></pre> <p>The subselection that is applied to this view.</p> RETURNS DESCRIPTION <code>Union[BooleanColumnView, FloatColumn, FloatColumnView]</code> <p>The subselection that is applied to this view.</p>"},{"location":"reference/data/view/#getml.data.View.shape","title":"shape  <code>property</code>","text":"<pre><code>shape: Tuple[Union[int, str], int]\n</code></pre> <p>A tuple containing the number of rows and columns of the View.</p>"},{"location":"reference/data/view/#getml.data.View.check","title":"check","text":"<pre><code>check()\n</code></pre> <p>Checks whether the underlying data frame has been changed after the creation of the view.</p> Source code in <code>getml/data/view.py</code> <pre><code>def check(self):\n    \"\"\"\n    Checks whether the underlying data frame has been changed\n    after the creation of the view.\n    \"\"\"\n    last_change = self.last_change\n    if last_change != self.__dict__[\"_initial_timestamp\"]:\n        logger.warning(\n            \"The data frame underlying view '\"\n            + self.name\n            + \"' was last changed at \"\n            + last_change\n            + \", which was after the creation of the view. \"\n            + \"This might lead to unexpected results. You might \"\n            + \"want to recreate the view. (Views are lazily \"\n            + \"evaluated, so recreating them is a very \"\n            + \"inexpensive operation).\"\n        )\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.drop","title":"drop","text":"<pre><code>drop(cols: Union[str, List[str]]) -&gt; View\n</code></pre> <p>Returns a new <code>View</code> that has one or several columns removed.</p> PARAMETER DESCRIPTION <code>cols</code> <p>The names of the columns to be dropped.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> RETURNS DESCRIPTION <code>View</code> <p>A new view with the specified columns removed.</p> Source code in <code>getml/data/view.py</code> <pre><code>def drop(self, cols: Union[str, List[str]]) -&gt; View:\n    \"\"\"Returns a new [`View`][getml.data.View] that has one or several columns removed.\n\n    Args:\n        cols:\n            The names of the columns to be dropped.\n\n    Returns:\n            A new view with the specified columns removed.\n    \"\"\"\n    if isinstance(cols, str):\n        cols = [cols]\n\n    if not _is_typed_list(cols, str):\n        raise TypeError(\"'cols' must be a string or a list of strings.\")\n\n    return View(base=self, dropped=cols)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.ncols","title":"ncols","text":"<pre><code>ncols() -&gt; int\n</code></pre> <p>Number of columns in the current instance.</p> RETURNS DESCRIPTION <code>int</code> <p>Overall number of columns</p> Source code in <code>getml/data/view.py</code> <pre><code>def ncols(self) -&gt; int:\n    \"\"\"\n    Number of columns in the current instance.\n\n    Returns:\n            Overall number of columns\n    \"\"\"\n    return len(self.colnames)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.nrows","title":"nrows","text":"<pre><code>nrows(force: bool = False) -&gt; Union[int, str]\n</code></pre> <p>Returns the number of rows in the current instance.</p> PARAMETER DESCRIPTION <code>force</code> <p>If the number of rows is unknown, do you want to force the Engine to calculate it anyway? This is a relatively expensive operation, therefore you might not necessarily want this.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[int, str]</code> <p>The number of rows in the current instance.</p> Source code in <code>getml/data/view.py</code> <pre><code>def nrows(self, force: bool = False) -&gt; Union[int, str]:\n    \"\"\"\n    Returns the number of rows in the current instance.\n\n    Args:\n        force:\n            If the number of rows is unknown,\n            do you want to force the Engine to calculate it anyway?\n            This is a relatively expensive operation, therefore\n            you might not necessarily want this.\n\n    Returns:\n            The number of rows in the current instance.\n    \"\"\"\n\n    self.refresh()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"View.get_nrows\"\n    cmd[\"name_\"] = \"\"\n\n    cmd[\"cols_\"] = [self[cname].cmd for cname in self.colnames]\n    cmd[\"force_\"] = force\n\n    with comm.send_and_get_socket(cmd) as sock:\n        json_str = comm.recv_string(sock)\n\n    if json_str[0] != \"{\":\n        comm.handle_engine_exception(json_str)\n\n    response = json.loads(json_str)\n\n    if \"recordsTotal\" in response:\n        return response[\"recordsTotal\"]\n\n    # ensure that we do not display \"unknown\" if the number of rows is\n    # less than or equal to the maximum number of diplayed rows\n    nrows_to_display = len(self[: _ViewFormatter.max_rows + 1])\n    if nrows_to_display &lt;= _ViewFormatter.max_rows:\n        return nrows_to_display\n\n    return \"unknown\"\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.refresh","title":"refresh","text":"<pre><code>refresh() -&gt; View\n</code></pre> <p>Aligns meta-information of the current instance with the corresponding data frame in the getML Engine.</p> RETURNS DESCRIPTION <code>View</code> <p>Updated handle the underlying data frame in the getML</p> <code>View</code> <p>Engine.</p> Source code in <code>getml/data/view.py</code> <pre><code>def refresh(self) -&gt; View:\n    \"\"\"Aligns meta-information of the current instance with the\n    corresponding data frame in the getML Engine.\n\n    Returns:\n            Updated handle the underlying data frame in the getML\n            Engine.\n\n    \"\"\"\n    self._base = self.__dict__[\"_base\"].refresh()\n    return self\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.to_arrow","title":"to_arrow","text":"<pre><code>to_arrow() -&gt; Table\n</code></pre> <p>Creates a <code>pyarrow.Table</code> from the view.</p> <p>Loads the underlying data from the getML Engine and constructs a <code>pyarrow.Table</code>.</p> RETURNS DESCRIPTION <code>Table</code> <p>Pyarrow equivalent of the current instance including</p> <code>Table</code> <p>its underlying data.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_arrow(self) -&gt; pyarrow.Table:\n    \"\"\"Creates a `pyarrow.Table` from the view.\n\n    Loads the underlying data from the getML Engine and constructs\n    a `pyarrow.Table`.\n\n    Returns:\n            Pyarrow equivalent of the current instance including\n            its underlying data.\n    \"\"\"\n    return to_arrow(self)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.to_json","title":"to_json","text":"<pre><code>to_json() -&gt; str\n</code></pre> <p>Creates a JSON string from the current instance.</p> <p>Loads the underlying data from the getML Engine and constructs a JSON string.</p> RETURNS DESCRIPTION <code>str</code> <p>JSON string of the current instance including its</p> <code>str</code> <p>underlying data.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"Creates a JSON string from the current instance.\n\n    Loads the underlying data from the getML Engine and constructs\n    a JSON string.\n\n    Returns:\n            JSON string of the current instance including its\n            underlying data.\n    \"\"\"\n    return self.to_pandas().to_json()\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.to_csv","title":"to_csv","text":"<pre><code>to_csv(\n    fname: str,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    batch_size: int = 0,\n    quoting_style: str = \"needed\",\n)\n</code></pre> <p>Writes the underlying data into a newly created CSV file.</p> PARAMETER DESCRIPTION <code>fname</code> <p>The name of the CSV file. The ending \".csv\" and an optional batch number will be added automatically.</p> <p> TYPE: <code>str</code> </p> <code>quotechar</code> <p>The character used to wrap strings.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'\"'</code> </p> <code>sep</code> <p>The character used for separating fields.</p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>batch_size</code> <p>Maximum number of lines per file. Set to 0 to read the entire data frame into a single file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>quoting_style</code> <p>The quoting style to use. Delegated to pyarrow.</p> <p>The following values are accepted: - <code>\"needed\"</code> (default): only enclose values in quotes when needed. - <code>\"all_valid\"</code>: enclose all valid values in quotes; nulls are not   quoted. - <code>\"none\"</code>: do not enclose any values in quotes; values containing   special characters (such as quotes, cell delimiters or line   endings) will raise an error.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'needed'</code> </p> Deprecated <p>1.5: The <code>quotechar</code> parameter is deprecated.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_csv(\n    self,\n    fname: str,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    batch_size: int = 0,\n    quoting_style: str = \"needed\",\n):\n    \"\"\"\n    Writes the underlying data into a newly created CSV file.\n\n    Args:\n        fname:\n            The name of the CSV file.\n            The ending \".csv\" and an optional batch number will\n            be added automatically.\n\n        quotechar:\n            The character used to wrap strings.\n\n        sep:\n            The character used for separating fields.\n\n        batch_size:\n            Maximum number of lines per file. Set to 0 to read\n            the entire data frame into a single file.\n\n        quoting_style (str):\n            The quoting style to use. Delegated to pyarrow.\n\n            The following values are accepted:\n            - `\"needed\"` (default): only enclose values in quotes when needed.\n            - `\"all_valid\"`: enclose all valid values in quotes; nulls are not\n              quoted.\n            - `\"none\"`: do not enclose any values in quotes; values containing\n              special characters (such as quotes, cell delimiters or line\n              endings) will raise an error.\n\n    Deprecated:\n       1.5: The `quotechar` parameter is deprecated.\n    \"\"\"\n\n    if quotechar != '\"':\n        warnings.warn(\n            \"'quotechar' is deprecated, use 'quoting_style' instead.\",\n            DeprecationWarning,\n        )\n\n    to_csv(self, fname, sep, batch_size, quoting_style)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.to_db","title":"to_db","text":"<pre><code>to_db(table_name: str, conn: Optional[Connection] = None)\n</code></pre> <p>Writes the underlying data into a newly created table in the database.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to be created.</p> <p>If a table of that name already exists, it will be replaced.</p> <p> TYPE: <code>str</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the Engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/data/view.py</code> <pre><code>def to_db(self, table_name: str, conn: Optional[Connection] = None):\n    \"\"\"Writes the underlying data into a newly created table in the\n    database.\n\n    Args:\n        table_name:\n            Name of the table to be created.\n\n            If a table of that name already exists, it will be\n            replaced.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the Engine will use the default connection.\n    \"\"\"\n\n    conn = conn or Connection()\n\n    self.refresh()\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be of type str\")\n\n    if not isinstance(conn, Connection):\n        raise TypeError(\"'conn' must be a getml.database.Connection object or None\")\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"View.to_db\"\n    cmd[\"name_\"] = \"\"\n\n    cmd[\"view_\"] = self._getml_deserialize()\n    cmd[\"table_name_\"] = table_name\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.to_df","title":"to_df","text":"<pre><code>to_df(name) -&gt; DataFrame\n</code></pre> <p>Creates a <code>DataFrame</code> from the view.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_df(self, name) -&gt; DataFrame:\n    \"\"\"Creates a [`DataFrame`][getml.DataFrame] from the view.\"\"\"\n    self.check()\n    self = self.refresh()\n    df = data.DataFrame(name)\n    return df.read_view(self)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas() -&gt; DataFrame\n</code></pre> <p>Creates a <code>pandas.DataFrame</code> from the view.</p> <p>Loads the underlying data from the getML Engine and constructs a <code>pandas.DataFrame</code>.</p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Pandas equivalent of the current instance including</p> <code>DataFrame</code> <p>its underlying data.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"Creates a `pandas.DataFrame` from the view.\n\n    Loads the underlying data from the getML Engine and constructs\n    a `pandas.DataFrame`.\n\n    Returns:\n            Pandas equivalent of the current instance including\n            its underlying data.\n    \"\"\"\n    return to_arrow(self).to_pandas()\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.to_placeholder","title":"to_placeholder","text":"<pre><code>to_placeholder(name: Optional[str] = None) -&gt; Placeholder\n</code></pre> <p>Generates a <code>Placeholder</code> from the current <code>View</code>.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the placeholder. If no name is passed, then the name of the placeholder will be identical to the name of the current view.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Placeholder</code> <p>A placeholder with the same name as this data frame.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_placeholder(self, name: Optional[str] = None) -&gt; Placeholder:\n    \"\"\"Generates a [`Placeholder`][getml.data.Placeholder] from the\n    current [`View`][getml.data.View].\n\n    Args:\n        name:\n            The name of the placeholder. If no\n            name is passed, then the name of the placeholder will\n            be identical to the name of the current view.\n\n    Returns:\n            A placeholder with the same name as this data frame.\n\n\n    \"\"\"\n    self.refresh()\n    return Placeholder(name=name or self.name, roles=self.roles)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.to_parquet","title":"to_parquet","text":"<pre><code>to_parquet(\n    fname: str,\n    compression: Literal[\n        \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"\n    ] = \"snappy\",\n)\n</code></pre> <p>Writes the underlying data into a newly created parquet file.</p> PARAMETER DESCRIPTION <code>fname</code> <p>The name of the parquet file. The ending \".parquet\" will be added automatically.</p> <p> TYPE: <code>str</code> </p> <code>compression</code> <p>The compression format to use. Supported values are \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"</p> <p> TYPE: <code>Literal['brotli', 'gzip', 'lz4', 'snappy', 'zstd']</code> DEFAULT: <code>'snappy'</code> </p> Source code in <code>getml/data/view.py</code> <pre><code>def to_parquet(\n    self,\n    fname: str,\n    compression: Literal[\"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"] = \"snappy\",\n):\n    \"\"\"\n    Writes the underlying data into a newly created parquet file.\n\n    Args:\n        fname:\n            The name of the parquet file.\n            The ending \".parquet\" will be added automatically.\n\n        compression:\n            The compression format to use.\n            Supported values are \"brotli\", \"gzip\", \"lz4\", \"snappy\", \"zstd\"\n    \"\"\"\n    to_parquet(self, fname, compression)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.to_pyspark","title":"to_pyspark","text":"<pre><code>to_pyspark(\n    spark: SparkSession, name: Optional[str] = None\n) -&gt; DataFrame\n</code></pre> <p>Creates a <code>pyspark.sql.DataFrame</code> from the current instance.</p> <p>Loads the underlying data from the getML Engine and constructs a <code>pyspark.sql.DataFrame</code>.</p> PARAMETER DESCRIPTION <code>spark</code> <p>The pyspark session in which you want to create the data frame.</p> <p> TYPE: <code>SparkSession</code> </p> <code>name</code> <p>The name of the temporary view to be created on top of the <code>pyspark.sql.DataFrame</code>, with which it can be referred to in Spark SQL (refer to <code>pyspark.sql.DataFrame.createOrReplaceTempView</code>). If none is passed, then the name of this <code>DataFrame</code> will be used.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>Pyspark equivalent of the current instance including</p> <code>DataFrame</code> <p>its underlying data.</p> Source code in <code>getml/data/view.py</code> <pre><code>def to_pyspark(\n    self, spark: pyspark.sql.SparkSession, name: Optional[str] = None\n) -&gt; pyspark.sql.DataFrame:\n    \"\"\"Creates a `pyspark.sql.DataFrame` from the current instance.\n\n    Loads the underlying data from the getML Engine and constructs\n    a `pyspark.sql.DataFrame`.\n\n    Args:\n        spark:\n            The pyspark session in which you want to\n            create the data frame.\n\n        name:\n            The name of the temporary view to be created on top\n            of the `pyspark.sql.DataFrame`,\n            with which it can be referred to\n            in Spark SQL (refer to\n            `pyspark.sql.DataFrame.createOrReplaceTempView`).\n            If none is passed, then the name of this\n            [`DataFrame`][getml.DataFrame] will be used.\n\n    Returns:\n            Pyspark equivalent of the current instance including\n            its underlying data.\n\n    \"\"\"\n    return _to_pyspark(self, name, spark)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.to_s3","title":"to_s3","text":"<pre><code>to_s3(\n    bucket: str,\n    key: str,\n    region: str,\n    sep: str = \",\",\n    batch_size: int = 50000,\n)\n</code></pre> <p>Writes the underlying data into a newly created CSV file located in an S3 bucket.</p> Note <p>S3 is not supported on Windows.</p> PARAMETER DESCRIPTION <code>bucket</code> <p>The bucket from which to read the files.</p> <p> TYPE: <code>str</code> </p> <code>key</code> <p>The key in the S3 bucket in which you want to write the output. The ending \".csv\" and an optional batch number will be added automatically.</p> <p> TYPE: <code>str</code> </p> <code>region</code> <p>The region in which the bucket is located.</p> <p> TYPE: <code>str</code> </p> <code>sep</code> <p>The character used for separating fields.</p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>batch_size</code> <p>Maximum number of lines per file. Set to 0 to read the entire data frame into a single file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>50000</code> </p> Example <pre><code>getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\ngetml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\nyour_view.to_s3(\n    bucket=\"your-bucket-name\",\n    key=\"filename-on-s3\",\n    region=\"us-east-2\",\n    sep=';'\n)\n</code></pre> Source code in <code>getml/data/view.py</code> <pre><code>def to_s3(\n    self,\n    bucket: str,\n    key: str,\n    region: str,\n    sep: str = \",\",\n    batch_size: int = 50000,\n):\n    \"\"\"\n    Writes the underlying data into a newly created CSV file\n    located in an S3 bucket.\n\n    Note:\n        S3 is not supported on Windows.\n\n    Args:\n        bucket:\n            The bucket from which to read the files.\n\n        key:\n            The key in the S3 bucket in which you want to\n            write the output. The ending \".csv\" and an optional\n            batch number will be added automatically.\n\n        region:\n            The region in which the bucket is located.\n\n        sep:\n            The character used for separating fields.\n\n        batch_size:\n            Maximum number of lines per file. Set to 0 to read\n            the entire data frame into a single file.\n\n    ??? example\n        ```python\n        getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n        getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n        your_view.to_s3(\n            bucket=\"your-bucket-name\",\n            key=\"filename-on-s3\",\n            region=\"us-east-2\",\n            sep=';'\n        )\n        ```\n    \"\"\"\n\n    self.refresh()\n\n    if not isinstance(bucket, str):\n        raise TypeError(\"'bucket' must be of type str\")\n\n    if not isinstance(key, str):\n        raise TypeError(\"'fname' must be of type str\")\n\n    if not isinstance(region, str):\n        raise TypeError(\"'region' must be of type str\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be of type str\")\n\n    if not isinstance(batch_size, numbers.Real):\n        raise TypeError(\"'batch_size' must be a real number\")\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"View.to_s3\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"view_\"] = self._getml_deserialize()\n    cmd[\"bucket_\"] = bucket\n    cmd[\"key_\"] = key\n    cmd[\"region_\"] = region\n    cmd[\"sep_\"] = sep\n    cmd[\"batch_size_\"] = batch_size\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.where","title":"where","text":"<pre><code>where(\n    index: Optional[\n        Union[\n            BooleanColumnView, FloatColumn, FloatColumnView\n        ]\n    ]\n) -&gt; View\n</code></pre> <p>Extract a subset of rows.</p> <p>Creates a new <code>View</code> as a subselection of the current instance.</p> PARAMETER DESCRIPTION <code>index</code> <p>Boolean column indicating the rows you want to select.</p> <p> TYPE: <code>Optional[Union[BooleanColumnView, FloatColumn, FloatColumnView]]</code> </p> RETURNS DESCRIPTION <code>View</code> <p>A new view containing only the rows that satisfy the condition.</p> Example <p>Generate example data: <pre><code>data = dict(\n    fruit=[\"banana\", \"apple\", \"cherry\", \"cherry\", \"melon\", \"pineapple\"],\n    price=[2.4, 3.0, 1.2, 1.4, 3.4, 3.4],\n    join_key=[\"0\", \"1\", \"2\", \"2\", \"3\", \"3\"])\n\nfruits = getml.DataFrame.from_dict(data, name=\"fruits\",\nroles={\"categorical\": [\"fruit\"], \"join_key\": [\"join_key\"], \"numerical\": [\"price\"]})\n\nfruits\n</code></pre> <pre><code>| join_key | fruit       | price     |\n| join key | categorical | numerical |\n--------------------------------------\n| 0        | banana      | 2.4       |\n| 1        | apple       | 3         |\n| 2        | cherry      | 1.2       |\n| 2        | cherry      | 1.4       |\n| 3        | melon       | 3.4       |\n| 3        | pineapple   | 3.4       |\n</code></pre> Apply where condition. This creates a new DataFrame called \"cherries\":</p> <p><pre><code>cherries = fruits.where(\n    fruits[\"fruit\"] == \"cherry\")\n\ncherries\n</code></pre> <pre><code>| join_key | fruit       | price     |\n| join key | categorical | numerical |\n--------------------------------------\n| 2        | cherry      | 1.2       |\n| 2        | cherry      | 1.4       |\n</code></pre></p> Source code in <code>getml/data/view.py</code> <pre><code>def where(\n    self, index: Optional[Union[BooleanColumnView, FloatColumn, FloatColumnView]]\n) -&gt; View:\n    \"\"\"Extract a subset of rows.\n\n    Creates a new [`View`][getml.data.View] as a\n    subselection of the current instance.\n\n    Args:\n        index:\n            Boolean column indicating the rows you want to select.\n\n    Returns:\n        A new view containing only the rows that satisfy the condition.\n\n    ??? example\n        Generate example data:\n        ```python\n        data = dict(\n            fruit=[\"banana\", \"apple\", \"cherry\", \"cherry\", \"melon\", \"pineapple\"],\n            price=[2.4, 3.0, 1.2, 1.4, 3.4, 3.4],\n            join_key=[\"0\", \"1\", \"2\", \"2\", \"3\", \"3\"])\n\n        fruits = getml.DataFrame.from_dict(data, name=\"fruits\",\n        roles={\"categorical\": [\"fruit\"], \"join_key\": [\"join_key\"], \"numerical\": [\"price\"]})\n\n        fruits\n        ```\n        ```\n        | join_key | fruit       | price     |\n        | join key | categorical | numerical |\n        --------------------------------------\n        | 0        | banana      | 2.4       |\n        | 1        | apple       | 3         |\n        | 2        | cherry      | 1.2       |\n        | 2        | cherry      | 1.4       |\n        | 3        | melon       | 3.4       |\n        | 3        | pineapple   | 3.4       |\n        ```\n        Apply where condition. This creates a new DataFrame called \"cherries\":\n\n        ```python\n\n        cherries = fruits.where(\n            fruits[\"fruit\"] == \"cherry\")\n\n        cherries\n        ```\n        ```\n        | join_key | fruit       | price     |\n        | join key | categorical | numerical |\n        --------------------------------------\n        | 2        | cherry      | 1.2       |\n        | 2        | cherry      | 1.4       |\n        ```\n    \"\"\"\n    return _where(self, index)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.with_column","title":"with_column","text":"<pre><code>with_column(\n    col: Union[\n        bool,\n        str,\n        float,\n        int,\n        datetime64,\n        FloatColumn,\n        FloatColumnView,\n        StringColumn,\n        StringColumnView,\n        BooleanColumnView,\n    ],\n    name: str,\n    role: Optional[Role] = None,\n    unit: str = \"\",\n    subroles: Optional[\n        Union[Subrole, Iterable[str]]\n    ] = None,\n    time_formats: Optional[List[str]] = None,\n) -&gt; View\n</code></pre> <p>Returns a new <code>View</code> that contains an additional column.</p> PARAMETER DESCRIPTION <code>col</code> <p>The column to be added.</p> <p> TYPE: <code>Union[bool, str, float, int, datetime64, FloatColumn, FloatColumnView, StringColumn, StringColumnView, BooleanColumnView]</code> </p> <code>name</code> <p>Name of the new column.</p> <p> TYPE: <code>str</code> </p> <code>role</code> <p>Role of the new column. Must be from <code>roles</code>.</p> <p> TYPE: <code>Optional[Role]</code> DEFAULT: <code>None</code> </p> <code>subroles</code> <p>Subroles of the new column. Must be from <code>subroles</code>.</p> <p> TYPE: <code>Optional[Union[Subrole, Iterable[str]]]</code> DEFAULT: <code>None</code> </p> <code>unit</code> <p>Unit of the column.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>time_formats</code> <p>Formats to be used to parse the time stamps.</p> <p>This is only necessary, if an implicit conversion from a <code>StringColumn</code> to a time stamp is taking place.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>View</code> <p>A new view containing the additional column.</p> Source code in <code>getml/data/view.py</code> <pre><code>def with_column(\n    self,\n    col: Union[\n        bool,\n        str,\n        float,\n        int,\n        np.datetime64,\n        FloatColumn,\n        FloatColumnView,\n        StringColumn,\n        StringColumnView,\n        BooleanColumnView,\n    ],\n    name: str,\n    role: Optional[Role] = None,\n    unit: str = \"\",\n    subroles: Optional[Union[Subrole, Iterable[str]]] = None,\n    time_formats: Optional[List[str]] = None,\n) -&gt; View:\n    \"\"\"Returns a new [`View`][getml.data.View] that contains an additional column.\n\n    Args:\n        col:\n            The column to be added.\n\n        name:\n            Name of the new column.\n\n        role:\n            Role of the new column. Must be from [`roles`][getml.data.roles].\n\n        subroles:\n            Subroles of the new column. Must be from [`subroles`][getml.data.subroles].\n\n        unit:\n            Unit of the column.\n\n        time_formats:\n            Formats to be used to parse the time stamps.\n\n            This is only necessary, if an implicit conversion from\n            a [`StringColumn`][getml.data.columns.StringColumn] to a time\n            stamp is taking place.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n    Returns:\n        A new view containing the additional column.\n    \"\"\"\n    col, role, subroles = _with_column(\n        col, name, role, subroles, unit, time_formats\n    )\n    return View(\n        base=self,\n        added={\n            \"col_\": col,\n            \"name_\": name,\n            \"role_\": role,\n            \"subroles_\": subroles,\n            \"unit_\": unit,\n        },\n    )\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.with_name","title":"with_name","text":"<pre><code>with_name(name: str) -&gt; View\n</code></pre> <p>Returns a new <code>View</code> with a new name.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the new view.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>View</code> <p>A new view with the new name.</p> Source code in <code>getml/data/view.py</code> <pre><code>def with_name(self, name: str) -&gt; View:\n    \"\"\"Returns a new [`View`][getml.data.View] with a new name.\n\n    Args:\n        name (str):\n            The name of the new view.\n\n    Returns:\n        A new view with the new name.\n    \"\"\"\n    return View(base=self, name=name)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.with_role","title":"with_role","text":"<pre><code>with_role(\n    names: Union[str, List[str]],\n    role: str,\n    time_formats: Optional[List[str]] = None,\n) -&gt; View\n</code></pre> <p>Returns a new <code>View</code> with modified roles.</p> <p>When switching from a role based on type float to a role based on type string or vice verse, an implicit type conversion will be conducted. The <code>time_formats</code> argument is used to interpret time format string: <code>annotating_roles_time_stamp</code>. For more information on roles, please refer to the User Guide.</p> PARAMETER DESCRIPTION <code>names</code> <p>The name or names of the column.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>role</code> <p>The role to be assigned.</p> <p> TYPE: <code>str</code> </p> <code>time_formats</code> <p>Formats to be used to parse the time stamps. This is only necessary, if an implicit conversion from a StringColumn to a time stamp is taking place.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>View</code> <p>A new view with the modified roles.</p> Source code in <code>getml/data/view.py</code> <pre><code>def with_role(\n    self,\n    names: Union[str, List[str]],\n    role: str,\n    time_formats: Optional[List[str]] = None,\n) -&gt; View:\n    \"\"\"Returns a new [`View`][getml.data.View] with modified roles.\n\n    When switching from a role based on type float to a role based on type\n    string or vice verse, an implicit type conversion will be conducted.\n    The `time_formats` argument is used to interpret time\n    format string: `annotating_roles_time_stamp`. For more information on\n    roles, please refer to the [User Guide][annotating-data].\n\n    Args:\n        names:\n            The name or names of the column.\n\n        role:\n            The role to be assigned.\n\n        time_formats:\n            Formats to be used to parse the time stamps.\n            This is only necessary, if an implicit conversion from a StringColumn to\n            a time stamp is taking place.\n\n    Returns:\n        A new view with the modified roles.\n    \"\"\"\n    return _with_role(self, names, role, time_formats)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.with_subroles","title":"with_subroles","text":"<pre><code>with_subroles(\n    names: Union[str, List[str]],\n    subroles: Union[Subrole, Iterable[str]],\n    append: bool = True,\n) -&gt; View\n</code></pre> <p>Returns a new view with one or several new subroles on one or more columns.</p> PARAMETER DESCRIPTION <code>names</code> <p>The name or names of the column.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>subroles</code> <p>The subroles to be assigned.</p> <p> TYPE: <code>Union[Subrole, Iterable[str]]</code> </p> <code>append</code> <p>Whether you want to append the new subroles to the existing subroles.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>View</code> <p>A new view with the modified subroles.</p> Source code in <code>getml/data/view.py</code> <pre><code>def with_subroles(\n    self,\n    names: Union[str, List[str]],\n    subroles: Union[Subrole, Iterable[str]],\n    append: bool = True,\n) -&gt; View:\n    \"\"\"Returns a new view with one or several new subroles on one or more columns.\n\n    Args:\n        names:\n            The name or names of the column.\n\n        subroles:\n            The subroles to be assigned.\n\n        append:\n            Whether you want to append the\n            new subroles to the existing subroles.\n\n    Returns:\n        A new view with the modified subroles.\n    \"\"\"\n    return _with_subroles(self, names, subroles, append)\n</code></pre>"},{"location":"reference/data/view/#getml.data.View.with_unit","title":"with_unit","text":"<pre><code>with_unit(\n    names: Union[str, List[str]],\n    unit: str,\n    comparison_only: bool = False,\n) -&gt; View\n</code></pre> <p>Returns a view that contains a new unit on one or more columns.</p> PARAMETER DESCRIPTION <code>names</code> <p>The name or names of the column.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>unit</code> <p>The unit to be assigned.</p> <p> TYPE: <code>str</code> </p> <code>comparison_only</code> <p>Whether you want the column to be used for comparison only. This means that the column can only be used in comparison to other columns of the same unit.</p> <p>An example might be a bank account number: The number in itself is hardly interesting, but it might be useful to know how often we have seen that same bank account number in another table.</p> <p>If True, this will append \", comparison only\" to the unit. The feature learning algorithms and the feature selectors will interpret this accordingly.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>View</code> <p>A new view with the modified unit.</p> Source code in <code>getml/data/view.py</code> <pre><code>def with_unit(\n    self, names: Union[str, List[str]], unit: str, comparison_only: bool = False\n) -&gt; View:\n    \"\"\"Returns a view that contains a new unit on one or more columns.\n\n    Args:\n        names:\n            The name or names of the column.\n\n        unit:\n            The unit to be assigned.\n\n        comparison_only:\n            Whether you want the column to\n            be used for comparison only. This means that the column can\n            only be used in comparison to other columns of the same unit.\n\n            An example might be a bank account number: The number in itself\n            is hardly interesting, but it might be useful to know how often\n            we have seen that same bank account number in another table.\n\n            If True, this will append \", comparison only\" to the unit.\n            The feature learning algorithms and the feature selectors will\n            interpret this accordingly.\n\n    Returns:\n        A new view with the modified unit.\n    \"\"\"\n    return _with_unit(self, names, unit, comparison_only)\n</code></pre>"},{"location":"reference/database/database/","title":"Database","text":""},{"location":"reference/database/database/#getml.database","title":"getml.database","text":"<p>This module contains communication routines to access various databases.</p> <p>The <code>connect_bigquery</code>, <code>connect_hana</code>, <code>connect_greenplum</code>, <code>connect_mariadb</code>, <code>connect_mysql</code>, <code>connect_postgres</code>, and <code>connect_sqlite3</code> functions establish a connection between a database and the getML Engine. During the data import using either the <code>read_db</code> or <code>read_query</code> methods of a <code>DataFrame</code> instance or the corresponding <code>from_db</code> class method all data will be directly loaded from the database into the Engine without ever passing the Python interpreter.</p> <p>In addition, several auxiliary functions that might be handy during the analysis and interaction with the database are provided.</p>"},{"location":"reference/database/database/#getml.database.Connection","title":"Connection","text":"<pre><code>Connection(conn_id: str = 'default')\n</code></pre> <p>A handle to a database connection on the getML Engine.</p> ATTRIBUTE DESCRIPTION <code>conn_id</code> <p>The name you want to use to reference the connection. You can call it anything you want to. If a database connection with the same conn_id already exists, that connection will be removed automatically and the new connection will take its place. The default conn_id is \"default\", which refers to the default connection. If you do not explicitly pass a connection handle to any function that relates to a database, the default connection will be used automatically.</p> <p> </p> Source code in <code>getml/database/connection.py</code> <pre><code>def __init__(self, conn_id: str = \"default\"):\n    self.conn_id = conn_id\n</code></pre>"},{"location":"reference/database/database/#getml.database.connect_bigquery.connect_bigquery","title":"connect_bigquery","text":"<pre><code>connect_bigquery(\n    database_id: str,\n    project_id: str,\n    google_application_credentials: Union[str, Path],\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection\n</code></pre> <p>Creates a new BigQuery database connection.</p> <p>enterprise-adm: Enterprise edition This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> PARAMETER DESCRIPTION <code>database_id</code> <p>The ID of the database to connect to.</p> <p> TYPE: <code>str</code> </p> <code>project_id</code> <p>The ID of the project to connect to.</p> <p> TYPE: <code>str</code> </p> <code>google_application_credentials</code> <p>The path of the Google application credentials. (Must be located on the machine hosting the getML Engine).</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>time_formats</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn_id</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> RETURNS DESCRIPTION <code>Connection</code> <p>The connection object.</p> Source code in <code>getml/database/connect_bigquery.py</code> <pre><code>def connect_bigquery(\n    database_id: str,\n    project_id: str,\n    google_application_credentials: Union[str, Path],\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection:\n    \"\"\"\n    Creates a new BigQuery database connection.\n\n    enterprise-adm: Enterprise edition\n    This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the [benefits of the Enterprise edition][enterprise-benefits] and [compare their features][enterprise-feature-list].\n\n    For licensing information and technical support, please [contact us][contact-page].\n\n    Args:\n        database_id:\n            The ID of the database to connect to.\n\n        project_id:\n            The ID of the project to connect to.\n\n        google_application_credentials:\n            The path of the Google application credentials.\n            (Must be located on the machine hosting the getML Engine).\n\n        time_formats:\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id:\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Returns:\n        The connection object.\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"database_id_\"] = database_id\n    cmd[\"project_id_\"] = project_id\n    cmd[\"google_application_credentials_\"] = os.path.abspath(\n        str(google_application_credentials)\n    )\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.new\"\n    cmd[\"db_\"] = \"bigquery\"\n\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The API expects a password, but in this case there is none\n        comm.send_string(sock, \"\")\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.handle_engine_exception(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/database/#getml.database.connect_greenplum.connect_greenplum","title":"connect_greenplum","text":"<pre><code>connect_greenplum(\n    dbname: str,\n    user: str,\n    password: str,\n    host: str,\n    hostaddr: str,\n    port: int = 5432,\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection\n</code></pre> <p>Creates a new Greenplum database connection.</p> <p>But first, make sure your database is running, and you can reach it from your command line.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> PARAMETER DESCRIPTION <code>dbname</code> <p>The name of the database to which you want to connect.</p> <p> TYPE: <code>str</code> </p> <code>user</code> <p>Username with which to log into the Greenplum database.</p> <p> TYPE: <code>str</code> </p> <code>password</code> <p>Password with which to log into the Greenplum database.</p> <p> TYPE: <code>str</code> </p> <code>host</code> <p>Host of the Greenplum database.</p> <p> TYPE: <code>str</code> </p> <code>hostaddr</code> <p>IP address of the Greenplum database.</p> <p> TYPE: <code>str</code> </p> <code>port</code> <p>Port of the Greenplum database.</p> <p>The default port used by Greenplum is 5432.</p> <p>If you do not know, which port to use, type the following into your Greenplum client:</p> <pre><code>SELECT setting FROM pg_settings WHERE name = 'port';\n</code></pre> <p> TYPE: <code>int</code> DEFAULT: <code>5432</code> </p> <code>time_formats</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn_id</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> RETURNS DESCRIPTION <code>Connection</code> <p>The connection object.</p> Note <p>By selecting an existing table of your database in <code>from_db</code> function, you can create a new <code>DataFrame</code> containing all its data. Alternatively you can use the <code>read_db</code> and <code>read_query</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p> <p>You can also write your results back into the Greenplum database. By passing the name for the destination table to <code>transform</code>, the features generated from your raw data will be written back. Passing them into <code>predict</code>, instead, makes predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p> Source code in <code>getml/database/connect_greenplum.py</code> <pre><code>def connect_greenplum(\n    dbname: str,\n    user: str,\n    password: str,\n    host: str,\n    hostaddr: str,\n    port: int = 5432,\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection:\n    \"\"\"Creates a new Greenplum database connection.\n\n    But first, make sure your database is running, and you can reach it\n    from your command line.\n\n    enterprise-adm: Enterprise edition\n        This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the [benefits of the Enterprise edition][enterprise-benefits] and [compare their features][enterprise-feature-list].\n\n        For licensing information and technical support, please [contact us][contact-page].\n\n    Args:\n        dbname:\n            The name of the database to which you want to connect.\n\n        user:\n            Username with which to log into the Greenplum database.\n\n        password:\n            Password with which to log into the Greenplum database.\n\n        host:\n            Host of the Greenplum database.\n\n        hostaddr:\n            IP address of the Greenplum database.\n\n        port:\n            Port of the Greenplum database.\n\n            The default port used by Greenplum is 5432.\n\n            If you do not know, which port to use, type the following into your\n            Greenplum client:\n\n            ```sql\n            SELECT setting FROM pg_settings WHERE name = 'port';\n            ```\n        time_formats:\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id:\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Returns:\n        The connection object.\n\n    Note:\n        By selecting an existing table of your database in\n        [`from_db`][getml.DataFrame.from_db] function, you can create\n        a new [`DataFrame`][getml.DataFrame] containing all its data.\n        Alternatively you can use the\n        [`read_db`][getml.DataFrame.read_db] and\n        [`read_query`][getml.DataFrame.read_query] methods to replace\n        the content of the current [`DataFrame`][getml.DataFrame]\n        instance or append further rows based on either a table or a\n        specific query.\n\n        You can also write your results back into the Greenplum\n        database. By passing the name for the destination table to\n        [`transform`][getml.Pipeline.transform], the features\n        generated from your raw data will be written back. Passing\n        them into [`predict`][getml.Pipeline.predict], instead,\n        makes predictions of the target variables to new, unseen data\n        and stores the result into the corresponding table.\n\n\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.new\"\n    cmd[\"db_\"] = \"greenplum\"\n\n    cmd[\"host_\"] = host\n    cmd[\"hostaddr_\"] = hostaddr\n    cmd[\"port_\"] = port\n    cmd[\"dbname_\"] = dbname\n    cmd[\"user_\"] = user\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The password is sent separately, so it doesn't\n        # end up in the logs.\n        comm.send_string(sock, password)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.handle_engine_exception(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/database/#getml.database.connect_hana.connect_hana","title":"connect_hana","text":"<pre><code>connect_hana(\n    user: str,\n    password: str,\n    host: str,\n    port: int = 39017,\n    default_schema: Optional[str] = \"public\",\n    ping_interval: int = 0,\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection\n</code></pre> <p>Creates a new HANA database connection.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> PARAMETER DESCRIPTION <code>user</code> <p>Username with which to log into the HANA database.</p> <p> TYPE: <code>str</code> </p> <code>password</code> <p>Password with which to log into the HANA database.</p> <p> TYPE: <code>str</code> </p> <code>host</code> <p>Host of the HANA database.</p> <p> TYPE: <code>str</code> </p> <code>port</code> <p>Port of the database.</p> <p> TYPE: <code>int</code> DEFAULT: <code>39017</code> </p> <code>default_schema</code> <p>The schema within the database you want to connect use unless another schema is explicitly set.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>'public'</code> </p> <code>ping_interval</code> <p>The interval at which you want to ping the database, in seconds. Set to 0 for no pings at all.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>time_formats</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn_id</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> RETURNS DESCRIPTION <code>Connection</code> <p>The connection object.</p> Source code in <code>getml/database/connect_hana.py</code> <pre><code>def connect_hana(\n    user: str,\n    password: str,\n    host: str,\n    port: int = 39017,\n    default_schema: Optional[str] = \"public\",\n    ping_interval: int = 0,\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection:\n    \"\"\"\n    Creates a new HANA database connection.\n\n    enterprise-adm: Enterprise edition\n        This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the [benefits of the Enterprise edition][enterprise-benefits] and [compare their features][enterprise-feature-list].\n\n        For licensing information and technical support, please [contact us][contact-page].\n\n    Args:\n        user:\n            Username with which to log into the HANA database.\n\n        password:\n            Password with which to log into the HANA database.\n\n        host:\n            Host of the HANA database.\n\n        port:\n            Port of the database.\n\n        default_schema:\n            The schema within the database you want to connect\n            use unless another schema is explicitly set.\n\n        ping_interval:\n            The interval at which you want to ping the database,\n            in seconds. Set to 0 for no pings at all.\n\n        time_formats:\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id (str, optional):\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Returns:\n        The connection object.\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.new\"\n    cmd[\"db_\"] = \"sap_hana\"\n\n    cmd[\"host_\"] = host\n    cmd[\"port_\"] = port\n    cmd[\"default_schema_\"] = default_schema\n    cmd[\"user_\"] = user\n    cmd[\"ping_interval_\"] = ping_interval\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The password is sent separately, so it doesn't\n        # end up in the logs.\n        comm.send_string(sock, password)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.handle_engine_exception(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/database/#getml.database.connect_mariadb.connect_mariadb","title":"connect_mariadb","text":"<pre><code>connect_mariadb(\n    dbname: str,\n    user: str,\n    password: str,\n    host: str,\n    port: int = 3306,\n    unix_socket: str = \"/var/run/mysqld/mysqld.sock\",\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection\n</code></pre> <p>Creates a new MariaDB database connection.</p> <p>But first, make sure your database is running and you can reach it from via your command line.</p> PARAMETER DESCRIPTION <code>dbname</code> <p>The name of the database to which you want to connect.</p> <p> TYPE: <code>str</code> </p> <code>user</code> <p>Username with which to log into the MariaDB database.</p> <p> TYPE: <code>str</code> </p> <code>password</code> <p>Password with which to log into the MariaDB database.</p> <p> TYPE: <code>str</code> </p> <code>host</code> <p>Host of the MariaDB database.</p> <p> TYPE: <code>str</code> </p> <code>port</code> <p>Port of the MariaDB database.</p> <p>The default port for MariaDB is 3306.</p> <p>If you do not know which port to use, type</p> <p><pre><code>SELECT @@port;\n</code></pre> into your MariaDB client.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3306</code> </p> <code>unix_socket</code> <p>The UNIX socket used to connect to the MariaDB database.</p> <p>If you do not know which UNIX socket to use, type</p> <p><pre><code>SELECT @@socket;\n</code></pre> into your MariaDB client.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'/var/run/mysqld/mysqld.sock'</code> </p> <code>time_formats</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn_id</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> RETURNS DESCRIPTION <code>Connection</code> <p>The connection object.</p> Note <p>By selecting an existing table of your database in <code>from_db</code> function, you can create a new <code>DataFrame</code> containing all its data. Alternatively you can use the <code>read_db</code> and <code>read_query</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p> <p>You can also write your results back into the MariaDB database. By passing the name for the destination table to <code>transform</code>, the features generated from your raw data will be written back. Passing them into <code>predict</code>, instead, makes predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p> Source code in <code>getml/database/connect_mariadb.py</code> <pre><code>def connect_mariadb(\n    dbname: str,\n    user: str,\n    password: str,\n    host: str,\n    port: int = 3306,\n    unix_socket: str = \"/var/run/mysqld/mysqld.sock\",\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection:\n    \"\"\"\n    Creates a new MariaDB database connection.\n\n    But first, make sure your database is running and you can reach it\n    from via your command line.\n\n    Args:\n        dbname:\n            The name of the database to which you want to connect.\n\n        user:\n            Username with which to log into the MariaDB database.\n\n        password:\n            Password with which to log into the MariaDB database.\n\n        host:\n            Host of the MariaDB database.\n\n        port:\n            Port of the MariaDB database.\n\n            The default port for MariaDB is 3306.\n\n            If you do not know which port to use, type\n\n            ```sql\n            SELECT @@port;\n            ```\n            into your MariaDB client.\n\n        unix_socket:\n            The UNIX socket used to connect to the MariaDB database.\n\n            If you do not know which UNIX socket to use, type\n\n            ```sql\n            SELECT @@socket;\n            ```\n            into your MariaDB client.\n\n        time_formats:\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id:\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Returns:\n        The connection object.\n\n    Note:\n        By selecting an existing table of your database in\n        [`from_db`][getml.DataFrame.from_db] function, you can create\n        a new [`DataFrame`][getml.DataFrame] containing all its data.\n        Alternatively you can use the\n        [`read_db`][getml.DataFrame.read_db] and\n        [`read_query`][getml.DataFrame.read_query] methods to replace\n        the content of the current [`DataFrame`][getml.DataFrame]\n        instance or append further rows based on either a table or a\n        specific query.\n\n        You can also write your results back into the MariaDB\n        database. By passing the name for the destination table to\n        [`transform`][getml.Pipeline.transform], the features\n        generated from your raw data will be written back. Passing\n        them into [`predict`][getml.Pipeline.predict], instead,\n        makes predictions\n        of the target variables to new, unseen data and stores the result into\n        the corresponding table.\n\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.new\"\n    cmd[\"db_\"] = \"mariadb\"\n\n    cmd[\"host_\"] = host\n    cmd[\"port_\"] = port\n    cmd[\"dbname_\"] = dbname\n    cmd[\"user_\"] = user\n    cmd[\"unix_socket_\"] = unix_socket\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The password is sent separately, so it doesn't\n        # end up in the logs.\n        comm.send_string(sock, password)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.handle_engine_exception(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/database/#getml.database.connect_mysql.connect_mysql","title":"connect_mysql","text":"<pre><code>connect_mysql(\n    dbname: str,\n    user: str,\n    password: str,\n    host: str,\n    port: int = 3306,\n    unix_socket: str = \"/var/run/mysqld/mysqld.sock\",\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection\n</code></pre> <p>Creates a new MySQL database connection.</p> <p>But first, make sure your database is running and you can reach it from via your command line.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> PARAMETER DESCRIPTION <code>dbname</code> <p>The name of the database to which you want to connect.</p> <p> TYPE: <code>str</code> </p> <code>user</code> <p>Username with which to log into the MySQL database.</p> <p> TYPE: <code>str</code> </p> <code>password</code> <p>Password with which to log into the MySQL database.</p> <p> TYPE: <code>str</code> </p> <code>host</code> <p>Host of the MySQL database.</p> <p> TYPE: <code>str</code> </p> <code>port</code> <p>Port of the MySQL database.</p> <p>The default port for MySQL is 3306.</p> <p>If you do not know which port to use, type</p> <p><pre><code>SELECT @@port;\n</code></pre> into your mysql client.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3306</code> </p> <code>unix_socket</code> <p>The UNIX socket used to connect to the MySQL database.</p> <p>If you do not know which UNIX socket to use, type</p> <p><pre><code>SELECT @@socket;\n</code></pre> into your mysql client.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'/var/run/mysqld/mysqld.sock'</code> </p> <code>time_formats</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn_id</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> RETURNS DESCRIPTION <code>Connection</code> <p>The connection object.</p> Note <p>By selecting an existing table of your database in <code>from_db</code> function, you can create a new <code>DataFrame</code> containing all its data. Alternatively you can use the <code>read_db</code> and <code>read_query</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p> <p>You can also write your results back into the MySQL database. By passing the name for the destination table to <code>transform</code>, the features generated from your raw data will be written back. Passing them into <code>predict</code>, instead, makes predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p> Source code in <code>getml/database/connect_mysql.py</code> <pre><code>def connect_mysql(\n    dbname: str,\n    user: str,\n    password: str,\n    host: str,\n    port: int = 3306,\n    unix_socket: str = \"/var/run/mysqld/mysqld.sock\",\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection:\n    \"\"\"\n    Creates a new MySQL database connection.\n\n    But first, make sure your database is running and you can reach it\n    from via your command line.\n\n    enterprise-adm: Enterprise edition\n        This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the [benefits of the Enterprise edition][enterprise-benefits] and [compare their features][enterprise-feature-list].\n\n        For licensing information and technical support, please [contact us][contact-page].\n\n    Args:\n        dbname:\n            The name of the database to which you want to connect.\n\n        user:\n            Username with which to log into the MySQL database.\n\n        password:\n            Password with which to log into the MySQL database.\n\n        host:\n            Host of the MySQL database.\n\n        port:\n            Port of the MySQL database.\n\n            The default port for MySQL is 3306.\n\n            If you do not know which port to use, type\n\n            ```sql\n            SELECT @@port;\n            ```\n            into your mysql client.\n\n        unix_socket:\n            The UNIX socket used to connect to the MySQL database.\n\n            If you do not know which UNIX socket to use, type\n\n            ```sql\n            SELECT @@socket;\n            ```\n            into your mysql client.\n\n        time_formats:\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id:\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Returns:\n        The connection object.\n\n    Note:\n        By selecting an existing table of your database in\n        [`from_db`][getml.DataFrame.from_db] function, you can create\n        a new [`DataFrame`][getml.DataFrame] containing all its data.\n        Alternatively you can use the\n        [`read_db`][getml.DataFrame.read_db] and\n        [`read_query`][getml.DataFrame.read_query] methods to replace\n        the content of the current [`DataFrame`][getml.DataFrame]\n        instance or append further rows based on either a table or a\n        specific query.\n\n        You can also write your results back into the MySQL\n        database. By passing the name for the destination table to\n        [`transform`][getml.Pipeline.transform], the features\n        generated from your raw data will be written back. Passing\n        them into [`predict`][getml.Pipeline.predict], instead,\n        makes predictions of the target variables to new, unseen data\n        and stores the result into the corresponding table.\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.new\"\n    cmd[\"db_\"] = \"mariadb\"\n\n    cmd[\"host_\"] = host\n    cmd[\"port_\"] = port\n    cmd[\"dbname_\"] = dbname\n    cmd[\"user_\"] = user\n    cmd[\"unix_socket_\"] = unix_socket\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The password is sent separately, so it doesn't\n        # end up in the logs.\n        comm.send_string(sock, password)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.handle_engine_exception(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/database/#getml.database.connect_odbc.connect_odbc","title":"connect_odbc","text":"<pre><code>connect_odbc(\n    server_name: str,\n    user: str = \"\",\n    password: str = \"\",\n    escape_chars: str = '\"',\n    double_precision: str = \"DOUBLE PRECISION\",\n    integer: str = \"INTEGER\",\n    text: str = \"TEXT\",\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection\n</code></pre> <p>Creates a new ODBC database connection.</p> <p>ODBC is standardized format that can be used to connect to almost any database.</p> <p>Before you use the ODBC connector, make sure the database is up and running and that the appropriate ODBC drivers are installed.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> PARAMETER DESCRIPTION <code>server_name</code> <p>The server name, as referenced in your .obdc.ini file.</p> <p> TYPE: <code>str</code> </p> <code>user</code> <p>Username with which to log into the database. You do not need to pass this, if it is already contained in your .odbc.ini.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>password</code> <p>Password with which to log into the database. You do not need to pass this, if it is already contained in your .odbc.ini.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>escape_chars</code> <p>ODBC drivers are supposed to support escaping table and column names using '\"' characters irrespective of the syntax in the target database. Unfortunately, not all ODBC drivers follow this standard. This is why some tuning might be necessary.</p> <p>The escape_chars value behaves as follows:</p> <ul> <li> <p>If you pass an empty string, schema, table and column names will not   be escaped at all. This is not a problem unless some table   or column names are identical to SQL keywords.</p> </li> <li> <p>If you pass a single character, schema, table and column names will   be enveloped in that character: \"TABLE_NAME\".\"COLUMN_NAME\" (standard SQL)   or <code>TABLE_NAME</code>.<code>COLUMN_NAME</code> (MySQL/MariaDB style).</p> </li> <li> <p>If you pass two characters, table, column and schema names will be   enveloped between these to characters. For instance, if you pass \"[]\",   the produced queries look as follows:   [TABLE_NAME].[COLUMN_NAME] (MS SQL Server style).</p> </li> <li> <p>If you pass more than two characters, the Engine will throw an exception.</p> </li> </ul> <p> TYPE: <code>str</code> DEFAULT: <code>'\"'</code> </p> <code>double_precision</code> <p>The keyword used for double precision columns.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'DOUBLE PRECISION'</code> </p> <code>integer</code> <p>The keyword used for integer columns.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'INTEGER'</code> </p> <code>text</code> <p>The keyword used for text columns.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'TEXT'</code> </p> <code>time_formats</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>List[str]</code> DEFAULT: <code>None</code> </p> <code>conn_id</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> RETURNS DESCRIPTION <code>Connection</code> <p>The connection object.</p> Source code in <code>getml/database/connect_odbc.py</code> <pre><code>def connect_odbc(\n    server_name: str,\n    user: str = \"\",\n    password: str = \"\",\n    escape_chars: str = '\"',\n    double_precision: str = \"DOUBLE PRECISION\",\n    integer: str = \"INTEGER\",\n    text: str = \"TEXT\",\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection:\n    \"\"\"\n    Creates a new ODBC database connection.\n\n    ODBC is standardized format that can be used to connect to almost any\n    database.\n\n    Before you use the ODBC connector, make sure the database is up and\n    running and that the appropriate ODBC drivers are installed.\n\n    enterprise-adm: Enterprise edition\n        This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the [benefits of the Enterprise edition][enterprise-benefits] and [compare their features][enterprise-feature-list].\n\n        For licensing information and technical support, please [contact us][contact-page].\n\n    Args:\n        server_name:\n            The server name, as referenced in your .obdc.ini file.\n\n        user:\n            Username with which to log into the database.\n            You do not need to pass this, if it is already contained in your\n            .odbc.ini.\n\n        password:\n            Password with which to log into the database.\n            You do not need to pass this, if it is already contained in your\n            .odbc.ini.\n\n        escape_chars:\n            ODBC drivers are supposed to support\n            escaping table and column names using '\"' characters irrespective of the\n            syntax in the target database. Unfortunately, not all ODBC drivers\n            follow this standard. This is why some\n            tuning might be necessary.\n\n            The escape_chars value behaves as follows:\n\n            * If you pass an empty string, schema, table and column names will not\n              be escaped at all. This is not a problem unless some table\n              or column names are identical to SQL keywords.\n\n            * If you pass a single character, schema, table and column names will\n              be enveloped in that character: \"TABLE_NAME\".\"COLUMN_NAME\" (standard SQL)\n              or `TABLE_NAME`.`COLUMN_NAME` (MySQL/MariaDB style).\n\n            * If you pass two characters, table, column and schema names will be\n              enveloped between these to characters. For instance, if you pass \"[]\",\n              the produced queries look as follows:\n              [TABLE_NAME].[COLUMN_NAME] (MS SQL Server style).\n\n            * If you pass more than two characters, the Engine will throw an exception.\n\n        double_precision:\n            The keyword used for double precision columns.\n\n        integer:\n            The keyword used for integer columns.\n\n        text:\n            The keyword used for text columns.\n\n        time_formats (List[str], optional):\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id:\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Returns:\n        The connection object.\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.new\"\n    cmd[\"db_\"] = \"odbc\"\n\n    cmd[\"server_name_\"] = server_name\n    cmd[\"user_\"] = user\n    cmd[\"escape_chars_\"] = escape_chars\n    cmd[\"double_precision_\"] = double_precision\n    cmd[\"integer_\"] = integer\n    cmd[\"text_\"] = text\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The password is sent separately, so it doesn't\n        # end up in the logs.\n        comm.send_string(sock, password)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.handle_engine_exception(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/database/#getml.database.connect_postgres.connect_postgres","title":"connect_postgres","text":"<pre><code>connect_postgres(\n    dbname: str,\n    user: str,\n    password: str,\n    host: Optional[str] = None,\n    hostaddr: Optional[str] = None,\n    port: int = 5432,\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection\n</code></pre> <p>Creates a new PostgreSQL database connection.</p> <p>But first, make sure your database is running, and you can reach it from via your command line.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> PARAMETER DESCRIPTION <code>dbname</code> <p>The name of the database to which you want to connect.</p> <p> TYPE: <code>str</code> </p> <code>user</code> <p>Username with which to log into the PostgreSQL database.</p> <p> TYPE: <code>str</code> </p> <code>password</code> <p>Password with which to log into the PostgreSQL database.</p> <p> TYPE: <code>str</code> </p> <code>host</code> <p>Host of the PostgreSQL database.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>hostaddr</code> <p>IP address of the PostgreSQL database. This should be in the standard IPv4 address format, e.g., 172.28.40.9. If your machine supports IPv6, you can also use those addresses. TCP/IP communication is always used when a nonempty string is specified for this parameter.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>port</code> <p>Port of the PostgreSQL database.</p> <p>The default port used by PostgreSQL is 5432.</p> <p>If you do not know, which port to use, type the following into your PostgreSQL client</p> <pre><code>SELECT setting FROM pg_settings WHERE name = 'port';\n</code></pre> <p> TYPE: <code>int</code> DEFAULT: <code>5432</code> </p> <code>time_formats</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn_id</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> RETURNS DESCRIPTION <code>Connection</code> <p>The connection object.</p> Note <p>By selecting an existing table of your database in <code>from_db</code> function, you can create a new <code>DataFrame</code> containing all its data. Alternatively you can use the <code>read_db</code> and <code>read_query</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p> <p>You can also write your results back into the PostgreSQL database. By passing the name for the destination table to <code>transform</code>, the features generated from your raw data will be written back. Passing them into <code>predict</code>, instead, makes predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p> Source code in <code>getml/database/connect_postgres.py</code> <pre><code>def connect_postgres(\n    dbname: str,\n    user: str,\n    password: str,\n    host: Optional[str] = None,\n    hostaddr: Optional[str] = None,\n    port: int = 5432,\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection:\n    \"\"\"\n    Creates a new PostgreSQL database connection.\n\n    But first, make sure your database is running, and you can reach it\n    from via your command line.\n\n    enterprise-adm: Enterprise edition\n        This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the [benefits of the Enterprise edition][enterprise-benefits] and [compare their features][enterprise-feature-list].\n\n        For licensing information and technical support, please [contact us][contact-page].\n\n    Args:\n        dbname:\n            The name of the database to which you want to connect.\n\n        user:\n            Username with which to log into the PostgreSQL database.\n\n        password:\n            Password with which to log into the PostgreSQL database.\n\n        host:\n            Host of the PostgreSQL database.\n\n        hostaddr:\n            IP address of the PostgreSQL database.\n            This should be in the standard IPv4 address format, e.g., 172.28.40.9.\n            If your machine supports IPv6, you can also use those addresses.\n            TCP/IP communication is always used when a nonempty string is specified\n            for this parameter.\n\n        port:\n            Port of the PostgreSQL database.\n\n            The default port used by PostgreSQL is 5432.\n\n            If you do not know, which port to use, type the following into your\n            PostgreSQL client\n\n            ```sql\n            SELECT setting FROM pg_settings WHERE name = 'port';\n            ```\n\n        time_formats:\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id:\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Returns:\n        The connection object.\n\n    Note:\n        By selecting an existing table of your database in\n        [`from_db`][getml.DataFrame.from_db] function, you can create\n        a new [`DataFrame`][getml.DataFrame] containing all its data.\n        Alternatively you can use the\n        [`read_db`][getml.DataFrame.read_db] and\n        [`read_query`][getml.DataFrame.read_query] methods to replace\n        the content of the current [`DataFrame`][getml.DataFrame]\n        instance or append further rows based on either a table or a\n        specific query.\n\n        You can also write your results back into the PostgreSQL\n        database. By passing the name for the destination table to\n        [`transform`][getml.Pipeline.transform], the features\n        generated from your raw data will be written back. Passing\n        them into [`predict`][getml.Pipeline.predict], instead,\n        makes predictions of the target variables to new, unseen data\n        and stores the result into the corresponding table.\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.new\"\n    cmd[\"db_\"] = \"postgres\"\n\n    if host is not None:\n        cmd[\"host_\"] = host\n\n    if hostaddr is not None:\n        cmd[\"hostaddr_\"] = hostaddr\n\n    cmd[\"port_\"] = port\n    cmd[\"dbname_\"] = dbname\n    cmd[\"user_\"] = user\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The password is sent separately, so it doesn't\n        # end up in the logs.\n        comm.send_string(sock, password)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.handle_engine_exception(msg)\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/database/#getml.database.connect_sqlite3.connect_sqlite3","title":"connect_sqlite3","text":"<pre><code>connect_sqlite3(\n    name: str = \":memory:\",\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection\n</code></pre> <p>Creates a new SQLite3 database connection.</p> <p>SQLite3 is a popular in-memory database. It is faster than distributed databases, like PostgreSQL, but less stable under massive parallel access, consumes more memory and requires all contained data sets to be loaded into memory, which might fill up too much of your RAM, especially for large data sets.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the sqlite3 file.  If the file does not exist, it will be created. Set to \":memory:\" for a purely in-memory SQLite3 database.</p> <p> TYPE: <code>str</code> DEFAULT: <code>':memory:'</code> </p> <code>time_formats</code> <p>The list of formats tried when parsing time stamps.</p> <p>The formats are allowed to contain the following special characters:</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign</li> </ul> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn_id</code> <p>The name to be used to reference the connection. If you do not pass anything, this will create a new default connection.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default'</code> </p> RETURNS DESCRIPTION <code>Connection</code> <p>The new SQLite3 database connection.</p> Note <p>By selecting an existing table of your database in <code>from_db</code> function, you can create a new <code>DataFrame</code> containing all its data. Alternatively you can use the <code>read_db</code> and <code>read_query</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p> <p>You can also write your results back into the SQLite3 database. By passing the name for the destination table to <code>transform</code>, the features generated from your raw data will be written back. Passing them into <code>predict</code>, instead, makes predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p> Source code in <code>getml/database/connect_sqlite3.py</code> <pre><code>def connect_sqlite3(\n    name: str = \":memory:\",\n    time_formats: Optional[List[str]] = None,\n    conn_id: str = \"default\",\n) -&gt; Connection:\n    \"\"\"Creates a new SQLite3 database connection.\n\n    SQLite3 is a popular in-memory database. It is faster than\n    distributed databases, like PostgreSQL, but less stable under massive\n    parallel access, consumes more memory and requires all contained\n    data sets to be loaded into memory, which might fill up too much\n    of your RAM, especially for large data sets.\n\n    Args:\n        name:\n            Name of the sqlite3 file.  If the file does not exist, it\n            will be created. Set to \":memory:\" for a purely in-memory SQLite3\n            database.\n\n        time_formats:\n            The list of formats tried when parsing time stamps.\n\n            The formats are allowed to contain the following\n            special characters:\n\n            * %w - abbreviated weekday (Mon, Tue, ...)\n            * %W - full weekday (Monday, Tuesday, ...)\n            * %b - abbreviated month (Jan, Feb, ...)\n            * %B - full month (January, February, ...)\n            * %d - zero-padded day of month (01 .. 31)\n            * %e - day of month (1 .. 31)\n            * %f - space-padded day of month ( 1 .. 31)\n            * %m - zero-padded month (01 .. 12)\n            * %n - month (1 .. 12)\n            * %o - space-padded month ( 1 .. 12)\n            * %y - year without century (70)\n            * %Y - year with century (1970)\n            * %H - hour (00 .. 23)\n            * %h - hour (00 .. 12)\n            * %a - am/pm\n            * %A - AM/PM\n            * %M - minute (00 .. 59)\n            * %S - second (00 .. 59)\n            * %s - seconds and microseconds (equivalent to %S.%F)\n            * %i - millisecond (000 .. 999)\n            * %c - centisecond (0 .. 9)\n            * %F - fractional seconds/microseconds (000000 - 999999)\n            * %z - time zone differential in ISO 8601 format (Z or +NN.NN)\n            * %Z - time zone differential in RFC format (GMT or +NNNN)\n            * %% - percent sign\n\n        conn_id:\n            The name to be used to reference the connection.\n            If you do not pass anything, this will create a new default connection.\n\n    Returns:\n        The new SQLite3 database connection.\n\n    Note:\n        By selecting an existing table of your database in\n        [`from_db`][getml.DataFrame.from_db] function, you can create\n        a new [`DataFrame`][getml.DataFrame] containing all its data.\n        Alternatively you can use the\n        [`read_db`][getml.DataFrame.read_db] and\n        [`read_query`][getml.DataFrame.read_query] methods to replace\n        the content of the current [`DataFrame`][getml.DataFrame]\n        instance or append further rows based on either a table or a\n        specific query.\n\n        You can also write your results back into the SQLite3\n        database. By passing the name for the destination table to\n        [`transform`][getml.Pipeline.transform], the features\n        generated from your raw data will be written back. Passing\n        them into [`predict`][getml.Pipeline.predict], instead,\n        makes predictions of the target variables to new, unseen data\n        and stores the result into the corresponding table.\n\n    \"\"\"\n\n    time_formats = time_formats or constants.TIME_FORMATS\n\n    # We want to use the absolute path, unless it is a pure\n    # in-memory version.\n    name_ = name\n\n    if name_ != \":memory:\":\n        name_ = os.path.abspath(name)\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = name_\n    cmd[\"type_\"] = \"Database.new\"\n\n    cmd[\"db_\"] = \"sqlite3\"\n    cmd[\"time_formats_\"] = time_formats\n    cmd[\"conn_id_\"] = conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        # The password is usually sent separately,\n        # so it doesn't\n        # end up in the logs. However, Sqlite3 does not\n        # need a password, so we just send a dummy.\n        comm.send_string(sock, \"none\")\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.handle_engine_exception(msg, extra={\"name\": name_})\n\n    return Connection(conn_id=conn_id)\n</code></pre>"},{"location":"reference/database/database/#getml.database.copy_table.copy_table","title":"copy_table","text":"<pre><code>copy_table(\n    source_conn: Connection,\n    target_conn: Connection,\n    source_table: str,\n    target_table: Optional[str] = None,\n)\n</code></pre> <p>Copies a table from one database connection to another.</p> PARAMETER DESCRIPTION <code>source_conn</code> <p>The database connection to be copied from.</p> <p> TYPE: <code>Connection</code> </p> <code>target_conn</code> <p>The database connection to be copied to.</p> <p> TYPE: <code>Connection</code> </p> <code>source_table</code> <p>The name of the table in the source connection.</p> <p> TYPE: <code>str</code> </p> <code>target_table</code> <p>The name of the table in the target connection. If you do not explicitly pass a target_table, the name will be identical to the source_table.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Example <p>A frequent use case for this function is to copy data from a data source into sqlite. This is a good idea, because sqlite is faster than most standard, ACID-compliant databases, and also you want to avoid messing up a productive environment.</p> <p>It is important to explicitly pass conn_id, otherwise the source connection will be closed when you create the target connection. What you pass as conn_id is entirely up to you, as long as the conn_id of the source and the target are different. It can be any name you see fit.</p> <pre><code>source_conn = getml.database.connect_odbc(\n    \"MY-SERVER-NAME\", conn_id=\"MY-SOURCE\")\n\ntarget_conn = getml.database.connect_sqlite3(\n    \"MY-SQLITE.db\", conn_id=\"MY-TARGET\")\n\ndata.database.copy_table(\n        source_conn=source_conn,\n        target_conn=target_conn,\n        source_table=\"MY-TABLE\"\n)\n</code></pre> Source code in <code>getml/database/copy_table.py</code> <pre><code>def copy_table(\n    source_conn: Connection,\n    target_conn: Connection,\n    source_table: str,\n    target_table: Optional[str] = None,\n):\n    \"\"\"\n    Copies a table from one database connection to another.\n\n    Args:\n        source_conn:\n            The database connection to be copied from.\n\n        target_conn:\n            The database connection to be copied to.\n\n        source_table:\n            The name of the table in the source connection.\n\n        target_table:\n            The name of the table in the target\n            connection. If you do not explicitly pass a target_table, the\n            name will be identical to the source_table.\n\n    ??? example\n        A frequent use case for this function is to copy data from a data source into\n        sqlite. This is a good idea, because sqlite is faster than most standard,\n        ACID-compliant databases, and also you want to avoid messing up a productive\n        environment.\n\n        It is important to explicitly pass conn_id, otherwise the source connection\n        will be closed\n        when you create the target connection. What you pass as conn_id is entirely\n        up to you,\n        as long as the conn_id of the source and the target are different. It can\n        be any name you see fit.\n\n        ```python\n        source_conn = getml.database.connect_odbc(\n            \"MY-SERVER-NAME\", conn_id=\"MY-SOURCE\")\n\n        target_conn = getml.database.connect_sqlite3(\n            \"MY-SQLITE.db\", conn_id=\"MY-TARGET\")\n\n        data.database.copy_table(\n                source_conn=source_conn,\n                target_conn=target_conn,\n                source_table=\"MY-TABLE\"\n        )\n        ```\n\n    \"\"\"\n\n    # -------------------------------------------\n\n    target_table = target_table or source_table\n\n    # -------------------------------------------\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.copy_table\"\n\n    cmd[\"source_conn_id_\"] = source_conn.conn_id\n    cmd[\"target_conn_id_\"] = target_conn.conn_id\n    cmd[\"source_table_\"] = source_table\n    cmd[\"target_table_\"] = target_table\n\n    # -------------------------------------------\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/database/database/#getml.database.drop_table.drop_table","title":"drop_table","text":"<pre><code>drop_table(name: str, conn: Optional[Connection] = None)\n</code></pre> <p>Drops a table from the database.</p> PARAMETER DESCRIPTION <code>name</code> <p>The table to be dropped.</p> <p> TYPE: <code>str</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the Engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/database/drop_table.py</code> <pre><code>def drop_table(name: str, conn: Optional[Connection] = None):\n    \"\"\"\n    Drops a table from the database.\n\n    Args:\n        name:\n            The table to be dropped.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the Engine will use the default connection.\n    \"\"\"\n\n    # -------------------------------------------\n\n    conn = conn or Connection()\n\n    # -------------------------------------------\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = name\n    cmd[\"type_\"] = \"Database.drop_table\"\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    # -------------------------------------------\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/database/database/#getml.database.execute.execute","title":"execute","text":"<pre><code>execute(query: str, conn: Optional[Connection] = None)\n</code></pre> <p>Executes an SQL query on the database.</p> <p>Please note that this is not meant to return results. If you want to get results, use <code>database.get()</code> instead.</p> PARAMETER DESCRIPTION <code>query</code> <p>The SQL query to be executed.</p> <p> TYPE: <code>str</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the Engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/database/execute.py</code> <pre><code>def execute(query: str, conn: Optional[Connection] = None):\n    \"\"\"\n    Executes an SQL query on the database.\n\n    Please note that this is not meant to return results. If you want to\n    get results, use `database.get()` instead.\n\n    Args:\n        query:\n            The SQL query to be executed.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the Engine will use the default connection.\n    \"\"\"\n\n    conn = conn or Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = conn.conn_id\n    cmd[\"type_\"] = \"Database.execute\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        comm.send_string(sock, query)\n        msg = comm.recv_string(sock)\n\n    if msg != \"Success!\":\n        comm.handle_engine_exception(msg)\n</code></pre>"},{"location":"reference/database/database/#getml.database.get.get","title":"get","text":"<pre><code>get(\n    query: str, conn: Optional[Connection] = None\n) -&gt; DataFrame\n</code></pre> <p>Executes an SQL query on the database and returns the result as a pandas dataframe.</p> PARAMETER DESCRIPTION <code>query</code> <p>The SQL query to be executed.</p> <p> TYPE: <code>str</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the Engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>The result of the query as a pandas dataframe.</p> Source code in <code>getml/database/get.py</code> <pre><code>def get(query: str, conn: Optional[Connection] = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Executes an SQL query on the database and returns the result as\n    a pandas dataframe.\n\n    Args:\n        query:\n            The SQL query to be executed.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the Engine will use the default connection.\n\n    Returns:\n        The result of the query as a pandas dataframe.\n    \"\"\"\n\n    conn = conn or Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = conn.conn_id\n    cmd[\"type_\"] = \"Database.get\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        comm.send_string(sock, query)\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.handle_engine_exception(msg)\n        json_str = comm.recv_string(sock)\n\n    return pd.read_json(json_str)\n</code></pre>"},{"location":"reference/database/database/#getml.database.get_colnames.get_colnames","title":"get_colnames","text":"<pre><code>get_colnames(\n    name: str, conn: Optional[Connection] = None\n) -&gt; List[str]\n</code></pre> <p>Lists the colnames of a table held in the database.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the table in the database.</p> <p> TYPE: <code>str</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the Engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of strings containing the names of the columns in the table.</p> Source code in <code>getml/database/get_colnames.py</code> <pre><code>def get_colnames(name: str, conn: Optional[Connection] = None) -&gt; List[str]:\n    \"\"\"\n    Lists the colnames of a table held in the database.\n\n    Args:\n        name:\n            The name of the table in the database.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the Engine will use the default connection.\n\n    Returns:\n        A list of strings containing the names of the columns in the table.\n    \"\"\"\n\n    conn = conn or Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = name\n    cmd[\"type_\"] = \"Database.get_colnames\"\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.handle_engine_exception(msg)\n        arr = json.loads(comm.recv_string(sock))\n\n    return arr\n</code></pre>"},{"location":"reference/database/database/#getml.database.list_connections.list_connections","title":"list_connections","text":"<pre><code>list_connections() -&gt; List[Connection]\n</code></pre> <p>Returns a list handles to all connections that are currently active on the Engine.</p> RETURNS DESCRIPTION <code>List[Connection]</code> <p>A list of Connection objects.</p> Source code in <code>getml/database/list_connections.py</code> <pre><code>def list_connections() -&gt; List[Connection]:\n    \"\"\"\n    Returns a list handles to all connections\n    that are currently active on the Engine.\n\n    Returns:\n        A list of Connection objects.\n    \"\"\"\n\n    cmd: Dict[Any, str] = {}\n\n    cmd[\"name_\"] = \"\"\n    cmd[\"type_\"] = \"Database.list_connections\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.handle_engine_exception(msg)\n        arr = json.loads(comm.recv_string(sock))\n\n    return [Connection(elem) for elem in arr]\n</code></pre>"},{"location":"reference/database/database/#getml.database.list_tables.list_tables","title":"list_tables","text":"<pre><code>list_tables(conn: Optional[Connection] = None) -&gt; List[str]\n</code></pre> <p>Lists all tables and views currently held in the database.</p> PARAMETER DESCRIPTION <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the Engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>A list of strings containing the names of the tables and views.</p> Source code in <code>getml/database/list_tables.py</code> <pre><code>def list_tables(conn: Optional[Connection] = None) -&gt; List[str]:\n    \"\"\"\n    Lists all tables and views currently held in the database.\n\n    Args:\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the Engine will use the default connection.\n\n    Returns:\n        A list of strings containing the names of the tables and views.\n    \"\"\"\n\n    conn = conn or Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = conn.conn_id\n    cmd[\"type_\"] = \"Database.list_tables\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.handle_engine_exception(msg)\n        return json.loads(comm.recv_string(sock))\n</code></pre>"},{"location":"reference/database/database/#getml.database.read_csv.read_csv","title":"read_csv","text":"<pre><code>read_csv(\n    name: str,\n    fnames: Union[str, List[str]],\n    quotechar: str = '\"',\n    sep: str = \",\",\n    num_lines_read: int = 0,\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    conn: Optional[Connection] = None,\n) -&gt; None\n</code></pre> <p>Reads a CSV file into the database.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the table in which the data is to be inserted.</p> <p> TYPE: <code>str</code> </p> <code>fnames</code> <p>The list of CSV file names to be read.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>quotechar</code> <p>The character used to wrap strings. Default:<code>\"</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>'\"'</code> </p> <code>sep</code> <p>The separator used for separating fields. Default:<code>,</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>num_lines_read</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>skip</code> <p>Number of lines to skip at the beginning of each file (Default: 0).</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>colnames</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the Engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> Example <p>Let's assume you have two CSV files - file1.csv and file2.csv . You can import their data into the database using the following commands:</p> <pre><code>stmt = data.database.sniff_csv(\n        fnames=[\"file1.csv\", \"file2.csv\"],\n        name=\"MY_TABLE\",\n        sep=';'\n)\n\ngetml.database.execute(stmt)\n\ndata.database.read_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY_TABLE\",\n    sep=';'\n)\n</code></pre> Source code in <code>getml/database/read_csv.py</code> <pre><code>def read_csv(\n    name: str,\n    fnames: Union[str, List[str]],\n    quotechar: str = '\"',\n    sep: str = \",\",\n    num_lines_read: int = 0,\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    conn: Optional[Connection] = None,\n) -&gt; None:\n    \"\"\"\n    Reads a CSV file into the database.\n\n    Args:\n        name:\n            Name of the table in which the data is to be inserted.\n\n        fnames:\n            The list of CSV file names to be read.\n\n        quotechar:\n            The character used to wrap strings. Default:`\"`\n\n        sep:\n            The separator used for separating fields. Default:`,`\n\n        num_lines_read:\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        skip:\n            Number of lines to skip at the beginning of each\n            file (Default: 0).\n\n        colnames:\n            The first line of a CSV file\n            usually contains the column names. When this is not the case, you need to\n            explicitly pass them.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the Engine will use the default connection.\n\n    ??? example\n        Let's assume you have two CSV files - *file1.csv* and\n        *file2.csv* . You can import their data into the database\n        using the following commands:\n\n        ```python\n        stmt = data.database.sniff_csv(\n                fnames=[\"file1.csv\", \"file2.csv\"],\n                name=\"MY_TABLE\",\n                sep=';'\n        )\n\n        getml.database.execute(stmt)\n\n        data.database.read_csv(\n            fnames=[\"file1.csv\", \"file2.csv\"],\n            name=\"MY_TABLE\",\n            sep=';'\n        )\n        ```\n\n    \"\"\"\n\n    return _read_csv(\n        CSVCmdType.READ,\n        name,\n        fnames,\n        num_lines_read,\n        quotechar,\n        sep,\n        skip,\n        colnames,\n        conn,\n    )\n</code></pre>"},{"location":"reference/database/database/#getml.database.read_s3.read_s3","title":"read_s3","text":"<pre><code>read_s3(\n    name: str,\n    bucket: str,\n    keys: List[str],\n    region: str,\n    sep: str = \",\",\n    num_lines_read: int = 0,\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    conn: Optional[Connection] = None,\n) -&gt; None\n</code></pre> <p>Reads a list of CSV files located in an S3 bucket.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the table in which the data is to be inserted.</p> <p> TYPE: <code>str</code> </p> <code>bucket</code> <p>The bucket from which to read the files.</p> <p> TYPE: <code>str</code> </p> <code>keys</code> <p>The list of keys (files in the bucket) to be read.</p> <p> TYPE: <code>List[str]</code> </p> <code>region</code> <p>The region in which the bucket is located.</p> <p> TYPE: <code>str</code> </p> <code>sep</code> <p>The separator used for separating fields. Default:<code>,</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>num_lines_read</code> <p>Number of lines read from each file. Set to 0 to read in the entire file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>skip</code> <p>Number of lines to skip at the beginning of each file (Default: 0).</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>colnames</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the Engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> Example <p>Let's assume you have two CSV files - file1.csv and file2.csv - in the bucket. You can import their data into the getML Engine using the following commands: <pre><code>getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n\ngetml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\nstmt = data.database.sniff_s3(\n        bucket=\"your-bucket-name\",\n        keys=[\"file1.csv\", \"file2.csv\"],\n        region=\"us-east-2\",\n        name=\"MY_TABLE\",\n        sep=';'\n)\n\ngetml.database.execute(stmt)\n\ndata.database.read_s3(\n    bucket=\"your-bucket-name\",\n    keys=[\"file1.csv\", \"file2.csv\"],\n    region=\"us-east-2\",\n    name=\"MY_TABLE\",\n    sep=';'\n)\n</code></pre> You can also set the access credential as environment variables before you launch the getML Engine.</p> Source code in <code>getml/database/read_s3.py</code> <pre><code>def read_s3(\n    name: str,\n    bucket: str,\n    keys: List[str],\n    region: str,\n    sep: str = \",\",\n    num_lines_read: int = 0,\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    conn: Optional[Connection] = None,\n) -&gt; None:\n    \"\"\"\n    Reads a list of CSV files located in an S3 bucket.\n\n    Args:\n        name:\n            Name of the table in which the data is to be inserted.\n\n        bucket:\n            The bucket from which to read the files.\n\n        keys:\n            The list of keys (files in the bucket) to be read.\n\n        region:\n            The region in which the bucket is located.\n\n        sep:\n            The separator used for separating fields. Default:`,`\n\n        num_lines_read:\n            Number of lines read from each file.\n            Set to 0 to read in the entire file.\n\n        skip:\n            Number of lines to skip at the beginning of each\n            file (Default: 0).\n\n        colnames:\n            The first line of a CSV file\n            usually contains the column names. When this is not the case, you need to\n            explicitly pass them.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the Engine will use the default connection.\n\n    ??? example\n        Let's assume you have two CSV files - *file1.csv* and\n        *file2.csv* - in the bucket. You can\n        import their data into the getML Engine using the following\n        commands:\n        ```python\n        getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n\n        getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n        stmt = data.database.sniff_s3(\n                bucket=\"your-bucket-name\",\n                keys=[\"file1.csv\", \"file2.csv\"],\n                region=\"us-east-2\",\n                name=\"MY_TABLE\",\n                sep=';'\n        )\n\n        getml.database.execute(stmt)\n\n        data.database.read_s3(\n            bucket=\"your-bucket-name\",\n            keys=[\"file1.csv\", \"file2.csv\"],\n            region=\"us-east-2\",\n            name=\"MY_TABLE\",\n            sep=';'\n        )\n        ```\n        You can also set the access credential as environment variables\n        before you launch the getML Engine.\n\n    \"\"\"\n    # -------------------------------------------\n\n    conn = conn or Connection()\n\n    # -------------------------------------------\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = name\n    cmd[\"type_\"] = \"Database.read_s3\"\n\n    cmd[\"bucket_\"] = bucket\n    cmd[\"keys_\"] = keys\n    cmd[\"num_lines_read_\"] = num_lines_read\n    cmd[\"region_\"] = region\n    cmd[\"sep_\"] = sep\n    cmd[\"skip_\"] = skip\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    if colnames is not None:\n        cmd[\"colnames_\"] = colnames\n\n    # -------------------------------------------\n\n    comm.send(cmd)\n</code></pre>"},{"location":"reference/database/database/#getml.database.sniff_csv.sniff_csv","title":"sniff_csv","text":"<pre><code>sniff_csv(\n    name: str,\n    fnames: Union[str, List[str]],\n    num_lines_sniffed: int = 1000,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    conn: Optional[Connection] = None,\n) -&gt; str\n</code></pre> <p>Sniffs a list of CSV files.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the table in which the data is to be inserted.</p> <p> TYPE: <code>str</code> </p> <code>fnames</code> <p>The list of CSV file names to be read.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>num_lines_sniffed</code> <p>Number of lines analyzed by the sniffer.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>quotechar</code> <p>The character used to wrap strings. Default:<code>\"</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>'\"'</code> </p> <code>sep</code> <p>The separator used for separating fields. Default:<code>,</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>skip</code> <p>Number of lines to skip at the beginning of each file (Default: 0).</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>colnames</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the Engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Appropriate <code>CREATE TABLE</code> statement.</p> Source code in <code>getml/database/sniff_csv.py</code> <pre><code>def sniff_csv(\n    name: str,\n    fnames: Union[str, List[str]],\n    num_lines_sniffed: int = 1000,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    conn: Optional[Connection] = None,\n) -&gt; str:\n    \"\"\"\n    Sniffs a list of CSV files.\n\n    Args:\n        name:\n            Name of the table in which the data is to be inserted.\n\n        fnames:\n            The list of CSV file names to be read.\n\n        num_lines_sniffed:\n            Number of lines analyzed by the sniffer.\n\n        quotechar:\n            The character used to wrap strings. Default:`\"`\n\n        sep:\n            The separator used for separating fields. Default:`,`\n\n        skip:\n            Number of lines to skip at the beginning of each\n            file (Default: 0).\n\n        colnames:\n            The first line of a CSV file\n            usually contains the column names. When this is not the case, you need to\n            explicitly pass them.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the Engine will use the default connection.\n\n    Returns:\n        Appropriate `CREATE TABLE` statement.\n    \"\"\"\n\n    return _read_csv(\n        CSVCmdType.SNIFF,\n        name,\n        fnames,\n        num_lines_sniffed,\n        quotechar,\n        sep,\n        skip,\n        colnames,\n        conn,\n    )\n</code></pre>"},{"location":"reference/database/database/#getml.database.sniff_s3.sniff_s3","title":"sniff_s3","text":"<pre><code>sniff_s3(\n    name: str,\n    bucket: str,\n    keys: List[str],\n    region: str,\n    num_lines_sniffed: int = 1000,\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    conn: Optional[Connection] = None,\n) -&gt; str\n</code></pre> <p>Sniffs a list of CSV files located in an S3 bucket.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the table in which the data is to be inserted.</p> <p> TYPE: <code>str</code> </p> <code>bucket</code> <p>The bucket from which to read the files.</p> <p> TYPE: <code>str</code> </p> <code>keys</code> <p>The list of keys (files in the bucket) to be read.</p> <p> TYPE: <code>List[str]</code> </p> <code>region</code> <p>The region in which the bucket is located.</p> <p> TYPE: <code>str</code> </p> <code>num_lines_sniffed</code> <p>Number of lines analyzed by the sniffer.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>sep</code> <p>The character used for separating fields.</p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>skip</code> <p>Number of lines to skip at the beginning of each file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>colnames</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you need to explicitly pass them.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>conn</code> <p>The database connection to be used. If you don't explicitly pass a connection, the Engine will use the default connection.</p> <p> TYPE: <code>Optional[Connection]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Appropriate <code>CREATE TABLE</code> statement.</p> Example <p>Let's assume you have two CSV files - file1.csv and file2.csv - in the bucket. You can import their data into the getML Engine using the following commands: <pre><code>getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n\ngetml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\nstmt = data.database.sniff_s3(\n        bucket=\"your-bucket-name\",\n        keys=[\"file1.csv\", \"file2.csv\"],\n        region=\"us-east-2\",\n        name=\"MY_TABLE\",\n        sep=';'\n)\n</code></pre> You can also set the access credential as environment variables before you launch the getML Engine.</p> Source code in <code>getml/database/sniff_s3.py</code> <pre><code>def sniff_s3(\n    name: str,\n    bucket: str,\n    keys: List[str],\n    region: str,\n    num_lines_sniffed: int = 1000,\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n    conn: Optional[Connection] = None,\n) -&gt; str:\n    \"\"\"\n    Sniffs a list of CSV files located in an S3 bucket.\n\n\n    Args:\n        name:\n            Name of the table in which the data is to be inserted.\n\n        bucket:\n            The bucket from which to read the files.\n\n        keys:\n            The list of keys (files in the bucket) to be read.\n\n        region:\n            The region in which the bucket is located.\n\n        num_lines_sniffed:\n            Number of lines analyzed by the sniffer.\n\n        sep:\n            The character used for separating fields.\n\n        skip:\n            Number of lines to skip at the beginning of each file.\n\n        colnames:\n            The first line of a CSV file\n            usually contains the column names. When this is not the case, you need to\n            explicitly pass them.\n\n        conn:\n            The database connection to be used.\n            If you don't explicitly pass a connection,\n            the Engine will use the default connection.\n\n    Returns:\n        Appropriate `CREATE TABLE` statement.\n\n    ??? example\n        Let's assume you have two CSV files - *file1.csv* and\n        *file2.csv* - in the bucket. You can\n        import their data into the getML Engine using the following\n        commands:\n        ```python\n        getml.engine.set_s3_access_key_id(\"YOUR-ACCESS-KEY-ID\")\n\n        getml.engine.set_s3_secret_access_key(\"YOUR-SECRET-ACCESS-KEY\")\n\n        stmt = data.database.sniff_s3(\n                bucket=\"your-bucket-name\",\n                keys=[\"file1.csv\", \"file2.csv\"],\n                region=\"us-east-2\",\n                name=\"MY_TABLE\",\n                sep=';'\n        )\n        ```\n        You can also set the access credential as environment variables\n        before you launch the getML Engine.\n\n    \"\"\"\n\n    conn = conn or Connection()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = name\n    cmd[\"type_\"] = \"Database.sniff_s3\"\n\n    cmd[\"bucket_\"] = bucket\n    cmd[\"keys_\"] = keys\n    cmd[\"num_lines_sniffed_\"] = num_lines_sniffed\n    cmd[\"region_\"] = region\n    cmd[\"sep_\"] = sep\n    cmd[\"skip_\"] = skip\n    cmd[\"conn_id_\"] = conn.conn_id\n\n    if colnames is not None:\n        cmd[\"colnames_\"] = colnames\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.handle_engine_exception(msg)\n        return comm.recv_string(sock)\n</code></pre>"},{"location":"reference/datasets/datasets/","title":"Datasets","text":""},{"location":"reference/datasets/datasets/#getml.datasets","title":"getml.datasets","text":"<p>The <code>datasets</code> module includes utilities to load datasets, including methods to load and fetch popular reference datasets. It also features some artificial data generators.</p>"},{"location":"reference/datasets/datasets/#getml.datasets.load_air_pollution","title":"load_air_pollution","text":"<pre><code>load_air_pollution(\n    roles: bool = True, as_pandas: bool = False\n) -&gt; DataFrameT\n</code></pre> <p>Regression dataset on air pollution in Beijing, China</p> <p>The dataset consists of a single table split into train and test sets around 2014-01-01.</p> <p>Reference</p> <p>Liang, X., Zou, T., Guo, B., Li, S., Zhang, H., Zhang, S., Huang, H. and Chen, S. X. (2015). Assessing Beijing's PM2.5 pollution: severity, weather impact, APEC and winter heating. Proceedings of the Royal Society A, 471, 20150257.</p> PARAMETER DESCRIPTION <code>as_pandas</code> <p>Return data as <code>pandas.DataFrame</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>roles</code> <p>Return data with roles set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>DataFrameT</code> <p>A DataFrame holding the data described above.</p> <p>The following DataFrames are returned:</p> <ul> <li><code>air_pollution</code></li> </ul> Example <p><pre><code>air_pollution = getml.datasets.load_air_pollution()\ntype(air_pollution)\ngetml.data.data_frame.DataFrame\n</code></pre> For a full analysis of the atherosclerosis dataset including all necessary preprocessing steps please refer to getml-demo .</p> Note <p>Roles can be set ad-hoc by supplying the respective flag. If <code>roles</code> is <code>False</code>, all columns in the returned <code>DataFrame</code> have roles <code>unused_string</code> or <code>unused_float</code>. This dataset contains no units. Before using them in an analysis, a data model needs to be constructed using <code>Placeholder</code>.</p> Source code in <code>getml/datasets/base.py</code> <pre><code>def load_air_pollution(\n    roles: bool = True,\n    as_pandas: bool = False,\n) -&gt; DataFrameT:\n    \"\"\"\n    Regression dataset on air pollution in Beijing, China\n\n    The dataset consists of a single table split into train and test sets\n    around 2014-01-01.\n\n    !!! abstract \"Reference\"\n        Liang, X., Zou, T., Guo, B., Li, S., Zhang, H., Zhang, S., Huang, H. and\n        Chen, S. X. (2015). Assessing Beijing's PM2.5 pollution: severity, weather\n        impact, APEC and winter heating. Proceedings of the Royal Society A, 471,\n        20150257.\n\n    Args:\n        as_pandas:\n            Return data as `pandas.DataFrame`\n\n        roles:\n            Return data with roles set\n\n    Returns:\n        A DataFrame holding the data described above.\n\n            The following DataFrames are returned:\n\n            * `air_pollution`\n\n    ??? example\n        ```python\n        air_pollution = getml.datasets.load_air_pollution()\n        type(air_pollution)\n        getml.data.data_frame.DataFrame\n        ```\n        For a full analysis of the atherosclerosis dataset including all necessary\n        preprocessing steps please refer to [getml-demo\n        ](https://github.com/getml/getml-demo/blob/master/air_pollution.ipynb).\n\n\n    Note:\n        Roles can be set ad-hoc by supplying the respective flag. If\n        `roles` is `False`, all columns in the returned\n        [`DataFrame`][getml.data.DataFrame] have roles\n        [`unused_string`][getml.data.roles.unused_string] or\n        [`unused_float`][getml.data.roles.unused_float]. This dataset contains no units.\n        Before using them in an analysis, a data model needs to be constructed\n        using [`Placeholder`][getml.data.Placeholder].\n\n    \"\"\"\n\n    ds_name = \"air_pollution\"\n\n    dataset = _load_dataset(\n        ds_name=ds_name,\n        roles=roles,\n        as_pandas=as_pandas,\n    )\n    assert isinstance(dataset, tuple), \"Expected a tuple\"\n    return dataset[0]\n</code></pre>"},{"location":"reference/datasets/datasets/#getml.datasets.load_atherosclerosis","title":"load_atherosclerosis","text":"<pre><code>load_atherosclerosis(\n    roles: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]\n</code></pre> <p>Binary classification dataset on the lethality of atherosclerosis</p> <p>The atherosclerosis dataset is a medical dataset from the Relational Dataset Repository (former CTU Prague Relational Learning Repository) . It contains information from a longitudinal study on 1417 middle-aged men observed over the course of 20 years. After preprocessing, it consists of 2 tables with 76 and 66 columns:</p> <ul> <li> <p><code>population</code>: Data on the study's participants</p> </li> <li> <p><code>contr</code>: Data on control dates</p> </li> </ul> <p>The population table is split into a training (70%), a testing (15%) set and a validation (15%) set.</p> PARAMETER DESCRIPTION <code>as_pandas</code> <p>Return data as <code>pandas.DataFrame</code> s</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>roles</code> <p>Return data with roles set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>as_dict</code> <p>Return data as dict with <code>df.name</code> as keys and <code>df</code> as values.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>Tuple containing (sorted alphabetically by <code>df.name</code>) the data as <code>DataFrame</code> or <code>pandas.DataFrame</code> (if <code>as_pandas</code> is True) or if <code>as_dict</code> is <code>True</code>: Dictionary containing the data as <code>DataFrame</code> or <code>pandas.DataFrame</code> (if <code>as_pandas</code> is True). The keys correspond to the name of the DataFrame on the <code>engine</code>.</p> <p>The following DataFrames are returned:</p> <ul> <li><code>population</code></li> <li><code>contr</code></li> </ul> Example <p><pre><code>population, contr = getml.datasets.load_atherosclerosis()\ntype(population)\ngetml.data.data_frame.DataFrame\n</code></pre> For a full analysis of the atherosclerosis dataset including all necessary preprocessing steps please refer to getml-examples .</p> Note <p>Roles can be set ad-hoc by supplying the respective flag. If <code>roles</code> is <code>False</code>, all columns in the returned <code>DataFrame</code> have roles <code>unused_string</code> or <code>unused_float</code>. This dataset contains no units. Before using them in an analysis, a data model needs to be constructed using <code>Placeholder</code>.</p> Source code in <code>getml/datasets/base.py</code> <pre><code>def load_atherosclerosis(\n    roles: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]:\n    \"\"\"\n    Binary classification dataset on the lethality of atherosclerosis\n\n    The atherosclerosis dataset is a medical dataset from the [Relational Dataset Repository (former CTU Prague\n    Relational Learning Repository)\n    ](https://relational-data.org/dataset/Atherosclerosis). It contains\n    information from a longitudinal study on 1417 middle-aged men observed over\n    the course of 20 years. After preprocessing, it consists of 2 tables with 76\n    and 66 columns:\n\n    * `population`: Data on the study's participants\n\n    * `contr`: Data on control dates\n\n    The population table is split into a training (70%), a testing (15%) set and a\n    validation (15%) set.\n\n    Args:\n        as_pandas:\n            Return data as `pandas.DataFrame` s\n\n        roles:\n            Return data with roles set\n\n        as_dict:\n            Return data as dict with `df.name` as keys and\n            `df` as values.\n\n    Returns:\n        Tuple containing (sorted alphabetically by `df.name`) the data as\n            [`DataFrame`][getml.DataFrame] or `pandas.DataFrame` (if `as_pandas`\n            is True) or\n            if `as_dict` is `True`: Dictionary containing the data as\n            [`DataFrame`][getml.DataFrame] or `pandas.DataFrame` (if `as_pandas`\n            is True). The keys correspond to the name of the DataFrame on the\n            [`engine`][getml.engine].\n\n            The following DataFrames are returned:\n\n            - `population`\n            - `contr`\n\n    ??? example\n        ```python\n        population, contr = getml.datasets.load_atherosclerosis()\n        type(population)\n        getml.data.data_frame.DataFrame\n        ```\n        For a full analysis of the atherosclerosis dataset including all necessary\n        preprocessing steps please refer to [getml-examples\n        ](https://github.com/getml/getml-demo/blob/master/atherosclerosis.ipynb).\n\n\n    Note:\n        Roles can be set ad-hoc by supplying the respective flag. If\n        `roles` is `False`, all columns in the returned\n        [`DataFrame`][getml.data.DataFrame] have roles\n        [`unused_string`][getml.data.roles.unused_string] or\n        [`unused_float`][getml.data.roles.unused_float]. This dataset contains no units.\n        Before using them in an analysis, a data model needs to be constructed\n        using [`Placeholder`][getml.data.Placeholder].\n    \"\"\"\n\n    ds_name = \"atherosclerosis\"\n\n    return _load_dataset(\n        ds_name=ds_name,\n        roles=roles,\n        as_pandas=as_pandas,\n        as_dict=as_dict,\n    )\n</code></pre>"},{"location":"reference/datasets/datasets/#getml.datasets.load_biodegradability","title":"load_biodegradability","text":"<pre><code>load_biodegradability(\n    roles: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]\n</code></pre> <p>Regression dataset on molecule weight prediction</p> <p>The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group (Universita degli Studi Milano-Bicocca, Milano, Italy). The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE).</p> <p>Reference</p> <p>Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878</p> <p>The dataset was collected through the Relational Dataset Repository (former CTU Prague Relational Learning Repository)</p> <p>It contains information on 1309 molecules with 6166 bonds. It consists of 5 tables.</p> <p>The population table is split into a training (50 %) and a testing (25%) and validation (25%) sets.</p> PARAMETER DESCRIPTION <code>as_pandas</code> <p>Return data as <code>pandas.DataFrame</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>roles</code> <p>Return data with roles set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>as_dict</code> <p>Return data as dict with <code>df.name</code> as keys and <code>df</code> as values.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>Tuple containing (sorted alphabetically by <code>df.name</code>) the data as <code>DataFrame</code> or <code>pandas.DataFrame</code> (if <code>as_pandas</code> is True) or if <code>as_dict</code> is <code>True</code>: Dictionary containing the data as <code>DataFrame</code> or <code>pandas.DataFrame</code> (if <code>as_pandas</code> is True). The keys correspond to the name of the DataFrame on the <code>engine</code>.</p> <p>The following DataFrames are returned:</p> <ul> <li><code>molecule</code></li> <li><code>atom</code></li> <li><code>bond</code></li> <li><code>gmember</code></li> <li><code>group</code></li> </ul> Example <pre><code>biodegradability = getml.datasets.load_biodegradability(as_dict=True)\ntype(biodegradability[\"molecule_train\"])\ngetml.data.data_frame.DataFrame\n</code></pre> Note <p>Roles can be set ad-hoc by supplying the respective flag. If <code>roles</code> is <code>False</code>, all columns in the returned <code>DataFrame</code> have roles <code>unused_string</code> or <code>unused_float</code>. This dataset contains no units. Before using them in an analysis, a data model needs to be constructed using <code>Placeholder</code>.</p> Source code in <code>getml/datasets/base.py</code> <pre><code>def load_biodegradability(\n    roles: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]:\n    \"\"\"\n    Regression dataset on molecule weight prediction\n\n    The QSAR biodegradation dataset was built in the Milano Chemometrics and\n    QSAR Research Group (Universita degli Studi Milano-Bicocca, Milano, Italy).\n    The data have been used to develop QSAR (Quantitative Structure Activity\n    Relationships) models for the study of the relationships between chemical\n    structure and biodegradation of molecules. Biodegradation experimental\n    values of 1055 chemicals were collected from the webpage of the National\n    Institute of Technology and Evaluation of Japan (NITE).\n\n    !!! abstract \"Reference\"\n        Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V.\n        (2013). Quantitative Structure - Activity Relationship models for ready\n        biodegradability of chemicals. Journal of Chemical Information and Modeling,\n        53, 867-878\n\n    The dataset was collected through the [Relational Dataset Repository (former CTU Prague\n    Relational Learning Repository)](https://relational-data.org/dataset/Biodegradability)\n\n    It contains information on 1309 molecules with 6166 bonds. It consists of 5\n    tables.\n\n    The population table is split into a training (50 %) and a testing (25%) and\n    validation (25%) sets.\n\n    Args:\n        as_pandas:\n            Return data as `pandas.DataFrame`\n\n        roles:\n            Return data with roles set\n\n        as_dict:\n            Return data as dict with `df.name` as keys and\n            `df` as values.\n\n    Returns:\n        Tuple containing (sorted alphabetically by `df.name`) the data as\n            [`DataFrame`][getml.DataFrame] or `pandas.DataFrame` (if `as_pandas`\n            is True) or if `as_dict` is `True`: Dictionary containing the data as\n            [`DataFrame`][getml.DataFrame] or `pandas.DataFrame` (if `as_pandas`\n            is True). The keys correspond to the name of the DataFrame on the\n            [`engine`][getml.engine].\n\n            The following DataFrames are returned:\n\n            * `molecule`\n            * `atom`\n            * `bond`\n            * `gmember`\n            * `group`\n\n    ??? example\n        ```python\n        biodegradability = getml.datasets.load_biodegradability(as_dict=True)\n        type(biodegradability[\"molecule_train\"])\n        getml.data.data_frame.DataFrame\n        ```\n\n    Note:\n        Roles can be set ad-hoc by supplying the respective flag. If\n        `roles` is `False`, all columns in the returned\n        [`DataFrame`][getml.data.DataFrame] have roles\n        [`unused_string`][getml.data.roles.unused_string] or\n        [`unused_float`][getml.data.roles.unused_float]. This dataset contains no units.\n        Before using them in an analysis, a data model needs to be constructed\n        using [`Placeholder`][getml.data.Placeholder].\n    \"\"\"\n\n    ds_name = \"biodegradability\"\n\n    return _load_dataset(\n        ds_name=ds_name,\n        roles=roles,\n        as_pandas=as_pandas,\n        as_dict=as_dict,\n    )\n</code></pre>"},{"location":"reference/datasets/datasets/#getml.datasets.load_consumer_expenditures","title":"load_consumer_expenditures","text":"<pre><code>load_consumer_expenditures(\n    roles: bool = True,\n    units: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]\n</code></pre> <p>Binary classification dataset on consumer expenditures</p> <p>The Consumer Expenditure Data Set is a public domain data set provided by the American Bureau of Labor Statistics. It includes the diary entries, where American consumers are asked to keep record of the products they have purchased each month.</p> <p>We use this dataset to classify whether an item was purchased as a gift or not.</p> PARAMETER DESCRIPTION <code>roles</code> <p>Return data with roles set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>units</code> <p>Return data with units set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>as_pandas</code> <p>Return data as <code>pandas.DataFrame</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>as_dict</code> <p>Return data as dict with <code>df.name</code> as keys and <code>df</code> as values.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>Tuple containing (sorted alphabetically by <code>df.name</code>) the data as <code>DataFrame</code> or <code>pandas.DataFrame</code> (if <code>as_pandas</code> is True) or if <code>as_dict</code> is <code>True</code>: Dictionary containing the data as <code>DataFrame</code> or <code>pandas.DataFrame</code> (if <code>as_pandas</code> is True). The keys correspond to the name of the DataFrame on the <code>engine</code>.</p> <p>The following DataFrames are returned:</p> <ul> <li><code>population</code></li> <li><code>expd</code></li> <li><code>fmld</code></li> <li><code>memd</code></li> </ul> Example <p><pre><code>ce = getml.datasets.load_consumer_expenditures(as_dict=True)\ntype(ce[\"expd\"])\ngetml.data.data_frame.DataFrame\n</code></pre> For a full analysis of the occupancy dataset including all necessary preprocessing steps please refer to getml-examples .</p> Note <p>Roles and units can be set ad-hoc by supplying the respective flag. If <code>roles</code> is <code>False</code>, all columns in the returned <code>DataFrame</code> have roles <code>unused_string</code> or <code>unused_float</code>. Before using them in an analysis, a data model needs to be constructed using <code>Placeholder</code>.</p> Source code in <code>getml/datasets/base.py</code> <pre><code>def load_consumer_expenditures(\n    roles: bool = True,\n    units: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]:\n    \"\"\"\n    Binary classification dataset on consumer expenditures\n\n    The Consumer Expenditure Data Set is a public domain data set provided by\n    the [American Bureau of Labor Statistics](https://www.bls.gov/cex/pumd.htm).\n    It includes the diary entries, where American consumers are asked to keep\n    record of the products they have purchased each month.\n\n    We use this dataset to classify whether an item was purchased as a gift or not.\n\n    Args:\n        roles:\n            Return data with roles set\n\n        units:\n            Return data with units set\n\n        as_pandas:\n            Return data as `pandas.DataFrame`\n\n        as_dict:\n            Return data as dict with `df.name` as keys and\n            `df` as values.\n\n    Returns:\n        Tuple containing (sorted alphabetically by `df.name`) the data as\n            [`DataFrame`][getml.DataFrame] or `pandas.DataFrame` (if `as_pandas`\n            is True) or\n            if `as_dict` is `True`: Dictionary containing the data as\n            [`DataFrame`][getml.DataFrame] or `pandas.DataFrame` (if `as_pandas`\n            is True). The keys correspond to the name of the DataFrame on the\n            [`engine`][getml.engine].\n\n            The following DataFrames are returned:\n\n            * `population`\n            * `expd`\n            * `fmld`\n            * `memd`\n\n    ??? example\n        ```python\n        ce = getml.datasets.load_consumer_expenditures(as_dict=True)\n        type(ce[\"expd\"])\n        getml.data.data_frame.DataFrame\n        ```\n        For a full analysis of the occupancy dataset including all necessary\n        preprocessing steps please refer to [getml-examples\n        ](https://github.com/getml/getml-demo/blob/master/consumer_expenditures.ipynb).\n\n    Note:\n        Roles and units can be set ad-hoc by supplying the respective flag. If\n        `roles` is `False`, all columns in the returned\n        [`DataFrame`][getml.data.DataFrame] have roles\n        [`unused_string`][getml.data.roles.unused_string] or\n        [`unused_float`][getml.data.roles.unused_float].\n        Before using them in an analysis, a data model needs to be constructed\n        using [`Placeholder`][getml.data.Placeholder].\n    \"\"\"\n\n    ds_name = \"consumer_expenditures\"\n\n    return _load_dataset(\n        ds_name=ds_name,\n        roles=roles,\n        units=units,\n        as_pandas=as_pandas,\n        as_dict=as_dict,\n    )\n</code></pre>"},{"location":"reference/datasets/datasets/#getml.datasets.load_interstate94","title":"load_interstate94","text":"<pre><code>load_interstate94(\n    roles: bool = True,\n    units: bool = True,\n    as_pandas: bool = False,\n) -&gt; DataFrameT\n</code></pre> <p>Regression dataset on traffic volume prediction</p> <p>The interstate94 dataset is a multivariate time series containing the hourly traffic volume on I-94 westbound from Minneapolis-St Paul. It is based on data provided by the MN Department of Transportation. Some additional data preparation done by John Hogue. The dataset features some particular interesting characteristics common for time series, which classical models may struggle to appropriately deal with. Such characteristics are:</p> <ul> <li>High frequency (hourly)</li> <li>Dependence on irregular events (holidays)</li> <li>Strong and overlapping cycles (daily, weekly)</li> <li>Anomalies</li> <li>Multiple seasonalities</li> </ul> PARAMETER DESCRIPTION <code>roles</code> <p>Return data with roles set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>units</code> <p>Return data with units set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>as_pandas</code> <p>Return data as <code>pandas.DataFrame</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrameT</code> <p>A DataFrame holding the data described above.</p> <p>The following DataFrames are returned:</p> <ul> <li><code>traffic</code></li> </ul> Example <p><pre><code>traffic = getml.datasets.load_interstate94()\ntype(traffic)\ngetml.data.data_frame.DataFrame\n</code></pre> For a full analysis of the interstate94 dataset including all necessary preprocessing steps please refer to getml-examples.</p> Note <p>Roles and units can be set ad-hoc by supplying the respective flags. If <code>roles</code> is <code>False</code>, all columns in the returned <code>DataFrame</code> have roles <code>unused_string</code> or <code>unused_float</code>. Before using them in an analysis, a data model needs to be constructed using <code>Placeholder</code>.</p> Source code in <code>getml/datasets/base.py</code> <pre><code>def load_interstate94(\n    roles: bool = True,\n    units: bool = True,\n    as_pandas: bool = False,\n) -&gt; DataFrameT:\n    \"\"\"\n    Regression dataset on traffic volume prediction\n\n    The interstate94 dataset is a multivariate time series containing the\n    hourly traffic volume on I-94 westbound from Minneapolis-St Paul. It is\n    based on data provided by the\n    [MN Department of Transportation](https://www.dot.state.mn.us/).\n    Some additional data preparation done by\n    [John Hogue](https://github.com/dreyco676/Anomaly_Detection_A_to_Z/). The\n    dataset features some particular interesting characteristics common for\n    time series, which classical models may struggle to appropriately deal\n    with. Such characteristics are:\n\n    * High frequency (hourly)\n    * Dependence on irregular events (holidays)\n    * Strong and overlapping cycles (daily, weekly)\n    * Anomalies\n    * Multiple seasonalities\n\n    Args:\n        roles:\n            Return data with roles set\n\n        units:\n            Return data with units set\n\n        as_pandas:\n            Return data as `pandas.DataFrame`\n\n    Returns:\n        A DataFrame holding the data described above.\n\n            The following DataFrames are returned:\n\n            * `traffic`\n\n    ??? example\n        ```python\n        traffic = getml.datasets.load_interstate94()\n        type(traffic)\n        getml.data.data_frame.DataFrame\n        ```\n        For a full analysis of the interstate94 dataset including all necessary\n        preprocessing steps please refer to [getml-examples](https://github.com/getml/getml-demo/blob/master/interstate94.ipynb).\n\n    Note:\n        Roles and units can be set ad-hoc by supplying the respective flags. If\n        `roles` is `False`, all columns in the returned\n        [`DataFrame`][getml.data.DataFrame] have roles\n        [`unused_string`][getml.data.roles.unused_string] or\n        [`unused_float`][getml.data.roles.unused_float]. Before using them in an\n        analysis, a data model needs to be constructed using\n        [`Placeholder`][getml.data.Placeholder].\n    \"\"\"\n\n    ds_name = \"interstate94\"\n    dataset = _load_dataset(\n        ds_name=ds_name,\n        roles=roles,\n        units=units,\n        as_pandas=as_pandas,\n    )\n    assert isinstance(dataset, tuple), \"Expected a tuple\"\n    return dataset[0]\n</code></pre>"},{"location":"reference/datasets/datasets/#getml.datasets.load_loans","title":"load_loans","text":"<pre><code>load_loans(\n    roles: bool = True,\n    units: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]\n</code></pre> <p>Binary classification dataset on loan default</p> <p>The loans dataset is based on a financial dataset from the Relational Dataset Repository (former CTU Prague Relational Learning Repository).</p> <p>Reference</p> <p>Berka, Petr (1999). Workshop notes on Discovery Challenge PKDD'99.</p> <p>The dataset contains information on 606 successful and 76 unsuccessful loans. After some preprocessing it contains 5 tables</p> <ul> <li> <p><code>account</code>: Information about the borrower(s) of a given loan.</p> </li> <li> <p><code>loan</code>: Information about the loans themselves, such as the date of creation, the amount, and the planned duration of the loan. The target variable is the status of the loan (default/no default)</p> </li> <li> <p><code>meta</code>: Meta information about the obligor, such as gender and geo-information</p> </li> <li> <p><code>order</code>: Information about permanent orders, debited payments and account balances.</p> </li> <li> <p><code>trans</code>: Information about transactions and accounts balances.</p> </li> </ul> <p>The population table is split into a training and a testing set at 80% of the main population.</p> PARAMETER DESCRIPTION <code>roles</code> <p>Return data with roles set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>units</code> <p>Return data with units set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>as_pandas</code> <p>Return data as <code>pandas.DataFrame</code></p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>as_dict</code> <p>Return data as dict with <code>df.name</code> as keys and <code>df</code> as values.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>Tuple containing (sorted alphabetically by <code>df.name</code>) the data as <code>DataFrame</code> or <code>pandas.DataFrame</code> (if <code>as_pandas</code> is True) or if <code>as_dict</code> is <code>True</code>: Dictionary containing the data as <code>DataFrame</code> or <code>pandas.DataFrame</code> (if <code>as_pandas</code> is True). The keys correspond to the name of the DataFrame on the <code>engine</code>.</p> <p>The following DataFrames are returned:</p> <ul> <li><code>account</code></li> <li><code>loan</code></li> <li><code>meta</code></li> <li><code>order</code></li> <li><code>trans</code></li> </ul> Example <p><pre><code>loans = getml.datasets.load_loans(as_dict=True)\ntype(loans[\"population_train\"])\ngetml.data.data_frame.DataFrame\n</code></pre> For a full analysis of the loans dataset including all necessary preprocessing steps please refer to getml-examples .</p> Note <p>Roles and units can be set ad-hoc by supplying the respective flags. If <code>roles</code> is <code>False</code>, all columns in the returned <code>DataFrame</code> have roles <code>unused_string</code> or <code>unused_float</code>. Before using them in an analysis, a data model needs to be constructed using <code>Placeholder</code>.</p> Source code in <code>getml/datasets/base.py</code> <pre><code>def load_loans(\n    roles: bool = True,\n    units: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]:\n    \"\"\"\n    Binary classification dataset on loan default\n\n    The loans dataset is based on a financial dataset from the [Relational Dataset Repository (former CTU Prague\n    Relational Learning Repository)](https://relational-data.org/dataset/Financial).\n\n    !!! abstract \"Reference\"\n        Berka, Petr (1999). Workshop notes on Discovery Challenge PKDD'99.\n\n    The dataset contains information on 606 successful and 76 unsuccessful\n    loans. After some preprocessing it contains 5 tables\n\n    * `account`: Information about the borrower(s) of a given loan.\n\n    * `loan`: Information about the loans themselves, such as the date of creation, the amount, and the planned duration of the loan. The target variable is the status of the loan (default/no default)\n\n    * `meta`: Meta information about the obligor, such as gender and geo-information\n\n    * `order`: Information about permanent orders, debited payments and account balances.\n\n    * `trans`: Information about transactions and accounts balances.\n\n    The population table is split into a training and a testing set at 80% of the main population.\n\n    Args:\n        roles:\n            Return data with roles set\n\n        units:\n            Return data with units set\n\n        as_pandas:\n            Return data as `pandas.DataFrame`\n\n        as_dict:\n            Return data as dict with `df.name` as keys and\n            `df` as values.\n\n    Returns:\n        Tuple containing (sorted alphabetically by `df.name`) the data as\n            [`DataFrame`][getml.DataFrame] or `pandas.DataFrame` (if `as_pandas`\n            is True) or\n            if `as_dict` is `True`: Dictionary containing the data as\n            [`DataFrame`][getml.DataFrame] or `pandas.DataFrame` (if `as_pandas`\n            is True). The keys correspond to the name of the DataFrame on the\n            [`engine`][getml.engine].\n\n            The following DataFrames are returned:\n\n            * `account`\n            * `loan`\n            * `meta`\n            * `order`\n            * `trans`\n\n    ??? example\n        ```python\n        loans = getml.datasets.load_loans(as_dict=True)\n        type(loans[\"population_train\"])\n        getml.data.data_frame.DataFrame\n        ```\n        For a full analysis of the loans dataset including all necessary\n        preprocessing steps please refer to [getml-examples\n        ](https://github.com/getml/getml-demo/blob/master/loans.ipynb).\n\n    Note:\n        Roles and units can be set ad-hoc by supplying the respective flags. If\n        `roles` is `False`, all columns in the returned\n        [`DataFrame`][getml.data.DataFrame] have roles\n        [`unused_string`][getml.data.roles.unused_string] or\n        [`unused_float`][getml.data.roles.unused_float]. Before using them in an\n        analysis, a data model needs to be constructed using\n        [`Placeholder`][getml.data.Placeholder].\n    \"\"\"\n\n    ds_name = \"loans\"\n\n    return _load_dataset(\n        ds_name=ds_name,\n        roles=roles,\n        units=units,\n        as_pandas=as_pandas,\n        as_dict=as_dict,\n    )\n</code></pre>"},{"location":"reference/datasets/datasets/#getml.datasets.load_occupancy","title":"load_occupancy","text":"<pre><code>load_occupancy(\n    roles: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]\n</code></pre> <p>Binary classification dataset on occupancy detection</p> <p>The occupancy detection dataset is a very simple multivariate time series from the UCI Machine Learning Repository . It is a binary classification problem. The task is to predict room occupancy from Temperature, Humidity, Light and CO2.</p> <p>Reference</p> <p>Candanedo, L. M., &amp; Feldheim, V. (2016). Accurate occupancy detection of an office room from light, temperature, humidity and CO2 measurements using statistical learning models. Energy and Buildings, 112, 28-39.</p> PARAMETER DESCRIPTION <code>roles</code> <p>Return data with roles set</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>as_pandas</code> <p>Return data as <code>pandas.DataFrame</code> s</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>as_dict</code> <p>Return data as dict with <code>df.name</code> as keys and <code>df</code> as values.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]</code> <p>Tuple containing (sorted alphabetically by <code>df.name</code>) the data as <code>DataFrame</code> or <code>pandas.DataFrame</code> (if <code>as_pandas</code> is True) or if <code>as_dict</code> is <code>True</code>: Dictionary containing the data as <code>DataFrame</code> or <code>pandas.DataFrame</code> (if <code>as_pandas</code> is True). The keys correspond to the name of the DataFrame on the <code>engine</code>.</p> <p>The following DataFrames are returned:</p> <ul> <li><code>population_train</code></li> <li><code>population_test</code></li> <li><code>population_validation</code></li> </ul> Example <p><pre><code>population_train, population_test, _ = getml.datasets.load_occupancy()\ntype(occupancy_train)\ngetml.data.data_frame.DataFrame\n</code></pre> For a full analysis of the occupancy dataset including all necessary preprocessing steps please refer to getml-examples .</p> Note <p>Roles can be set ad-hoc by supplying the respective flag. If <code>roles</code> is <code>False</code>, all columns in the returned <code>DataFrame</code> have roles <code>unused_string</code> or <code>unused_float</code>. This dataset contains no units. Before using them in an analysis, a data model needs to be constructed using <code>Placeholder</code>.</p> Source code in <code>getml/datasets/base.py</code> <pre><code>def load_occupancy(\n    roles: bool = True,\n    as_pandas: bool = False,\n    as_dict: bool = False,\n) -&gt; Union[Tuple[DataFrameT, ...], Dict[str, DataFrameT]]:\n    \"\"\"\n    Binary classification dataset on occupancy detection\n\n    The occupancy detection dataset is a very simple multivariate time series\n    from the [UCI Machine Learning Repository\n    ](https://archive.ics.uci.edu/dataset/357/occupancy+detection). It is a\n    binary classification problem. The task is to predict room occupancy\n    from Temperature, Humidity, Light and CO2.\n\n    !!! abstract \"Reference\"\n        Candanedo, L. M., &amp; Feldheim, V. (2016). Accurate occupancy detection of an\n        office room from light, temperature, humidity and CO2 measurements using\n        statistical learning models. Energy and Buildings, 112, 28-39.\n\n    Args:\n        roles:\n            Return data with roles set\n\n        as_pandas:\n            Return data as `pandas.DataFrame` s\n\n        as_dict:\n            Return data as dict with `df.name` as keys and\n            `df` as values.\n\n    Returns:\n        Tuple containing (sorted alphabetically by `df.name`) the data as\n            [`DataFrame`][getml.DataFrame] or `pandas.DataFrame` (if `as_pandas`\n            is True) or\n            if `as_dict` is `True`: Dictionary containing the data as\n            [`DataFrame`][getml.DataFrame] or `pandas.DataFrame` (if `as_pandas`\n            is True). The keys correspond to the name of the DataFrame on the\n            [`engine`][getml.engine].\n\n            The following DataFrames are returned:\n\n            * `population_train`\n            * `population_test`\n            * `population_validation`\n\n    ??? example\n        ```python\n        population_train, population_test, _ = getml.datasets.load_occupancy()\n        type(occupancy_train)\n        getml.data.data_frame.DataFrame\n        ```\n        For a full analysis of the occupancy dataset including all necessary\n        preprocessing steps please refer to [getml-examples\n        ](https://github.com/getml/getml-demo/blob/master/occupancy.ipynb).\n\n\n    Note:\n        Roles can be set ad-hoc by supplying the respective flag. If\n        `roles` is `False`, all columns in the returned\n        [`DataFrame`][getml.data.DataFrame] have roles\n        [`unused_string`][getml.data.roles.unused_string] or\n        [`unused_float`][getml.data.roles.unused_float]. This dataset contains no units.\n        Before using them in an analysis, a data model needs to be constructed\n        using [`Placeholder`][getml.data.Placeholder].\n    \"\"\"\n\n    ds_name = \"occupancy\"\n\n    return _load_dataset(\n        ds_name=ds_name,\n        roles=roles,\n        as_pandas=as_pandas,\n        as_dict=as_dict,\n    )\n</code></pre>"},{"location":"reference/datasets/datasets/#getml.datasets.make_categorical","title":"make_categorical","text":"<pre><code>make_categorical(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.COUNT,\n) -&gt; Tuple[DataFrame, DataFrame]\n</code></pre> <p>Generate a random dataset with categorical variables</p> <p>The dataset consists of a population table and one peripheral table.</p> <p>The peripheral table has 3 columns:</p> <ul> <li><code>column_01</code>: random categorical variable between '0' and '9'</li> <li><code>join_key</code>: random integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> </ul> <p>The population table has 4 columns:</p> <ul> <li><code>column_01</code>: random categorical variable between '0' and '9'</li> <li><code>join_key</code>: unique integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> <li><code>targets</code>: target variable. Defined as the number of matching entries in   the peripheral table for which <code>time_stamp_peripheral &lt;   time_stamp_population</code> and the category in the peripheral table is not   1, 2 or 9. The SQL definition of the target variable read like this</li> </ul> <pre><code>SELECT aggregation( column_01 )\nFROM POPULATION_TABLE t1\nLEFT JOIN PERIPHERAL_TABLE t2\nON t1.join_key = t2.join_key\nWHERE (\n   ( t2.column_01 != '1' AND t2.column_01 != '2' AND t2.column_01 != '9' )\n) AND t2.time_stamps &lt;= t1.time_stamps\nGROUP BY t1.join_key,\n     t1.time_stamp;\n</code></pre> PARAMETER DESCRIPTION <code>n_rows_population</code> <p>Number of rows in the population table.</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>n_rows_peripheral</code> <p>Number of rows in the peripheral table.</p> <p> TYPE: <code>int</code> DEFAULT: <code>125000</code> </p> <code>random_state</code> <p>Seed to initialize the random number generator used for the dataset creation. If set to None, the seed will be the 'microsecond' component of <code>datetime.datetime.now()</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>population_name</code> <p>Name assigned to the <code>DataFrame</code> holding the population table. If set to a name already existing on the getML Engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>categorical_population_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>peripheral_name</code> <p>Name assigned to the <code>DataFrame</code> holding the peripheral table. If set to a name already existing on the getML Engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>categorical_peripheral_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>aggregation</code> <p><code>aggregations</code> used to generate the 'target' column.</p> <p> TYPE: <code>str</code> DEFAULT: <code>COUNT</code> </p> RETURNS DESCRIPTION <code>Tuple[DataFrame, DataFrame]</code> <p>The dataframes are:</p> <ul> <li>population (<code>DataFrame</code>): Population table</li> <li>peripheral (<code>DataFrame</code>): Peripheral table</li> </ul> Source code in <code>getml/datasets/samples_generator.py</code> <pre><code>def make_categorical(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.COUNT,\n) -&gt; Tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Generate a random dataset with categorical variables\n\n    The dataset consists of a population table and one peripheral table.\n\n    The peripheral table has 3 columns:\n\n    * `column_01`: random categorical variable between '0' and '9'\n    * `join_key`: random integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n\n    The population table has 4 columns:\n\n    * `column_01`: random categorical variable between '0' and '9'\n    * `join_key`: unique integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n    * `targets`: target variable. Defined as the number of matching entries in\n      the peripheral table for which ``time_stamp_peripheral &lt;\n      time_stamp_population`` and the category in the peripheral table is not\n      1, 2 or 9. The SQL definition of the target variable read like this\n\n    ```sql\n    SELECT aggregation( column_01 )\n    FROM POPULATION_TABLE t1\n    LEFT JOIN PERIPHERAL_TABLE t2\n    ON t1.join_key = t2.join_key\n    WHERE (\n       ( t2.column_01 != '1' AND t2.column_01 != '2' AND t2.column_01 != '9' )\n    ) AND t2.time_stamps &lt;= t1.time_stamps\n    GROUP BY t1.join_key,\n         t1.time_stamp;\n    ```\n\n    Args:\n        n_rows_population:\n            Number of rows in the population table.\n\n        n_rows_peripheral:\n            Number of rows in the peripheral table.\n\n        random_state:\n            Seed to initialize the random number generator used for\n            the dataset creation. If set to None, the seed will be the\n            'microsecond' component of\n            `datetime.datetime.now()`.\n\n        population_name:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the population\n            table. If set to a name already existing on the getML\n            Engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `categorical_population_` and the seed of the random\n            number generator.\n\n        peripheral_name:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the peripheral\n            table. If set to a name already existing on the getML\n            Engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `categorical_peripheral_` and the seed of the random\n            number generator.\n\n        aggregation:\n            [`aggregations`][getml.feature_learning.aggregations] used to generate the 'target'\n            column.\n\n    Returns:\n        The dataframes are:\n\n            * population ([`DataFrame`][getml.DataFrame]): Population table\n            * peripheral ([`DataFrame`][getml.DataFrame]): Peripheral table\n    \"\"\"\n\n    if random_state is None:\n        random_state = datetime.datetime.now().microsecond\n\n    random = np.random.RandomState(random_state)  # pylint: disable=E1101\n    population_table = pd.DataFrame()\n    population_table[\"column_01\"] = random.randint(0, 10, n_rows_population).astype(str)\n    population_table[\"join_key\"] = np.arange(n_rows_population)\n    population_table[\"time_stamp_population\"] = random.rand(n_rows_population)\n\n    peripheral_table = pd.DataFrame()\n    peripheral_table[\"column_01\"] = random.randint(0, 10, n_rows_peripheral).astype(str)\n    peripheral_table[\"join_key\"] = random.randint(\n        0, n_rows_population, n_rows_peripheral\n    )\n    peripheral_table[\"time_stamp_peripheral\"] = random.rand(n_rows_peripheral)\n\n    # Compute targets\n    temp = peripheral_table.merge(\n        population_table[[\"join_key\", \"time_stamp_population\"]],\n        how=\"left\",\n        on=\"join_key\",\n    )\n\n    # Apply some conditions\n    temp = temp[\n        (temp[\"time_stamp_peripheral\"] &lt;= temp[\"time_stamp_population\"])\n        &amp; (temp[\"column_01\"] != \"1\")\n        &amp; (temp[\"column_01\"] != \"2\")\n        &amp; (temp[\"column_01\"] != \"9\")\n    ]\n\n    # Define the aggregation\n    temp = _aggregate(temp, aggregation, \"column_01\", \"join_key\")\n\n    temp = temp.rename(index=str, columns={\"column_01\": \"targets\"})\n\n    population_table = population_table.merge(temp, how=\"left\", on=\"join_key\")\n\n    del temp\n\n    population_table = population_table.rename(\n        index=str, columns={\"time_stamp_population\": \"time_stamp\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"time_stamp_peripheral\": \"time_stamp\"}\n    )\n\n    # Replace NaN targets with 0.0 - target values may never be NaN!.\n    population_table.targets = np.where(\n        np.isnan(population_table[\"targets\"]), 0, population_table[\"targets\"]\n    )\n\n    # Set default names if none where provided.\n    if not population_name:\n        population_name = \"categorical_population_\" + str(random_state)\n    if not peripheral_name:\n        peripheral_name = \"categorical_peripheral_\" + str(random_state)\n\n    # Create the data.DataFrame counterpart.\n    population_on_engine = data.DataFrame(\n        name=population_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"categorical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n            \"target\": [\"targets\"],\n        },\n    ).read_pandas(population_table)\n\n    peripheral_on_engine = data.DataFrame(\n        name=peripheral_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"categorical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n        },\n    ).read_pandas(peripheral_table)\n\n    return population_on_engine, peripheral_on_engine\n</code></pre>"},{"location":"reference/datasets/datasets/#getml.datasets.make_discrete","title":"make_discrete","text":"<pre><code>make_discrete(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.COUNT,\n) -&gt; Tuple[DataFrame, DataFrame]\n</code></pre> <p>Generate a random dataset with categorical variables</p> <p>The dataset consists of a population table and one peripheral table.</p> <p>The peripheral table has 3 columns:</p> <ul> <li><code>column_01</code>: random integer between -10 and 10</li> <li><code>join_key</code>: random integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> </ul> <p>The population table has 4 columns:</p> <ul> <li><code>column_01</code>: random number between -1 and 1</li> <li><code>join_key</code>: unique integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> <li><code>targets</code>: target variable. Defined as the minimum value greater than 0   in the peripheral table for which   <code>time_stamp_peripheral &lt; time_stamp_population</code>   and the join key matches <pre><code>SELECT aggregation( column_01 )\nFROM POPULATION t1\nLEFT JOIN PERIPHERAL t2\nON t1.join_key = t2.join_key\nWHERE (\n   ( t2.column_01 &gt; 0 )\n) AND t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.join_key,\n         t1.time_stamp;\n</code></pre></li> </ul> PARAMETER DESCRIPTION <code>n_rows_population</code> <p>Number of rows in the population table.</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>n_rows_peripheral</code> <p>Number of rows in the peripheral table.</p> <p> TYPE: <code>int</code> DEFAULT: <code>125000</code> </p> <code>random_state</code> <p>Seed to initialize the random number generator used for the dataset creation. If set to None, the seed will be the 'microsecond' component of <code>datetime.datetime.now()</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>population_name</code> <p>Name assigned to the <code>DataFrame</code> holding the population table. If set to a name already existing on the getML Engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>discrete_population_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>peripheral_name</code> <p>Name assigned to the <code>DataFrame</code> holding the peripheral table. If set to a name already existing on the getML Engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>discrete_peripheral_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>aggregation</code> <p>aggregations used to generate the 'target' column.</p> <p> TYPE: <code>str</code> DEFAULT: <code>COUNT</code> </p> RETURNS DESCRIPTION <code>Tuple[DataFrame, DataFrame]</code> <p>The dataframes are:</p> <ul> <li>population (<code>DataFrame</code>): Population table</li> <li>peripheral (<code>DataFrame</code>): Peripheral table</li> </ul> Source code in <code>getml/datasets/samples_generator.py</code> <pre><code>def make_discrete(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.COUNT,\n) -&gt; Tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Generate a random dataset with categorical variables\n\n    The dataset consists of a population table and one peripheral table.\n\n    The peripheral table has 3 columns:\n\n    * `column_01`: random integer between -10 and 10\n    * `join_key`: random integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n\n    The population table has 4 columns:\n\n    * `column_01`: random number between -1 and 1\n    * `join_key`: unique integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n    * `targets`: target variable. Defined as the minimum value greater than 0\n      in the peripheral table for which\n      ``time_stamp_peripheral &lt; time_stamp_population``\n      and the join key matches\n    ```sql\n    SELECT aggregation( column_01 )\n    FROM POPULATION t1\n    LEFT JOIN PERIPHERAL t2\n    ON t1.join_key = t2.join_key\n    WHERE (\n       ( t2.column_01 &gt; 0 )\n    ) AND t2.time_stamp &lt;= t1.time_stamp\n    GROUP BY t1.join_key,\n             t1.time_stamp;\n    ```\n\n    Args:\n        n_rows_population:\n            Number of rows in the population table.\n\n        n_rows_peripheral:\n            Number of rows in the peripheral table.\n\n        random_state:\n            Seed to initialize the random number generator used for\n            the dataset creation. If set to None, the seed will be the\n            'microsecond' component of\n            `datetime.datetime.now()`.\n\n        population_name:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the population\n            table. If set to a name already existing on the getML\n            Engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `discrete_population_` and the seed of the random\n            number generator.\n\n        peripheral_name:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the peripheral\n            table. If set to a name already existing on the getML\n            Engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `discrete_peripheral_` and the seed of the random\n            number generator.\n\n        aggregation:\n            [aggregations][getml.feature_learning.aggregations] used to generate the 'target'\n            column.\n\n    Returns:\n        The dataframes are:\n\n            * population ([`DataFrame`][getml.DataFrame]): Population table\n            * peripheral ([`DataFrame`][getml.DataFrame]): Peripheral table\n    \"\"\"\n\n    if random_state is None:\n        random_state = datetime.datetime.now().microsecond\n\n    random = np.random.RandomState(random_state)  # pylint: disable=E1101\n\n    population_table = pd.DataFrame()\n    population_table[\"column_01\"] = random.randint(0, 10, n_rows_population).astype(str)\n    population_table[\"join_key\"] = np.arange(n_rows_population)\n    population_table[\"time_stamp_population\"] = random.rand(n_rows_population)\n\n    peripheral_table = pd.DataFrame()\n    peripheral_table[\"column_01\"] = random.randint(-11, 11, n_rows_peripheral)\n    peripheral_table[\"join_key\"] = random.randint(\n        0, n_rows_population, n_rows_peripheral\n    )\n    peripheral_table[\"time_stamp_peripheral\"] = random.rand(n_rows_peripheral)\n\n    # Compute targets\n    temp = peripheral_table.merge(\n        population_table[[\"join_key\", \"time_stamp_population\"]],\n        how=\"left\",\n        on=\"join_key\",\n    )\n\n    # Apply some conditions\n    temp = temp[\n        (temp[\"time_stamp_peripheral\"] &lt;= temp[\"time_stamp_population\"])\n        &amp; (temp[\"column_01\"] &gt; 0.0)\n    ]\n\n    # Define the aggregation\n    temp = _aggregate(temp, aggregation, \"column_01\", \"join_key\")\n\n    temp = temp.rename(index=str, columns={\"column_01\": \"targets\"})\n\n    population_table = population_table.merge(temp, how=\"left\", on=\"join_key\")\n\n    del temp\n\n    population_table = population_table.rename(\n        index=str, columns={\"time_stamp_population\": \"time_stamp\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"time_stamp_peripheral\": \"time_stamp\"}\n    )\n\n    # Replace NaN targets with 0.0 - target values may never be NaN!.\n    population_table.targets = np.where(\n        np.isnan(population_table[\"targets\"]), 0, population_table[\"targets\"]\n    )\n\n    # Set default names if none where provided.\n    if not population_name:\n        population_name = \"discrete_population_\" + str(random_state)\n    if not peripheral_name:\n        peripheral_name = \"discrete_peripheral_\" + str(random_state)\n\n    # Create the data.DataFrame counterpart.\n    population_on_engine = data.DataFrame(\n        name=population_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n            \"target\": [\"targets\"],\n        },\n    ).read_pandas(population_table)\n\n    peripheral_on_engine = data.DataFrame(\n        name=peripheral_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n        },\n    ).read_pandas(peripheral_table)\n\n    return population_on_engine, peripheral_on_engine\n</code></pre>"},{"location":"reference/datasets/datasets/#getml.datasets.make_numerical","title":"make_numerical","text":"<pre><code>make_numerical(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.COUNT,\n) -&gt; Tuple[DataFrame, DataFrame]\n</code></pre> <p>Generate a random dataset with continuous numerical variables</p> <p>The dataset consists of a population table and one peripheral table.</p> <p>The peripheral table has 3 columns:</p> <ul> <li><code>column_01</code>:  random number between -1 and 1</li> <li><code>join_key</code>: random integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> </ul> <p>The population table has 4 columns:</p> <ul> <li><code>column_01</code>:  random number between -1 and 1</li> <li><code>join_key</code>: unique integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> <li><code>targets</code>: target variable. Defined as the number of matching entries in   the peripheral table for which <code>time_stamp_peripheral &lt;   time_stamp_population &lt; time_stamp_peripheral + 0.5</code></li> </ul> <pre><code>SELECT aggregation( column_01 )\nFROM POPULATION t1\nLEFT JOIN PERIPHERAL t2\nON t1.join_key = t2.join_key\nWHERE (\n   ( t1.time_stamp - t2.time_stamp &lt;= 0.5 )\n) AND t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.join_key,\n     t1.time_stamp;\n</code></pre> PARAMETER DESCRIPTION <code>n_rows_population</code> <p>Number of rows in the population table.</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>n_rows_peripheral</code> <p>Number of rows in the peripheral table.</p> <p> TYPE: <code>int</code> DEFAULT: <code>125000</code> </p> <code>random_state</code> <p>Seed to initialize the random number generator used for the dataset creation. If set to None, the seed will be the 'microsecond' component of <code>datetime.datetime.now()</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>population_name</code> <p>Name assigned to the <code>DataFrame</code> holding the population table. If set to a name already existing on the getML Engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>numerical_population_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>peripheral_name</code> <p>Name assigned to the <code>DataFrame</code> holding the peripheral table. If set to a name already existing on the getML Engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>numerical_peripheral_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>aggregation</code> <p>aggregations used to generate the 'target' column.</p> <p> TYPE: <code>str</code> DEFAULT: <code>COUNT</code> </p> RETURNS DESCRIPTION <code>Tuple[DataFrame, DataFrame]</code> <p>The dataframes are:</p> <ul> <li>population (<code>DataFrame</code>): Population table</li> <li>peripheral (<code>DataFrame</code>): Peripheral table</li> </ul> Source code in <code>getml/datasets/samples_generator.py</code> <pre><code>def make_numerical(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.COUNT,\n) -&gt; Tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Generate a random dataset with continuous numerical variables\n\n    The dataset consists of a population table and one peripheral table.\n\n    The peripheral table has 3 columns:\n\n    * `column_01`:  random number between -1 and 1\n    * `join_key`: random integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n\n    The population table has 4 columns:\n\n    * `column_01`:  random number between -1 and 1\n    * `join_key`: unique integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n    * `targets`: target variable. Defined as the number of matching entries in\n      the peripheral table for which ``time_stamp_peripheral &lt;\n      time_stamp_population &lt; time_stamp_peripheral + 0.5``\n\n    ```sql\n    SELECT aggregation( column_01 )\n    FROM POPULATION t1\n    LEFT JOIN PERIPHERAL t2\n    ON t1.join_key = t2.join_key\n    WHERE (\n       ( t1.time_stamp - t2.time_stamp &lt;= 0.5 )\n    ) AND t2.time_stamp &lt;= t1.time_stamp\n    GROUP BY t1.join_key,\n         t1.time_stamp;\n    ```\n\n    Args:\n        n_rows_population:\n            Number of rows in the population table.\n\n        n_rows_peripheral:\n            Number of rows in the peripheral table.\n\n        random_state:\n            Seed to initialize the random number generator used for\n            the dataset creation. If set to None, the seed will be the\n            'microsecond' component of\n            `datetime.datetime.now()`.\n\n        population_name:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the population\n            table. If set to a name already existing on the getML\n            Engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `numerical_population_` and the seed of the random\n            number generator.\n\n        peripheral_name:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the peripheral\n            table. If set to a name already existing on the getML\n            Engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `numerical_peripheral_` and the seed of the random\n            number generator.\n\n        aggregation:\n            [aggregations][getml.feature_learning.aggregations] used to generate the 'target'\n            column.\n\n    Returns:\n        The dataframes are:\n\n            * population ([`DataFrame`][getml.DataFrame]): Population table\n            * peripheral ([`DataFrame`][getml.DataFrame]): Peripheral table\n    \"\"\"\n\n    if random_state is None:\n        random_state = datetime.datetime.now().microsecond\n\n    random = np.random.RandomState(random_state)  # pylint: disable=E1101\n\n    population_table = pd.DataFrame()\n    population_table[\"column_01\"] = random.rand(n_rows_population) * 2.0 - 1.0\n    population_table[\"join_key\"] = np.arange(n_rows_population)\n    population_table[\"time_stamp_population\"] = random.rand(n_rows_population)\n\n    peripheral_table = pd.DataFrame()\n    peripheral_table[\"column_01\"] = random.rand(n_rows_peripheral) * 2.0 - 1.0\n    peripheral_table[\"join_key\"] = random.randint(\n        0, n_rows_population, n_rows_peripheral\n    )\n    peripheral_table[\"time_stamp_peripheral\"] = random.rand(n_rows_peripheral)\n\n    # Compute targets\n    temp = peripheral_table.merge(\n        population_table[[\"join_key\", \"time_stamp_population\"]],\n        how=\"left\",\n        on=\"join_key\",\n    )\n\n    # Apply some conditions\n    temp = temp[\n        (temp[\"time_stamp_peripheral\"] &lt;= temp[\"time_stamp_population\"])\n        &amp; (temp[\"time_stamp_peripheral\"] &gt;= temp[\"time_stamp_population\"] - 0.5)\n    ]\n\n    # Define the aggregation\n    temp = _aggregate(temp, aggregation, \"column_01\", \"join_key\")\n\n    temp = temp.rename(index=str, columns={\"column_01\": \"targets\"})\n\n    population_table = population_table.merge(temp, how=\"left\", on=\"join_key\")\n\n    del temp\n\n    population_table = population_table.rename(\n        index=str, columns={\"time_stamp_population\": \"time_stamp\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"time_stamp_peripheral\": \"time_stamp\"}\n    )\n\n    # Replace NaN targets with 0.0 - target values may never be NaN!.\n    population_table.targets = np.where(\n        np.isnan(population_table[\"targets\"]), 0, population_table[\"targets\"]\n    )\n\n    # Set default names if none where provided.\n    if not population_name:\n        population_name = \"numerical_population_\" + str(random_state)\n    if not peripheral_name:\n        peripheral_name = \"numerical_peripheral_\" + str(random_state)\n\n    # Create the data.DataFrame counterpart.\n    population_on_engine = data.DataFrame(\n        name=population_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n            \"target\": [\"targets\"],\n        },\n    ).read_pandas(population_table)\n\n    peripheral_on_engine = data.DataFrame(\n        name=peripheral_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n        },\n    ).read_pandas(peripheral_table)\n\n    return population_on_engine, peripheral_on_engine\n</code></pre>"},{"location":"reference/datasets/datasets/#getml.datasets.make_same_units_categorical","title":"make_same_units_categorical","text":"<pre><code>make_same_units_categorical(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.COUNT,\n) -&gt; Tuple[DataFrame, DataFrame]\n</code></pre> <p>Generate a random dataset with categorical variables</p> <p>The dataset consists of a population table and one peripheral table.</p> <p>The peripheral table has 3 columns:</p> <ul> <li><code>column_01</code>: random categorical variable between '0' and '9'</li> <li><code>join_key</code>: random integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> </ul> <p>The population table has 4 columns:</p> <ul> <li><code>column_01</code>: random categorical variable between '0' and '9'</li> <li><code>join_key</code>: unique integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> <li><code>targets</code>: target variable. Defined as the number of matching entries in   the peripheral table for which <code>time_stamp_peripheral &lt;   time_stamp_population</code> and the category in the peripheral table is not   1, 2 or 9</li> </ul> <pre><code>SELECT aggregation( column_02 )\nFROM POPULATION_TABLE t1\nLEFT JOIN PERIPHERAL_TABLE t2\nON t1.join_key = t2.join_key\nWHERE (\n   ( t1.column_01 == t2.column_01 )\n) AND t2.time_stamps &lt;= t1.time_stamps\nGROUP BY t1.join_key,\n     t1.time_stamp;\n</code></pre> PARAMETER DESCRIPTION <code>n_rows_population</code> <p>Number of rows in the population table.</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>n_rows_peripheral</code> <p>Number of rows in the peripheral table.</p> <p> TYPE: <code>int</code> DEFAULT: <code>125000</code> </p> <code>random_state</code> <p>Seed to initialize the random number generator used for the dataset creation. If set to None, the seed will be the 'microsecond' component of <code>datetime.datetime.now()</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>population_name</code> <p>Name assigned to the <code>DataFrame</code> holding the population table. If set to a name already existing on the getML Engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>make_same_units_categorical_population_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>peripheral_name</code> <p>Name assigned to the <code>DataFrame</code> holding the peripheral table. If set to a name already existing on the getML Engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>make_same_units_categorical_peripheral_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>aggregation</code> <p>aggregations used to generate the 'target' column.</p> <p> TYPE: <code>str</code> DEFAULT: <code>COUNT</code> </p> RETURNS DESCRIPTION <code>Tuple[DataFrame, DataFrame]</code> <p>The dataframes are:</p> <ul> <li>population (<code>DataFrame</code>): Population table</li> <li>peripheral (<code>DataFrame</code>): Peripheral table</li> </ul> Source code in <code>getml/datasets/samples_generator.py</code> <pre><code>def make_same_units_categorical(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.COUNT,\n) -&gt; Tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Generate a random dataset with categorical variables\n\n    The dataset consists of a population table and one peripheral table.\n\n    The peripheral table has 3 columns:\n\n    * `column_01`: random categorical variable between '0' and '9'\n    * `join_key`: random integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n\n    The population table has 4 columns:\n\n    * `column_01`: random categorical variable between '0' and '9'\n    * `join_key`: unique integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n    * `targets`: target variable. Defined as the number of matching entries in\n      the peripheral table for which ``time_stamp_peripheral &lt;\n      time_stamp_population`` and the category in the peripheral table is not\n      1, 2 or 9\n\n    ```sql\n    SELECT aggregation( column_02 )\n    FROM POPULATION_TABLE t1\n    LEFT JOIN PERIPHERAL_TABLE t2\n    ON t1.join_key = t2.join_key\n    WHERE (\n       ( t1.column_01 == t2.column_01 )\n    ) AND t2.time_stamps &lt;= t1.time_stamps\n    GROUP BY t1.join_key,\n         t1.time_stamp;\n    ```\n\n    Args:\n        n_rows_population:\n            Number of rows in the population table.\n\n        n_rows_peripheral:\n            Number of rows in the peripheral table.\n\n        random_state:\n            Seed to initialize the random number generator used for\n            the dataset creation. If set to None, the seed will be the\n            'microsecond' component of\n            `datetime.datetime.now()`.\n\n        population_name:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the population\n            table. If set to a name already existing on the getML\n            Engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `make_same_units_categorical_population_` and the seed of the random\n            number generator.\n\n        peripheral_name:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the peripheral\n            table. If set to a name already existing on the getML\n            Engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `make_same_units_categorical_peripheral_` and the seed of the random\n            number generator.\n\n        aggregation:\n            [aggregations][getml.feature_learning.aggregations] used to generate the 'target'\n            column.\n\n    Returns:\n        The dataframes are:\n\n            * population ([`DataFrame`][getml.DataFrame]): Population table\n            * peripheral ([`DataFrame`][getml.DataFrame]): Peripheral table\n    \"\"\"\n\n    if random_state is None:\n        random_state = datetime.datetime.now().microsecond\n\n    random = np.random.RandomState(random_state)  # pylint: disable=E1101\n\n    population_table = pd.DataFrame()\n    population_table[\"column_01_population\"] = (\n        (random.rand(n_rows_population) * 10.0).astype(np.int32).astype(str)\n    )\n    population_table[\"join_key\"] = range(n_rows_population)\n    population_table[\"time_stamp_population\"] = random.rand(n_rows_population)\n\n    peripheral_table = pd.DataFrame()\n    peripheral_table[\"column_01_peripheral\"] = (\n        (random.rand(n_rows_peripheral) * 10.0).astype(np.int32).astype(str)\n    )\n    peripheral_table[\"column_02\"] = random.rand(n_rows_peripheral) * 2.0 - 1.0\n    peripheral_table[\"join_key\"] = [\n        int(float(n_rows_population) * random.rand(1)[0])\n        for i in range(n_rows_peripheral)\n    ]\n    peripheral_table[\"time_stamp_peripheral\"] = random.rand(n_rows_peripheral)\n\n    # ----------------\n\n    temp = peripheral_table.merge(\n        population_table[[\"join_key\", \"time_stamp_population\", \"column_01_population\"]],\n        how=\"left\",\n        on=\"join_key\",\n    )\n\n    # Apply some conditions\n    temp = temp[\n        (temp[\"time_stamp_peripheral\"] &lt;= temp[\"time_stamp_population\"])\n        &amp; (temp[\"column_01_peripheral\"] == temp[\"column_01_population\"])\n    ]\n\n    # Define the aggregation\n    temp = _aggregate(temp, aggregation, \"column_02\", \"join_key\")\n\n    temp = temp.rename(index=str, columns={\"column_02\": \"targets\"})\n\n    population_table = population_table.merge(temp, how=\"left\", on=\"join_key\")\n\n    population_table = population_table.rename(\n        index=str, columns={\"column_01_population\": \"column_01\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"column_01_peripheral\": \"column_01\"}\n    )\n\n    del temp\n\n    # ----------------\n\n    population_table = population_table.rename(\n        index=str, columns={\"time_stamp_population\": \"time_stamp\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"time_stamp_peripheral\": \"time_stamp\"}\n    )\n\n    # ----------------\n\n    # Replace NaN targets with 0.0 - target values may never be NaN!.\n    population_table[\"targets\"] = [\n        0.0 if val != val else val for val in population_table[\"targets\"]\n    ]\n\n    # ----------------\n\n    # Set default names if none where provided.\n    population_name = (\n        population_name\n        or \"make_same_units_categorical_population__\" + str(random_state)\n    )\n\n    peripheral_name = (\n        peripheral_name\n        or \"make_same_units_categorical_peripheral__\" + str(random_state)\n    )\n\n    # Create the data.DataFrame counterpart.\n    population_on_engine = data.DataFrame(\n        name=population_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"categorical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n            \"target\": [\"targets\"],\n        },\n    ).read_pandas(population_table)\n\n    peripheral_on_engine = data.DataFrame(\n        name=peripheral_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"categorical\": [\"column_01\"],\n            \"numerical\": [\"column_02\"],\n            \"time_stamp\": [\"time_stamp\"],\n        },\n    ).read_pandas(peripheral_table)\n\n    # ----------------\n\n    return population_on_engine, peripheral_on_engine\n</code></pre>"},{"location":"reference/datasets/datasets/#getml.datasets.make_same_units_numerical","title":"make_same_units_numerical","text":"<pre><code>make_same_units_numerical(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.COUNT,\n) -&gt; Tuple[DataFrame, DataFrame]\n</code></pre> <p>Generate a random dataset with continuous numerical variables</p> <p>The dataset consists of a population table and one peripheral table.</p> <p>The peripheral table has 3 columns:</p> <ul> <li><code>column_01</code>:  random number between -1 and 1</li> <li><code>join_key</code>: random integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> </ul> <p>The population table has 4 columns:</p> <ul> <li><code>column_01</code>:  random number between -1 and 1</li> <li><code>join_key</code>: unique integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> <li><code>targets</code>: target variable. Defined as the number of matching entries in   the peripheral table for which <code>time_stamp_peripheral &lt;   time_stamp_population &lt; time_stamp_peripheral + 0.5</code></li> </ul> <pre><code>SELECT aggregation( column_01 )\nFROM POPULATION t1\nLEFT JOIN PERIPHERAL t2\nON t1.join_key = t2.join_key\nWHERE (\n   ( t1.column_01 - t2.column_01 &lt;= 0.5 )\n) AND t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.join_key,\n     t1.time_stamp;\n</code></pre> PARAMETER DESCRIPTION <code>n_rows_population</code> <p>Number of rows in the population table.</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>n_rows_peripheral</code> <p>Number of rows in the peripheral table.</p> <p> TYPE: <code>int</code> DEFAULT: <code>125000</code> </p> <code>random_state</code> <p>Seed to initialize the random number generator used for the dataset creation. If set to None, the seed will be the 'microsecond' component of <code>datetime.datetime.now()</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>population_name</code> <p>Name assigned to the <code>DataFrame</code> holding the population table. If set to a name already existing on the getML Engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>make_same_units_numerical_population_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>peripheral_name</code> <p>Name assigned to <code>DataFrame</code> holding the peripheral table. If set to a name already existing on the getML Engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>make_same_units_numerical_peripheral_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>aggregation</code> <p>aggregations used to generate the 'target' column.</p> <p> TYPE: <code>str</code> DEFAULT: <code>COUNT</code> </p> RETURNS DESCRIPTION <code>Tuple[DataFrame, DataFrame]</code> <p>The dataframes are:</p> <ul> <li>population (<code>DataFrame</code>): Population table</li> <li>peripheral (<code>DataFrame</code>): Peripheral table</li> </ul> Source code in <code>getml/datasets/samples_generator.py</code> <pre><code>def make_same_units_numerical(\n    n_rows_population: int = 500,\n    n_rows_peripheral: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name: str = \"\",\n    aggregation: str = aggregations.COUNT,\n) -&gt; Tuple[DataFrame, DataFrame]:\n    \"\"\"\n    Generate a random dataset with continuous numerical variables\n\n    The dataset consists of a population table and one peripheral table.\n\n    The peripheral table has 3 columns:\n\n    * `column_01`:  random number between -1 and 1\n    * `join_key`: random integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n\n    The population table has 4 columns:\n\n    * `column_01`:  random number between -1 and 1\n    * `join_key`: unique integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n    * `targets`: target variable. Defined as the number of matching entries in\n      the peripheral table for which ``time_stamp_peripheral &lt;\n      time_stamp_population &lt; time_stamp_peripheral + 0.5``\n\n    ```sql\n    SELECT aggregation( column_01 )\n    FROM POPULATION t1\n    LEFT JOIN PERIPHERAL t2\n    ON t1.join_key = t2.join_key\n    WHERE (\n       ( t1.column_01 - t2.column_01 &lt;= 0.5 )\n    ) AND t2.time_stamp &lt;= t1.time_stamp\n    GROUP BY t1.join_key,\n         t1.time_stamp;\n    ```\n\n    Args:\n        n_rows_population:\n            Number of rows in the population table.\n\n        n_rows_peripheral:\n            Number of rows in the peripheral table.\n\n        random_state:\n            Seed to initialize the random number generator used for\n            the dataset creation. If set to None, the seed will be the\n            'microsecond' component of\n            `datetime.datetime.now()`.\n\n        population_name:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the population\n            table. If set to a name already existing on the getML\n            Engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `make_same_units_numerical_population_` and the seed of the random\n            number generator.\n\n        peripheral_name:\n            Name assigned to\n            [`DataFrame`][getml.DataFrame] holding the peripheral\n            table. If set to a name already existing on the getML\n            Engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `make_same_units_numerical_peripheral_` and the seed of the random\n            number generator.\n\n        aggregation:\n            [aggregations][getml.feature_learning.aggregations] used to generate the 'target'\n            column.\n\n    Returns:\n        The dataframes are:\n\n            * population ([`DataFrame`][getml.DataFrame]): Population table\n            * peripheral ([`DataFrame`][getml.DataFrame]): Peripheral table\n    \"\"\"\n\n    if random_state is None:\n        random_state = datetime.datetime.now().microsecond\n\n    random = np.random.RandomState(random_state)  # pylint: disable=E1101\n\n    population_table = pd.DataFrame()\n    population_table[\"column_01_population\"] = (\n        random.rand(n_rows_population) * 2.0 - 1.0\n    )\n    population_table[\"join_key\"] = range(n_rows_population)\n    population_table[\"time_stamp_population\"] = random.rand(n_rows_population)\n\n    peripheral_table = pd.DataFrame()\n    peripheral_table[\"column_01_peripheral\"] = (\n        random.rand(n_rows_peripheral) * 2.0 - 1.0\n    )\n    peripheral_table[\"join_key\"] = [\n        int(float(n_rows_population) * random.rand(1)[0])\n        for i in range(n_rows_peripheral)\n    ]\n    peripheral_table[\"time_stamp_peripheral\"] = random.rand(n_rows_peripheral)\n\n    # ----------------\n\n    temp = peripheral_table.merge(\n        population_table[[\"join_key\", \"time_stamp_population\", \"column_01_population\"]],\n        how=\"left\",\n        on=\"join_key\",\n    )\n\n    # Apply some conditions\n    temp = temp[\n        (temp[\"time_stamp_peripheral\"] &lt;= temp[\"time_stamp_population\"])\n        &amp; (temp[\"column_01_peripheral\"] &gt; temp[\"column_01_population\"] - 0.5)\n    ]\n\n    # Define the aggregation\n    temp = (\n        temp[[\"column_01_peripheral\", \"join_key\"]]\n        .groupby([\"join_key\"], as_index=False)\n        .count()\n    )\n\n    temp = temp.rename(index=str, columns={\"column_01_peripheral\": \"targets\"})\n\n    population_table = population_table.merge(temp, how=\"left\", on=\"join_key\")\n\n    population_table = population_table.rename(\n        index=str, columns={\"column_01_population\": \"column_01\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"column_01_peripheral\": \"column_01\"}\n    )\n\n    del temp\n\n    # ----------------\n\n    population_table = population_table.rename(\n        index=str, columns={\"time_stamp_population\": \"time_stamp\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"time_stamp_peripheral\": \"time_stamp\"}\n    )\n\n    # ----------------\n\n    # Replace NaN targets with 0.0 - target values may never be NaN!.\n    population_table[\"targets\"] = [\n        0.0 if val != val else val for val in population_table[\"targets\"]\n    ]\n\n    # ----------------\n\n    # Set default names if none where provided.\n    if not population_name:\n        population_name = \"same_unit_numerical_population_\" + str(random_state)\n    if not peripheral_name:\n        peripheral_name = \"same_unit_numerical_peripheral_\" + str(random_state)\n\n    # Create the data.DataFrame counterpart.\n    population_on_engine = data.DataFrame(\n        name=population_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n            \"target\": [\"targets\"],\n        },\n    ).read_pandas(population_table)\n\n    peripheral_on_engine = data.DataFrame(\n        name=peripheral_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n        },\n    ).read_pandas(peripheral_table)\n\n    return population_on_engine, peripheral_on_engine\n</code></pre>"},{"location":"reference/datasets/datasets/#getml.datasets.make_snowflake","title":"make_snowflake","text":"<pre><code>make_snowflake(\n    n_rows_population: int = 500,\n    n_rows_peripheral1: int = 5000,\n    n_rows_peripheral2: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name1: str = \"\",\n    peripheral_name2: str = \"\",\n    aggregation1: str = aggregations.SUM,\n    aggregation2: str = aggregations.COUNT,\n) -&gt; Tuple[DataFrame, DataFrame, DataFrame]\n</code></pre> <p>Generate a random dataset with continuous numerical variables</p> <p>The dataset consists of a population table and two peripheral tables.</p> <p>The first peripheral table has 4 columns:</p> <ul> <li><code>column_01</code>:  random number between -1 and 1</li> <li><code>join_key</code>: random integer in the range from 0 to <code>n_rows_population</code></li> <li><code>join_key2</code>: unique integer in the range from 0 to <code>n_rows_peripheral1</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> </ul> <p>The second peripheral table has 3 columns:</p> <ul> <li><code>column_01</code>:  random number between -1 and 1</li> <li><code>join_key2</code>: random integer in the range from 0 to <code>n_rows_peripheral1</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> </ul> <p>The population table has 4 columns:</p> <ul> <li><code>column_01</code>:  random number between -1 and 1</li> <li><code>join_key</code>: unique integer in the range from 0 to <code>n_rows_population</code></li> <li><code>time_stamp</code>: random number between 0 and 1</li> <li><code>targets</code>: target variable as defined by the SQL block below:</li> </ul> <pre><code>SELECT aggregation1( feature_1_1 )\nFROM POPULATION t1\nLEFT JOIN (\n    SELECT aggregation2( t4.column_01 ) AS feature_1_1\n    FROM PERIPHERAL t3\n    LEFT JOIN PERIPHERAL2 t4\n    ON t3.join_key2 = t4.join_key2\n    WHERE (\n       ( t3.time_stamp - t4.time_stamp &lt;= 0.5 )\n    ) AND t4.time_stamp &lt;= t3.time_stamp\n    GROUP BY t3.join_key,\n         t3.time_stamp\n) t2\nON t1.join_key = t2.join_key\nWHERE t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.join_key,\n     t1.time_stamp;\n</code></pre> PARAMETER DESCRIPTION <code>n_rows_population</code> <p>Number of rows in the population table.</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>n_rows_peripheral1</code> <p>Number of rows in the first peripheral table.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5000</code> </p> <code>n_rows_peripheral2</code> <p>Number of rows in the second peripheral table.</p> <p> TYPE: <code>int</code> DEFAULT: <code>125000</code> </p> <code>random_state</code> <p>Seed to initialize the random number generator used for the dataset creation. If set to None, the seed will be the 'microsecond' component of <code>datetime.datetime.now()</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>population_name</code> <p>Name assigned to the <code>DataFrame</code> holding the population table. If set to a name already existing on the getML Engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>snowflake_population_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>peripheral_name1</code> <p>Name assigned to the <code>DataFrame</code> holding the first peripheral table. If set to a name already existing on the getML Engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>snowflake_peripheral_1_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>peripheral_name2</code> <p>Name assigned to the <code>DataFrame</code> holding the second peripheral table. If set to a name already existing on the getML Engine, the corresponding <code>DataFrame</code> will be overwritten. If set to an empty string, a unique name will be generated by concatenating <code>snowflake_peripheral_2_</code> and the seed of the random number generator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>aggregation1</code> <p>aggregations used to generate the 'target' column in the first peripheral table.</p> <p> TYPE: <code>str</code> DEFAULT: <code>SUM</code> </p> <code>aggregation2</code> <p>aggregations used to generate the 'target' column in the second peripheral table.</p> <p> TYPE: <code>str</code> DEFAULT: <code>COUNT</code> </p> RETURNS DESCRIPTION <code>Tuple[DataFrame, DataFrame, DataFrame]</code> <p>The dataframes are:</p> <ul> <li>population (<code>DataFrame</code>): Population table</li> <li>peripheral (<code>DataFrame</code>): Peripheral table</li> <li>peripheral_2 (<code>DataFrame</code>): Peripheral table</li> </ul> Source code in <code>getml/datasets/samples_generator.py</code> <pre><code>def make_snowflake(\n    n_rows_population: int = 500,\n    n_rows_peripheral1: int = 5000,\n    n_rows_peripheral2: int = 125000,\n    random_state: Optional[int] = None,\n    population_name: str = \"\",\n    peripheral_name1: str = \"\",\n    peripheral_name2: str = \"\",\n    aggregation1: str = aggregations.SUM,\n    aggregation2: str = aggregations.COUNT,\n) -&gt; Tuple[DataFrame, DataFrame, DataFrame]:\n    \"\"\"\n    Generate a random dataset with continuous numerical variables\n\n    The dataset consists of a population table and two peripheral tables.\n\n    The first peripheral table has 4 columns:\n\n    * `column_01`:  random number between -1 and 1\n    * `join_key`: random integer in the range from 0 to ``n_rows_population``\n    * `join_key2`: unique integer in the range from 0 to ``n_rows_peripheral1``\n    * `time_stamp`: random number between 0 and 1\n\n    The second peripheral table has 3 columns:\n\n    * `column_01`:  random number between -1 and 1\n    * `join_key2`: random integer in the range from 0 to ``n_rows_peripheral1``\n    * `time_stamp`: random number between 0 and 1\n\n    The population table has 4 columns:\n\n    * `column_01`:  random number between -1 and 1\n    * `join_key`: unique integer in the range from 0 to ``n_rows_population``\n    * `time_stamp`: random number between 0 and 1\n    * `targets`: target variable as defined by the SQL block below:\n\n    ```sql\n    SELECT aggregation1( feature_1_1 )\n    FROM POPULATION t1\n    LEFT JOIN (\n        SELECT aggregation2( t4.column_01 ) AS feature_1_1\n        FROM PERIPHERAL t3\n        LEFT JOIN PERIPHERAL2 t4\n        ON t3.join_key2 = t4.join_key2\n        WHERE (\n           ( t3.time_stamp - t4.time_stamp &lt;= 0.5 )\n        ) AND t4.time_stamp &lt;= t3.time_stamp\n        GROUP BY t3.join_key,\n             t3.time_stamp\n    ) t2\n    ON t1.join_key = t2.join_key\n    WHERE t2.time_stamp &lt;= t1.time_stamp\n    GROUP BY t1.join_key,\n         t1.time_stamp;\n    ```\n\n    Args:\n        n_rows_population:\n            Number of rows in the population table.\n\n        n_rows_peripheral1:\n            Number of rows in the first peripheral table.\n\n        n_rows_peripheral2:\n            Number of rows in the second peripheral table.\n\n        random_state:\n            Seed to initialize the random number generator used for\n            the dataset creation. If set to None, the seed will be the\n            'microsecond' component of\n            `datetime.datetime.now()`.\n\n        population_name:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the population\n            table. If set to a name already existing on the getML\n            Engine, the corresponding [`DataFrame`][getml.DataFrame]\n            will be overwritten. If set to an empty string, a unique\n            name will be generated by concatenating\n            `snowflake_population_` and the seed of the random\n            number generator.\n\n        peripheral_name1:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the first\n            peripheral table. If set to a name already existing on the\n            getML Engine, the corresponding\n            [`DataFrame`][getml.DataFrame] will be overwritten. If\n            set to an empty string, a unique name will be generated by\n            concatenating `snowflake_peripheral_1_` and the seed of the\n            random number generator.\n\n        peripheral_name2:\n            Name assigned to the\n            [`DataFrame`][getml.DataFrame] holding the second\n            peripheral table. If set to a name already existing on the\n            getML Engine, the corresponding\n            [`DataFrame`][getml.DataFrame] will be overwritten. If\n            set to an empty string, a unique name will be generated by\n            concatenating `snowflake_peripheral_2_` and the seed of the\n            random number generator.\n\n        aggregation1:\n            [aggregations][getml.feature_learning.aggregations] used to generate the 'target'\n            column in the first peripheral table.\n\n        aggregation2:\n            [aggregations][getml.feature_learning.aggregations] used to generate the 'target'\n            column in the second peripheral table.\n\n    Returns:\n        The dataframes are:\n\n            * population ([`DataFrame`][getml.DataFrame]): Population table\n            * peripheral ([`DataFrame`][getml.DataFrame]): Peripheral table\n            * peripheral_2 ([`DataFrame`][getml.DataFrame]): Peripheral table\n    \"\"\"\n\n    if random_state is None:\n        random_state = datetime.datetime.now().microsecond\n\n    random = np.random.RandomState(random_state)  # pylint: disable=E1101\n\n    population_table = pd.DataFrame()\n    population_table[\"column_01\"] = random.rand(n_rows_population) * 2.0 - 1.0\n    population_table[\"join_key\"] = range(n_rows_population)\n    population_table[\"time_stamp_population\"] = random.rand(n_rows_population)\n\n    peripheral_table = pd.DataFrame()\n    peripheral_table[\"column_01\"] = random.rand(n_rows_peripheral1) * 2.0 - 1.0\n    peripheral_table[\"join_key\"] = [\n        int(float(n_rows_population) * random.rand(1)[0])\n        for i in range(n_rows_peripheral1)\n    ]\n    peripheral_table[\"join_key2\"] = range(n_rows_peripheral1)\n    peripheral_table[\"time_stamp_peripheral\"] = random.rand(n_rows_peripheral1)\n\n    peripheral_table2 = pd.DataFrame()\n    peripheral_table2[\"column_01\"] = random.rand(n_rows_peripheral2) * 2.0 - 1.0\n    peripheral_table2[\"join_key2\"] = [\n        int(float(n_rows_peripheral1) * random.rand(1)[0])\n        for i in range(n_rows_peripheral2)\n    ]\n    peripheral_table2[\"time_stamp_peripheral2\"] = random.rand(n_rows_peripheral2)\n\n    # ----------------\n    # Merge peripheral_table with peripheral_table2\n\n    temp = peripheral_table2.merge(\n        peripheral_table[[\"join_key2\", \"time_stamp_peripheral\"]],\n        how=\"left\",\n        on=\"join_key2\",\n    )\n\n    # Apply some conditions\n    temp = temp[\n        (temp[\"time_stamp_peripheral2\"] &lt;= temp[\"time_stamp_peripheral\"])\n        &amp; (temp[\"time_stamp_peripheral2\"] &gt;= temp[\"time_stamp_peripheral\"] - 0.5)\n    ]\n\n    # Define the aggregation\n    temp = _aggregate(temp, aggregation2, \"column_01\", \"join_key2\")\n\n    temp = temp.rename(index=str, columns={\"column_01\": \"temporary\"})\n\n    peripheral_table = peripheral_table.merge(temp, how=\"left\", on=\"join_key2\")\n\n    del temp\n\n    # Replace NaN with 0.0\n    peripheral_table[\"temporary\"] = [\n        0.0 if val != val else val for val in peripheral_table[\"temporary\"]\n    ]\n\n    # ----------------\n    # Merge population_table with peripheral_table\n\n    temp2 = peripheral_table.merge(\n        population_table[[\"join_key\", \"time_stamp_population\"]],\n        how=\"left\",\n        on=\"join_key\",\n    )\n\n    # Apply some conditions\n    temp2 = temp2[(temp2[\"time_stamp_peripheral\"] &lt;= temp2[\"time_stamp_population\"])]\n\n    # Define the aggregation\n    temp2 = _aggregate(temp2, aggregation1, \"temporary\", \"join_key\")\n\n    temp2 = temp2.rename(index=str, columns={\"temporary\": \"targets\"})\n\n    population_table = population_table.merge(temp2, how=\"left\", on=\"join_key\")\n\n    del temp2\n\n    # Replace NaN targets with 0.0 - target values may never be NaN!.\n    population_table[\"targets\"] = [\n        0.0 if val != val else val for val in population_table[\"targets\"]\n    ]\n\n    # Remove temporary column.\n    del peripheral_table[\"temporary\"]\n\n    # ----------------\n\n    population_table = population_table.rename(\n        index=str, columns={\"time_stamp_population\": \"time_stamp\"}\n    )\n\n    peripheral_table = peripheral_table.rename(\n        index=str, columns={\"time_stamp_peripheral\": \"time_stamp\"}\n    )\n\n    peripheral_table2 = peripheral_table2.rename(\n        index=str, columns={\"time_stamp_peripheral2\": \"time_stamp\"}\n    )\n\n    # ----------------\n\n    # Set default names if none where provided.\n    if not population_name:\n        population_name = \"snowflake_population_\" + str(random_state)\n    if not peripheral_name1:\n        peripheral_name1 = \"snowflake_peripheral_1_\" + str(random_state)\n    if not peripheral_name2:\n        peripheral_name2 = \"snowflake_peripheral_2_\" + str(random_state)\n\n    # Create the data.DataFrame counterpart.\n    population_on_engine = data.DataFrame(\n        name=population_name,\n        roles={\n            \"join_key\": [\"join_key\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n            \"target\": [\"targets\"],\n        },\n    ).read_pandas(population_table)\n\n    peripheral_on_engine = data.DataFrame(\n        name=peripheral_name1,\n        roles={\n            \"join_key\": [\"join_key\", \"join_key2\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n        },\n    ).read_pandas(peripheral_table)\n\n    peripheral_on_engine2 = data.DataFrame(\n        name=peripheral_name2,\n        roles={\n            \"join_key\": [\"join_key2\"],\n            \"numerical\": [\"column_01\"],\n            \"time_stamp\": [\"time_stamp\"],\n        },\n    ).read_pandas(peripheral_table2)\n\n    # ----------------\n\n    return population_on_engine, peripheral_on_engine, peripheral_on_engine2\n</code></pre>"},{"location":"reference/datasets/datasets/#getml.datasets.base.DataFrameT","title":"DataFrameT  <code>module-attribute</code>","text":"<pre><code>DataFrameT = Union[DataFrame, DataFrame]\n</code></pre> <p>DataFrame types for builtin demonstration datasets</p>"},{"location":"reference/engine/engine/","title":"Engine","text":""},{"location":"reference/engine/engine/#getml.engine","title":"getml.engine","text":"<p>This module is a collection of utility functions for the overall communication and the session management of the getML Engine.</p> <p>In order to log into the Engine, you have to open your favorite internet browser and enter http://localhost:1709 in the navigation bar. This tells it to connect to a local TCP socket at port 1709 opened by the getML Monitor. This will only be possible from within the same device!</p> Example <p>First of all, you need to start the getML Engine. Next, you need to create a new project or load an existing one.</p> <p><pre><code>getml.engine.list_projects()\ngetml.engine.set_project('test')\n</code></pre> After doing all calculations for today you can shut down the getML Engine.</p> <pre><code>print(getml.engine.is_alive())\ngetml.engine.shutdown()\n</code></pre> Note <p>The Python process and the getML Engine must be located on the same machine. If you intend to run the Engine on a remote host, make sure to start your Python session on that device as well. Also, when using SSH sessions, make sure to start Python using <code>python &amp;</code> followed by <code>disown</code> or using <code>nohup python</code>. This ensures the Python process and all the script it has to run won't be killed the moment your remote connection becomes unstable, and you are able to recover them later on (see <code>remote_access</code>).</p> <p>All data frame objects and models in the getML Engine are bundled in projects. When loading an existing project, the current memory of the Engine will be flushed and all changes applied to <code>DataFrame</code> instances after calling their <code>save</code> method will be lost. Afterwards, all <code>Pipeline</code> will be loaded into memory automatically. The data frame objects will not be loaded automatically since they consume significantly more memory than the pipelines. They can be loaded manually using <code>load_data_frame</code> or <code>load</code>.</p> <p>The getML Engine reflects the separation of data into individual projects on the level of the filesystem too. All data belonging to a single project is stored in a dedicated folder in the 'projects' directory located in '.getML' in your home folder. These projects can be copied and shared between different platforms and architectures without any loss of information. However, you must copy the entire project and not just individual data frames or pipelines.</p>"},{"location":"reference/engine/engine/#getml.engine.delete_project","title":"delete_project","text":"<pre><code>delete_project(name: str)\n</code></pre> <p>Deletes a project.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of your project.</p> <p> TYPE: <code>str</code> </p> Note <p>All data and models contained in the project directory will be permanently lost.</p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def delete_project(name: str):\n    \"\"\"Deletes a project.\n\n    Args:\n        name:\n            Name of your project.\n\n    Note:\n        All data and models contained in the project directory will be\n        permanently lost.\n\n    \"\"\"\n    _delete_project(name)\n</code></pre>"},{"location":"reference/engine/engine/#getml.engine.is_alive","title":"is_alive","text":"<pre><code>is_alive() -&gt; bool\n</code></pre> <p>Checks if the getML Engine is running.</p> RETURNS DESCRIPTION <code>bool</code> <p>True if the getML Engine is running and ready to accept commands and False otherwise.</p> Source code in <code>getml/communication.py</code> <pre><code>def is_engine_alive() -&gt; bool:\n    \"\"\"Checks if the getML Engine is running.\n\n    Returns:\n            True if the getML Engine is running and ready to accept\n                commands and False otherwise.\n\n    \"\"\"\n\n    if not _list_projects_impl(running_only=True):\n        return False\n\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        sock.connect((\"localhost\", port))\n    except ConnectionRefusedError:\n        return False\n    finally:\n        sock.close()\n\n    return True\n</code></pre>"},{"location":"reference/engine/engine/#getml.engine.is_engine_alive","title":"is_engine_alive","text":"<pre><code>is_engine_alive() -&gt; bool\n</code></pre> <p>Checks if the getML Engine is running.</p> RETURNS DESCRIPTION <code>bool</code> <p>True if the getML Engine is running and ready to accept commands and False otherwise.</p> Source code in <code>getml/communication.py</code> <pre><code>def is_engine_alive() -&gt; bool:\n    \"\"\"Checks if the getML Engine is running.\n\n    Returns:\n            True if the getML Engine is running and ready to accept\n                commands and False otherwise.\n\n    \"\"\"\n\n    if not _list_projects_impl(running_only=True):\n        return False\n\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        sock.connect((\"localhost\", port))\n    except ConnectionRefusedError:\n        return False\n    finally:\n        sock.close()\n\n    return True\n</code></pre>"},{"location":"reference/engine/engine/#getml.engine.launch","title":"launch","text":"<pre><code>launch(\n    allow_push_notifications: bool = True,\n    allow_remote_ips: bool = False,\n    home_directory: Optional[str] = None,\n    http_port: Optional[int] = None,\n    in_memory: bool = True,\n    install: bool = False,\n    launch_browser: bool = True,\n    log: bool = False,\n    project_directory: Optional[str] = None,\n    proxy_url: Optional[str] = None,\n    token: Optional[str] = None,\n)\n</code></pre> <p>Launches the getML Engine.</p> PARAMETER DESCRIPTION <code>allow_push_notifications</code> <p>Whether you want to allow the getML Monitor to send push notifications to your desktop.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>allow_remote_ips</code> <p>Whether you want to allow remote IPs to access the http-port.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>home_directory</code> <p>The directory which should be treated as the home directory by getML. getML will create a hidden folder named '.getML' in said directory. All binaries will be installed there.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>http_port</code> <p>The local port of the getML Monitor. This port can only be accessed from your local computer, unless you set <code>allow_remote_ips=True</code>.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>in_memory</code> <p>Whether you want the Engine to process everything in memory.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>install</code> <p>Reinstalls getML, even if it is already installed.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>launch_browser</code> <p>Whether you want to automatically launch your browser.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>log</code> <p>Whether you want the Engine log to appear in the logfile (more detailed logging). The Engine log also appears in the 'Log' page of the Monitor.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>project_directory</code> <p>The directory in which to store all of your projects.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>proxy_url</code> <p>The URL of any proxy server that that redirects to the getML Monitor.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>token</code> <p>The token used for authentication. Authentication is required when remote IPs are allowed to access the Monitor. If authentication is required and no token is passed, a random hexcode will be generated as the token.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/engine/_launch.py</code> <pre><code>def launch(\n    allow_push_notifications: bool = True,\n    allow_remote_ips: bool = False,\n    home_directory: Optional[str] = None,\n    http_port: Optional[int] = None,\n    in_memory: bool = True,\n    install: bool = False,\n    launch_browser: bool = True,\n    log: bool = False,\n    project_directory: Optional[str] = None,\n    proxy_url: Optional[str] = None,\n    token: Optional[str] = None,\n):\n    \"\"\"\n    Launches the getML Engine.\n\n    Args:\n      allow_push_notifications:\n        Whether you want to allow the getML Monitor to send push notifications to your desktop.\n\n      allow_remote_ips:\n        Whether you want to allow remote IPs to access the http-port.\n\n      home_directory:\n        The directory which should be treated as the home directory by getML.\n        getML will create a hidden folder named '.getML' in said directory.\n        All binaries will be installed there.\n\n      http_port:\n        The local port of the getML Monitor.\n        This port can only be accessed from your local computer,\n        unless you set `allow_remote_ips=True`.\n\n      in_memory:\n        Whether you want the Engine to process everything in memory.\n\n      install:\n        Reinstalls getML, even if it is already installed.\n\n      launch_browser:\n        Whether you want to automatically launch your browser.\n\n      log:\n        Whether you want the Engine log to appear in the logfile (more detailed logging).\n        The Engine log also appears in the 'Log' page of the Monitor.\n\n      project_directory:\n        The directory in which to store all of your projects.\n\n      proxy_url:\n        The URL of any proxy server that that redirects to the getML Monitor.\n\n      token:\n        The token used for authentication.\n        Authentication is required when remote IPs are allowed to access the Monitor.\n        If authentication is required and no token is passed,\n        a random hexcode will be generated as the token.\"\"\"\n\n    if _is_monitor_alive():\n        print(\"getML Engine is already running.\")\n        return\n    if platform.system() != System.LINUX:\n        raise OSError(\n            PLATFORM_NOT_SUPPORTED_NATIVELY_ERROR_MSG_TEMPLATE.format(\n                platform=platform.system(),\n                docker_docs_url=DOCKER_DOCS_URL,\n                compose_file_url=COMPOSE_FILE_URL,\n            )\n        )\n    home_path = _make_home_path(home_directory)\n    binary_path = _find_binary(home_path)\n    log_path = _make_log_path(home_path)\n    logfile = open(str(log_path), \"w\", encoding=\"utf-8\")\n    cmd = _Options(\n        allow_push_notifications=allow_push_notifications,\n        allow_remote_ips=allow_remote_ips,\n        home_directory=str(home_path),\n        http_port=http_port,\n        in_memory=in_memory,\n        install=install,\n        launch_browser=launch_browser,\n        log=log,\n        project_directory=project_directory,\n        proxy_url=proxy_url,\n        token=token,\n    ).to_cmd(binary_path)\n    cwd = str(binary_path.parent)\n    print(f\"Launching {' '.join(cmd)} in {cwd}...\")\n    Popen(cmd, cwd=cwd, shell=False, stdout=logfile, stdin=logfile, stderr=logfile)\n    while not _is_monitor_alive():\n        sleep(0.1)\n    print(f\"Launched the getML Engine. The log output will be stored in {log_path}.\")\n</code></pre>"},{"location":"reference/engine/engine/#getml.engine.list_projects","title":"list_projects","text":"<pre><code>list_projects() -&gt; List[str]\n</code></pre> <p>List all projects on the getML Engine.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>Lists the name of all the projects.</p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def list_projects() -&gt; List[str]:\n    \"\"\"\n    List all projects on the getML Engine.\n\n    Returns:\n            Lists the name of all the projects.\n    \"\"\"\n    return _list_projects_impl(running_only=False)\n</code></pre>"},{"location":"reference/engine/engine/#getml.engine.list_running_projects","title":"list_running_projects","text":"<pre><code>list_running_projects() -&gt; List[str]\n</code></pre> <p>List all projects on the getML Engine that are currently running.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>Lists the name of all the projects currently running.</p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def list_running_projects() -&gt; List[str]:\n    \"\"\"\n    List all projects on the getML Engine that are currently running.\n\n    Returns:\n        Lists the name of all the projects currently running.\n    \"\"\"\n    return _list_projects_impl(running_only=True)\n</code></pre>"},{"location":"reference/engine/engine/#getml.engine.set_project","title":"set_project","text":"<pre><code>set_project(name: str)\n</code></pre> <p>Creates a new project or loads an existing one.</p> <p>If there is no project called <code>name</code> present on the Engine, a new one will be created.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the new project.</p> <p> TYPE: <code>str</code> </p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def set_project(name: str):\n    \"\"\"Creates a new project or loads an existing one.\n\n    If there is no project called `name` present on the Engine, a new one will\n    be created.\n\n    Args:\n           name: Name of the new project.\n    \"\"\"\n    _set_project(name)\n</code></pre>"},{"location":"reference/engine/engine/#getml.engine.shutdown","title":"shutdown","text":"<pre><code>shutdown()\n</code></pre> <p>Shuts down the getML Engine.</p> Warning <p>All changes applied to the <code>DataFrame</code> after calling their <code>save</code> method will be lost.</p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def shutdown():\n    \"\"\"Shuts down the getML Engine.\n\n    Warning:\n        All changes applied to the [`DataFrame`][getml.DataFrame]\n        after calling their [`save`][getml.DataFrame.save]\n        method will be lost.\n\n    \"\"\"\n    _shutdown()\n</code></pre>"},{"location":"reference/engine/engine/#getml.engine.suspend_project","title":"suspend_project","text":"<pre><code>suspend_project(name: str)\n</code></pre> <p>Suspends a project that is currently running.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of your project.</p> <p> TYPE: <code>str</code> </p> Source code in <code>getml/engine/helpers.py</code> <pre><code>def suspend_project(name: str):\n    \"\"\"Suspends a project that is currently running.\n\n    Args:\n        name:\n            Name of your project.\n    \"\"\"\n    _suspend_project(name)\n</code></pre>"},{"location":"reference/feature_learning/","title":"Feature Learning","text":"<p>Feature Learners are getML's powerhouse leveraging  the high performance of C++ to ensure efficient execution and effective memory use.</p> <p>There are five different algorithms, each capitalizing on unique strengths and  suitable for different use cases. Most of them are available in the  getML Enterprise edition.</p>"},{"location":"reference/feature_learning/#getml.feature_learning","title":"getml.feature_learning","text":""},{"location":"reference/feature_learning/#fastprop","title":"<code>FastProp</code>","text":"<ul> <li>FastProp utilizes aggregation-based  operations, enabling rapid generation of numerous features through simple aggregations. This  makes FastProp ideal for the exploration phase of a data science project, delivering  quick, decent results. </li> <li>FastProp is available in both, the getML Community edition and getML enterprise    edition.</li> </ul>"},{"location":"reference/feature_learning/#multirel","title":"<code>Multirel</code>","text":"<ul> <li>Multirel focuses on minimizing algorithmic redundancies through incremental  updates and combining  these improvements with ensemble learning methods. </li> <li>Recalculations are only performed where changes occur,  significantly increasing efficiency while  integrating methods such as bagging and gradient boosting.</li> <li>This feature learner is available in the getML Enterprise edition.</li> </ul>"},{"location":"reference/feature_learning/#relboost","title":"<code>Relboost</code>","text":"<ul> <li>Relboost extends the gradient boosting approach, to relational learning by focusing on aggregating learnable weights rather than columns, addressing computational complexity and exponentially growing feature  space. </li> <li>While Relboost often surpasses Multirel  in predictive accuracy and training efficiency, its generated features are less  intuitive.</li> <li>This feature learner is available in the getML Enterprise edition.</li> </ul>"},{"location":"reference/feature_learning/#fastboost","title":"<code>Fastboost</code>","text":"<ul> <li> <p>Fastboost uses a simpler, faster, and more scalable algorithm than Relboost, making  it ideal for large datasets and many cross-joins. Fastboost can outperform FastProp  in speed for datasets with many columns.</p> </li> <li> <p>Fastboost requires free disk space due to extensive memory mapping and has  difficulty applying to multiple targets as it must learn separate rules for each.</p> </li> <li> <p>This feature learner is available in the getML Enterprise edition.</p> </li> </ul>"},{"location":"reference/feature_learning/#relmt","title":"<code>RelMT</code>","text":"<ul> <li>RelMT adapts linear model trees to relational data, combining linear models at each  tree leaf to effectively capture both linear and non-linear relationships, making it  particularly advantageous for modeling time-series data. </li> <li>This feature learner is available in the getML Enterprise edition.</li> </ul>"},{"location":"reference/feature_learning/fastboost/","title":"Fastboost","text":""},{"location":"reference/feature_learning/fastboost/#getml.feature_learning.Fastboost","title":"getml.feature_learning.Fastboost  <code>dataclass</code>","text":"<pre><code>Fastboost(\n    gamma: float = 0.0,\n    loss_function: Optional[\n        Union[CrossEntropyLossType, SquareLossType]\n    ] = None,\n    max_depth: int = 5,\n    min_child_weights: float = 1.0,\n    num_features: int = 100,\n    num_threads: int = 1,\n    reg_lambda: float = 1.0,\n    seed: int = 5543,\n    shrinkage: float = 0.1,\n    silent: bool = True,\n    subsample: float = 1.0,\n)\n</code></pre> <p>               Bases: <code>_FeatureLearner</code></p> <p>Feature learning based on Gradient Boosting.</p> <p><code>Fastboost</code> automates feature learning for relational data and time series. The algorithm used is slightly simpler than <code>Relboost</code> and much faster.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> PARAMETER DESCRIPTION <code>gamma</code> <p>During the training of Fastboost, which is based on gradient tree boosting, this value serves as the minimum improvement in terms of the <code>loss_function</code> required for a split of the tree to be applied. Larger <code>gamma</code> will lead to fewer partitions of the tree and a more conservative algorithm. Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>loss_function</code> <p>Objective function used by the feature learning algorithm to optimize your features. For regression problems use <code>SquareLoss</code> and for classification problems use <code>CrossEntropyLoss</code>.</p> <p> TYPE: <code>Optional[Union[CrossEntropyLossType, SquareLossType]]</code> DEFAULT: <code>None</code> </p> <code>max_depth</code> <p>Maximum depth of the trees generated during the gradient tree boosting. Deeper trees will result in more complex models and increase the risk of overfitting. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>min_child_weights</code> <p>Determines the minimum sum of the weights a subcondition should apply to in order for it to be considered. Higher values lead to less complex statements and less danger of overfitting. Range: [1, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>num_features</code> <p>Number of features generated by the feature learning algorithm. Range: [1, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>num_threads</code> <p>Number of threads used by the feature learning algorithm. If set to zero or a negative value, the number of threads will be determined automatically by the getML Engine. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>reg_lambda</code> <p>L2 regularization on the weights in the gradient boosting routine. This is one of the most important hyperparameters in the <code>Relboost</code> as it allows for the most direct regularization. Larger values will make the resulting model more conservative. Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>seed</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5543</code> </p> <code>shrinkage</code> <p>Since Fastboost works using a gradient-boosting-like algorithm, <code>shrinkage</code> (or learning rate) scales down the weights and thus the impact of each new tree. This gives more room for future ones to improve the overall performance of the model in this greedy algorithm. It must be between 0.0 and 1.0 with higher values leading to a higher danger of overfitting. Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>silent</code> <p>Controls the logging during training.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>subsample</code> <p>Fastboost uses a bootstrapping procedure (sampling with replacement) to train each of the features. The sampling factor is proportional to the share of the samples randomly drawn from the population table every time Fastboost generates a new feature. A lower sampling factor (but still greater than 0.0), will lead to less danger of overfitting, less complex statements and faster training. When set to 1.0, the number of samples drawn will be identical to the size of the population table. When set to 0.0, there will be no sampling at all. Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p>"},{"location":"reference/feature_learning/fastprop/","title":"FastProp","text":""},{"location":"reference/feature_learning/fastprop/#getml.feature_learning.FastProp","title":"getml.feature_learning.FastProp  <code>dataclass</code>","text":"<pre><code>FastProp(\n    aggregation: Iterable[\n        FastPropAggregations\n    ] = FASTPROP.default,\n    delta_t: float = 0.0,\n    loss_function: Optional[\n        Union[CrossEntropyLossType, SquareLossType]\n    ] = None,\n    max_lag: int = 0,\n    min_df: int = 30,\n    n_most_frequent: int = 0,\n    num_features: int = 200,\n    num_threads: int = 0,\n    sampling_factor: float = 1.0,\n    silent: bool = True,\n    vocab_size: int = 500,\n)\n</code></pre> <p>               Bases: <code>_FeatureLearner</code></p> <p>Generates simple features based on propositionalization.</p> <p><code>FastProp</code> generates simple and easily interpretable features for relational data and time series. It is based on a propositionalization approach and has been optimized for speed and memory efficiency. <code>FastProp</code> generates a large number of features and selects the most relevant ones based on the pair-wise correlation with the target(s).</p> <p>It is recommended to combine <code>FastProp</code> with the <code>Mapping</code> and <code>Seasonal</code> preprocessors, which can drastically improve predictive accuracy.</p> <p>For more information on the underlying feature learning algorithm, check out the User guide: FastProp.</p> ATTRIBUTE DESCRIPTION <code>agg_sets</code> <p>It is a class variable holding the available aggregation sets for the FastProp feature learner. Value: <code>FASTPROP</code>.</p> <p> TYPE: <code>FastPropAggregationsSets</code> </p> PARAMETER DESCRIPTION <code>aggregation</code> <p>Mathematical operations used by the automated feature learning algorithm to create new features.</p> <p>Must be an aggregation supported by FastProp feature learner (<code>FASTPROP_AGGREGATIONS</code>).</p> <p> TYPE: <code>Iterable[FastPropAggregations]</code> DEFAULT: <code>default</code> </p> <code>delta_t</code> <p>Frequency with which lag variables will be explored in a time series setting. When set to 0.0, there will be no lag variables. Please note that you must also pass a value to max_lag.</p> <p>For more information please refer to Data Model Time Series. Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>loss_function</code> <p>Objective function used by the feature learning algorithm to optimize your features. For regression problems use <code>SquareLoss</code> and for classification problems use <code>CrossEntropyLoss</code>.</p> <p> TYPE: <code>Optional[Union[CrossEntropyLossType, SquareLossType]]</code> DEFAULT: <code>None</code> </p> <code>max_lag</code> <p>Maximum number of steps taken into the past to form lag variables. The step size is determined by delta_t. Please note that you must also pass a value to delta_t.</p> <p>For more information please refer to Time Series. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>min_df</code> <p>Only relevant for columns with role <code>text</code>. The minimum number of fields (i.e. rows) in <code>text</code> column a given word is required to appear in to be included in the bag of words. Range: [1, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> <code>num_features</code> <p>Number of features generated by the feature learning algorithm. Range: [1, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>n_most_frequent</code> <p><code>FastProp</code> can find the N most frequent categories in a categorical column and derive features from them. The parameter determines how many categories should be used. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>num_threads</code> <p>Number of threads used by the feature learning algorithm. If set to zero or a negative value, the number of threads will be determined automatically by the getML Engine. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>sampling_factor</code> <p>FastProp uses a bootstrapping procedure (sampling with replacement) to train each of the features. The sampling factor is proportional to the share of the samples randomly drawn from the population table every time Multirel generates a new feature. A lower sampling factor (but still greater than 0.0), will lead to less danger of overfitting, less complex statements and faster training. When set to 1.0, roughly 2,000 samples are drawn from the population table. If the population table contains less than 2,000 samples, it will use standard bagging. When set to 0.0, there will be no sampling at all. Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>silent</code> <p>Controls the logging during training.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>vocab_size</code> <p>Determines the maximum number of words that are extracted in total from <code>text</code> columns. This can be interpreted as the maximum size of the bag of words. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p>"},{"location":"reference/feature_learning/fastprop/#getml.feature_learning.FastProp.validate","title":"validate","text":"<pre><code>validate(params: Optional[Dict[str, Any]] = None) -&gt; None\n</code></pre> <p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> PARAMETER DESCRIPTION <code>params</code> <p>A dictionary containing the parameters to validate. params can hold the full set or a subset of the parameters explained in <code>FastProp</code>. If params is None, the current set of parameters in the instance dictionary will be validated.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/feature_learning/fastprop.py</code> <pre><code>def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params:\n            A dictionary containing the parameters to validate.\n            params can hold the full set or a subset of the\n            parameters explained in\n            [`FastProp`][getml.feature_learning.FastProp].\n            If params is None, the\n            current set of parameters in the\n            instance dictionary will be validated.\n\n\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    for kkey in params:\n        if kkey not in type(self)._supported_params:\n            raise KeyError(\n                f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n            )\n\n    _validate_dfs_model_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/loss_functions/","title":"loss_functions","text":""},{"location":"reference/feature_learning/loss_functions/#getml.feature_learning.loss_functions","title":"getml.feature_learning.loss_functions","text":"<p>Loss functions used by the feature learning algorithms.</p> <p>The getML Python API contains two different loss functions. We recommend using <code>SQUARELOSS</code> for regression problems and <code>CROSSENTROPYLOSS</code> for classification problems.</p> <p>Please note that these loss functions will only be used by the feature learning algorithms and not by the <code>predictors</code>.</p>"},{"location":"reference/feature_learning/loss_functions/#getml.feature_learning.loss_functions.CrossEntropyLossType","title":"CrossEntropyLossType  <code>module-attribute</code>","text":"<pre><code>CrossEntropyLossType = Literal['CrossEntropyLoss']\n</code></pre> <p>Type of the cross entropy loss function.</p>"},{"location":"reference/feature_learning/loss_functions/#getml.feature_learning.loss_functions.CROSSENTROPYLOSS","title":"CROSSENTROPYLOSS  <code>module-attribute</code>","text":"<pre><code>CROSSENTROPYLOSS: Final[CrossEntropyLossType] = (\n    \"CrossEntropyLoss\"\n)\n</code></pre> <p>The cross entropy between two probability distributions \\(p(x)\\) and \\(q(x)\\) is a combination of the information contained in \\(p(x)\\) and the additional information stored in \\(q(x)\\) with respect to \\(p(x)\\). In technical terms: it is the entropy of \\(p(x)\\) plus the Kullback-Leibler divergence - a distance in probability space - from \\(q(x)\\) to \\(p(x)\\).</p> \\[ H(p,q) = H(p) + D_{KL}(p||q) \\] <p>For discrete probability distributions the cross entropy loss can be calculated by</p> \\[ H(p,q) = - \\sum_{x \\in X} p(x) \\log q(x) \\] <p>and for continuous probability distributions by</p> \\[ H(p,q) = - \\int_{X} p(x) \\log q(x) dx \\] <p>with \\(X\\) being the support of the samples and \\(p(x)\\) and \\(q(x)\\) being two discrete or continuous probability distributions over \\(X\\).</p> Note <p>Recommended loss function for classification problems.</p>"},{"location":"reference/feature_learning/loss_functions/#getml.feature_learning.loss_functions.SquareLossType","title":"SquareLossType  <code>module-attribute</code>","text":"<pre><code>SquareLossType = Literal['SquareLoss']\n</code></pre> <p>Type of the square loss function.</p>"},{"location":"reference/feature_learning/loss_functions/#getml.feature_learning.loss_functions.SQUARELOSS","title":"SQUARELOSS  <code>module-attribute</code>","text":"<pre><code>SQUARELOSS: Final[SquareLossType] = 'SquareLoss'\n</code></pre> <p>The Square loss (aka mean squared error (MSE)) measures the loss by calculating the average of all squared deviations of the predictions \\(\\hat{y}\\) from the observed (given) outcomes \\(y\\). Depending on the context this measure is also known as mean squared error (MSE) or mean squared deviation (MSD).</p> \\[ L(y,\\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i -\\hat{y}_i)^2  \\] <p>with \\(n\\) being the number of samples, \\(y\\) the observed outcome, and \\(\\hat{y}\\) the estimate.</p> Note <p>Recommended loss function for regression problems.</p>"},{"location":"reference/feature_learning/multirel/","title":"Multirel","text":""},{"location":"reference/feature_learning/multirel/#getml.feature_learning.Multirel","title":"getml.feature_learning.Multirel  <code>dataclass</code>","text":"<pre><code>Multirel(\n    aggregation: Iterable[\n        MultirelAggregations\n    ] = MULTIREL.default,\n    allow_sets: bool = True,\n    delta_t: float = 0.0,\n    grid_factor: float = 1.0,\n    loss_function: Optional[\n        Union[CrossEntropyLossType, SquareLossType]\n    ] = None,\n    max_length: int = 4,\n    min_df: int = 30,\n    min_num_samples: int = 1,\n    num_features: int = 100,\n    num_subfeatures: int = 5,\n    num_threads: int = 0,\n    propositionalization: FastProp = FastProp(),\n    regularization: float = 0.01,\n    round_robin: bool = False,\n    sampling_factor: float = 1.0,\n    seed: int = 5543,\n    share_aggregations: float = 0.0,\n    share_conditions: float = 1.0,\n    shrinkage: float = 0.0,\n    silent: bool = True,\n    vocab_size: int = 500,\n)\n</code></pre> <p>               Bases: <code>_FeatureLearner</code></p> <p>Feature learning based on Multi-Relational Decision Tree Learning.</p> <p><code>Multirel</code> automates feature learning for relational data and time series. It is based on an efficient variation of the Multi-Relational Decision Tree Learning (MRDTL).</p> <p>For more information on the underlying feature learning algorithm, check out the User guide: Multirel.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> ATTRIBUTE DESCRIPTION <code>agg_sets</code> <p>It is a class variable holding the available aggregation sets for the Multirel feature learner. Value: <code>MULTIREL</code>.</p> <p> TYPE: <code>MultirelAggregationsSets</code> </p> PARAMETER DESCRIPTION <code>aggregation</code> <p>Mathematical operations used by the automated feature learning algorithm to create new features.</p> <p>Must be an aggregation supported by Multirel feature learner (<code>MULTIREL_AGGREGATIONS</code>).</p> <p> TYPE: <code>Iterable[MultirelAggregations]</code> DEFAULT: <code>default</code> </p> <code>allow_sets</code> <p>Multirel can summarize different categories into sets for producing conditions. When expressed as SQL statements these sets might look like this: <pre><code>t2.category IN ( 'value_1', 'value_2', ... )\n</code></pre> This can be very powerful, but it can also produce features that are hard to read and might be prone to overfitting when the <code>sampling_factor</code> is too low.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>delta_t</code> <p>Frequency with which lag variables will be explored in a time series setting. When set to 0.0, there will be no lag variables.</p> <p>For more information please refer to Time Series in the User Guide. Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>grid_factor</code> <p>Multirel will try a grid of critical values for your numerical features. A higher <code>grid_factor</code> will lead to a larger number of critical values being considered. This can increase the training time, but also lead to more accurate features. Range: (0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>loss_function</code> <p>Objective function used by the feature learning algorithm to optimize your features. For regression problems use <code>SquareLoss</code> and for classification problems use <code>CrossEntropyLoss</code>.</p> <p> TYPE: <code>Optional[Union[CrossEntropyLossType, SquareLossType]]</code> DEFAULT: <code>None</code> </p> <code>max_length</code> <p>The maximum length a subcondition might have. Multirel will create conditions in the form <pre><code>(condition 1.1 AND condition 1.2 AND condition 1.3 )\nOR ( condition 2.1 AND condition 2.2 AND condition 2.3 )\n...\n</code></pre> Using this parameter you can set the maximum number of conditions allowed in the brackets. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>min_df</code> <p>Only relevant for columns with role <code>text</code>. The minimum number of fields (i.e. rows) in <code>text</code> column a given word is required to appear in to be included in the bag of words. Range: [1, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> <code>min_num_samples</code> <p>Determines the minimum number of samples a subcondition should apply to in order for it to be considered. Higher values lead to less complex statements and less danger of overfitting. Range: [1, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>num_features</code> <p>Number of features generated by the feature learning algorithm. Range: [1, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>num_subfeatures</code> <p>The number of subfeatures you would like to extract in a subensemble (for snowflake data model only). See Snowflake Schema for more information. Range: [1, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>num_threads</code> <p>Number of threads used by the feature learning algorithm. If set to zero or a negative value, the number of threads will be determined automatically by the getML Engine. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>propositionalization</code> <p>The feature learner used for joins which are flagged to be propositionalized (by setting a join's <code>relationship</code> parameter to <code>propositionalization</code>)</p> <p> TYPE: <code>FastProp</code> DEFAULT: <code>FastProp()</code> </p> <code>regularization</code> <p>Most important regularization parameter for the quality of the features produced by Multirel. Higher values will lead to less complex features and less danger of overfitting. A <code>regularization</code> of 1.0 is very strong and allows no conditions. Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>round_robin</code> <p>If True, the Multirel picks a different <code>aggregation</code> every time a new feature is generated.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>sampling_factor</code> <p>Multirel uses a bootstrapping procedure (sampling with replacement) to train each of the features. The sampling factor is proportional to the share of the samples randomly drawn from the population table every time Multirel generates a new feature. A lower sampling factor (but still greater than 0.0), will lead to less danger of overfitting, less complex statements and faster training. When set to 1.0, roughly 20,000 samples are drawn from the population table. If the population table contains less than 20,000 samples, it will use standard bagging. When set to 0.0, there will be no sampling at all. Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>seed</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5543</code> </p> <code>share_aggregations</code> <p>Every time a new feature is generated, the <code>aggregation</code> will be taken from a random subsample of possible aggregations and values to be aggregated. This parameter determines the size of that subsample. Only relevant when <code>round_robin</code> is False. Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>share_conditions</code> <p>Every time a new column is tested for applying conditions, it might be skipped at random. This parameter determines the probability that a column will not be skipped. Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>shrinkage</code> <p>Since Multirel works using a gradient-boosting-like algorithm, <code>shrinkage</code> (or learning rate) scales down the weights and thus the impact of each new tree. This gives more room for future ones to improve the overall performance of the model in this greedy algorithm. Higher values will lead to more danger of overfitting. Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>silent</code> <p>Controls the logging during training.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>vocab_size</code> <p>Determines the maximum number of words that are extracted in total from <code>text</code> columns. This can be interpreted as the maximum size of the bag of words. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p>"},{"location":"reference/feature_learning/multirel/#getml.feature_learning.Multirel.validate","title":"validate","text":"<pre><code>validate(params: Optional[Dict[str, Any]] = None) -&gt; None\n</code></pre> <p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> PARAMETER DESCRIPTION <code>params</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/feature_learning/multirel.py</code> <pre><code>def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params: A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    # ------------------------------------------------------------\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    # ------------------------------------------------------------\n\n    for kkey in params:\n        if kkey not in type(self)._supported_params:\n            raise KeyError(\n                f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n            )\n\n    # ------------------------------------------------------------\n\n    _validate_multirel_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/relboost/","title":"Relboost","text":""},{"location":"reference/feature_learning/relboost/#getml.feature_learning.Relboost","title":"getml.feature_learning.Relboost  <code>dataclass</code>","text":"<pre><code>Relboost(\n    allow_null_weights: bool = False,\n    delta_t: float = 0.0,\n    gamma: float = 0.0,\n    loss_function: Optional[\n        Union[CrossEntropyLossType, SquareLossType]\n    ] = None,\n    max_depth: int = 3,\n    min_df: int = 30,\n    min_num_samples: int = 1,\n    num_features: int = 100,\n    num_subfeatures: int = 100,\n    num_threads: int = 0,\n    propositionalization: FastProp = FastProp(),\n    reg_lambda: float = 0.0,\n    sampling_factor: float = 1.0,\n    seed: int = 5543,\n    shrinkage: float = 0.1,\n    silent: bool = True,\n    vocab_size: int = 500,\n)\n</code></pre> <p>               Bases: <code>_FeatureLearner</code></p> <p>Feature learning based on Gradient Boosting.</p> <p><code>Relboost</code> automates feature learning for relational data and time series. It is based on a generalization of the XGBoost algorithm to relational data, hence the name.</p> <p>For more information on the underlying feature learning algorithm, check out the User Guide: Relboost.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> PARAMETER DESCRIPTION <code>allow_null_weights</code> <p>Whether you want to allow <code>Relboost</code> to set weights to NULL.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>delta_t</code> <p>Frequency with which lag variables will be explored in a time series setting. When set to 0.0, there will be no lag variables.</p> <p>For more information, please refer to Time Series in the User Guide. Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>gamma</code> <p>During the training of Relboost, which is based on gradient tree boosting, this value serves as the minimum improvement in terms of the <code>loss_function</code> required for a split of the tree to be applied. Larger <code>gamma</code> will lead to fewer partitions of the tree and a more conservative algorithm. Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>loss_function</code> <p>Objective function used by the feature learning algorithm to optimize your features. For regression problems use <code>SquareLoss</code> and for classification problems use <code>CrossEntropyLoss</code>.</p> <p> TYPE: <code>Optional[Union[CrossEntropyLossType, SquareLossType]]</code> DEFAULT: <code>None</code> </p> <code>max_depth</code> <p>Maximum depth of the trees generated during the gradient tree boosting. Deeper trees will result in more complex models and increase the risk of overfitting. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>min_df</code> <p>Only relevant for columns with role <code>text</code>. The minimum number of fields (i.e. rows) in <code>text</code> column a given word is required to appear in to be included in the bag of words. Range: [1, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> <code>min_num_samples</code> <p>Determines the minimum number of samples a subcondition should apply to in order for it to be considered. Higher values lead to less complex statements and less danger of overfitting. Range: [1, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>num_features</code> <p>Number of features generated by the feature learning algorithm. Range: [1, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>num_subfeatures</code> <p>The number of subfeatures you would like to extract in a subensemble (for snowflake data model only). See Snowflake Schema for more information. Range: [1, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>num_threads</code> <p>Number of threads used by the feature learning algorithm. If set to zero or a negative value, the number of threads will be determined automatically by the getML Engine. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>propositionalization</code> <p>The feature learner used for joins which are flagged to be propositionalized (by setting a join's <code>relationship</code> parameter to <code>propositionalization</code>)</p> <p> TYPE: <code>FastProp</code> DEFAULT: <code>FastProp()</code> </p> <code>reg_lambda</code> <p>L2 regularization on the weights in the gradient boosting routine. This is one of the most important hyperparameters in the <code>Relboost</code> as it allows for the most direct regularization. Larger values will make the resulting model more conservative. Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>sampling_factor</code> <p>Relboost uses a bootstrapping procedure (sampling with replacement) to train each of the features. The sampling factor is proportional to the share of the samples randomly drawn from the population table every time Relboost generates a new feature. A lower sampling factor (but still greater than 0.0), will lead to less danger of overfitting, less complex statements and faster training. When set to 1.0, roughly 20,000 samples are drawn from the population table. If the population table contains less than 20,000 samples, it will use standard bagging. When set to 0.0, there will be no sampling at all. Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>seed</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5543</code> </p> <code>shrinkage</code> <p>Since Relboost works using a gradient-boosting-like algorithm, <code>shrinkage</code> (or learning rate) scales down the weights and thus the impact of each new tree. This gives more room for future ones to improve the overall performance of the model in this greedy algorithm. It must be between 0.0 and 1.0 with higher values leading to a higher danger of overfitting. Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>silent</code> <p>Controls the logging during training.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>vocab_size</code> <p>Determines the maximum number of words that are extracted in total from <code>text</code> columns. This can be interpreted as the maximum size of the bag of words. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p>"},{"location":"reference/feature_learning/relboost/#getml.feature_learning.Relboost.validate","title":"validate","text":"<pre><code>validate(params: Optional[Dict[str, Any]] = None) -&gt; None\n</code></pre> <p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> PARAMETER DESCRIPTION <code>params</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/feature_learning/relboost.py</code> <pre><code>def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params: A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    # ------------------------------------------------------------\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    # ------------------------------------------------------------\n\n    for kkey in params:\n        if kkey not in type(self)._supported_params:\n            raise KeyError(\n                f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n            )\n\n    # ------------------------------------------------------------\n\n    if not isinstance(params[\"silent\"], bool):\n        raise TypeError(\"'silent' must be of type bool\")\n\n    # ------------------------------------------------------------\n\n    _validate_relboost_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/relmt/","title":"RelMT","text":""},{"location":"reference/feature_learning/relmt/#getml.feature_learning.RelMT","title":"getml.feature_learning.RelMT  <code>dataclass</code>","text":"<pre><code>RelMT(\n    allow_avg: bool = True,\n    delta_t: float = 0.0,\n    gamma: float = 0.0,\n    loss_function: Optional[\n        Union[CrossEntropyLossType, SquareLossType]\n    ] = None,\n    max_depth: int = 2,\n    min_df: int = 30,\n    min_num_samples: int = 1,\n    num_features: int = 30,\n    num_subfeatures: int = 30,\n    num_threads: int = 0,\n    propositionalization: FastProp = FastProp(),\n    reg_lambda: float = 0.0,\n    sampling_factor: float = 1.0,\n    seed: int = 5543,\n    shrinkage: float = 0.1,\n    silent: bool = True,\n    vocab_size: int = 500,\n)\n</code></pre> <p>               Bases: <code>_FeatureLearner</code></p> <p>Feature learning based on relational linear model trees.</p> <p><code>RelMT</code> automates feature learning for relational data and time series. It is based on a generalization of linear model trees to relational data, hence the name. A linear model tree is a decision tree with linear models on its leaves.</p> <p>For more information on the underlying feature learning algorithm, check out the User Guide: RelMT.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> PARAMETER DESCRIPTION <code>allow_avg</code> <p>Whether to allow an AVG aggregation. Particularly for time series problems, AVG aggregations are not necessary and you can save some time by taking them out.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>delta_t</code> <p>Frequency with which lag variables will be explored in a time series setting. When set to 0.0, there will be no lag variables.</p> <p>For more information, please refer to Time Series in the User Guide. Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>gamma</code> <p>During the training of RelMT, which is based on gradient tree boosting, this value serves as the minimum improvement in terms of the <code>loss_function</code> required for a split of the tree to be applied. Larger <code>gamma</code> will lead to fewer partitions of the tree and a more conservative algorithm. Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>loss_function</code> <p>Objective function used by the feature learning algorithm to optimize your features. For regression problems use <code>SquareLoss</code> and for classification problems use <code>CrossEntropyLoss</code>.</p> <p> TYPE: <code>Optional[Union[CrossEntropyLossType, SquareLossType]]</code> DEFAULT: <code>None</code> </p> <code>max_depth</code> <p>Maximum depth of the trees generated during the gradient tree boosting. Deeper trees will result in more complex models and increase the risk of overfitting. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>min_df</code> <p>Only relevant for columns with role <code>text</code>. The minimum number of fields (i.e. rows) in <code>text</code> column a given word is required to appear in to be included in the bag of words. Range: [1, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> <code>min_num_samples</code> <p>Determines the minimum number of samples a subcondition should apply to in order for it to be considered. Higher values lead to less complex statements and less danger of overfitting. Range: [1, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>num_features</code> <p>Number of features generated by the feature learning algorithm. Range: [1, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> <code>num_subfeatures</code> <p>The number of subfeatures you would like to extract in a subensemble (for snowflake data model only). See :ref:<code>data_model_snowflake_schema</code> for more information. Range: [1, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> <code>num_threads</code> <p>Number of threads used by the feature learning algorithm. If set to zero or a negative value, the number of threads will be determined automatically by the getML Engine. Range: [-\u221e, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>propositionalization</code> <p>The feature learner used for joins which are flagged to be propositionalized (by setting a join's <code>relationship</code> parameter to <code>propositionalization</code>)</p> <p> TYPE: <code>FastProp</code> DEFAULT: <code>FastProp()</code> </p> <code>reg_lambda</code> <p>L2 regularization on the weights in the gradient boosting routine. This is one of the most important hyperparameters in the <code>RelMT</code> as it allows for the most direct regularization. Larger values will make the resulting model more conservative. Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>sampling_factor</code> <p>RelMT uses a bootstrapping procedure (sampling with replacement) to train each of the features. The sampling factor is proportional to the share of the samples randomly drawn from the population table every time RelMT generates a new feature. A lower sampling factor (but still greater than 0.0), will lead to less danger of overfitting, less complex statements and faster training. When set to 1.0, roughly 20,000 samples are drawn from the population table. If the population table contains less than 20,000 samples, it will use standard bagging. When set to 0.0, there will be no sampling at all. Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>seed</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5543</code> </p> <code>shrinkage</code> <p>Since RelMT works using a gradient-boosting-like algorithm, <code>shrinkage</code> (or learning rate) scales down the weights and thus the impact of each new tree. This gives more room for future ones to improve the overall performance of the model in this greedy algorithm. It must be between 0.0 and 1.0 with higher values leading to more danger of overfitting. Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>silent</code> <p>Controls the logging during training.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>vocab_size</code> <p>Determines the maximum number of words that are extracted in total from <code>text</code> columns. This can be interpreted as the maximum size of the bag of words. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p>"},{"location":"reference/feature_learning/relmt/#getml.feature_learning.RelMT.validate","title":"validate","text":"<pre><code>validate(params: Optional[Dict[str, Any]] = None) -&gt; None\n</code></pre> <p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> PARAMETER DESCRIPTION <code>params</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/feature_learning/relmt.py</code> <pre><code>def validate(self, params: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params: A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    # ------------------------------------------------------------\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    # ------------------------------------------------------------\n\n    for kkey in params:\n        if kkey not in type(self)._supported_params:\n            raise KeyError(\n                f\"Instance variable '{kkey}' is not supported in {self.type}.\"\n            )\n\n    # ------------------------------------------------------------\n\n    if not isinstance(params[\"silent\"], bool):\n        raise TypeError(\"'silent' must be of type bool\")\n\n    # ------------------------------------------------------------\n\n    _validate_relboost_parameters(**params)\n</code></pre>"},{"location":"reference/feature_learning/aggregations/","title":"Index","text":""},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations","title":"getml.feature_learning.aggregations","text":""},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.FASTPROP","title":"FASTPROP  <code>module-attribute</code>","text":"<pre><code>FASTPROP = FastPropAggregationsSets(\n    all=FASTPROP_AGGREGATIONS,\n    default=frozenset(\n        {\n            AVG,\n            COUNT,\n            COUNT_DISTINCT,\n            COUNT_MINUS_COUNT_DISTINCT,\n            FIRST,\n            LAST,\n            MAX,\n            MEDIAN,\n            MIN,\n            MODE,\n            STDDEV,\n            SUM,\n            TREND,\n        }\n    ),\n    minimal=frozenset([AVG, COUNT, MAX, MIN, SUM]),\n)\n</code></pre> <p>Set of default aggregations for <code>FastProp</code>.  <code>all</code> contains all aggregations supported by FastProp, <code>default</code> contains the subset  of reasonable default aggregations, <code>minimal</code> is minimal set.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.MULTIREL","title":"MULTIREL  <code>module-attribute</code>","text":"<pre><code>MULTIREL = MultirelAggregationsSets(\n    all=MULTIREL_AGGREGATIONS,\n    default=frozenset({AVG, COUNT, MAX, MIN, SUM}),\n    minimal=frozenset([AVG, COUNT, SUM]),\n)\n</code></pre> <p>Set of default aggregations for <code>Multirel</code>.  <code>all</code> contains all aggregations supported by Multirel, <code>default</code> contains the subset  of reasonable default aggregations, <code>minimal</code> is minimal set.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.MAPPING","title":"MAPPING  <code>module-attribute</code>","text":"<pre><code>MAPPING = MappingAggregationsSets(\n    all=MAPPING_AGGREGATIONS,\n    default=frozenset({AVG}),\n    minimal=frozenset({AVG}),\n)\n</code></pre> <p>Set of default aggregations for <code>Mapping</code>. <code>all</code>  contains all aggregations supported by the mapping preprocessor. <code>default</code> and  <code>minimal</code> are identical and include only the AVG aggregation, which is the  recommended setting for classification problems.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.MULTIREL_AGGREGATIONS","title":"MULTIREL_AGGREGATIONS  <code>module-attribute</code>","text":"<pre><code>MULTIREL_AGGREGATIONS: FrozenSet[MultirelAggregations] = (\n    frozenset(\n        {\n            AVG,\n            COUNT,\n            COUNT_DISTINCT,\n            COUNT_MINUS_COUNT_DISTINCT,\n            FIRST,\n            LAST,\n            MAX,\n            MEDIAN,\n            MIN,\n            STDDEV,\n            SUM,\n            VAR,\n        }\n    )\n)\n</code></pre> <p>Set of all aggregations supported by Multirel.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.FASTPROP_AGGREGATIONS","title":"FASTPROP_AGGREGATIONS  <code>module-attribute</code>","text":"<pre><code>FASTPROP_AGGREGATIONS: FrozenSet[FastPropAggregations] = (\n    frozenset(\n        {\n            AVG,\n            COUNT,\n            COUNT_DISTINCT,\n            COUNT_MINUS_COUNT_DISTINCT,\n            FIRST,\n            LAST,\n            MAX,\n            MEDIAN,\n            MIN,\n            STDDEV,\n            SUM,\n            VAR,\n            COUNT_DISTINCT_OVER_COUNT,\n            EWMA_1S,\n            EWMA_1M,\n            EWMA_1H,\n            EWMA_1D,\n            EWMA_7D,\n            EWMA_30D,\n            EWMA_90D,\n            EWMA_365D,\n            EWMA_TREND_1S,\n            EWMA_TREND_1M,\n            EWMA_TREND_1H,\n            EWMA_TREND_1D,\n            EWMA_TREND_7D,\n            EWMA_TREND_30D,\n            EWMA_TREND_90D,\n            EWMA_TREND_365D,\n            KURTOSIS,\n            MODE,\n            NUM_MAX,\n            NUM_MIN,\n            Q_1,\n            Q_5,\n            Q_10,\n            Q_25,\n            Q_75,\n            Q_90,\n            Q_95,\n            Q_99,\n            SKEW,\n            TIME_SINCE_FIRST_MAXIMUM,\n            TIME_SINCE_FIRST_MINIMUM,\n            TIME_SINCE_LAST_MAXIMUM,\n            TIME_SINCE_LAST_MINIMUM,\n            TREND,\n            VARIATION_COEFFICIENT,\n        }\n    )\n)\n</code></pre> <p>Set of all aggregations supported by FastProp.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.MAPPING_AGGREGATIONS","title":"MAPPING_AGGREGATIONS  <code>module-attribute</code>","text":"<pre><code>MAPPING_AGGREGATIONS: FrozenSet[MappingAggregations] = (\n    frozenset(\n        {\n            AVG,\n            COUNT,\n            COUNT_DISTINCT,\n            COUNT_DISTINCT_OVER_COUNT,\n            COUNT_MINUS_COUNT_DISTINCT,\n            KURTOSIS,\n            MAX,\n            MEDIAN,\n            MIN,\n            MODE,\n            NUM_MAX,\n            NUM_MIN,\n            Q_1,\n            Q_5,\n            Q_10,\n            Q_25,\n            Q_75,\n            Q_90,\n            Q_95,\n            Q_99,\n            SKEW,\n            STDDEV,\n            SUM,\n            VAR,\n            VARIATION_COEFFICIENT,\n        }\n    )\n)\n</code></pre> <p>Set of all aggregations supported by the mapping preprocessor.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.AGGREGATIONS","title":"AGGREGATIONS  <code>module-attribute</code>","text":"<pre><code>AGGREGATIONS: FrozenSet[Aggregations] = (\n    MULTIREL_AGGREGATIONS\n    | FASTPROP_AGGREGATIONS\n    | MAPPING_AGGREGATIONS\n)\n</code></pre> <p>Set of all possible aggregations.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.AVG","title":"AVG  <code>module-attribute</code>","text":"<pre><code>AVG: Final[Avg] = 'AVG'\n</code></pre> <p>Average value of a given numerical column.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.COUNT","title":"COUNT  <code>module-attribute</code>","text":"<pre><code>COUNT: Final[Count] = 'COUNT'\n</code></pre> <p>Number of rows in a given column.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.COUNT_DISTINCT","title":"COUNT_DISTINCT  <code>module-attribute</code>","text":"<pre><code>COUNT_DISTINCT: Final[CountDistinct] = 'COUNT DISTINCT'\n</code></pre> <p>Count function with distinct clause. This only counts unique elements.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.COUNT_DISTINCT_OVER_COUNT","title":"COUNT_DISTINCT_OVER_COUNT  <code>module-attribute</code>","text":"<pre><code>COUNT_DISTINCT_OVER_COUNT: Final[CountDistinctOverCount] = (\n    \"COUNT DISTINCT OVER COUNT\"\n)\n</code></pre> <p>COUNT DISTINCT divided by COUNT. Please note that this aggregation is not  supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.COUNT_MINUS_COUNT_DISTINCT","title":"COUNT_MINUS_COUNT_DISTINCT  <code>module-attribute</code>","text":"<pre><code>COUNT_MINUS_COUNT_DISTINCT: Final[\n    CountMinusCountDistinct\n] = \"COUNT MINUS COUNT DISTINCT\"\n</code></pre> <p>Counts minus counts distinct. Substracts COUNT DISTINCT from COUNT.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_1S","title":"EWMA_1S  <code>module-attribute</code>","text":"<pre><code>EWMA_1S: Final[EWMA_1s] = 'EWMA_1S'\n</code></pre> <p>Exponentially weighted moving average with a half-life of 1 second.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_1M","title":"EWMA_1M  <code>module-attribute</code>","text":"<pre><code>EWMA_1M: Final[EWMA_1m] = 'EWMA_1M'\n</code></pre> <p>Exponentially weighted moving average with a half-life of 1 minute. Please note  that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_1H","title":"EWMA_1H  <code>module-attribute</code>","text":"<pre><code>EWMA_1H: Final[EWMA_1h] = 'EWMA_1H'\n</code></pre> <p>Exponentially weighted moving average with a half-life of 1 hour. Please note that  this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_1D","title":"EWMA_1D  <code>module-attribute</code>","text":"<pre><code>EWMA_1D: Final[EWMA_1d] = 'EWMA_1D'\n</code></pre> <p>Exponentially weighted moving average with a half-life of 1 day. Please note that  this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_7D","title":"EWMA_7D  <code>module-attribute</code>","text":"<pre><code>EWMA_7D: Final[EWMA_7d] = 'EWMA_7D'\n</code></pre> <p>Exponentially weighted moving average with a half-life of 7 days. Please note that  this aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_30D","title":"EWMA_30D  <code>module-attribute</code>","text":"<pre><code>EWMA_30D: Final[EWMA_30d] = 'EWMA_30D'\n</code></pre> <p>Exponentially weighted moving average with a half-life of 30 days. Please note  that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_90D","title":"EWMA_90D  <code>module-attribute</code>","text":"<pre><code>EWMA_90D: Final[EWMA_90d] = 'EWMA_90D'\n</code></pre> <p>Exponentially weighted moving average with a half-life of 90 days. Please note  that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_365D","title":"EWMA_365D  <code>module-attribute</code>","text":"<pre><code>EWMA_365D: Final[EWMA_365d] = 'EWMA_365D'\n</code></pre> <p>Exponentially weighted moving average with a half-life of 365 days. Please note  that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_1S","title":"EWMA_TREND_1S  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_1S: Final[EWMA_TREND_1s] = 'EWMA_TREND_1S'\n</code></pre> <p>Exponentially weighted trend with a half-life of 1 second.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_1M","title":"EWMA_TREND_1M  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_1M: Final[EWMA_TREND_1m] = 'EWMA_TREND_1M'\n</code></pre> <p>Exponentially weighted trend with a half-life of 1 minute. Please note that this  aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_1H","title":"EWMA_TREND_1H  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_1H: Final[EWMA_TREND_1h] = 'EWMA_TREND_1H'\n</code></pre> <p>Exponentially weighted trend with a half-life of 1 hour. Please note that this  aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_1D","title":"EWMA_TREND_1D  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_1D: Final[EWMA_TREND_1d] = 'EWMA_TREND_1D'\n</code></pre> <p>Exponentially weighted trend with a half-life of 1 day. Please note that this  aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_7D","title":"EWMA_TREND_7D  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_7D: Final[EWMA_TREND_7d] = 'EWMA_TREND_7D'\n</code></pre> <p>Exponentially weighted trend with a half-life of 7 days. Please note that this  aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_30D","title":"EWMA_TREND_30D  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_30D: Final[EWMA_TREND_30d] = 'EWMA_TREND_30D'\n</code></pre> <p>Exponentially weighted trend with a half-life of 30 days. Please note that this  aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_90D","title":"EWMA_TREND_90D  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_90D: Final[EWMA_TREND_90d] = 'EWMA_TREND_90D'\n</code></pre> <p>Exponentially weighted trend with a half-life of 90 days. Please note that this  aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.EWMA_TREND_365D","title":"EWMA_TREND_365D  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_365D: Final[EWMA_TREND_365d] = 'EWMA_TREND_365D'\n</code></pre> <p>Exponentially weighted trend with a half-life of 365 days. Please note that this  aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.FIRST","title":"FIRST  <code>module-attribute</code>","text":"<pre><code>FIRST: Final[First] = 'FIRST'\n</code></pre> <p>First value of a given column, when ordered by the time stamp.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.KURTOSIS","title":"KURTOSIS  <code>module-attribute</code>","text":"<pre><code>KURTOSIS: Final[Kurtosis] = 'KURTOSIS'\n</code></pre> <p>The kurtosis of a given column. Please note that this aggregation is not supported  by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.LAST","title":"LAST  <code>module-attribute</code>","text":"<pre><code>LAST: Final[Last] = 'LAST'\n</code></pre> <p>Last value of a given column, when ordered by the time stamp.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.MAX","title":"MAX  <code>module-attribute</code>","text":"<pre><code>MAX: Final[Max] = 'MAX'\n</code></pre> <p>Largest value of a given column.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.MEDIAN","title":"MEDIAN  <code>module-attribute</code>","text":"<pre><code>MEDIAN: Final[Median] = 'MEDIAN'\n</code></pre> <p>Median of a given column.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.MIN","title":"MIN  <code>module-attribute</code>","text":"<pre><code>MIN: Final[Min] = 'MIN'\n</code></pre> <p>Smallest value of a given column.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.MODE","title":"MODE  <code>module-attribute</code>","text":"<pre><code>MODE: Final[Mode] = 'MODE'\n</code></pre> <p>Most frequent value of a given column. Please note that this aggregation is not  supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.NUM_MAX","title":"NUM_MAX  <code>module-attribute</code>","text":"<pre><code>NUM_MAX: Final[NumMax] = 'NUM MAX'\n</code></pre> <p>The number of times we observe the maximum value. Please note that this  aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.NUM_MIN","title":"NUM_MIN  <code>module-attribute</code>","text":"<pre><code>NUM_MIN: Final[NumMin] = 'NUM MIN'\n</code></pre> <p>The number of times we observe the minimum value. Please note that this  aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q_1","title":"Q_1  <code>module-attribute</code>","text":"<pre><code>Q_1: Final[Q1] = 'Q1'\n</code></pre> <p>The 1%-quantile. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q_5","title":"Q_5  <code>module-attribute</code>","text":"<pre><code>Q_5: Final[Q5] = 'Q5'\n</code></pre> <p>The 5%-quantile. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q_10","title":"Q_10  <code>module-attribute</code>","text":"<pre><code>Q_10: Final[Q10] = 'Q10'\n</code></pre> <p>The 10%-quantile. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q_25","title":"Q_25  <code>module-attribute</code>","text":"<pre><code>Q_25: Final[Q25] = 'Q25'\n</code></pre> <p>The 25%-quantile. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q_75","title":"Q_75  <code>module-attribute</code>","text":"<pre><code>Q_75: Final[Q75] = 'Q75'\n</code></pre> <p>The 75%-quantile. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q_90","title":"Q_90  <code>module-attribute</code>","text":"<pre><code>Q_90: Final[Q90] = 'Q90'\n</code></pre> <p>The 90%-quantile. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q_95","title":"Q_95  <code>module-attribute</code>","text":"<pre><code>Q_95: Final[Q95] = 'Q95'\n</code></pre> <p>The 95%-quantile. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.Q_99","title":"Q_99  <code>module-attribute</code>","text":"<pre><code>Q_99: Final[Q99] = 'Q99'\n</code></pre> <p>The 99%-quantile. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.SKEW","title":"SKEW  <code>module-attribute</code>","text":"<pre><code>SKEW: Final[Skew] = 'SKEW'\n</code></pre> <p>Skewness of a given column. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.STDDEV","title":"STDDEV  <code>module-attribute</code>","text":"<pre><code>STDDEV: Final[Stddev] = 'STDDEV'\n</code></pre> <p>Standard deviation of a given column.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.SUM","title":"SUM  <code>module-attribute</code>","text":"<pre><code>SUM: Final[Sum] = 'SUM'\n</code></pre> <p>Total sum of a given numerical column.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.TIME_SINCE_FIRST_MAXIMUM","title":"TIME_SINCE_FIRST_MAXIMUM  <code>module-attribute</code>","text":"<pre><code>TIME_SINCE_FIRST_MAXIMUM: Final[TimeSinceFirstMaximum] = (\n    \"TIME SINCE FIRST MAXIMUM\"\n)\n</code></pre> <p>The time difference between the first time we see the maximum value and the time  stamp in the population table. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.TIME_SINCE_FIRST_MINIMUM","title":"TIME_SINCE_FIRST_MINIMUM  <code>module-attribute</code>","text":"<pre><code>TIME_SINCE_FIRST_MINIMUM: Final[TimeSinceFirstMinimum] = (\n    \"TIME SINCE FIRST MINIMUM\"\n)\n</code></pre> <p>The time difference between the first time we see the minimum value and the time  stamp in the population table. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.TIME_SINCE_LAST_MAXIMUM","title":"TIME_SINCE_LAST_MAXIMUM  <code>module-attribute</code>","text":"<pre><code>TIME_SINCE_LAST_MAXIMUM: Final[TimeSinceLastMaximum] = (\n    \"TIME SINCE LAST MAXIMUM\"\n)\n</code></pre> <p>The time difference between the last time we see the maximum value and the time  stamp in the population table. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.TIME_SINCE_LAST_MINIMUM","title":"TIME_SINCE_LAST_MINIMUM  <code>module-attribute</code>","text":"<pre><code>TIME_SINCE_LAST_MINIMUM: Final[TimeSinceLastMinimum] = (\n    \"TIME SINCE LAST MINIMUM\"\n)\n</code></pre> <p>The time difference between the last time we see the minimum value and the time  stamp in the population table. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.TREND","title":"TREND  <code>module-attribute</code>","text":"<pre><code>TREND: Final[Trend] = 'TREND'\n</code></pre> <p>Extracts a linear trend from a variable over time and extrapolates this trend to  the current time stamp. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.VAR","title":"VAR  <code>module-attribute</code>","text":"<pre><code>VAR: Final[Var] = 'VAR'\n</code></pre> <p>Statistical variance of a given numerical column. Please note that this  aggregation is not supported by <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/#getml.feature_learning.aggregations.VARIATION_COEFFICIENT","title":"VARIATION_COEFFICIENT  <code>module-attribute</code>","text":"<pre><code>VARIATION_COEFFICIENT: Final[VariationCoefficient] = (\n    \"VARIATION COEFFICIENT\"\n)\n</code></pre> <p>VAR divided by MEAN. Please note that this aggregation is not supported by  <code>Multirel</code>.</p>"},{"location":"reference/feature_learning/aggregations/sets/","title":"sets","text":""},{"location":"reference/feature_learning/aggregations/sets/#getml.feature_learning.aggregations.sets","title":"getml.feature_learning.aggregations.sets","text":""},{"location":"reference/feature_learning/aggregations/sets/#getml.feature_learning.aggregations.sets.AggregationsSets","title":"AggregationsSets  <code>dataclass</code>","text":"<pre><code>AggregationsSets(\n    all: FrozenSet[Aggregations],\n    default: FrozenSet[Aggregations],\n    minimal: FrozenSet[Aggregations],\n)\n</code></pre> <p>Base Class for aggregations sets</p>"},{"location":"reference/feature_learning/aggregations/sets/#getml.feature_learning.aggregations.sets.FastPropAggregationsSets","title":"FastPropAggregationsSets  <code>dataclass</code>","text":"<pre><code>FastPropAggregationsSets(\n    all: FrozenSet[FastPropAggregations],\n    default: FrozenSet[FastPropAggregations],\n    minimal: FrozenSet[FastPropAggregations],\n)\n</code></pre> <p>               Bases: <code>AggregationsSets</code></p> <p>Base class for FastProp aggregation sets</p>"},{"location":"reference/feature_learning/aggregations/sets/#getml.feature_learning.aggregations.sets.MultirelAggregationsSets","title":"MultirelAggregationsSets  <code>dataclass</code>","text":"<pre><code>MultirelAggregationsSets(\n    all: FrozenSet[MultirelAggregations],\n    default: FrozenSet[MultirelAggregations],\n    minimal: FrozenSet[MultirelAggregations],\n)\n</code></pre> <p>               Bases: <code>AggregationsSets</code></p> <p>Base class for Multirel aggregation sets</p>"},{"location":"reference/feature_learning/aggregations/sets/#getml.feature_learning.aggregations.sets.MappingAggregationsSets","title":"MappingAggregationsSets  <code>dataclass</code>","text":"<pre><code>MappingAggregationsSets(\n    all: FrozenSet[MappingAggregations],\n    default: FrozenSet[MappingAggregations],\n    minimal: FrozenSet[MappingAggregations],\n)\n</code></pre> <p>               Bases: <code>AggregationsSets</code></p> <p>Base class for Mapping aggregation sets</p>"},{"location":"reference/feature_learning/aggregations/types/","title":"types","text":""},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types","title":"getml.feature_learning.aggregations.types","text":""},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Avg","title":"Avg  <code>module-attribute</code>","text":"<pre><code>Avg = Literal['AVG']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Count","title":"Count  <code>module-attribute</code>","text":"<pre><code>Count = Literal['COUNT']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.CountDistinct","title":"CountDistinct  <code>module-attribute</code>","text":"<pre><code>CountDistinct = Literal['COUNT DISTINCT']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.CountDistinctOverCount","title":"CountDistinctOverCount  <code>module-attribute</code>","text":"<pre><code>CountDistinctOverCount = Literal[\n    \"COUNT DISTINCT OVER COUNT\"\n]\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.CountMinusCountDistinct","title":"CountMinusCountDistinct  <code>module-attribute</code>","text":"<pre><code>CountMinusCountDistinct = Literal[\n    \"COUNT MINUS COUNT DISTINCT\"\n]\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.EWMA_1s","title":"EWMA_1s  <code>module-attribute</code>","text":"<pre><code>EWMA_1s = Literal['EWMA_1S']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.EWMA_1m","title":"EWMA_1m  <code>module-attribute</code>","text":"<pre><code>EWMA_1m = Literal['EWMA_1M']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.EWMA_1h","title":"EWMA_1h  <code>module-attribute</code>","text":"<pre><code>EWMA_1h = Literal['EWMA_1H']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.EWMA_1d","title":"EWMA_1d  <code>module-attribute</code>","text":"<pre><code>EWMA_1d = Literal['EWMA_1D']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.EWMA_7d","title":"EWMA_7d  <code>module-attribute</code>","text":"<pre><code>EWMA_7d = Literal['EWMA_7D']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.EWMA_30d","title":"EWMA_30d  <code>module-attribute</code>","text":"<pre><code>EWMA_30d = Literal['EWMA_30D']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.EWMA_90d","title":"EWMA_90d  <code>module-attribute</code>","text":"<pre><code>EWMA_90d = Literal['EWMA_90D']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.EWMA_365d","title":"EWMA_365d  <code>module-attribute</code>","text":"<pre><code>EWMA_365d = Literal['EWMA_365D']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.EWMA_TREND_1s","title":"EWMA_TREND_1s  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_1s = Literal['EWMA_TREND_1S']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.EWMA_TREND_1m","title":"EWMA_TREND_1m  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_1m = Literal['EWMA_TREND_1M']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.EWMA_TREND_1h","title":"EWMA_TREND_1h  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_1h = Literal['EWMA_TREND_1H']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.EWMA_TREND_1d","title":"EWMA_TREND_1d  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_1d = Literal['EWMA_TREND_1D']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.EWMA_TREND_7d","title":"EWMA_TREND_7d  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_7d = Literal['EWMA_TREND_7D']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.EWMA_TREND_30d","title":"EWMA_TREND_30d  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_30d = Literal['EWMA_TREND_30D']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.EWMA_TREND_90d","title":"EWMA_TREND_90d  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_90d = Literal['EWMA_TREND_90D']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.EWMA_TREND_365d","title":"EWMA_TREND_365d  <code>module-attribute</code>","text":"<pre><code>EWMA_TREND_365d = Literal['EWMA_TREND_365D']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.First","title":"First  <code>module-attribute</code>","text":"<pre><code>First = Literal['FIRST']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Kurtosis","title":"Kurtosis  <code>module-attribute</code>","text":"<pre><code>Kurtosis = Literal['KURTOSIS']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Last","title":"Last  <code>module-attribute</code>","text":"<pre><code>Last = Literal['LAST']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Max","title":"Max  <code>module-attribute</code>","text":"<pre><code>Max = Literal['MAX']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Median","title":"Median  <code>module-attribute</code>","text":"<pre><code>Median = Literal['MEDIAN']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Min","title":"Min  <code>module-attribute</code>","text":"<pre><code>Min = Literal['MIN']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Mode","title":"Mode  <code>module-attribute</code>","text":"<pre><code>Mode = Literal['MODE']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.NumMax","title":"NumMax  <code>module-attribute</code>","text":"<pre><code>NumMax = Literal['NUM MAX']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.NumMin","title":"NumMin  <code>module-attribute</code>","text":"<pre><code>NumMin = Literal['NUM MIN']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Q1","title":"Q1  <code>module-attribute</code>","text":"<pre><code>Q1 = Literal['Q1']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Q5","title":"Q5  <code>module-attribute</code>","text":"<pre><code>Q5 = Literal['Q5']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Q10","title":"Q10  <code>module-attribute</code>","text":"<pre><code>Q10 = Literal['Q10']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Q25","title":"Q25  <code>module-attribute</code>","text":"<pre><code>Q25 = Literal['Q25']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Q75","title":"Q75  <code>module-attribute</code>","text":"<pre><code>Q75 = Literal['Q75']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Q90","title":"Q90  <code>module-attribute</code>","text":"<pre><code>Q90 = Literal['Q90']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Q95","title":"Q95  <code>module-attribute</code>","text":"<pre><code>Q95 = Literal['Q95']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Q99","title":"Q99  <code>module-attribute</code>","text":"<pre><code>Q99 = Literal['Q99']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Skew","title":"Skew  <code>module-attribute</code>","text":"<pre><code>Skew = Literal['SKEW']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Stddev","title":"Stddev  <code>module-attribute</code>","text":"<pre><code>Stddev = Literal['STDDEV']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Sum","title":"Sum  <code>module-attribute</code>","text":"<pre><code>Sum = Literal['SUM']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.TimeSinceFirstMaximum","title":"TimeSinceFirstMaximum  <code>module-attribute</code>","text":"<pre><code>TimeSinceFirstMaximum = Literal['TIME SINCE FIRST MAXIMUM']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.TimeSinceFirstMinimum","title":"TimeSinceFirstMinimum  <code>module-attribute</code>","text":"<pre><code>TimeSinceFirstMinimum = Literal['TIME SINCE FIRST MINIMUM']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.TimeSinceLastMaximum","title":"TimeSinceLastMaximum  <code>module-attribute</code>","text":"<pre><code>TimeSinceLastMaximum = Literal['TIME SINCE LAST MAXIMUM']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.TimeSinceLastMinimum","title":"TimeSinceLastMinimum  <code>module-attribute</code>","text":"<pre><code>TimeSinceLastMinimum = Literal['TIME SINCE LAST MINIMUM']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Trend","title":"Trend  <code>module-attribute</code>","text":"<pre><code>Trend = Literal['TREND']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Var","title":"Var  <code>module-attribute</code>","text":"<pre><code>Var = Literal['VAR']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.VariationCoefficient","title":"VariationCoefficient  <code>module-attribute</code>","text":"<pre><code>VariationCoefficient = Literal['VARIATION COEFFICIENT']\n</code></pre>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.MultirelAggregations","title":"MultirelAggregations  <code>module-attribute</code>","text":"<pre><code>MultirelAggregations = Union[\n    Avg,\n    Count,\n    CountDistinct,\n    CountMinusCountDistinct,\n    First,\n    Last,\n    Max,\n    Median,\n    Min,\n    Stddev,\n    Sum,\n    Var,\n]\n</code></pre> <p>Union of all Multirel aggregation types.</p>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.FastPropAggregations","title":"FastPropAggregations  <code>module-attribute</code>","text":"<pre><code>FastPropAggregations = Union[\n    Avg,\n    Count,\n    CountDistinct,\n    CountMinusCountDistinct,\n    First,\n    Last,\n    Max,\n    Median,\n    Min,\n    Stddev,\n    Sum,\n    Var,\n    CountDistinctOverCount,\n    EWMA_1s,\n    EWMA_1m,\n    EWMA_1h,\n    EWMA_1d,\n    EWMA_7d,\n    EWMA_30d,\n    EWMA_90d,\n    EWMA_365d,\n    EWMA_TREND_1s,\n    EWMA_TREND_1m,\n    EWMA_TREND_1h,\n    EWMA_TREND_1d,\n    EWMA_TREND_7d,\n    EWMA_TREND_30d,\n    EWMA_TREND_90d,\n    EWMA_TREND_365d,\n    Kurtosis,\n    Mode,\n    NumMax,\n    NumMin,\n    Q1,\n    Q5,\n    Q10,\n    Q25,\n    Q75,\n    Q90,\n    Q95,\n    Q99,\n    Skew,\n    TimeSinceFirstMaximum,\n    TimeSinceFirstMinimum,\n    TimeSinceLastMaximum,\n    TimeSinceLastMinimum,\n    Trend,\n    VariationCoefficient,\n]\n</code></pre> <p>Union of all FastProp aggregation types.</p>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.MappingAggregations","title":"MappingAggregations  <code>module-attribute</code>","text":"<pre><code>MappingAggregations = Union[\n    Avg,\n    Count,\n    CountDistinct,\n    CountDistinctOverCount,\n    CountMinusCountDistinct,\n    Kurtosis,\n    Max,\n    Median,\n    Min,\n    Mode,\n    NumMax,\n    NumMin,\n    Q1,\n    Q5,\n    Q10,\n    Q25,\n    Q75,\n    Q90,\n    Q95,\n    Q99,\n    Skew,\n    Stddev,\n    Sum,\n    Var,\n    VariationCoefficient,\n]\n</code></pre> <p>Union of all Mapping aggregation types.</p>"},{"location":"reference/feature_learning/aggregations/types/#getml.feature_learning.aggregations.types.Aggregations","title":"Aggregations  <code>module-attribute</code>","text":"<pre><code>Aggregations = Union[\n    MultirelAggregations,\n    FastPropAggregations,\n    MappingAggregations,\n]\n</code></pre> <p>Union of all possible aggregation types.</p>"},{"location":"reference/hyperopt/","title":"Hyperparameter Optimization","text":""},{"location":"reference/hyperopt/#getml.hyperopt","title":"getml.hyperopt","text":"<p>Automatically find the best parameters for</p> <ul> <li><code>Multirel</code></li> <li><code>Relboost</code></li> <li><code>RelMT</code></li> <li><code>FastProp</code></li> <li><code>FastBoost</code></li> <li><code>LinearRegression</code></li> <li><code>LogisticRegression</code></li> <li><code>XGBoostClassifier</code></li> <li><code>XGBoostRegressor</code></li> </ul> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> Example <p>The easiest way to conduct a hyperparameter optimization is to use the built-in tuning routines. Note that these tuning routines usually take a day to complete unless we use very small data sets as we do in this example.</p> <p><pre><code>from getml import data\nfrom getml import datasets\nfrom getml import engine\nfrom getml import feature_learning\nfrom getml.feature_learning import aggregations\nfrom getml.feature_learning import loss_functions\nfrom getml import hyperopt\nfrom getml import pipeline\nfrom getml import predictors\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n\npopulation_table, peripheral_table = datasets.make_numerical()\n\n# ----------------\n# Construct placeholders\n\npopulation_placeholder = data.Placeholder(\"POPULATION\")\nperipheral_placeholder = data.Placeholder(\"PERIPHERAL\")\npopulation_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n# ----------------\n\nfeature_learner1 = feature_learning.Multirel(\n    aggregation=[\n        aggregations.COUNT,\n        aggregations.SUM\n    ],\n    loss_function=loss_functions.SquareLoss,\n    num_features=10,\n    share_aggregations=1.0,\n    max_length=1,\n    num_threads=0\n)\n\n# ----------------\n\nfeature_learner2 = feature_learning.Relboost(\n    loss_function=loss_functions.SquareLoss,\n    num_features=10\n)\n\n# ----------------\n\npredictor = predictors.LinearRegression()\n\n# ----------------\n\npipe = pipeline.Pipeline(\n    population=population_placeholder,\n    peripheral=[peripheral_placeholder],\n    feature_learners=[feature_learner1, feature_learner2],\n    predictors=[predictor]\n)\n\n# ----------------\n\ntuned_pipeline = getml.hyperopt.tune_feature_learners(\n    pipeline=base_pipeline,\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n\n# ----------------\n\ntuned_pipeline = getml.hyperopt.tune_predictors(\n    pipeline=tuned_pipeline,\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n</code></pre>     If you want to define the hyperparameter space and     the tuning routing yourself, this is how you     can do that:</p> <pre><code>from getml import data\nfrom getml import datasets\nfrom getml import engine\nfrom getml import feature_learning\nfrom getml.feature_learning import aggregations\nfrom getml.feature_learning import loss_functions\nfrom getml import hyperopt\nfrom getml import pipeline\nfrom getml import predictors\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n\npopulation_table, peripheral_table = datasets.make_numerical()\n\n# ----------------\n# Construct placeholders\n\npopulation_placeholder = data.Placeholder(\"POPULATION\")\nperipheral_placeholder = data.Placeholder(\"PERIPHERAL\")\npopulation_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfeature_learner1 = feature_learning.Multirel(\n    aggregation=[\n        aggregations.COUNT,\n        aggregations.SUM\n    ],\n    loss_function=loss_functions.SquareLoss,\n    num_features=10,\n    share_aggregations=1.0,\n    max_length=1,\n    num_threads=0\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfeature_learner2 = feature_learning.Relboost(\n    loss_function=loss_functions.SquareLoss,\n    num_features=10\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\npredictor = predictors.LinearRegression()\n\n# ----------------\n\npipe = pipeline.Pipeline(\n    population=population_placeholder,\n    peripheral=[peripheral_placeholder],\n    feature_learners=[feature_learner1, feature_learner2],\n    predictors=[predictor]\n)\n\n# ----------------\n# Build a hyperparameter space.\n# We have two feature learners and one\n# predictor, so this is how we must\n# construct our hyperparameter space.\n# If we only wanted to optimize the predictor,\n# we could just leave out the feature_learners.\n\nparam_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n\n# ----------------\n# Wrap a GaussianHyperparameterSearch around the reference model\n\ngaussian_search = hyperopt.GaussianHyperparameterSearch(\n    pipeline=pipe,\n    param_space=param_space,\n    n_iter=30,\n    score=pipeline.scores.rsquared\n)\n\ngaussian_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n\n# ----------------\n\n# We want 5 additional iterations.\ngaussian_search.n_iter = 5\n\n# We do not want another burn-in-phase,\n# so we set ratio_iter to 0.\ngaussian_search.ratio_iter = 0.0\n\n# This widens the hyperparameter space.\ngaussian_search.param_space[\"feature_learners\"][1][\"num_features\"] = [10, 100]\n\n# This narrows the hyperparameter space.\ngaussian_search.param_space[\"predictors\"][0][\"reg_lambda\"] = [0.0, 0.0]\n\n# This continues the hyperparameter search using the previous iterations as\n# prior knowledge.\ngaussian_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n\n# ----------------\n\nall_hyp = hyperopt.list_hyperopts()\n\nbest_pipeline = gaussian_search.best_pipeline\n</code></pre>"},{"location":"reference/hyperopt/#getml.hyperopt.list_hyperopts","title":"list_hyperopts","text":"<pre><code>list_hyperopts() -&gt; List[str]\n</code></pre> <p>Lists all hyperparameter optimization objects present in the Engine.</p> <p>Note that this function only lists hyperopts which are part of the current project. See <code>set_project</code> for changing projects.</p> <p>To subsequently load one of them, use <code>load_hyperopt</code>.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>list containing the names of all hyperopts.</p> Source code in <code>getml/hyperopt/helpers.py</code> <pre><code>def list_hyperopts() -&gt; List[str]:\n    \"\"\"Lists all hyperparameter optimization objects present in the Engine.\n\n    Note that this function only lists hyperopts which are part of the\n    current project. See [`set_project`][getml.engine.set_project] for\n    changing projects.\n\n    To subsequently load one of them, use\n    [`load_hyperopt`][getml.hyperopt.load_hyperopt.load_hyperopt].\n\n    Returns:\n        list containing the names of all hyperopts.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = \"list_hyperopts\"\n    cmd[\"name_\"] = \"\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.handle_engine_exception(msg)\n        json_str = comm.recv_string(sock)\n\n    return json.loads(json_str)[\"names\"]\n</code></pre>"},{"location":"reference/hyperopt/#getml.hyperopt.tune_feature_learners","title":"tune_feature_learners","text":"<pre><code>tune_feature_learners(\n    pipeline: Pipeline,\n    container: Container,\n    train: str = \"train\",\n    validation: str = \"validation\",\n    n_iter: int = 0,\n    score: Optional[str] = None,\n    num_threads: int = 0,\n) -&gt; Pipeline\n</code></pre> <p>A high-level interface for optimizing the feature learners of a <code>Pipeline</code>.</p> <p>Efficiently optimizes the hyperparameters for the set of feature learners (from <code>feature_learning</code>) of a given pipeline by breaking each feature learner's hyperparameter space down into carefully curated subspaces: <code>hyperopt_tuning_subspaces</code> and optimizing the hyperparameters for each subspace in a sequential multi-step process.  For further details about the actual recipes behind the tuning routines refer to tuning routines: <code>hyperopt_tuning</code>.</p> PARAMETER DESCRIPTION <code>pipeline</code> <p>Base pipeline used to derive all models fitted and scored during the hyperparameter optimization. It defines the data schema and any hyperparameters that are not optimized.</p> <p> TYPE: <code>Pipeline</code> </p> <code>container</code> <p>The data container used for the hyperparameter tuning.</p> <p> TYPE: <code>Container</code> </p> <code>train</code> <p>The name of the subset in 'container' used for training.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'train'</code> </p> <code>validation</code> <p>The name of the subset in 'container' used for validation.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'validation'</code> </p> <code>n_iter</code> <p>The number of iterations.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>score</code> <p>The score to optimize. Must be from <code>metrics</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>num_threads</code> <p>The number of parallel threads to use. If set to 0, the number of threads will be inferred.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>Pipeline</code> <p>Pipeline containing tuned versions of the feature learners.</p> Example <p>We assume that you have already set up your <code>Pipeline</code> and <code>Container</code>.</p> <pre><code>tuned_pipeline = getml.hyperopt.tune_predictors(\n    pipeline=base_pipeline,\n    container=container)\n</code></pre> Source code in <code>getml/hyperopt/tuning.py</code> <pre><code>def tune_feature_learners(\n    pipeline: Pipeline,\n    container: Container,\n    train: str = \"train\",\n    validation: str = \"validation\",\n    n_iter: int = 0,\n    score: Optional[str] = None,\n    num_threads: int = 0,\n) -&gt; Pipeline:\n    \"\"\"\n    A high-level interface for optimizing the feature learners of a\n    [`Pipeline`][getml.pipeline.Pipeline].\n\n    Efficiently optimizes the hyperparameters for the set of feature learners\n    (from [`feature_learning`][getml.feature_learning]) of a given pipeline by breaking each\n    feature learner's hyperparameter space down into carefully curated\n    subspaces: `hyperopt_tuning_subspaces` and optimizing the hyperparameters for\n    each subspace in a sequential multi-step process.  For further details about\n    the actual recipes behind the tuning routines refer\n    to tuning routines: `hyperopt_tuning`.\n\n    Args:\n        pipeline:\n            Base pipeline used to derive all models fitted and scored\n            during the hyperparameter optimization. It defines the data\n            schema and any hyperparameters that are not optimized.\n\n        container:\n            The data container used for the hyperparameter tuning.\n\n        train:\n            The name of the subset in 'container' used for training.\n\n        validation:\n            The name of the subset in 'container' used for validation.\n\n        n_iter:\n            The number of iterations.\n\n        score:\n            The score to optimize. Must be from\n            [`metrics`][getml.pipeline.metrics].\n\n        num_threads:\n            The number of parallel threads to use. If set to 0,\n            the number of threads will be inferred.\n\n    Returns:\n        Pipeline containing tuned versions of the feature learners.\n\n    ??? example\n        We assume that you have already set up your\n        [`Pipeline`][getml.Pipeline] and\n        [`Container`][getml.data.Container].\n\n        ```python\n        tuned_pipeline = getml.hyperopt.tune_predictors(\n            pipeline=base_pipeline,\n            container=container)\n        ```\n    \"\"\"\n\n    if not isinstance(pipeline, getml.pipeline.Pipeline):\n        raise TypeError(\"'pipeline' must be a pipeline!\")\n\n    pipeline._validate()\n\n    if not score:\n        score = _infer_score(pipeline)\n\n    tuned_feature_learners = []\n\n    for feature_learner in pipeline.feature_learners:\n        tuned_pipeline = _tune_feature_learner(\n            feature_learner=feature_learner,\n            pipeline=pipeline,\n            container=container,\n            train=train,\n            validation=validation,\n            n_iter=n_iter,\n            score=score,\n            num_threads=num_threads,\n        )\n\n        assert len(tuned_pipeline.feature_learners) == 1, (\n            \"Expected exactly one feature learner, got \"\n            + str(len(tuned_pipeline.feature_learners))\n        )\n\n        tuned_feature_learners.append(tuned_pipeline.feature_learners[0])\n\n    return _make_final_pipeline(\n        pipeline,\n        tuned_feature_learners,\n        copy.deepcopy(pipeline.predictors),\n        container,\n        train,\n        validation,\n    )\n</code></pre>"},{"location":"reference/hyperopt/#getml.hyperopt.tune_predictors","title":"tune_predictors","text":"<pre><code>tune_predictors(\n    pipeline: Pipeline,\n    container: Container,\n    train: str = \"train\",\n    validation: str = \"validation\",\n    n_iter: int = 0,\n    score: Optional[str] = None,\n    num_threads: int = 0,\n) -&gt; Pipeline\n</code></pre> <p>A high-level interface for optimizing the predictors of a <code>Pipeline</code>.</p> <p>Efficiently optimizes the hyperparameters for the set of predictors (from <code>getml.predictors</code>) of a given pipeline by breaking each predictor's hyperparameter space down into carefully curated subspaces: <code>hyperopt_tuning_subspaces</code> and optimizing the hyperparameters for each subspace in a sequential multi-step process.  For further details about the actual recipes behind the tuning routines refer to tuning routines: <code>hyperopt_tuning</code>.</p> PARAMETER DESCRIPTION <code>pipeline</code> <p>Base pipeline used to derive all models fitted and scored during the hyperparameter optimization. It defines the data schema and any hyperparameters that are not optimized.</p> <p> TYPE: <code>Pipeline</code> </p> <code>container</code> <p>The data container used for the hyperparameter tuning.</p> <p> TYPE: <code>Container</code> </p> <code>train</code> <p>The name of the subset in 'container' used for training.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'train'</code> </p> <code>validation</code> <p>The name of the subset in 'container' used for validation.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'validation'</code> </p> <code>n_iter</code> <p>The number of iterations.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>score</code> <p>The score to optimize. Must be from <code>metrics</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>num_threads</code> <p>The number of parallel threads to use. If set to 0, the number of threads will be inferred.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> Example <p>We assume that you have already set up your <code>Pipeline</code> and <code>Container</code>.</p> <pre><code>tuned_pipeline = getml.hyperopt.tune_predictors(\n    pipeline=base_pipeline,\n    container=container)\n</code></pre> RETURNS DESCRIPTION <code>Pipeline</code> <p>Pipeline containing tuned predictors.</p> Source code in <code>getml/hyperopt/tuning.py</code> <pre><code>def tune_predictors(\n    pipeline: Pipeline,\n    container: Container,\n    train: str = \"train\",\n    validation: str = \"validation\",\n    n_iter: int = 0,\n    score: Optional[str] = None,\n    num_threads: int = 0,\n) -&gt; Pipeline:\n    \"\"\"\n    A high-level interface for optimizing the predictors of a\n    [`Pipeline`][getml.Pipeline].\n\n    Efficiently optimizes the hyperparameters for the set of predictors (from\n    [`getml.predictors`][getml.predictors]) of a given pipeline by breaking each\n    predictor's\n    hyperparameter space down into carefully curated\n    subspaces: `hyperopt_tuning_subspaces` and optimizing the hyperparameters for\n    each subspace in a sequential multi-step process.  For further details about\n    the actual recipes behind the tuning routines refer to\n    tuning routines: `hyperopt_tuning`.\n\n    Args:\n        pipeline:\n            Base pipeline used to derive all models fitted and scored\n            during the hyperparameter optimization. It defines the data\n            schema and any hyperparameters that are not optimized.\n\n        container:\n            The data container used for the hyperparameter tuning.\n\n        train:\n            The name of the subset in 'container' used for training.\n\n        validation:\n            The name of the subset in 'container' used for validation.\n\n        n_iter:\n            The number of iterations.\n\n        score:\n            The score to optimize. Must be from\n            [`metrics`][getml.pipeline.metrics].\n\n        num_threads:\n            The number of parallel threads to use. If set to 0,\n            the number of threads will be inferred.\n\n    ??? example\n        We assume that you have already set up your\n        [`Pipeline`][getml.Pipeline] and\n        [`Container`][getml.data.Container].\n\n        ```python\n        tuned_pipeline = getml.hyperopt.tune_predictors(\n            pipeline=base_pipeline,\n            container=container)\n        ```\n\n    Returns:\n        Pipeline containing tuned predictors.\n\n    \"\"\"\n\n    if not isinstance(pipeline, getml.pipeline.Pipeline):\n        raise TypeError(\"'pipeline' must be a pipeline!\")\n\n    pipeline._validate()\n\n    if not score:\n        score = _infer_score(pipeline)\n\n    tuned_predictors = []\n\n    for predictor in pipeline.predictors:\n        tuned_pipeline = _tune_predictor(\n            predictor=predictor,\n            pipeline=pipeline,\n            container=container,\n            train=train,\n            validation=validation,\n            n_iter=n_iter,\n            score=score,\n            num_threads=num_threads,\n        )\n\n        assert len(tuned_pipeline.predictors) == 1, (\n            \"Expected exactly one predictor, got \" + str(len(tuned_pipeline.predictors))\n        )\n\n        tuned_predictors.append(tuned_pipeline.predictors[0])\n\n    return _make_final_pipeline(\n        pipeline,\n        copy.deepcopy(pipeline.feature_learners),\n        tuned_predictors,\n        container,\n        train,\n        validation,\n    )\n</code></pre>"},{"location":"reference/hyperopt/#getml.hyperopt.exists","title":"exists","text":"<pre><code>exists(name: str) -&gt; bool\n</code></pre> <p>Determines whether a hyperopt exists.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the hyperopt.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>A boolean indicating whether a hyperopt named 'name' exists.</p> Source code in <code>getml/hyperopt/helpers.py</code> <pre><code>def exists(name: str) -&gt; bool:\n    \"\"\"Determines whether a hyperopt exists.\n\n    Args:\n        name: The name of the hyperopt.\n\n    Returns:\n        A boolean indicating whether a hyperopt named 'name' exists.\n    \"\"\"\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    return name in list_hyperopts()\n</code></pre>"},{"location":"reference/hyperopt/#getml.hyperopt.delete","title":"delete","text":"<pre><code>delete(name: str) -&gt; None\n</code></pre> <p>If a hyperopt named 'name' exists, it is deleted.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the hyperopt.</p> <p> TYPE: <code>str</code> </p> Source code in <code>getml/hyperopt/helpers.py</code> <pre><code>def delete(name: str) -&gt; None:\n    \"\"\"\n    If a hyperopt named 'name' exists, it is deleted.\n\n    Args:\n        name: The name of the hyperopt.\n    \"\"\"\n\n    if not exists(name):\n        return\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Hyperopt.delete\"\n    cmd[\"name_\"] = name\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.handle_engine_exception(msg)\n</code></pre>"},{"location":"reference/hyperopt/#getml.hyperopt.load_hyperopt.load_hyperopt","title":"load_hyperopt","text":"<pre><code>load_hyperopt(\n    name: str,\n) -&gt; Union[\n    GaussianHyperparameterSearch,\n    LatinHypercubeSearch,\n    RandomSearch,\n]\n</code></pre> <p>Loads a hyperparameter optimization object from the getML Engine into Python.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the hyperopt to be loaded.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Union[GaussianHyperparameterSearch, LatinHypercubeSearch, RandomSearch]</code> <p>The hyperopt object.</p> Source code in <code>getml/hyperopt/load_hyperopt.py</code> <pre><code>def load_hyperopt(\n    name: str,\n) -&gt; Union[GaussianHyperparameterSearch, LatinHypercubeSearch, RandomSearch]:\n    \"\"\"Loads a hyperparameter optimization object from the getML Engine into Python.\n\n    Args:\n        name:\n            The name of the hyperopt to be loaded.\n\n    Returns:\n        The hyperopt object.\n\n    \"\"\"\n    # This will be overwritten by .refresh(...) anyway\n    dummy_pipeline = _make_dummy(\"123456\")\n\n    dummy_param_space = {\"predictors\": [{\"reg_lambda\": [0.0, 1.0]}]}\n\n    json_obj = _get_json_obj(name)\n\n    if json_obj[\"type_\"] == \"GaussianHyperparameterSearch\":\n        return GaussianHyperparameterSearch(\n            param_space=dummy_param_space, pipeline=dummy_pipeline\n        )._parse_json_obj(json_obj)\n\n    if json_obj[\"type_\"] == \"LatinHypercubeSearch\":\n        return LatinHypercubeSearch(\n            param_space=dummy_param_space, pipeline=dummy_pipeline\n        )._parse_json_obj(json_obj)\n\n    if json_obj[\"type_\"] == \"RandomSearch\":\n        return RandomSearch(\n            param_space=dummy_param_space, pipeline=dummy_pipeline\n        )._parse_json_obj(json_obj)\n\n    raise ValueError(\"Unknown type: '\" + json_obj[\"type_\"] + \"'!\")\n</code></pre>"},{"location":"reference/hyperopt/#getml.hyperopt.kernels","title":"kernels","text":"<p>Collection of kernel functions to be used by the hyperparameter optimizations.</p>"},{"location":"reference/hyperopt/#getml.hyperopt.kernels.exp","title":"exp  <code>module-attribute</code>","text":"<pre><code>exp = 'exp'\n</code></pre> <p>An exponential kernel yielding non-differentiable sample paths.</p>"},{"location":"reference/hyperopt/#getml.hyperopt.kernels.gauss","title":"gauss  <code>module-attribute</code>","text":"<pre><code>gauss = 'gauss'\n</code></pre> <p>A Gaussian kernel yielding analytic (infinitely--differentiable) sample paths.</p>"},{"location":"reference/hyperopt/#getml.hyperopt.kernels.matern32","title":"matern32  <code>module-attribute</code>","text":"<pre><code>matern32 = 'matern32'\n</code></pre> <p>A Mat\u00e9rn 3/2 kernel yielding once-differentiable sample paths.</p>"},{"location":"reference/hyperopt/#getml.hyperopt.kernels.matern52","title":"matern52  <code>module-attribute</code>","text":"<pre><code>matern52 = 'matern52'\n</code></pre> <p>A Mat\u00e9rn 5/2 kernel yielding twice-differentiable sample paths.</p>"},{"location":"reference/hyperopt/#getml.hyperopt.optimization","title":"optimization","text":"<p>Collection of optimization algorithms to be used by the hyperparameter optimizations.</p>"},{"location":"reference/hyperopt/#getml.hyperopt.optimization.bfgs","title":"bfgs  <code>module-attribute</code>","text":"<pre><code>bfgs = 'bfgs'\n</code></pre> <p>Broyden-Fletcher-Goldbarb-Shanno optimization algorithm.</p> <p>The BFGS algorithm is a quasi-Newton method that requires the function to be differentiable.</p>"},{"location":"reference/hyperopt/#getml.hyperopt.optimization.nelder_mead","title":"nelder_mead  <code>module-attribute</code>","text":"<pre><code>nelder_mead = 'nelderMead'\n</code></pre> <p>Nelder-Mead optimization algorithm.</p> <p>Nelder-Mead is a direct search method that does not require functions to be differentiable.</p>"},{"location":"reference/hyperopt/#getml.hyperopt.burn_in","title":"burn_in","text":"<p>Collection of burn-in algorithms to be used by the hyperparameter optimizations.</p>"},{"location":"reference/hyperopt/#getml.hyperopt.burn_in.latin_hypercube","title":"latin_hypercube  <code>module-attribute</code>","text":"<pre><code>latin_hypercube = 'latinHypercube'\n</code></pre> <p>Samples from the hyperparameter space almost randomly, but ensures that the different draws are sufficiently different from each other.</p>"},{"location":"reference/hyperopt/#getml.hyperopt.burn_in.random","title":"random  <code>module-attribute</code>","text":"<pre><code>random = 'random'\n</code></pre> <p>Samples from the hyperparameter space at random.</p>"},{"location":"reference/hyperopt/gaussian/","title":"Gaussian Process","text":""},{"location":"reference/hyperopt/gaussian/#getml.hyperopt.GaussianHyperparameterSearch","title":"GaussianHyperparameterSearch","text":"<pre><code>GaussianHyperparameterSearch(\n    param_space: Dict[str, Any],\n    pipeline: Pipeline,\n    score: str = metrics.rmse,\n    n_iter: int = 100,\n    seed: int = 5483,\n    ratio_iter: float = 0.8,\n    optimization_algorithm: str = nelder_mead,\n    optimization_burn_in_algorithm: str = latin_hypercube,\n    optimization_burn_ins: int = 500,\n    surrogate_burn_in_algorithm: str = latin_hypercube,\n    gaussian_kernel: str = matern52,\n    gaussian_optimization_burn_in_algorithm: str = latin_hypercube,\n    gaussian_optimization_algorithm: str = nelder_mead,\n    gaussian_optimization_burn_ins: int = 500,\n    gaussian_nugget: int = 50,\n    early_stopping: bool = True,\n)\n</code></pre> <p>               Bases: <code>_Hyperopt</code></p> <p>Bayesian hyperparameter optimization using a Gaussian process.</p> <p>After a burn-in period, a Gaussian process is used to pick the most promising parameter combination to be evaluated next based on the knowledge gathered throughout previous evaluations. Assessing the quality of potential combinations will be done using the expected information (EI).</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> PARAMETER DESCRIPTION <code>param_space</code> <p>Dictionary containing numerical arrays of length two holding the lower and upper bounds of all parameters which will be altered in <code>pipeline</code> during the hyperparameter optimization.</p> <p>If we have two feature learners and one predictor, the hyperparameter space might look like this:</p> <pre><code>param_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n</code></pre> <p>If we only want to optimize the predictor, then we can leave out the feature learners.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>pipeline</code> <p>Base pipeline used to derive all models fitted and scored during the hyperparameter optimization. Be careful when constructing it since only the parameters present in <code>param_space</code> will be overwritten. It defines the data schema and any hyperparameters that are not optimized.</p> <p> TYPE: <code>Pipeline</code> </p> <code>score</code> <p>The score to optimize. Must be from <code>metrics</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>rmse</code> </p> <code>n_iter</code> <p>Number of iterations in the hyperparameter optimization and thus the number of parameter combinations to draw and evaluate. Range: [1, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>seed</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Due to nature of the underlying algorithm, this is only the case if the fit is done without multithreading. To reflect this, a <code>seed</code> of None is only allowed to be set to an actual integer if both <code>num_threads</code> and <code>n_jobs</code> instance variables of the <code>predictor</code> and <code>feature_selector</code> in <code>model</code> - if they are instances of either <code>XGBoostRegressor</code> or <code>XGBoostClassifier</code> - are set to 1. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5483</code> </p> <code>ratio_iter</code> <p>Ratio of the iterations used for the burn-in. For a <code>ratio_iter</code> of 1.0, all iterations will be spent in the burn-in period resulting in an equivalence of this class to <code>LatinHypercubeSearch</code> or <code>RandomSearch</code> - depending on <code>surrogate_burn_in_algorithm</code>. Range: [0, 1]</p> <p>As a rule of thumb at least 70 percent of the evaluations should be spent in the burn-in phase. The more comprehensive the exploration of the <code>param_space</code> during the burn-in, the less likely it is that the Gaussian process gets stuck in local minima.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.8</code> </p> <code>optimization_algorithm</code> <p>Determines the optimization algorithm used for the local search in the optimization of the expected information (EI). Must be from <code>optimization</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>nelder_mead</code> </p> <code>optimization_burn_in_algorithm</code> <p>Specifies the algorithm used to draw initial points in the burn-in period of the optimization of the expected information (EI). Must be from <code>burn_in</code>.</p> <p> DEFAULT: <code>latin_hypercube</code> </p> <code>optimization_burn_ins</code> <p>Number of random evaluation points used during the burn-in of the minimization of the expected information (EI). After the surrogate model - the Gaussian process - was successfully fitted to the previous parameter combination, the algorithm is able to calculate the EI for a given point. In order to get to the next combination, the EI has to be maximized over the whole parameter space. Much like the GaussianProcess itself, this requires a burn-in phase. Range: [3, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>surrogate_burn_in_algorithm</code> <p>Specifies the algorithm used to draw new parameter combinations during the burn-in period. Must be from <code>burn_in</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>latin_hypercube</code> </p> <code>gaussian_kernel</code> <p>Specifies the 1-dimensional kernel of the Gaussian process which will be used along each dimension of the parameter space. All the choices below will result in continuous sample paths and their main difference is the degree of smoothness of the results with 'exp' yielding the least and 'gauss' yielding the most smooth paths. Must be from <code>kernels</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>matern52</code> </p> <code>gaussian_optimization_burn_in_algorithm</code> <p>Specifies the algorithm used to draw new parameter combinations during the burn-in period of the optimization of the Gaussian process. Must be from <code>burn_in</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>latin_hypercube</code> </p> <code>gaussian_optimization_algorithm</code> <p>Determines the optimization algorithm used for the local search in the fitting of the Gaussian process to the previous parameter combinations. Must be from <code>optimization</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>nelder_mead</code> </p> <code>gaussian_optimization_burn_ins</code> <p>Number of random evaluation points used during the burn-in of the fitting of the Gaussian process. Range: [3, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>500</code> </p> <code>early_stopping</code> <p>Whether you want to apply early stopping to the predictors.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Note <p>A Gaussian hyperparameter search works like this:</p> <ul> <li> <p>It begins with a burn-in phase, usually about 70% to 90%   of all iterations. During that burn-in phase, the hyperparameter   space is sampled more or less at random. You can control   this phase using <code>ratio_iter</code> and <code>surrogate_burn_in_algorithm</code>.</p> </li> <li> <p>Once enough information has been collected, it fits a   Gaussian process on the hyperparameters with the <code>score</code> we want to   maximize or minimize as the predicted variable. Note that the   Gaussian process has hyperparameters itself, which are also optimized.   You can control this phase using <code>gaussian_kernel</code>,   <code>gaussian_optimization_algorithm</code>,   <code>gaussian_optimization_burn_in_algorithm</code> and   <code>gaussian_optimization_burn_ins</code>.</p> </li> <li> <p>It then uses the Gaussian process to predict the expected information   (EI), which is how much additional information it might get from   evaluating   a particular point in the hyperparameter space. The expected information   is to be maximized. The point in the hyperparameter space with   the maximum expected information is the next point that is actually   evaluated (meaning a new pipeline with these hyperparameters is trained).   You can control this phase using <code>optimization_algorithm</code>,   <code>optimization_burn_ins</code> and <code>optimization_burn_in_algorithm</code>.</p> </li> </ul> <p>In a nutshell, the GaussianHyperparameterSearch behaves like human data scientists:</p> <ul> <li> <p>At first, it picks random hyperparameter combinations.</p> </li> <li> <p>Once it has gained a better understanding of the hyperparameter space,   it starts evaluating hyperparameter combinations that are   particularly interesting.</p> </li> </ul> References <ul> <li>Carl Edward Rasmussen and Christopher K. I. Williams, MIT   Press, 2006 </li> <li>Julien Villemonteix, Emmanuel Vazquez, and Eric Walter, 2009   </li> </ul> Example <pre><code>from getml import data\nfrom getml import datasets\nfrom getml import engine\nfrom getml import feature_learning\nfrom getml.feature_learning import aggregations\nfrom getml.feature_learning import loss_functions\nfrom getml import hyperopt\nfrom getml import pipeline\nfrom getml import predictors\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n\npopulation_table, peripheral_table = datasets.make_numerical()\n\n# ----------------\n# Construct placeholders\n\npopulation_placeholder = data.Placeholder(\"POPULATION\")\nperipheral_placeholder = data.Placeholder(\"PERIPHERAL\")\npopulation_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe1 = feature_learning.Multirel(\n    aggregation=[\n        aggregations.COUNT,\n        aggregations.SUM\n    ],\n    loss_function=loss_functions.SquareLoss,\n    num_features=10,\n    share_aggregations=1.0,\n    max_length=1,\n    num_threads=0\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe2 = feature_learning.Relboost(\n    loss_function=loss_functions.SquareLoss,\n    num_features=10\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\npredictor = predictors.LinearRegression()\n\n# ----------------\n\npipe = pipeline.Pipeline(\n    population=population_placeholder,\n    peripheral=[peripheral_placeholder],\n    feature_learners=[fe1, fe2],\n    predictors=[predictor]\n)\n\n# ----------------\n# Build a hyperparameter space.\n# We have two feature learners and one\n# predictor, so this is how we must\n# construct our hyperparameter space.\n# If we only wanted to optimize the predictor,\n# we could just leave out the feature_learners.\n\nparam_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n\n# ----------------\n# Wrap a GaussianHyperparameterSearch around the reference model\n\ngaussian_search = hyperopt.GaussianHyperparameterSearch(\n    pipeline=pipe,\n    param_space=param_space,\n    n_iter=30,\n    score=pipeline.metrics.rsquared\n)\n\ngaussian_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n\n# ----------------\n\n# We want 5 additional iterations.\ngaussian_search.n_iter = 5\n\n# We do not want another burn-in-phase,\n# so we set ratio_iter to 0.\ngaussian_search.ratio_iter = 0.0\n\n# This widens the hyperparameter space.\ngaussian_search.param_space[\"feature_learners\"][1][\"num_features\"] = [10, 100]\n\n# This narrows the hyperparameter space.\ngaussian_search.param_space[\"predictors\"][0][\"reg_lambda\"] = [0.0, 0.0]\n\n# This continues the hyperparameter search using the previous iterations as\n# prior knowledge.\ngaussian_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n\n# ----------------\n\nall_hyp = hyperopt.list_hyperopts()\n\nbest_pipeline = gaussian_search.best_pipeline\n</code></pre> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def __init__(\n    self,\n    param_space: Dict[str, Any],\n    pipeline: Pipeline,\n    score: str = metrics.rmse,\n    n_iter: int = 100,\n    seed: int = 5483,\n    ratio_iter: float = 0.80,\n    optimization_algorithm: str = nelder_mead,\n    optimization_burn_in_algorithm: str = latin_hypercube,\n    optimization_burn_ins: int = 500,\n    surrogate_burn_in_algorithm: str = latin_hypercube,\n    gaussian_kernel: str = matern52,\n    gaussian_optimization_burn_in_algorithm: str = latin_hypercube,\n    gaussian_optimization_algorithm: str = nelder_mead,\n    gaussian_optimization_burn_ins: int = 500,\n    gaussian_nugget: int = 50,\n    early_stopping: bool = True,\n):\n    super().__init__(\n        param_space=param_space,\n        pipeline=pipeline,\n        score=score,\n        n_iter=n_iter,\n        seed=seed,\n        ratio_iter=ratio_iter,\n        optimization_algorithm=optimization_algorithm,\n        optimization_burn_in_algorithm=optimization_burn_in_algorithm,\n        optimization_burn_ins=optimization_burn_ins,\n        surrogate_burn_in_algorithm=surrogate_burn_in_algorithm,\n        gaussian_kernel=gaussian_kernel,\n        gaussian_optimization_algorithm=gaussian_optimization_algorithm,\n        gaussian_optimization_burn_in_algorithm=gaussian_optimization_burn_in_algorithm,\n        gaussian_optimization_burn_ins=gaussian_optimization_burn_ins,\n        gaussian_nugget=gaussian_nugget,\n        early_stopping=early_stopping,\n    )\n\n    self._type = \"GaussianHyperparameterSearch\"\n\n    self.validate()\n</code></pre>"},{"location":"reference/hyperopt/gaussian/#getml.hyperopt.GaussianHyperparameterSearch.best_pipeline","title":"best_pipeline  <code>property</code>","text":"<pre><code>best_pipeline: Pipeline\n</code></pre> <p>The best pipeline that is part of the hyperparameter optimization.</p> <p>This is always based on the validation data you have passed even if you have chosen to score the pipeline on other data afterwards.</p> RETURNS DESCRIPTION <code>Pipeline</code> <p>The best pipeline.</p>"},{"location":"reference/hyperopt/gaussian/#getml.hyperopt.GaussianHyperparameterSearch.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Name of the hyperparameter optimization. This is used to uniquely identify it on the engine.</p> RETURNS DESCRIPTION <code>str</code> <p>The name of the hyperparameter optimization.</p>"},{"location":"reference/hyperopt/gaussian/#getml.hyperopt.GaussianHyperparameterSearch.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Returns the ID of the hyperparameter optimization. The name property is kept for backward compatibility.</p> RETURNS DESCRIPTION <code>str</code> <p>The name of the hyperparameter optimization.</p>"},{"location":"reference/hyperopt/gaussian/#getml.hyperopt.GaussianHyperparameterSearch.score","title":"score  <code>property</code>","text":"<pre><code>score: str\n</code></pre> <p>The score to be optimized.</p> RETURNS DESCRIPTION <code>str</code> <p>The score to be optimized.</p>"},{"location":"reference/hyperopt/gaussian/#getml.hyperopt.GaussianHyperparameterSearch.type","title":"type  <code>property</code>","text":"<pre><code>type: str\n</code></pre> <p>The algorithm used for the hyperparameter optimization.</p> RETURNS DESCRIPTION <code>str</code> <p>The algorithm used for the hyperparameter optimization.</p>"},{"location":"reference/hyperopt/gaussian/#getml.hyperopt.GaussianHyperparameterSearch.clean_up","title":"clean_up","text":"<pre><code>clean_up() -&gt; None\n</code></pre> <p>Deletes all pipelines associated with hyperparameter optimization, but the best pipeline.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def clean_up(self) -&gt; None:\n    \"\"\"\n    Deletes all pipelines associated with hyperparameter optimization,\n    but the best pipeline.\n    \"\"\"\n    best_pipeline = self._best_pipeline_name()\n    names = [obj[\"pipeline_name\"] for obj in self.evaluations]\n    for name in names:\n        if name == best_pipeline:\n            continue\n        if exists(name):\n            delete(name)\n</code></pre>"},{"location":"reference/hyperopt/gaussian/#getml.hyperopt.GaussianHyperparameterSearch.fit","title":"fit","text":"<pre><code>fit(\n    container: Union[Container, StarSchema, TimeSeries],\n    train: str = \"train\",\n    validation: str = \"validation\",\n) -&gt; _Hyperopt\n</code></pre> <p>Launches the hyperparameter optimization.</p> PARAMETER DESCRIPTION <code>container</code> <p>The data container used for the hyperparameter tuning.</p> <p> TYPE: <code>Union[Container, StarSchema, TimeSeries]</code> </p> <code>train</code> <p>The name of the subset in 'container' used for training.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'train'</code> </p> <code>validation</code> <p>The name of the subset in 'container' used for validation.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'validation'</code> </p> RETURNS DESCRIPTION <code>_Hyperopt</code> <p>The current instance.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def fit(\n    self,\n    container: Union[Container, StarSchema, TimeSeries],\n    train: str = \"train\",\n    validation: str = \"validation\",\n) -&gt; _Hyperopt:\n    \"\"\"Launches the hyperparameter optimization.\n\n    Args:\n        container:\n            The data container used for the hyperparameter tuning.\n\n        train:\n            The name of the subset in 'container' used for training.\n\n        validation:\n            The name of the subset in 'container' used for validation.\n\n    Returns:\n        The current instance.\n    \"\"\"\n\n    if isinstance(container, (StarSchema, TimeSeries)):\n        container = container.container\n\n    if not isinstance(container, Container):\n        raise TypeError(\n            \"'container' must be a `~getml.data.Container`, \"\n            + \"a `~getml.data.StarSchema` or a `~getml.data.TimeSeries`\"\n        )\n\n    if not isinstance(train, str):\n        raise TypeError(\"\"\"'train' must be a string\"\"\")\n\n    if not isinstance(validation, str):\n        raise TypeError(\"\"\"'validation' must be a string\"\"\")\n\n    self.pipeline.check(container[train])\n\n    population_table_training = container[train].population\n\n    population_table_validation = container[validation].population\n\n    peripheral_tables = _transform_peripheral(\n        container[train].peripheral, self.pipeline.peripheral\n    )\n\n    self._send()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = self.id\n    cmd[\"type_\"] = \"Hyperopt.launch\"\n\n    cmd[\"population_training_df_\"] = population_table_training._getml_deserialize()\n\n    cmd[\"population_validation_df_\"] = (\n        population_table_validation._getml_deserialize()\n    )\n\n    cmd[\"peripheral_dfs_\"] = [\n        elem._getml_deserialize() for elem in peripheral_tables\n    ]\n\n    with comm.send_and_get_socket(cmd) as sock:\n        begin = time.monotonic()\n        msg = comm.log(sock)\n        end = time.monotonic()\n\n    if msg != \"Success!\":\n        comm.handle_engine_exception(msg)\n\n    print()\n    _print_time_taken(begin, end, \"Time taken: \")\n\n    self._save()\n\n    return self.refresh()\n</code></pre>"},{"location":"reference/hyperopt/gaussian/#getml.hyperopt.GaussianHyperparameterSearch.refresh","title":"refresh","text":"<pre><code>refresh() -&gt; _Hyperopt\n</code></pre> <p>Reloads the hyperparameter optimization from the Engine.</p> RETURNS DESCRIPTION <code>_Hyperopt</code> <p>Current instance</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def refresh(self) -&gt; _Hyperopt:\n    \"\"\"Reloads the hyperparameter optimization from the Engine.\n\n    Returns:\n            Current instance\n\n    \"\"\"\n    json_obj = _get_json_obj(self.id)\n    return self._parse_json_obj(json_obj)\n</code></pre>"},{"location":"reference/hyperopt/gaussian/#getml.hyperopt.GaussianHyperparameterSearch.validate","title":"validate","text":"<pre><code>validate() -&gt; None\n</code></pre> <p>Validate the parameters of the hyperparameter optimization.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"\n    Validate the parameters of the hyperparameter optimization.\n    \"\"\"\n    _validate_hyperopt(_Hyperopt._supported_params, **self.__dict__)  # type: ignore\n</code></pre>"},{"location":"reference/hyperopt/latin/","title":"Latin Hypercube","text":""},{"location":"reference/hyperopt/latin/#getml.hyperopt.LatinHypercubeSearch","title":"LatinHypercubeSearch","text":"<pre><code>LatinHypercubeSearch(\n    param_space: Dict[str, Any],\n    pipeline: Pipeline,\n    score: str = metrics.rmse,\n    n_iter: int = 100,\n    seed: int = 5483,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>_Hyperopt</code></p> <p>Latin hypercube sampling of the hyperparameters.</p> <p>Uses a multidimensional, uniform cumulative distribution function to draw the random numbers from. For drawing <code>n_iter</code> samples, the distribution will be divided in <code>n_iter</code>*<code>n_iter</code> hypercubes of equal size (<code>n_iter</code> per dimension). <code>n_iter</code> of them will be selected in such a way only one per dimension is used and an independent and identically-distributed (iid) random number is drawn within the boundaries of the hypercube.</p> <p>A latin hypercube search can be seen as a compromise between a grid search, which iterates through the entire hyperparameter space, and a random search, which draws completely random samples from the hyperparameter space.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> PARAMETER DESCRIPTION <code>param_space</code> <p>Dictionary containing numerical arrays of length two holding the lower and upper bounds of all parameters which will be altered in <code>pipeline</code> during the hyperparameter optimization.</p> <p>If we have two feature learners and one predictor, the hyperparameter space might look like this:</p> <pre><code>param_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n</code></pre> <p>If we only want to optimize the predictor, then we can leave out the feature learners.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>pipeline</code> <p>Base pipeline used to derive all models fitted and scored during the hyperparameter optimization. Be careful in constructing it since only those parameters present in <code>param_space</code> will be overwritten. It defines the data schema and any hyperparameters that are not optimized.</p> <p> TYPE: <code>Pipeline</code> </p> <code>score</code> <p>The score to optimize. Must be from <code>metrics</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>rmse</code> </p> <code>n_iter</code> <p>Number of iterations in the hyperparameter optimization and thus the number of parameter combinations to draw and evaluate. Range: [1, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>seed</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Due to nature of the underlying algorithm this is only the case if the fit is done without multithreading. To reflect this, a <code>seed</code> of None represents an unreproducible and is only allowed to be set to an actual integer if both <code>num_threads</code> and <code>n_jobs</code> instance variables of the <code>predictor</code> and <code>feature_selector</code> in <code>model</code> - if they are instances of either <code>XGBoostRegressor</code> or <code>XGBoostClassifier</code> - are set to 1. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5483</code> </p> Example <pre><code>from getml import data\nfrom getml import datasets\nfrom getml import engine\nfrom getml import feature_learning\nfrom getml.feature_learning import aggregations\nfrom getml.feature_learning import loss_functions\nfrom getml import hyperopt\nfrom getml import pipeline\nfrom getml import predictors\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n\npopulation_table, peripheral_table = datasets.make_numerical()\n\n# ----------------\n# Construct placeholders\n\npopulation_placeholder = data.Placeholder(\"POPULATION\")\nperipheral_placeholder = data.Placeholder(\"PERIPHERAL\")\npopulation_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe1 = feature_learning.Multirel(\n    aggregation=[\n        aggregations.COUNT,\n        aggregations.SUM\n    ],\n    loss_function=loss_functions.SquareLoss,\n    num_features=10,\n    share_aggregations=1.0,\n    max_length=1,\n    num_threads=0\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe2 = feature_learning.Relboost(\n    loss_function=loss_functions.SquareLoss,\n    num_features=10\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\npredictor = predictors.LinearRegression()\n\n# ----------------\n\npipe = pipeline.Pipeline(\n    population=population_placeholder,\n    peripheral=[peripheral_placeholder],\n    feature_learners=[fe1, fe2],\n    predictors=[predictor]\n)\n\n# ----------------\n# Build a hyperparameter space.\n# We have two feature learners and one\n# predictor, so this is how we must\n# construct our hyperparameter space.\n# If we only wanted to optimize the predictor,\n# we could just leave out the feature_learners.\n\nparam_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n\n# ----------------\n# Wrap a LatinHypercubeSearch around the reference model\n\nlatin_search = hyperopt.LatinHypercubeSearch(\n    pipeline=pipe,\n    param_space=param_space,\n    n_iter=30,\n    score=pipeline.metrics.rsquared\n)\n\nlatin_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n</code></pre> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def __init__(\n    self,\n    param_space: Dict[str, Any],\n    pipeline: Pipeline,\n    score: str = metrics.rmse,\n    n_iter: int = 100,\n    seed: int = 5483,\n    **kwargs,\n):\n    super().__init__(\n        param_space=param_space,\n        pipeline=pipeline,\n        score=score,\n        n_iter=n_iter,\n        seed=seed,\n        **kwargs,\n    )\n\n    self._type = \"LatinHypercubeSearch\"\n\n    self.surrogate_burn_in_algorithm = latin_hypercube\n\n    self.validate()\n</code></pre>"},{"location":"reference/hyperopt/latin/#getml.hyperopt.LatinHypercubeSearch.best_pipeline","title":"best_pipeline  <code>property</code>","text":"<pre><code>best_pipeline: Pipeline\n</code></pre> <p>The best pipeline that is part of the hyperparameter optimization.</p> <p>This is always based on the validation data you have passed even if you have chosen to score the pipeline on other data afterwards.</p> RETURNS DESCRIPTION <code>Pipeline</code> <p>The best pipeline.</p>"},{"location":"reference/hyperopt/latin/#getml.hyperopt.LatinHypercubeSearch.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Name of the hyperparameter optimization. This is used to uniquely identify it on the engine.</p> RETURNS DESCRIPTION <code>str</code> <p>The name of the hyperparameter optimization.</p>"},{"location":"reference/hyperopt/latin/#getml.hyperopt.LatinHypercubeSearch.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Returns the ID of the hyperparameter optimization. The name property is kept for backward compatibility.</p> RETURNS DESCRIPTION <code>str</code> <p>The name of the hyperparameter optimization.</p>"},{"location":"reference/hyperopt/latin/#getml.hyperopt.LatinHypercubeSearch.score","title":"score  <code>property</code>","text":"<pre><code>score: str\n</code></pre> <p>The score to be optimized.</p> RETURNS DESCRIPTION <code>str</code> <p>The score to be optimized.</p>"},{"location":"reference/hyperopt/latin/#getml.hyperopt.LatinHypercubeSearch.type","title":"type  <code>property</code>","text":"<pre><code>type: str\n</code></pre> <p>The algorithm used for the hyperparameter optimization.</p> RETURNS DESCRIPTION <code>str</code> <p>The algorithm used for the hyperparameter optimization.</p>"},{"location":"reference/hyperopt/latin/#getml.hyperopt.LatinHypercubeSearch.clean_up","title":"clean_up","text":"<pre><code>clean_up() -&gt; None\n</code></pre> <p>Deletes all pipelines associated with hyperparameter optimization, but the best pipeline.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def clean_up(self) -&gt; None:\n    \"\"\"\n    Deletes all pipelines associated with hyperparameter optimization,\n    but the best pipeline.\n    \"\"\"\n    best_pipeline = self._best_pipeline_name()\n    names = [obj[\"pipeline_name\"] for obj in self.evaluations]\n    for name in names:\n        if name == best_pipeline:\n            continue\n        if exists(name):\n            delete(name)\n</code></pre>"},{"location":"reference/hyperopt/latin/#getml.hyperopt.LatinHypercubeSearch.fit","title":"fit","text":"<pre><code>fit(\n    container: Union[Container, StarSchema, TimeSeries],\n    train: str = \"train\",\n    validation: str = \"validation\",\n) -&gt; _Hyperopt\n</code></pre> <p>Launches the hyperparameter optimization.</p> PARAMETER DESCRIPTION <code>container</code> <p>The data container used for the hyperparameter tuning.</p> <p> TYPE: <code>Union[Container, StarSchema, TimeSeries]</code> </p> <code>train</code> <p>The name of the subset in 'container' used for training.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'train'</code> </p> <code>validation</code> <p>The name of the subset in 'container' used for validation.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'validation'</code> </p> RETURNS DESCRIPTION <code>_Hyperopt</code> <p>The current instance.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def fit(\n    self,\n    container: Union[Container, StarSchema, TimeSeries],\n    train: str = \"train\",\n    validation: str = \"validation\",\n) -&gt; _Hyperopt:\n    \"\"\"Launches the hyperparameter optimization.\n\n    Args:\n        container:\n            The data container used for the hyperparameter tuning.\n\n        train:\n            The name of the subset in 'container' used for training.\n\n        validation:\n            The name of the subset in 'container' used for validation.\n\n    Returns:\n        The current instance.\n    \"\"\"\n\n    if isinstance(container, (StarSchema, TimeSeries)):\n        container = container.container\n\n    if not isinstance(container, Container):\n        raise TypeError(\n            \"'container' must be a `~getml.data.Container`, \"\n            + \"a `~getml.data.StarSchema` or a `~getml.data.TimeSeries`\"\n        )\n\n    if not isinstance(train, str):\n        raise TypeError(\"\"\"'train' must be a string\"\"\")\n\n    if not isinstance(validation, str):\n        raise TypeError(\"\"\"'validation' must be a string\"\"\")\n\n    self.pipeline.check(container[train])\n\n    population_table_training = container[train].population\n\n    population_table_validation = container[validation].population\n\n    peripheral_tables = _transform_peripheral(\n        container[train].peripheral, self.pipeline.peripheral\n    )\n\n    self._send()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = self.id\n    cmd[\"type_\"] = \"Hyperopt.launch\"\n\n    cmd[\"population_training_df_\"] = population_table_training._getml_deserialize()\n\n    cmd[\"population_validation_df_\"] = (\n        population_table_validation._getml_deserialize()\n    )\n\n    cmd[\"peripheral_dfs_\"] = [\n        elem._getml_deserialize() for elem in peripheral_tables\n    ]\n\n    with comm.send_and_get_socket(cmd) as sock:\n        begin = time.monotonic()\n        msg = comm.log(sock)\n        end = time.monotonic()\n\n    if msg != \"Success!\":\n        comm.handle_engine_exception(msg)\n\n    print()\n    _print_time_taken(begin, end, \"Time taken: \")\n\n    self._save()\n\n    return self.refresh()\n</code></pre>"},{"location":"reference/hyperopt/latin/#getml.hyperopt.LatinHypercubeSearch.refresh","title":"refresh","text":"<pre><code>refresh() -&gt; _Hyperopt\n</code></pre> <p>Reloads the hyperparameter optimization from the Engine.</p> RETURNS DESCRIPTION <code>_Hyperopt</code> <p>Current instance</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def refresh(self) -&gt; _Hyperopt:\n    \"\"\"Reloads the hyperparameter optimization from the Engine.\n\n    Returns:\n            Current instance\n\n    \"\"\"\n    json_obj = _get_json_obj(self.id)\n    return self._parse_json_obj(json_obj)\n</code></pre>"},{"location":"reference/hyperopt/latin/#getml.hyperopt.LatinHypercubeSearch.validate","title":"validate","text":"<pre><code>validate() -&gt; None\n</code></pre> <p>Validate the parameters of the hyperparameter optimization.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"\n    Validate the parameters of the hyperparameter optimization.\n    \"\"\"\n    _validate_hyperopt(_Hyperopt._supported_params, **self.__dict__)  # type: ignore\n\n    if self.surrogate_burn_in_algorithm != latin_hypercube:\n        raise ValueError(\n            \"'surrogate_burn_in_algorithm' must be '\" + latin_hypercube + \"'.\"\n        )\n\n    if self.ratio_iter != 1.0:\n        raise ValueError(\"'ratio_iter' must be 1.0.\")\n</code></pre>"},{"location":"reference/hyperopt/random/","title":"Random","text":""},{"location":"reference/hyperopt/random/#getml.hyperopt.RandomSearch","title":"RandomSearch","text":"<pre><code>RandomSearch(\n    param_space: Dict[str, Any],\n    pipeline: Pipeline,\n    score: str = metrics.rmse,\n    n_iter: int = 100,\n    seed: int = 5483,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>_Hyperopt</code></p> <p>Uniformly distributed sampling of the hyperparameters.</p> <p>During every iteration, a new set of hyperparameters is chosen at random by uniformly drawing a random value in between the lower and upper bound for each dimension of <code>param_space</code> independently.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> PARAMETER DESCRIPTION <code>param_space</code> <p>Dictionary containing numerical arrays of length two holding the lower and upper bounds of all parameters which will be altered in <code>pipeline</code> during the hyperparameter optimization.</p> <p>If we have two feature learners and one predictor, the hyperparameter space might look like this:</p> <p><pre><code>param_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n</code></pre> If we only want to optimize the predictor, then we can leave out the feature learners.</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>pipeline</code> <p>Base pipeline used to derive all models fitted and scored during the hyperparameter optimization. Be careful in constructing it since only those parameters present in <code>param_space</code> will be overwritten. It defines the data schema and any hyperparameters that are not optimized.</p> <p> TYPE: <code>Pipeline</code> </p> <code>score</code> <p>The score to optimize. Must be from <code>metrics</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>rmse</code> </p> <code>n_iter</code> <p>Number of iterations in the hyperparameter optimization and thus the number of parameter combinations to draw and evaluate. Range: [1, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>seed</code> <p>Seed used for the random number generator that underlies the sampling procedure to make the calculation reproducible. Due to nature of the underlying algorithm this is only the case if the fit is done without multithreading. To reflect this, a <code>seed</code> of None represents an unreproducible and is only allowed to be set to an actual integer if both <code>num_threads</code> and <code>n_jobs</code> instance variables of the <code>predictor</code> and <code>feature_selector</code> in <code>model</code> - if they are instances of either <code>XGBoostRegressor</code> or <code>XGBoostClassifier</code> - are set to 1. Internally, a <code>seed</code> of None will be mapped to 5543. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5483</code> </p> Example <pre><code>from getml import data\nfrom getml import datasets\nfrom getml import engine\nfrom getml import feature_learning\nfrom getml.feature_learning import aggregations\nfrom getml.feature_learning import loss_functions\nfrom getml import hyperopt\nfrom getml import pipeline\nfrom getml import predictors\n\n# ----------------\n\nengine.set_project(\"examples\")\n\n# ----------------\n\npopulation_table, peripheral_table = datasets.make_numerical()\n\n# ----------------\n# Construct placeholders\n\npopulation_placeholder = data.Placeholder(\"POPULATION\")\nperipheral_placeholder = data.Placeholder(\"PERIPHERAL\")\npopulation_placeholder.join(peripheral_placeholder, \"join_key\", \"time_stamp\")\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe1 = feature_learning.Multirel(\n    aggregation=[\n        aggregations.COUNT,\n        aggregations.SUM\n    ],\n    loss_function=loss_functions.SquareLoss,\n    num_features=10,\n    share_aggregations=1.0,\n    max_length=1,\n    num_threads=0\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\nfe2 = feature_learning.Relboost(\n    loss_function=loss_functions.SquareLoss,\n    num_features=10\n)\n\n# ----------------\n# Base model - any parameters not included\n# in param_space will be taken from this.\n\npredictor = predictors.LinearRegression()\n\n# ----------------\n\npipe = pipeline.Pipeline(\n    population=population_placeholder,\n    peripheral=[peripheral_placeholder],\n    feature_learners=[fe1, fe2],\n    predictors=[predictor]\n)\n\n# ----------------\n# Build a hyperparameter space.\n# We have two feature learners and one\n# predictor, so this is how we must\n# construct our hyperparameter space.\n# If we only wanted to optimize the predictor,\n# we could just leave out the feature_learners.\n\nparam_space = {\n    \"feature_learners\": [\n        {\n            \"num_features\": [10, 50],\n        },\n        {\n            \"max_depth\": [1, 10],\n            \"min_num_samples\": [100, 500],\n            \"num_features\": [10, 50],\n            \"reg_lambda\": [0.0, 0.1],\n            \"shrinkage\": [0.01, 0.4]\n        }],\n    \"predictors\": [\n        {\n            \"reg_lambda\": [0.0, 10.0]\n        }\n    ]\n}\n\n# ----------------\n# Wrap a RandomSearch around the reference model\n\nrandom_search = hyperopt.RandomSearch(\n    pipeline=pipe,\n    param_space=param_space,\n    n_iter=30,\n    score=pipeline.metrics.rsquared\n)\n\nrandom_search.fit(\n    population_table_training=population_table,\n    population_table_validation=population_table,\n    peripheral_tables=[peripheral_table]\n)\n</code></pre> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def __init__(\n    self,\n    param_space: Dict[str, Any],\n    pipeline: Pipeline,\n    score: str = metrics.rmse,\n    n_iter: int = 100,\n    seed: int = 5483,\n    **kwargs,\n):\n    super().__init__(\n        param_space=param_space,\n        pipeline=pipeline,\n        score=score,\n        n_iter=n_iter,\n        seed=seed,\n        **kwargs,\n    )\n\n    self._type = \"RandomSearch\"\n\n    self.surrogate_burn_in_algorithm = random\n\n    self.validate()\n</code></pre>"},{"location":"reference/hyperopt/random/#getml.hyperopt.RandomSearch.best_pipeline","title":"best_pipeline  <code>property</code>","text":"<pre><code>best_pipeline: Pipeline\n</code></pre> <p>The best pipeline that is part of the hyperparameter optimization.</p> <p>This is always based on the validation data you have passed even if you have chosen to score the pipeline on other data afterwards.</p> RETURNS DESCRIPTION <code>Pipeline</code> <p>The best pipeline.</p>"},{"location":"reference/hyperopt/random/#getml.hyperopt.RandomSearch.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Name of the hyperparameter optimization. This is used to uniquely identify it on the engine.</p> RETURNS DESCRIPTION <code>str</code> <p>The name of the hyperparameter optimization.</p>"},{"location":"reference/hyperopt/random/#getml.hyperopt.RandomSearch.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Returns the ID of the hyperparameter optimization. The name property is kept for backward compatibility.</p> RETURNS DESCRIPTION <code>str</code> <p>The name of the hyperparameter optimization.</p>"},{"location":"reference/hyperopt/random/#getml.hyperopt.RandomSearch.score","title":"score  <code>property</code>","text":"<pre><code>score: str\n</code></pre> <p>The score to be optimized.</p> RETURNS DESCRIPTION <code>str</code> <p>The score to be optimized.</p>"},{"location":"reference/hyperopt/random/#getml.hyperopt.RandomSearch.type","title":"type  <code>property</code>","text":"<pre><code>type: str\n</code></pre> <p>The algorithm used for the hyperparameter optimization.</p> RETURNS DESCRIPTION <code>str</code> <p>The algorithm used for the hyperparameter optimization.</p>"},{"location":"reference/hyperopt/random/#getml.hyperopt.RandomSearch.clean_up","title":"clean_up","text":"<pre><code>clean_up() -&gt; None\n</code></pre> <p>Deletes all pipelines associated with hyperparameter optimization, but the best pipeline.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def clean_up(self) -&gt; None:\n    \"\"\"\n    Deletes all pipelines associated with hyperparameter optimization,\n    but the best pipeline.\n    \"\"\"\n    best_pipeline = self._best_pipeline_name()\n    names = [obj[\"pipeline_name\"] for obj in self.evaluations]\n    for name in names:\n        if name == best_pipeline:\n            continue\n        if exists(name):\n            delete(name)\n</code></pre>"},{"location":"reference/hyperopt/random/#getml.hyperopt.RandomSearch.fit","title":"fit","text":"<pre><code>fit(\n    container: Union[Container, StarSchema, TimeSeries],\n    train: str = \"train\",\n    validation: str = \"validation\",\n) -&gt; _Hyperopt\n</code></pre> <p>Launches the hyperparameter optimization.</p> PARAMETER DESCRIPTION <code>container</code> <p>The data container used for the hyperparameter tuning.</p> <p> TYPE: <code>Union[Container, StarSchema, TimeSeries]</code> </p> <code>train</code> <p>The name of the subset in 'container' used for training.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'train'</code> </p> <code>validation</code> <p>The name of the subset in 'container' used for validation.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'validation'</code> </p> RETURNS DESCRIPTION <code>_Hyperopt</code> <p>The current instance.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def fit(\n    self,\n    container: Union[Container, StarSchema, TimeSeries],\n    train: str = \"train\",\n    validation: str = \"validation\",\n) -&gt; _Hyperopt:\n    \"\"\"Launches the hyperparameter optimization.\n\n    Args:\n        container:\n            The data container used for the hyperparameter tuning.\n\n        train:\n            The name of the subset in 'container' used for training.\n\n        validation:\n            The name of the subset in 'container' used for validation.\n\n    Returns:\n        The current instance.\n    \"\"\"\n\n    if isinstance(container, (StarSchema, TimeSeries)):\n        container = container.container\n\n    if not isinstance(container, Container):\n        raise TypeError(\n            \"'container' must be a `~getml.data.Container`, \"\n            + \"a `~getml.data.StarSchema` or a `~getml.data.TimeSeries`\"\n        )\n\n    if not isinstance(train, str):\n        raise TypeError(\"\"\"'train' must be a string\"\"\")\n\n    if not isinstance(validation, str):\n        raise TypeError(\"\"\"'validation' must be a string\"\"\")\n\n    self.pipeline.check(container[train])\n\n    population_table_training = container[train].population\n\n    population_table_validation = container[validation].population\n\n    peripheral_tables = _transform_peripheral(\n        container[train].peripheral, self.pipeline.peripheral\n    )\n\n    self._send()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"name_\"] = self.id\n    cmd[\"type_\"] = \"Hyperopt.launch\"\n\n    cmd[\"population_training_df_\"] = population_table_training._getml_deserialize()\n\n    cmd[\"population_validation_df_\"] = (\n        population_table_validation._getml_deserialize()\n    )\n\n    cmd[\"peripheral_dfs_\"] = [\n        elem._getml_deserialize() for elem in peripheral_tables\n    ]\n\n    with comm.send_and_get_socket(cmd) as sock:\n        begin = time.monotonic()\n        msg = comm.log(sock)\n        end = time.monotonic()\n\n    if msg != \"Success!\":\n        comm.handle_engine_exception(msg)\n\n    print()\n    _print_time_taken(begin, end, \"Time taken: \")\n\n    self._save()\n\n    return self.refresh()\n</code></pre>"},{"location":"reference/hyperopt/random/#getml.hyperopt.RandomSearch.refresh","title":"refresh","text":"<pre><code>refresh() -&gt; _Hyperopt\n</code></pre> <p>Reloads the hyperparameter optimization from the Engine.</p> RETURNS DESCRIPTION <code>_Hyperopt</code> <p>Current instance</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def refresh(self) -&gt; _Hyperopt:\n    \"\"\"Reloads the hyperparameter optimization from the Engine.\n\n    Returns:\n            Current instance\n\n    \"\"\"\n    json_obj = _get_json_obj(self.id)\n    return self._parse_json_obj(json_obj)\n</code></pre>"},{"location":"reference/hyperopt/random/#getml.hyperopt.RandomSearch.validate","title":"validate","text":"<pre><code>validate() -&gt; None\n</code></pre> <p>Validate the parameters of the hyperparameter optimization.</p> Source code in <code>getml/hyperopt/hyperopt.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"\n    Validate the parameters of the hyperparameter optimization.\n    \"\"\"\n    _validate_hyperopt(_Hyperopt._supported_params, **self.__dict__)  # type: ignore\n\n    if self.surrogate_burn_in_algorithm != random:\n        raise ValueError(\"'surrogate_burn_in_algorithm' must be '\" + random + \"'.\")\n\n    if self.ratio_iter != 1.0:\n        raise ValueError(\"'ratio_iter' must be 1.0.\")\n</code></pre>"},{"location":"reference/pipeline/","title":"Pipeline","text":""},{"location":"reference/pipeline/#getml.pipeline","title":"getml.pipeline","text":"<p>Contains handlers for all steps involved in a data science project after data preparation:</p> <ul> <li>Automated feature learning</li> <li>Automated feature selection</li> <li>Training and evaluation of machine learning (ML) algorithms</li> <li>Deployment of the fitted models</li> </ul> Example <p>We assume that you have already set up your preprocessors (refer to <code>preprocessors</code>), your feature learners (refer to <code>feature_learning</code>) as well as your feature selectors and predictors (refer to <code>predictors</code>, which can be used for prediction and feature selection).</p> <p>You might also want to refer to <code>DataFrame</code>, <code>View</code>, <code>DataModel</code>, <code>Container</code>, <code>Placeholder</code> and <code>StarSchema</code>.</p> <p>If you want to create features for a time series problem, the easiest way to do so is to use the <code>TimeSeries</code> abstraction.</p> <p>Note that this example is taken from the robot notebook .</p> <pre><code># All rows before row 10500 will be used for training.\nsplit = getml.data.split.time(data_all, \"rowid\", test=10500)\n\ntime_series = getml.data.TimeSeries(\n    population=data_all,\n    time_stamps=\"rowid\",\n    split=split,\n    lagged_targets=False,\n    memory=30,\n)\n\npipe = getml.Pipeline(\n    data_model=time_series.data_model,\n    feature_learners=[...],\n    predictors=...\n)\n\npipe.check(time_series.train)\n\npipe.fit(time_series.train)\n\npipe.score(time_series.test)\n\n# To generate predictions on new data,\n# it is sufficient to use a Container.\n# You don't have to recreate the entire\n# TimeSeries, because the abstract data model\n# is stored in the pipeline.\ncontainer = getml.data.Container(\n    population=population_new,\n)\n\n# Add the data as a peripheral table, for the\n# self-join.\ncontainer.add(population=population_new)\n\npredictions = pipe.predict(container.full)\n</code></pre> Example <p>If your data can be organized in a simple star schema, you can use <code>StarSchema</code>. <code>StarSchema</code> unifies <code>Container</code> and <code>DataModel</code>: Note that this example is taken from the loans notebook .</p> <pre><code># First, we insert our data into a StarSchema.\n# population_train and population_test are either\n# DataFrames or Views. The population table\n# defines the statistical population of your\n# machine learning problem and contains the\n# target variables.\nstar_schema = getml.data.StarSchema(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views.\n# Because this is a star schema,\n# all joins take place on the population\n# table.\nstar_schema.join(\n    trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\nstar_schema.join(\n    order,\n    on=\"account_id\",\n)\n\nstar_schema.join(\n    meta,\n    on=\"account_id\",\n)\n\n# Now you can insert your data model,\n# your preprocessors, feature learners,\n# feature selectors and predictors\n# into the pipeline.\n# Note that the pipeline only knows\n# the abstract data model, but hasn't\n# seen the actual data yet.\npipe = getml.Pipeline(\n    data_model=star_schema.data_model,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=predictor,\n)\n\n# Now, we pass the actual data.\n# This passes 'population_train' and the\n# peripheral tables (meta, order and trans)\n# to the pipeline.\npipe.check(star_schema.train)\n\npipe.fit(star_schema.train)\n\npipe.score(star_schema.test)\n</code></pre> Example <p><code>StarSchema</code> is simpler, but cannot be used for more complex data models. The general approach is to use <code>Container</code> and <code>DataModel</code>:</p> <p><pre><code># First, we insert our data into a Container.\n# population_train and population_test are either\n# DataFrames or Views.\ncontainer = getml.data.Container(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views. They are given\n# aliases, so we can refer to them in the\n# DataModel.\ncontainer.add(\n    meta=meta,\n    order=order,\n    trans=trans\n)\n\n# Freezing makes the container immutable.\n# This is not required, but often a good idea.\ncontainer.freeze()\n\n# The abstract data model is constructed\n# using the DataModel class. A data model\n# does not contain any actual data. It just\n# defines the abstract relational structure.\ndm = getml.data.DataModel(\n    population_train.to_placeholder(\"population\")\n)\n\ndm.add(getml.data.to_placeholder(\n    meta=meta,\n    order=order,\n    trans=trans)\n)\n\ndm.population.join(\n    dm.trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\ndm.population.join(\n    dm.order,\n    on=\"account_id\",\n)\n\ndm.population.join(\n    dm.meta,\n    on=\"account_id\",\n)\n\n# Now you can insert your data model,\n# your preprocessors, feature learners,\n# feature selectors and predictors\n# into the pipeline.\n# Note that the pipeline only knows\n# the abstract data model, but hasn't\n# seen the actual data yet.\npipe = getml.Pipeline(\n    data_model=dm,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=predictor,\n)\n\n# This passes 'population_train' and the\n# peripheral tables (meta, order and trans)\n# to the pipeline.\npipe.check(container.train)\n\npipe.fit(container.train)\n\npipe.score(container.test)\n</code></pre> Technically, you don't actually have to use a <code>Container</code>. You might as well do this (in fact, a <code>Container</code> is just syntactic sugar for this approach):</p> <p><pre><code>pipe.check(\n    population_train,\n    {\"meta\": meta, \"order\": order, \"trans\": trans},\n)\n\npipe.fit(\n    population_train,\n    {\"meta\": meta, \"order\": order, \"trans\": trans},\n)\n\npipe.score(\n    population_test,\n    {\"meta\": meta, \"order\": order, \"trans\": trans},\n)\n</code></pre> Or you could even do this. The order of the peripheral tables can be inferred from the <code>__repr__()</code> method of the pipeline, and it is usually in alphabetical order.</p> <pre><code>pipe.check(\n    population_train,\n    [meta, order, trans],\n)\n\npipe.fit(\n    population_train,\n    [meta, order, trans],\n)\n\npipe.score(\n    population_test,\n    [meta, order, trans],\n)\n</code></pre>"},{"location":"reference/pipeline/#getml.pipeline.delete","title":"delete","text":"<pre><code>delete(name: str) -&gt; None\n</code></pre> <p>If a pipeline named 'name' exists, it is deleted.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the pipeline.</p> <p> TYPE: <code>str</code> </p> Source code in <code>getml/pipeline/helpers2.py</code> <pre><code>def delete(name: str) -&gt; None:\n    \"\"\"\n    If a pipeline named 'name' exists, it is deleted.\n\n    Args:\n        name:\n            Name of the pipeline.\n    \"\"\"\n\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    if exists(name):\n        _make_dummy(name).delete()\n</code></pre>"},{"location":"reference/pipeline/#getml.pipeline.exists","title":"exists","text":"<pre><code>exists(name: str) -&gt; bool\n</code></pre> <p>Returns true if a pipeline named 'name' exists.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the pipeline.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the pipeline exists, False otherwise.</p> Source code in <code>getml/pipeline/helpers2.py</code> <pre><code>def exists(name: str) -&gt; bool:\n    \"\"\"\n    Returns true if a pipeline named 'name' exists.\n\n    Args:\n        name (str):\n            Name of the pipeline.\n\n    Returns:\n            True if the pipeline exists, False otherwise.\n    \"\"\"\n    if not isinstance(name, str):\n        raise TypeError(\"'name' must be of type str\")\n\n    all_pipelines = list_pipelines()\n\n    return name in all_pipelines\n</code></pre>"},{"location":"reference/pipeline/#getml.pipeline.list_pipelines","title":"list_pipelines","text":"<pre><code>list_pipelines() -&gt; List[str]\n</code></pre> <p>Lists all pipelines present in the Engine.</p> <p>Note that this function only lists pipelines which are part of the current project. See <code>set_project</code> for changing projects and <code>pipelines</code> for more details about the lifecycles of the pipelines.</p> <p>To subsequently load one of them, use <code>load</code>.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>List containing the names of all pipelines.</p> Source code in <code>getml/pipeline/helpers2.py</code> <pre><code>def list_pipelines() -&gt; List[str]:\n    \"\"\"Lists all pipelines present in the Engine.\n\n    Note that this function only lists pipelines which are part of the\n    current project. See [`set_project`][getml.engine.set_project] for\n    changing projects and [`pipelines`][getml.pipeline] for more details about\n    the lifecycles of the pipelines.\n\n    To subsequently load one of them, use\n    [`load`][getml.pipeline.load].\n\n    Returns:\n        List containing the names of all pipelines.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"list_pipelines\"\n    cmd[\"name_\"] = \"\"\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.handle_engine_exception(msg)\n        json_str = comm.recv_string(sock)\n\n    return json.loads(json_str)[\"names\"]\n</code></pre>"},{"location":"reference/pipeline/#getml.pipeline.load","title":"load","text":"<pre><code>load(name: str) -&gt; Pipeline\n</code></pre> <p>Loads a pipeline from the getML Engine into Python.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the pipeline to be loaded.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Pipeline</code> <p>Pipeline that is a handler for the pipeline signified by name.</p> Source code in <code>getml/pipeline/helpers2.py</code> <pre><code>def load(name: str) -&gt; Pipeline:\n    \"\"\"Loads a pipeline from the getML Engine into Python.\n\n    Args:\n        name: The name of the pipeline to be loaded.\n\n    Returns:\n        Pipeline that is a handler for the pipeline signified by name.\n    \"\"\"\n\n    return _make_dummy(name).refresh()\n</code></pre>"},{"location":"reference/pipeline/column/","title":"Column","text":""},{"location":"reference/pipeline/column/#getml.pipeline.column.Column","title":"getml.pipeline.column.Column  <code>dataclass</code>","text":"<pre><code>Column(\n    index: int,\n    name: str,\n    marker: str,\n    table: str,\n    target: str,\n    importance: float = np.nan,\n)\n</code></pre> <p>Dataclass that holds data about a single column.</p> PARAMETER DESCRIPTION <code>index</code> <p>The index of the column.</p> <p> TYPE: <code>int</code> </p> <code>name</code> <p>The name of the column.</p> <p> TYPE: <code>str</code> </p> <code>marker</code> <p>The marker of the column.</p> <p> TYPE: <code>str</code> </p> <code>table</code> <p>The table the column is from.</p> <p> TYPE: <code>str</code> </p> <code>target</code> <p>The target the column is associated with.</p> <p> TYPE: <code>str</code> </p> <code>importance</code> <p>The importance of the column.</p> <p> TYPE: <code>float</code> DEFAULT: <code>nan</code> </p>"},{"location":"reference/pipeline/columns/","title":"Columns","text":""},{"location":"reference/pipeline/columns/#getml.pipeline.Columns","title":"getml.pipeline.Columns","text":"<pre><code>Columns(\n    pipeline: str,\n    targets: Sequence[str],\n    peripheral: Sequence[Placeholder],\n    data: Optional[Sequence[Column]] = None,\n)\n</code></pre> <p>Container which holds a pipeline's columns. These include the columns for which importance can be calculated, such as the ones with <code>roles</code> as <code>categorical</code>, <code>numerical</code> and <code>text</code>. The rest of the columns with roles <code>time_stamp</code>, <code>join_key</code>, <code>target</code>, <code>unused_float</code> and <code>unused_string</code> can not have importance of course.</p> <p>Columns can be accessed by name, index or with a NumPy array. The container supports slicing and is sort- and filterable. Further, the container holds global methods to request columns' importances and apply a column selection to data frames provided to the pipeline.</p> PARAMETER DESCRIPTION <code>pipeline</code> <p>The id of the pipeline.</p> <p> TYPE: <code>str</code> </p> <code>targets</code> <p>The names of the targets used for this pipeline.</p> <p> TYPE: <code>Sequence[str]</code> </p> <code>peripheral</code> <p>The abstract representation of peripheral tables used for this pipeline.</p> <p> TYPE: <code>Sequence[Placeholder]</code> </p> <code>data</code> <p>The columns to be stored in the container. If not provided, they are obtained from the Engine.</p> <p> TYPE: <code>Optional[Sequence[Column]]</code> DEFAULT: <code>None</code> </p> Note <p>The container is an iterable. So, in addition to <code>filter</code> you can also use python list comprehensions for filtering.</p> Example <pre><code>all_my_columns = my_pipeline.columns\n\nfirst_column = my_pipeline.columns[0]\n\nall_but_last_10_columns = my_pipeline.columns[:-10]\n\nimportant_columns = [column for column in my_pipeline.columns if\ncolumn.importance &gt; 0.1]\n\nnames, importances = my_pipeline.columns.importances()\n\n# Drops all categorical and numerical columns that are not # in the\ntop 20%. new_container = my_pipeline.columns.select(\n    container, share_selected_columns=0.2,\n)\n</code></pre> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def __init__(\n    self,\n    pipeline: str,\n    targets: Sequence[str],\n    peripheral: Sequence[Placeholder],\n    data: Optional[Sequence[Column]] = None,\n) -&gt; None:\n    if not isinstance(pipeline, str):\n        raise ValueError(\"'pipeline' must be a str.\")\n\n    if not _is_typed_list(targets, str):\n        raise TypeError(\"'targets' must be a list of str.\")\n\n    self.pipeline = pipeline\n\n    self.targets = targets\n\n    self.peripheral = peripheral\n\n    self.peripheral_names = [p.name for p in self.peripheral]\n\n    if data is not None:\n        self.data = data\n    else:\n        self._load_columns()\n</code></pre>"},{"location":"reference/pipeline/columns/#getml.pipeline.Columns.names","title":"names  <code>property</code>","text":"<pre><code>names: List[str]\n</code></pre> <p>Holds the names of a <code>Pipeline</code>'s columns.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>List containing the names.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/columns/#getml.pipeline.Columns.filter","title":"filter","text":"<pre><code>filter(conditional: Callable[[Column], bool]) -&gt; Columns\n</code></pre> <p>Filters the columns container.</p> PARAMETER DESCRIPTION <code>conditional</code> <p>A callable that evaluates to a boolean for a given item.</p> <p> TYPE: <code>Callable[[Column], bool]</code> </p> RETURNS DESCRIPTION <code>Columns</code> <p>A container of filtered Columns.</p> Example <pre><code>important_columns = my_pipeline.columns.filter(lambda column: column.importance &gt; 0.1)\nperipheral_columns = my_pipeline.columns.filter(lambda column: column.marker == \"[PERIPHERAL]\")\n</code></pre> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def filter(self, conditional: Callable[[Column], bool]) -&gt; Columns:\n    \"\"\"\n    Filters the columns container.\n\n    Args:\n        conditional:\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n        A container of filtered Columns.\n\n    ??? example\n        ```python\n        important_columns = my_pipeline.columns.filter(lambda column: column.importance &gt; 0.1)\n        peripheral_columns = my_pipeline.columns.filter(lambda column: column.marker == \"[PERIPHERAL]\")\n        ```\n    \"\"\"\n    columns_filtered = [column for column in self.data if conditional(column)]\n    return self._make_columns(columns_filtered)\n</code></pre>"},{"location":"reference/pipeline/columns/#getml.pipeline.Columns.importances","title":"importances","text":"<pre><code>importances(\n    target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[str_], NDArray[float_]]\n</code></pre> <p>Returns the data for the column importances.</p> <p>Column importances extend the idea of column importances to the columns originally inserted into the pipeline. Each column is assigned an importance value that measures its contribution to the predictive performance. All columns importances add up to 1.</p> <p>The importances can be calculated for columns with <code>roles</code> such as <code>categorical</code>, <code>numerical</code> and <code>text</code>. The rest of the columns with roles <code>time_stamp</code>, <code>join_key</code>, <code>target</code>, <code>unused_float</code> and <code>unused_string</code> can not have importance of course.</p> PARAMETER DESCRIPTION <code>target_num</code> <p>Indicates for which target you want to view the importances. (Pipelines can have more than one target.)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>sort</code> <p>Whether you want the results to be sorted.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>NDArray[str_]</code> <p>The first array contains the names of the columns.</p> <code>NDArray[float_]</code> <p>The second array contains their importances. By definition, all importances add up to 1.</p> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def importances(\n    self, target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n    \"\"\"\n    Returns the data for the column importances.\n\n    Column importances extend the idea of column importances\n    to the columns originally inserted into the pipeline.\n    Each column is assigned an importance value that measures\n    its contribution to the predictive performance. All\n    columns importances add up to 1.\n\n    The importances can be calculated for columns with\n    [`roles`][getml.data.roles] such as [`categorical`][getml.data.roles.categorical],\n    [`numerical`][getml.data.roles.numerical] and [`text`][getml.data.roles.text].\n    The rest of the columns with roles [`time_stamp`][getml.data.roles.time_stamp],\n    [`join_key`][getml.data.roles.join_key], [`target`][getml.data.roles.target],\n    [`unused_float`][getml.data.roles.unused_float] and\n    [`unused_string`][getml.data.roles.unused_string] can not have importance of course.\n\n    Args:\n        target_num:\n            Indicates for which target you want to view the\n            importances.\n            (Pipelines can have more than one target.)\n\n        sort:\n            Whether you want the results to be sorted.\n\n    Returns:\n        The first array contains the names of the columns.\n        The second array contains their importances. By definition, all importances add up to 1.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    descriptions, importances = self._get_column_importances(\n        target_num=target_num, sort=sort\n    )\n\n    # ------------------------------------------------------------\n\n    names = np.asarray(\n        [d[\"marker_\"] + \" \" + d[\"table_\"] + \".\" + d[\"name_\"] for d in descriptions]\n    )\n\n    # ------------------------------------------------------------\n\n    return names, importances\n</code></pre>"},{"location":"reference/pipeline/columns/#getml.pipeline.Columns.select","title":"select","text":"<pre><code>select(\n    container: Union[Container, StarSchema, TimeSeries],\n    share_selected_columns: float = 0.5,\n) -&gt; Container\n</code></pre> <p>Returns a new data container with all insufficiently important columns dropped.</p> PARAMETER DESCRIPTION <code>container</code> <p>The container containing the data you want to use.</p> <p> TYPE: <code>Union[Container, StarSchema, TimeSeries]</code> </p> <code>share_selected_columns</code> <p>The share of columns to keep. Must be between 0.0 and 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> RETURNS DESCRIPTION <code>Container</code> <p>A new container with the columns dropped.</p> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def select(\n    self,\n    container: Union[Container, StarSchema, TimeSeries],\n    share_selected_columns: float = 0.5,\n) -&gt; Container:\n    \"\"\"\n    Returns a new data container with all insufficiently important columns dropped.\n\n    Args:\n        container:\n            The container containing the data you want to use.\n\n        share_selected_columns: The share of columns\n            to keep. Must be between 0.0 and 1.0.\n\n    Returns:\n        A new container with the columns dropped.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if isinstance(container, (StarSchema, TimeSeries)):\n        data = self.select(\n            container.container, share_selected_columns=share_selected_columns\n        )\n        new_container = deepcopy(container)\n        new_container._container = data\n        return new_container\n\n    # ------------------------------------------------------------\n\n    if not isinstance(container, Container):\n        raise TypeError(\n            \"'container' must be a getml.data.Container, \"\n            + \"a getml.data.StarSchema or a getml.data.TimeSeries\"\n        )\n\n    if not isinstance(share_selected_columns, numbers.Real):\n        raise TypeError(\"'share_selected_columns' must be a real number!\")\n\n    if share_selected_columns &lt; 0.0 or share_selected_columns &gt; 1.0:\n        raise ValueError(\"'share_selected_columns' must be between 0 and 1!\")\n\n    # ------------------------------------------------------------\n\n    descriptions, _ = self._get_column_importances(target_num=-1, sort=True)\n\n    # ------------------------------------------------------------\n\n    num_keep = int(np.ceil(share_selected_columns * len(descriptions)))\n\n    keep_columns = descriptions[:num_keep]\n\n    # ------------------------------------------------------------\n\n    subsets = {\n        k: _drop(v, keep_columns, k, POPULATION)\n        for (k, v) in container.subsets.items()\n    }\n\n    peripheral = {\n        k: _drop(v, keep_columns, k, PERIPHERAL)\n        for (k, v) in container.peripheral.items()\n    }\n\n    # ------------------------------------------------------------\n\n    new_container = Container(**subsets)\n    new_container.add(**peripheral)\n    new_container.freeze()\n\n    # ------------------------------------------------------------\n\n    return new_container\n</code></pre>"},{"location":"reference/pipeline/columns/#getml.pipeline.Columns.sort","title":"sort","text":"<pre><code>sort(\n    by: Optional[str] = None,\n    key: Optional[Callable[[Column], Any]] = None,\n    descending: Optional[bool] = None,\n) -&gt; Columns\n</code></pre> <p>Sorts the Columns container. If no arguments are provided the container is sorted by target and name.</p> PARAMETER DESCRIPTION <code>by</code> <p>The name of field to sort by. Possible fields:     - name(s)     - table(s)     - importances(s)</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>key</code> <p>A callable that evaluates to a sort key for a given item.</p> <p> TYPE: <code>Optional[Callable[[Column], Any]]</code> DEFAULT: <code>None</code> </p> <code>descending</code> <p>Whether to sort in descending order.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Columns</code> <p>A container of sorted columns.</p> Example <pre><code>by_importance = my_pipeline.columns.sort(key=lambda column: column.importance)\n</code></pre> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def sort(\n    self,\n    by: Optional[str] = None,\n    key: Optional[Callable[[Column], Any]] = None,\n    descending: Optional[bool] = None,\n) -&gt; Columns:\n    \"\"\"\n    Sorts the Columns container. If no arguments are provided the\n    container is sorted by target and name.\n\n    Args:\n        by:\n            The name of field to sort by. Possible fields:\n                - name(s)\n                - table(s)\n                - importances(s)\n        key:\n            A callable that evaluates to a sort key for a given item.\n        descending:\n            Whether to sort in descending order.\n\n    Returns:\n            A container of sorted columns.\n\n    ??? example\n        ```python\n        by_importance = my_pipeline.columns.sort(key=lambda column: column.importance)\n        ```\n    \"\"\"\n\n    reverse = False if descending is None else descending\n\n    if (by is not None) and (key is not None):\n        raise ValueError(\"Only one of `by` and `key` can be provided.\")\n\n    if key is not None:\n        columns_sorted = sorted(self.data, key=key, reverse=reverse)\n        return self._make_columns(columns_sorted)\n\n    if by is None:\n        columns_sorted = sorted(\n            self.data, key=lambda column: column.name, reverse=reverse\n        )\n        columns_sorted.sort(key=lambda column: column.target)\n        return self._make_columns(columns_sorted)\n\n    if re.match(pattern=\"names?$\", string=by):\n        columns_sorted = sorted(\n            self.data, key=lambda column: column.name, reverse=reverse\n        )\n        return self._make_columns(columns_sorted)\n\n    if re.match(pattern=\"tables?$\", string=by):\n        columns_sorted = sorted(\n            self.data,\n            key=lambda column: column.table,\n        )\n        return self._make_columns(columns_sorted)\n\n    if re.match(pattern=\"importances?$\", string=by):\n        reverse = True if descending is None else descending\n        columns_sorted = sorted(\n            self.data, key=lambda column: column.importance, reverse=reverse\n        )\n        return self._make_columns(columns_sorted)\n\n    raise ValueError(f\"Cannot sort by: {by}.\")\n</code></pre>"},{"location":"reference/pipeline/columns/#getml.pipeline.Columns.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas() -&gt; DataFrame\n</code></pre> <p>Returns all information related to the columns in a pandas data frame.</p> Source code in <code>getml/pipeline/columns.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"Returns all information related to the columns in a pandas data frame.\"\"\"\n\n    names, markers, tables, importances, targets = (\n        self._pivot(field)\n        for field in [\"name\", \"marker\", \"table\", \"importance\", \"target\"]\n    )\n\n    data_frame = pd.DataFrame(index=np.arange(len(self.data)))\n\n    data_frame[\"name\"] = names\n\n    data_frame[\"marker\"] = markers\n\n    data_frame[\"table\"] = tables\n\n    data_frame[\"importance\"] = importances\n\n    data_frame[\"target\"] = targets\n\n    return data_frame\n</code></pre>"},{"location":"reference/pipeline/dialect/","title":"dialect","text":""},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect","title":"getml.pipeline.dialect","text":"<p>SQL dialects that can be used for the generated code.</p> <p>One way to productionize a <code>Pipeline</code> is to transpile its features to production-ready SQL code. This SQL code can be run on standard cloud infrastructure. Please also refer to <code>SQLCode</code>.</p> Example <pre><code>sql_code = my_pipeline.features.to_sql(\n    getml.pipeline.dialect.spark_sql)\n\n# Creates a folder called \"my_pipeline\"\n# which contains the SQL scripts.\nsql_code.save(\"my_pipeline\")\n</code></pre>"},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect.bigquery","title":"bigquery  <code>module-attribute</code>","text":"<pre><code>bigquery = _all_dialects[0]\n</code></pre> <p>BigQuery is a proprietary database system used by the Google Cloud.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p>"},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect.human_readable_sql","title":"human_readable_sql  <code>module-attribute</code>","text":"<pre><code>human_readable_sql = _all_dialects[1]\n</code></pre> <p>SQL that is not meant to be executed, but for interpretation by humans.</p>"},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect.mysql","title":"mysql  <code>module-attribute</code>","text":"<pre><code>mysql = _all_dialects[2]\n</code></pre> <p>MySQL and its fork MariaDB are among the most popular open-source database systems.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p>"},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect.postgres","title":"postgres  <code>module-attribute</code>","text":"<pre><code>postgres = _all_dialects[3]\n</code></pre> <p>The PostgreSQL or postgres dialect is a popular SQL dialect used by PostgreSQL and its many derivatives like Redshift or Greenplum.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p>"},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect.spark_sql","title":"spark_sql  <code>module-attribute</code>","text":"<pre><code>spark_sql = _all_dialects[4]\n</code></pre> <p>Spark SQL is the SQL dialect used by Apache Spark.</p> <p>Apache Spark is an open-source, distributed, in-memory engine for large-scale data processing and a popular choice for productionizing machine learning pipelines.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p>"},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect.sqlite3","title":"sqlite3  <code>module-attribute</code>","text":"<pre><code>sqlite3 = _all_dialects[5]\n</code></pre> <p>SQLite3 is a light-weight and widely used database system.</p> <p>It is recommended for live prediction systems or when the amount of data handled is unlikely to be too large.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p>"},{"location":"reference/pipeline/dialect/#getml.pipeline.dialect.tsql","title":"tsql  <code>module-attribute</code>","text":"<pre><code>tsql = _all_dialects[6]\n</code></pre> <p>TSQL or Transact-SQL is the dialect used by most Microsoft databases.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p>"},{"location":"reference/pipeline/feature/","title":"Feature","text":""},{"location":"reference/pipeline/feature/#getml.pipeline.feature","title":"getml.pipeline.feature","text":"<p>Custom representing a sole feature.</p>"},{"location":"reference/pipeline/feature/#getml.pipeline.feature.Feature","title":"Feature  <code>dataclass</code>","text":"<pre><code>Feature(\n    index: int,\n    name: str,\n    pipeline: str,\n    target: str,\n    targets: Sequence[str],\n    importance: float,\n    correlation: float,\n    sql: SQLString,\n)\n</code></pre> <p>Dataclass that holds data about a single feature.</p> PARAMETER DESCRIPTION <code>index</code> <p>The index of the feature.</p> <p> TYPE: <code>int</code> </p> <code>name</code> <p>The name of the feature.</p> <p> TYPE: <code>str</code> </p> <code>pipeline</code> <p>The pipeline the feature is from.</p> <p> TYPE: <code>str</code> </p> <code>target</code> <p>The target the feature is associated with.</p> <p> TYPE: <code>str</code> </p> <code>targets</code> <p>The targets the feature is associated with.</p> <p> TYPE: <code>Sequence[str]</code> </p> <code>importance</code> <p>The importance of the feature.</p> <p> TYPE: <code>float</code> </p> <code>correlation</code> <p>The correlation of the feature with the target.</p> <p> TYPE: <code>float</code> </p> <code>sql</code> <p>The SQL code of the feature.</p> <p> TYPE: <code>SQLString</code> </p>"},{"location":"reference/pipeline/features/","title":"Features","text":""},{"location":"reference/pipeline/features/#getml.pipeline.Features","title":"getml.pipeline.Features","text":"<pre><code>Features(\n    pipeline: str,\n    targets: Sequence[str],\n    data: Optional[Sequence[Feature]] = None,\n)\n</code></pre> <p>Container which holds a pipeline's features. Features can be accessed by name, index or with a numpy array. The container supports slicing and is sort- and filterable.</p> <p>Further, the container holds global methods to request features' importances, correlations and their respective transpiled sql representation.</p> PARAMETER DESCRIPTION <code>pipeline</code> <p>The name of the pipeline the features are associated with.</p> <p> TYPE: <code>str</code> </p> <code>targets</code> <p>The targets the features are associated with.</p> <p> TYPE: <code>Sequence[str]</code> </p> <code>data</code> <p>The features to be stored in the container.</p> <p> TYPE: <code>Optional[Sequence[Feature]]</code> DEFAULT: <code>None</code> </p> Note <p>The container is an iterable. So, in addition to <code>filter</code> you can also use python list comprehensions for filtering.</p> Example <pre><code>all_my_features = my_pipeline.features\n\nfirst_feature = my_pipeline.features[0]\n\nsecond_feature = my_pipeline.features[\"feature_1_2\"]\n\nall_but_last_10_features = my_pipeline.features[:-10]\n\nimportant_features = [feature for feature in my_pipeline.features if feature.importance &gt; 0.1]\n\nnames, importances = my_pipeline.features.importances()\n\nnames, correlations = my_pipeline.features.correlations()\n\nsql_code = my_pipeline.features.to_sql()\n</code></pre> Source code in <code>getml/pipeline/features.py</code> <pre><code>def __init__(\n    self,\n    pipeline: str,\n    targets: Sequence[str],\n    data: Optional[Sequence[Feature]] = None,\n) -&gt; None:\n    if not isinstance(pipeline, str):\n        raise ValueError(\"'pipeline' must be a str.\")\n\n    if not _is_typed_list(targets, str):\n        raise TypeError(\"'targets' must be a list of str.\")\n\n    self.pipeline = pipeline\n\n    self.targets = targets\n\n    if data is None:\n        self.data = self._load_features()\n\n    else:\n        self.data = list(data)\n</code></pre>"},{"location":"reference/pipeline/features/#getml.pipeline.Features.correlation","title":"correlation  <code>property</code>","text":"<pre><code>correlation: List[float]\n</code></pre> <p>Holds the correlations of a <code>Pipeline</code>'s features.</p> RETURNS DESCRIPTION <code>List[float]</code> <p>List containing the correlations.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/features/#getml.pipeline.Features.importance","title":"importance  <code>property</code>","text":"<pre><code>importance: List[float]\n</code></pre> <p>Holds the correlations of a <code>Pipeline</code>'s features.</p> RETURNS DESCRIPTION <code>List[float]</code> <p>List containing the correlations.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/features/#getml.pipeline.Features.name","title":"name  <code>property</code>","text":"<pre><code>name: List[str]\n</code></pre> <p>Holds the names of a <code>Pipeline</code>'s features.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>List containing the names.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/features/#getml.pipeline.Features.names","title":"names  <code>property</code>","text":"<pre><code>names: List[str]\n</code></pre> <p>Holds the names of a <code>Pipeline</code>'s features.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>List containing the names.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/features/#getml.pipeline.Features.correlations","title":"correlations","text":"<pre><code>correlations(\n    target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[str_], NDArray[float_]]\n</code></pre> <p>Returns the data for the feature correlations, as displayed in the getML Monitor.</p> PARAMETER DESCRIPTION <code>target_num</code> <p>Indicates for which target you want to view the importances. (Pipelines can have more than one target.)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>sort</code> <p>Whether you want the results to be sorted.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>NDArray[str_]</code> <p>The first array contains the names of the features.</p> <code>NDArray[float_]</code> <p>The second array contains the correlations with the target.</p> Source code in <code>getml/pipeline/features.py</code> <pre><code>def correlations(\n    self, target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n    \"\"\"\n    Returns the data for the feature correlations,\n    as displayed in the getML Monitor.\n\n    Args:\n        target_num:\n            Indicates for which target you want to view the\n            importances.\n            (Pipelines can have more than one target.)\n\n        sort:\n            Whether you want the results to be sorted.\n\n    Returns:\n        The first array contains the names of the features.\n        The second array contains the correlations with the target.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.feature_correlations\"\n    cmd[\"name_\"] = self.pipeline\n\n    cmd[\"target_num_\"] = target_num\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.handle_engine_exception(msg)\n        msg = comm.recv_string(sock)\n\n    json_obj = json.loads(msg)\n\n    names = np.asarray(json_obj[\"feature_names_\"])\n    correlations = np.asarray(json_obj[\"feature_correlations_\"])\n\n    assert len(correlations) &lt;= len(names), \"Correlations must be &lt;= names\"\n\n    if hasattr(self, \"data\"):\n        indices = np.asarray(\n            [\n                feature.index\n                for feature in self.data\n                if feature.target == self.targets[target_num]\n                and feature.index &lt; len(correlations)\n            ]\n        )\n\n        names = names[indices]\n        correlations = correlations[indices]\n\n    if not sort:\n        return names, correlations\n\n    indices = np.argsort(np.abs(correlations))[::-1]\n\n    return (names[indices], correlations[indices])\n</code></pre>"},{"location":"reference/pipeline/features/#getml.pipeline.Features.filter","title":"filter","text":"<pre><code>filter(conditional: Callable[[Feature], bool]) -&gt; Features\n</code></pre> <p>Filters the Features container.</p> PARAMETER DESCRIPTION <code>conditional</code> <p>A callable that evaluates to a boolean for a given item.</p> <p> TYPE: <code>Callable[[Feature], bool]</code> </p> RETURNS DESCRIPTION <code>Features</code> <p>A container of filtered Features.</p> Example <pre><code>important_features = my_pipeline.features.filter(lambda feature: feature.importance &gt; 0.1)\ncorrelated_features = my_pipeline.features.filter(lambda feature: feature.correlation &gt; 0.3)\n</code></pre> Source code in <code>getml/pipeline/features.py</code> <pre><code>def filter(self, conditional: Callable[[Feature], bool]) -&gt; Features:\n    \"\"\"\n     Filters the Features container.\n\n    Args:\n        conditional:\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered Features.\n\n    ??? example\n        ```python\n        important_features = my_pipeline.features.filter(lambda feature: feature.importance &gt; 0.1)\n        correlated_features = my_pipeline.features.filter(lambda feature: feature.correlation &gt; 0.3)\n        ```\n    \"\"\"\n    features_filtered = [feature for feature in self.data if conditional(feature)]\n    return Features(self.pipeline, self.targets, data=features_filtered)\n</code></pre>"},{"location":"reference/pipeline/features/#getml.pipeline.Features.importances","title":"importances","text":"<pre><code>importances(\n    target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[str_], NDArray[float_]]\n</code></pre> <p>Returns the data for the feature importances, as displayed in the getML Monitor.</p> PARAMETER DESCRIPTION <code>target_num</code> <p>Indicates for which target you want to view the importances. (Pipelines can have more than one target.)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>sort</code> <p>Whether you want the results to be sorted.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>NDArray[str_]</code> <p>The first array contains the names of the features.</p> <code>NDArray[float_]</code> <p>The second array contains their importances. By definition, all importances add up to 1.</p> Source code in <code>getml/pipeline/features.py</code> <pre><code>def importances(\n    self, target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n    \"\"\"\n    Returns the data for the feature importances,\n    as displayed in the getML Monitor.\n\n    Args:\n        target_num:\n            Indicates for which target you want to view the\n            importances.\n            (Pipelines can have more than one target.)\n\n        sort:\n            Whether you want the results to be sorted.\n\n    Returns:\n        The first array contains the names of the features.\n        The second array contains their importances. By definition, all importances add up to 1.\n\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.feature_importances\"\n    cmd[\"name_\"] = self.pipeline\n\n    cmd[\"target_num_\"] = target_num\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.handle_engine_exception(msg)\n        msg = comm.recv_string(sock)\n\n    json_obj = json.loads(msg)\n\n    names = np.asarray(json_obj[\"feature_names_\"])\n    importances = np.asarray(json_obj[\"feature_importances_\"])\n\n    if hasattr(self, \"data\"):\n        assert len(importances) &lt;= len(names), \"Importances must be &lt;= names\"\n\n        indices = np.asarray(\n            [\n                feature.index\n                for feature in self.data\n                if feature.target == self.targets[target_num]\n                and feature.index &lt; len(importances)\n            ]\n        )\n\n        names = names[indices]\n        importances = importances[indices]\n\n    if not sort:\n        return names, importances\n\n    assert len(importances) &lt;= len(names), \"Must have the same length\"\n\n    indices = np.argsort(importances)[::-1]\n\n    return (names[indices], importances[indices])\n</code></pre>"},{"location":"reference/pipeline/features/#getml.pipeline.Features.sort","title":"sort","text":"<pre><code>sort(\n    by: Optional[str] = None,\n    key: Optional[\n        Callable[[Feature], Union[float, int, str]]\n    ] = None,\n    descending: Optional[bool] = None,\n) -&gt; Features\n</code></pre> <p>Sorts the Features container. If no arguments are provided the container is sorted by target and name.</p> PARAMETER DESCRIPTION <code>by</code> <p>The name of field to sort by. Possible fields:     - name(s)     - correlation(s)     - importances(s)</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>key</code> <p>A callable that evaluates to a sort key for a given item.</p> <p> TYPE: <code>Optional[Callable[[Feature], Union[float, int, str]]]</code> DEFAULT: <code>None</code> </p> <code>descending</code> <p>Whether to sort in descending order.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Features</code> <p>A container of sorted Features.</p> Example <pre><code>by_correlation = my_pipeline.features.sort(by=\"correlation\")\n\nby_importance = my_pipeline.features.sort(key=lambda feature: feature.importance)\n</code></pre> Source code in <code>getml/pipeline/features.py</code> <pre><code>def sort(\n    self,\n    by: Optional[str] = None,\n    key: Optional[\n        Callable[\n            [Feature],\n            Union[\n                float,\n                int,\n                str,\n            ],\n        ]\n    ] = None,\n    descending: Optional[bool] = None,\n) -&gt; Features:\n    \"\"\"\n    Sorts the Features container. If no arguments are provided the\n    container is sorted by target and name.\n\n    Args:\n        by:\n            The name of field to sort by. Possible fields:\n                - name(s)\n                - correlation(s)\n                - importances(s)\n        key:\n            A callable that evaluates to a sort key for a given item.\n        descending:\n            Whether to sort in descending order.\n\n    Returns:\n            A container of sorted Features.\n\n    ??? example\n        ```python\n        by_correlation = my_pipeline.features.sort(by=\"correlation\")\n\n        by_importance = my_pipeline.features.sort(key=lambda feature: feature.importance)\n        ```\n    \"\"\"\n\n    reverse = False if descending is None else descending\n\n    if (by is not None) and (key is not None):\n        raise ValueError(\"Only one of `by` and `key` can be provided.\")\n\n    if key is not None:\n        features_sorted = sorted(self.data, key=key, reverse=reverse)\n        return self._make_features(features_sorted)\n\n    else:\n        if by is None:\n            features_sorted = sorted(\n                self.data, key=lambda feature: feature.index, reverse=reverse\n            )\n            features_sorted.sort(key=lambda feature: feature.target)\n            return self._make_features(features_sorted)\n\n        if re.match(pattern=\"names?$\", string=by):\n            features_sorted = sorted(\n                self.data, key=lambda feature: feature.name, reverse=reverse\n            )\n            return self._make_features(features_sorted)\n\n        if re.match(pattern=\"correlations?$\", string=by):\n            reverse = True if descending is None else descending\n            features_sorted = sorted(\n                self.data,\n                key=lambda feature: abs(feature.correlation),\n                reverse=reverse,\n            )\n            return self._make_features(features_sorted)\n\n        if re.match(pattern=\"importances?$\", string=by):\n            reverse = True if descending is None else descending\n            features_sorted = sorted(\n                self.data,\n                key=lambda feature: feature.importance,\n                reverse=reverse,\n            )\n            return self._make_features(features_sorted)\n\n        raise ValueError(f\"Cannot sort by: {by}.\")\n</code></pre>"},{"location":"reference/pipeline/features/#getml.pipeline.Features.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas() -&gt; DataFrame\n</code></pre> <p>Returns all information related to the features in a pandas data frame.</p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A pandas data frame containing the features' names, importances, correlations, and SQL code.</p> Source code in <code>getml/pipeline/features.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns all information related to the features in a pandas data frame.\n\n    Returns:\n        A pandas data frame containing the features' names, importances, correlations, and SQL code.\n    \"\"\"\n\n    return self._to_pandas()\n</code></pre>"},{"location":"reference/pipeline/features/#getml.pipeline.Features.to_sql","title":"to_sql","text":"<pre><code>to_sql(\n    targets: bool = True,\n    subfeatures: bool = True,\n    dialect: str = sqlite3,\n    schema: Optional[str] = None,\n    nchar_categorical: int = 128,\n    nchar_join_key: int = 128,\n    nchar_text: int = 4096,\n    size_threshold: Optional[int] = 50000,\n) -&gt; SQLCode\n</code></pre> <p>Returns SQL statements visualizing the features.</p> PARAMETER DESCRIPTION <code>targets</code> <p>Whether you want to include the target columns in the main table.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>subfeatures</code> <p>Whether you want to include the code for the subfeatures of a snowflake schema.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dialect</code> <p>The SQL dialect to use. Must be from <code>dialect</code>. Please note that not all dialects are supported in the getML Community edition.</p> <p> TYPE: <code>str</code> DEFAULT: <code>sqlite3</code> </p> <code>schema</code> <p>The schema in which to wrap all generated tables and indices. None for no schema. Not applicable to all dialects. For the BigQuery and MySQL dialects, the schema is identical to the database ID.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>nchar_categorical</code> <p>The maximum number of characters used in the VARCHAR for categorical columns. Not applicable to all dialects.</p> <p> TYPE: <code>int</code> DEFAULT: <code>128</code> </p> <code>nchar_join_key</code> <p>The maximum number of characters used in the VARCHAR for join keys. Not applicable to all dialects.</p> <p> TYPE: <code>int</code> DEFAULT: <code>128</code> </p> <code>nchar_text</code> <p>The maximum number of characters used in the VARCHAR for text columns. Not applicable to all dialects.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4096</code> </p> <code>size_threshold</code> <p>The maximum number of characters to display in a single feature. Displaying extremely complicated features can crash your iPython notebook or lead to unexpectedly high memory consumption, which is why a reasonable upper limit is advantageous. Set to None for no upper limit.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>50000</code> </p> RETURNS DESCRIPTION <code>SQLCode</code> <p>Object representing the features.</p> Example <pre><code>my_pipeline.features.to_sql()\n</code></pre> Note <p>Only fitted pipelines (<code>fit</code>) can hold trained features which can be returned as SQL statements.</p> Note <p>The getML Community edition only supports transpilation to human-readable SQL. Passing 'sqlite3' will also produce human-readable SQL.</p> Source code in <code>getml/pipeline/features.py</code> <pre><code>def to_sql(\n    self,\n    targets: bool = True,\n    subfeatures: bool = True,\n    dialect: str = sqlite3,\n    schema: Optional[str] = None,\n    nchar_categorical: int = 128,\n    nchar_join_key: int = 128,\n    nchar_text: int = 4096,\n    size_threshold: Optional[int] = 50000,\n) -&gt; SQLCode:\n    \"\"\"\n    Returns SQL statements visualizing the features.\n\n    Args:\n        targets:\n            Whether you want to include the target columns\n            in the main table.\n\n        subfeatures:\n            Whether you want to include the code for the\n            subfeatures of a snowflake schema.\n\n        dialect:\n            The SQL dialect to use. Must be from\n            [`dialect`][getml.pipeline.dialect]. Please\n            note that not all dialects are supported\n            in the getML Community edition.\n\n        schema:\n            The schema in which to wrap all generated tables and\n            indices. None for no schema. Not applicable to all dialects.\n            For the BigQuery and MySQL dialects, the schema is identical\n            to the database ID.\n\n        nchar_categorical:\n            The maximum number of characters used in the\n            VARCHAR for categorical columns. Not applicable\n            to all dialects.\n\n        nchar_join_key:\n            The maximum number of characters used in the\n            VARCHAR for join keys. Not applicable\n            to all dialects.\n\n        nchar_text:\n            The maximum number of characters used in the\n            VARCHAR for text columns. Not applicable\n            to all dialects.\n\n        size_threshold:\n            The maximum number of characters to display\n            in a single feature. Displaying extremely\n            complicated features can crash your iPython\n            notebook or lead to unexpectedly high memory\n            consumption, which is why a reasonable\n            upper limit is advantageous. Set to None\n            for no upper limit.\n\n    Returns:\n            Object representing the features.\n\n    ??? example\n        ```python\n        my_pipeline.features.to_sql()\n        ```\n\n    Note:\n        Only fitted pipelines\n        ([`fit`][getml.Pipeline.fit]) can hold trained\n        features which can be returned as SQL statements.\n\n    Note:\n        The getML Community edition only supports\n        transpilation to human-readable SQL. Passing\n        'sqlite3' will also produce human-readable SQL.\n\n    \"\"\"\n\n    if not isinstance(targets, bool):\n        raise TypeError(\"'targets' must be a bool!\")\n\n    if not isinstance(subfeatures, bool):\n        raise TypeError(\"'subfeatures' must be a bool!\")\n\n    if not isinstance(dialect, str):\n        raise TypeError(\"'dialect' must be a string!\")\n\n    if not isinstance(nchar_categorical, int):\n        raise TypeError(\"'nchar_categorical' must be an int!\")\n\n    if not isinstance(nchar_join_key, int):\n        raise TypeError(\"'nchar_join_key' must be an int!\")\n\n    if not isinstance(nchar_text, int):\n        raise TypeError(\"'nchar_text' must be an int!\")\n\n    if dialect not in _all_dialects:\n        raise ValueError(\n            \"'dialect' must from getml.pipeline.dialect, \"\n            + \"meaning that is must be one of the following: \"\n            + str(_all_dialects)\n            + \".\"\n        )\n\n    if size_threshold is not None and not isinstance(size_threshold, int):\n        raise TypeError(\"'size_threshold' must be an int or None!\")\n\n    if size_threshold is not None and size_threshold &lt;= 0:\n        raise ValueError(\"'size_threshold' must be a positive number!\")\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.to_sql\"\n    cmd[\"name_\"] = self.pipeline\n\n    cmd[\"targets_\"] = targets\n    cmd[\"subfeatures_\"] = subfeatures\n    cmd[\"dialect_\"] = dialect\n    cmd[\"schema_\"] = schema or \"\"\n    cmd[\"nchar_categorical_\"] = nchar_categorical\n    cmd[\"nchar_join_key_\"] = nchar_join_key\n    cmd[\"nchar_text_\"] = nchar_text\n\n    if size_threshold is not None:\n        cmd[\"size_threshold_\"] = size_threshold\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            comm.handle_engine_exception(msg)\n        sql = comm.recv_string(sock)\n\n    return SQLCode(sql.split(\"\\n\\n\\n\"), dialect)\n</code></pre>"},{"location":"reference/pipeline/metadata/","title":"Metadata","text":""},{"location":"reference/pipeline/metadata/#getml.pipeline.metadata","title":"getml.pipeline.metadata","text":"<p>Contains the metadata related to the data frames that were originally passed to .fit(...).</p>"},{"location":"reference/pipeline/metadata/#getml.pipeline.metadata.Metadata","title":"Metadata","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Contains the metadata related to a data frame that were originally passed to .fit(...).</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>The name of the data frame.</p> <p> TYPE: <code>str</code> </p> <code>roles</code> <p>The roles of the columns in the data frame.</p> <p> TYPE: <code>Roles</code> </p>"},{"location":"reference/pipeline/metadata/#getml.pipeline.metadata.Metadata.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"reference/pipeline/metadata/#getml.pipeline.metadata.Metadata.roles","title":"roles  <code>instance-attribute</code>","text":"<pre><code>roles: Roles\n</code></pre>"},{"location":"reference/pipeline/metadata/#getml.pipeline.metadata.AllMetadata","title":"AllMetadata","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Contains the metadata related to all the data frames that were originally passed to .fit(...).</p> ATTRIBUTE DESCRIPTION <code>peripheral</code> <p>The metadata of the peripheral tables.</p> <p> TYPE: <code>List[Metadata]</code> </p> <code>population</code> <p>The metadata of the population table.</p> <p> TYPE: <code>Metadata</code> </p>"},{"location":"reference/pipeline/metadata/#getml.pipeline.metadata.AllMetadata.peripheral","title":"peripheral  <code>instance-attribute</code>","text":"<pre><code>peripheral: List[Metadata]\n</code></pre>"},{"location":"reference/pipeline/metadata/#getml.pipeline.metadata.AllMetadata.population","title":"population  <code>instance-attribute</code>","text":"<pre><code>population: Metadata\n</code></pre>"},{"location":"reference/pipeline/metrics/","title":"metrics","text":""},{"location":"reference/pipeline/metrics/#getml.pipeline.metrics","title":"getml.pipeline.metrics","text":"<p>Signifies different scoring methods.</p>"},{"location":"reference/pipeline/metrics/#getml.pipeline.metrics.auc","title":"auc  <code>module-attribute</code>","text":"<pre><code>auc = _all_metrics[0]\n</code></pre> <p>Area under the curve - refers to the area under the receiver operating characteristic (ROC) curve.</p> <p>Used for classification problems.</p> <p>When handling a classification problem, the ROC curve maps the relationship between two conflicting goals:</p> <p>On the hand, we want a high true positive rate. The true positive rate, sometimes referred to as recall, measures the share of true positive predictions over all positives:</p> \\[ TPR = \\frac{number \\; of \\; true \\; positives}{number \\; of \\; all \\; positives} \\] <p>In other words, we want our classification algorithm to \"catch\" as many positives as possible.</p> <p>On the other hand, we also want a low false positive rate (FPR). The false positive rate measures the share of false positives over all negatives.</p> \\[ FPR = \\frac{number \\; of \\; false \\; positives}{number \\; of \\; all \\; negatives} \\] <p>In other words, we want as few \"false alarms\" as possible.</p> <p>However, unless we have a perfect classifier, these two goals conflict with each other.</p> <p>The ROC curve maps the TPR against the FPR. We now measure the area under said curve (AUC). A higher AUC implies that the trade-off between TPR and FPR is more beneficial. A perfect model would have an AUC of 1. An AUC of 0.5 implies that the model has no predictive value.</p>"},{"location":"reference/pipeline/metrics/#getml.pipeline.metrics.accuracy","title":"accuracy  <code>module-attribute</code>","text":"<pre><code>accuracy = _all_metrics[1]\n</code></pre> <p>Accuracy - measures the share of accurate predictions as of total samples in the testing set.</p> <p>Used for classification problems.</p> \\[ accuracy = \\frac{number \\; of \\; correct \\; predictions}{number \\; of \\; all \\; predictions} \\] <p>The number of correct predictions depends on the threshold used: For instance, we could interpret all predictions for which the probability is greater than 0.5 as a positive and all others as a negative. But we do not have to use a threshold of 0.5 - we might as well use any other threshold. Which threshold we choose will impact the calculated accuracy.</p> <p>When calculating the accuracy, the value returned is the accuracy returned by the best threshold.</p> <p>Even though accuracy is the most intuitive way to measure a classification algorithm, it can also be very misleading when the samples are very skewed. For instance, if only 2% of the samples are positive, a predictor that always predicts negative outcomes will have an accuracy of 98%. This sounds very good to the layman, but the predictor in this example actually has no predictive value.</p>"},{"location":"reference/pipeline/metrics/#getml.pipeline.metrics.cross_entropy","title":"cross_entropy  <code>module-attribute</code>","text":"<pre><code>cross_entropy = _all_metrics[2]\n</code></pre> <p>Cross entropy, also referred to as log-loss, is a measure of the likelihood of the classification model.</p> <p>Used for classification problems.</p> <p>Mathematically speaking, cross-entropy for a binary classification problem is defined as follows:</p> \\[ cross \\; entropy = - \\frac{1}{N} \\sum_{i}^{N} (y_i \\log p_i + (1 - y_i) \\log(1 - p_i), \\] <p>where \\(p_i\\) is the probability of a positive outcome as predicted by the classification algorithm and \\(y_i\\) is the target value, which is 1 for a positive outcome and 0 otherwise.</p> <p>There are several ways to justify the use of cross entropy to evaluate classification algorithms. But the most intuitive way is to think of it as a measure of likelihood. When we have a classification algorithm that gives us probabilities, we would like to know how likely it is that we observe a particular state of the world given the probabilities.</p> <p>We can calculate this likelihood as follows:</p> \\[ likelihood = \\prod_{i}^{N} (p_i^{y_i} * (1 - p_i)^{1 - y_i}). \\] <p>(Recall that \\(y_i\\) can only be 0 or 1.)</p> <p>If we take the logarithm of the likelihood as defined above, divide by \\(N\\) and then multiply by <code>-1</code> (because we want lower to mean better and 0 to mean perfect), the outcome will be cross entropy.</p>"},{"location":"reference/pipeline/metrics/#getml.pipeline.metrics.mae","title":"mae  <code>module-attribute</code>","text":"<pre><code>mae = _all_metrics[3]\n</code></pre> <p>Mean Absolute Error - measure of distance between two numerical targets.</p> <p>Used for regression problems.</p> \\[ MAE = \\frac{\\sum_{i=1}^n | \\mathbf{y}_i - \\mathbf{\\hat{y}}_i |}{n}, \\] <p>where \\(\\mathbf{y}_i\\) and \\(\\mathbf{\\hat{y}}_i\\) are the target values or prediction respectively for a particular data sample \\(i\\) (both multidimensional in case of using multiple targets) while \\(n\\) is the number of samples we consider during the scoring.</p>"},{"location":"reference/pipeline/metrics/#getml.pipeline.metrics.rmse","title":"rmse  <code>module-attribute</code>","text":"<pre><code>rmse = _all_metrics[4]\n</code></pre> <p>Root Mean Squared Error - measure of distance between two numerical targets.</p> <p>Used for regression problems.</p> \\[ RMSE = \\sqrt{\\frac{\\sum_{i=1}^n ( \\mathbf{y}_i - \\mathbf{\\hat{y}}_i )^2}{n}}, \\] <p>where \\(\\mathbf{y}_i\\) and \\(\\mathbf{\\hat{y}}_i\\) are the target values or prediction respectively for a particular data sample \\(i\\) (both multidimensional in case of using multiple targets) while \\(n\\) is the number of samples we consider during the scoring.</p>"},{"location":"reference/pipeline/metrics/#getml.pipeline.metrics.rsquared","title":"rsquared  <code>module-attribute</code>","text":"<pre><code>rsquared = _all_metrics[5]\n</code></pre> <p>\\(R^{2}\\) - squared correlation coefficient between predictions and targets.</p> <p>Used for regression problems.</p> <p>\\(R^{2}\\) is defined as follows:</p> \\[ R^{2} = \\frac{(\\sum_{i=1}^n ( y_i - \\bar{y_i} ) *  ( \\hat{y_i} - \\bar{\\hat{y_i}} ))^2 }{\\sum_{i=1}^n ( y_i - \\bar{y_i} )^2 \\sum_{i=1}^n ( \\hat{y_i} - \\bar{\\hat{y_i}} )^2 }, \\] <p>where \\(y_i\\) are the true values, \\(\\hat{y_i}\\) are the predictions and \\(\\bar{...}\\) denotes the mean operator.</p> <p>An \\(R^{2}\\) of 1 implies perfect correlation between the predictions and the targets and an \\(R^{2}\\) of 0 implies no correlation at all.</p>"},{"location":"reference/pipeline/pipeline/","title":"Pipeline","text":""},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline","title":"getml.pipeline.Pipeline","text":"<pre><code>Pipeline(\n    data_model: Optional[DataModel] = None,\n    peripheral: Optional[List[Placeholder]] = None,\n    preprocessors: Optional[\n        Union[\n            CategoryTrimmer,\n            EmailDomain,\n            Imputation,\n            Mapping,\n            Seasonal,\n            Substring,\n            TextFieldSplitter,\n            List[\n                Union[\n                    CategoryTrimmer,\n                    EmailDomain,\n                    Imputation,\n                    Mapping,\n                    Seasonal,\n                    Substring,\n                    TextFieldSplitter,\n                ]\n            ],\n        ]\n    ] = None,\n    feature_learners: Optional[\n        Union[\n            Union[\n                Fastboost,\n                FastProp,\n                Multirel,\n                Relboost,\n                RelMT,\n            ],\n            List[\n                Union[\n                    Fastboost,\n                    FastProp,\n                    Multirel,\n                    Relboost,\n                    RelMT,\n                ]\n            ],\n        ]\n    ] = None,\n    feature_selectors: Optional[\n        Union[\n            Union[\n                LinearRegression,\n                LogisticRegression,\n                XGBoostClassifier,\n                XGBoostRegressor,\n                ScaleGBMClassifier,\n                ScaleGBMRegressor,\n            ],\n            List[\n                Union[\n                    LinearRegression,\n                    LogisticRegression,\n                    XGBoostClassifier,\n                    XGBoostRegressor,\n                    ScaleGBMClassifier,\n                    ScaleGBMRegressor,\n                ]\n            ],\n        ]\n    ] = None,\n    predictors: Optional[\n        Union[\n            LinearRegression,\n            LogisticRegression,\n            XGBoostClassifier,\n            XGBoostRegressor,\n            ScaleGBMClassifier,\n            ScaleGBMRegressor,\n            List[\n                Union[\n                    LinearRegression,\n                    LogisticRegression,\n                    XGBoostClassifier,\n                    XGBoostRegressor,\n                    ScaleGBMClassifier,\n                    ScaleGBMRegressor,\n                ]\n            ],\n        ]\n    ] = None,\n    loss_function: Optional[str] = None,\n    tags: Optional[List[str]] = None,\n    include_categorical: bool = False,\n    share_selected_features: float = 0.5,\n)\n</code></pre> <p>A Pipeline is the main class for feature learning and prediction.</p> PARAMETER DESCRIPTION <code>data_model</code> <p>Abstract representation of the data_model, which defines the abstract relationships between the tables. Required for the feature learners.</p> <p> TYPE: <code>Optional[DataModel]</code> DEFAULT: <code>None</code> </p> <code>peripheral</code> <p>Abstract representations of the additional tables used to augment the information provided in <code>population</code>. These have to be the same objects that were <code>join</code> ed onto the <code>population</code> <code>Placeholder</code>. Their order determines the order of the peripheral <code>DataFrame</code> passed to the 'peripheral_tables' argument in <code>check</code>, <code>fit</code>, <code>predict</code>, <code>score</code>, and <code>transform</code>, if you pass the data frames as a list. If you omit the peripheral placeholders, they will be inferred from the data model and ordered alphabetically.</p> <p> TYPE: <code>Optional[List[Placeholder]]</code> DEFAULT: <code>None</code> </p> <code>preprocessors</code> <p>The preprocessor(s) to be used. Must be from <code>preprocessors</code>. A single preprocessor does not have to be wrapped in a list.</p> <p> TYPE: <code>Optional[Union[CategoryTrimmer, EmailDomain, Imputation, Mapping, Seasonal, Substring, TextFieldSplitter, List[Union[CategoryTrimmer, EmailDomain, Imputation, Mapping, Seasonal, Substring, TextFieldSplitter]]]]</code> DEFAULT: <code>None</code> </p> <code>feature_learners</code> <p>The feature learner(s) to be used. Must be from <code>feature_learning</code>. A single feature learner does not have to be wrapped in a list.</p> <p> TYPE: <code>Optional[Union[Union[Fastboost, FastProp, Multirel, Relboost, RelMT], List[Union[Fastboost, FastProp, Multirel, Relboost, RelMT]]]]</code> DEFAULT: <code>None</code> </p> <code>feature_selectors</code> <p>Predictor(s) used to select the best features. Must be from <code>predictors</code>. A single feature selector does not have to be wrapped in a list. Make sure to also set share_selected_features.</p> <p> TYPE: <code>Optional[Union[Union[LinearRegression, LogisticRegression, XGBoostClassifier, XGBoostRegressor, ScaleGBMClassifier, ScaleGBMRegressor], List[Union[LinearRegression, LogisticRegression, XGBoostClassifier, XGBoostRegressor, ScaleGBMClassifier, ScaleGBMRegressor]]]]</code> DEFAULT: <code>None</code> </p> <code>predictors</code> <p>Predictor(s) used to generate the predictions. If more than one predictor is passed, the predictions generated will be averaged. Must be from <code>predictors</code>. A single predictor does not have to be wrapped in a list.</p> <p> TYPE: <code>Optional[Union[LinearRegression, LogisticRegression, XGBoostClassifier, XGBoostRegressor, ScaleGBMClassifier, ScaleGBMRegressor, List[Union[LinearRegression, LogisticRegression, XGBoostClassifier, XGBoostRegressor, ScaleGBMClassifier, ScaleGBMRegressor]]]]</code> DEFAULT: <code>None</code> </p> <code>loss_function</code> <p>The loss function to use for the feature learners.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>tags</code> <p>Tags exist to help you organize your pipelines. You can add any tags that help you remember what you were trying to do.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> <code>include_categorical</code> <p>Whether you want to pass categorical columns in the population table to the predictor.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>share_selected_features</code> <p>The share of features you want the feature selection to keep. When set to 0.0, then all features will be kept.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> Example <p>We assume that you have already set up your preprocessors (refer to <code>preprocessors</code>), your feature learners (refer to <code>feature_learning</code>) as well as your feature selectors and predictors (refer to <code>predictors</code>, which can be used for prediction and feature selection).</p> <p>You might also want to refer to <code>DataFrame</code>, <code>View</code>, <code>DataModel</code>, <code>Container</code>, <code>Placeholder</code> and <code>StarSchema</code>.</p> <p>If you want to create features for a time series problem, the easiest way to do so is to use the <code>TimeSeries</code> abstraction.</p> <p>Note that this example is taken from the robot notebook .</p> <pre><code># All rows before row 10500 will be used for training.\nsplit = getml.data.split.time(data_all, \"rowid\", test=10500)\n\ntime_series = getml.data.TimeSeries(\n    population=data_all,\n    time_stamps=\"rowid\",\n    split=split,\n    lagged_targets=False,\n    memory=30,\n)\n\npipe = getml.Pipeline(\n    data_model=time_series.data_model,\n    feature_learners=[...],\n    predictors=...\n)\n\npipe.check(time_series.train)\n\npipe.fit(time_series.train)\n\npipe.score(time_series.test)\n\n# To generate predictions on new data,\n# it is sufficient to use a Container.\n# You don't have to recreate the entire\n# TimeSeries, because the abstract data model\n# is stored in the pipeline.\ncontainer = getml.data.Container(\n    population=population_new,\n)\n\n# Add the data as a peripheral table, for the\n# self-join.\ncontainer.add(population=population_new)\n\npredictions = pipe.predict(container.full)\n</code></pre> Example <p>If your data can be organized in a simple star schema, you can use <code>StarSchema</code>. <code>StarSchema</code> unifies <code>Container</code> and <code>DataModel</code>:</p> <p>Note that this example is taken from the loans notebook .</p> <pre><code># First, we insert our data into a StarSchema.\n# population_train and population_test are either\n# DataFrames or Views. The population table\n# defines the statistical population of your\n# machine learning problem and contains the\n# target variables.\nstar_schema = getml.data.StarSchema(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views.\n# Because this is a star schema,\n# all joins take place on the population\n# table.\nstar_schema.join(\n    trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\nstar_schema.join(\n    order,\n    on=\"account_id\",\n)\n\nstar_schema.join(\n    meta,\n    on=\"account_id\",\n)\n\n# Now you can insert your data model,\n# your preprocessors, feature learners,\n# feature selectors and predictors\n# into the pipeline.\n# Note that the pipeline only knows\n# the abstract data model, but hasn't\n# seen the actual data yet.\npipe = getml.Pipeline(\n    data_model=star_schema.data_model,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=predictor,\n)\n\n# Now, we pass the actual data.\n# This passes 'population_train' and the\n# peripheral tables (meta, order and trans)\n# to the pipeline.\npipe.check(star_schema.train)\n\npipe.fit(star_schema.train)\n\npipe.score(star_schema.test)\n</code></pre> Example <p><code>StarSchema</code> is simpler, but cannot be used for more complex data models. The general approach is to use <code>Container</code> and <code>DataModel</code>:</p> <pre><code># First, we insert our data into a Container.\n# population_train and population_test are either\n# DataFrames or Views.\ncontainer = getml.data.Container(\n    train=population_train,\n    test=population_test\n)\n\n# meta, order and trans are either\n# DataFrames or Views. They are given\n# aliases, so we can refer to them in the\n# DataModel.\ncontainer.add(\n    meta=meta,\n    order=order,\n    trans=trans\n)\n\n# Freezing makes the container immutable.\n# This is not required, but often a good idea.\ncontainer.freeze()\n\n# The abstract data model is constructed\n# using the DataModel class. A data model\n# does not contain any actual data. It just\n# defines the abstract relational structure.\ndm = getml.data.DataModel(\n    population_train.to_placeholder(\"population\")\n)\n\ndm.add(getml.data.to_placeholder(\n    meta=meta,\n    order=order,\n    trans=trans)\n)\n\ndm.population.join(\n    dm.trans,\n    on=\"account_id\",\n    time_stamps=(\"date_loan\", \"date\")\n)\n\ndm.population.join(\n    dm.order,\n    on=\"account_id\",\n)\n\ndm.population.join(\n    dm.meta,\n    on=\"account_id\",\n)\n\n# Now you can insert your data model,\n# your preprocessors, feature learners,\n# feature selectors and predictors\n# into the pipeline.\n# Note that the pipeline only knows\n# the abstract data model, but hasn't\n# seen the actual data yet.\npipe = getml.Pipeline(\n    data_model=dm,\n    preprocessors=[mapping],\n    feature_learners=[fast_prop],\n    feature_selectors=[feature_selector],\n    predictors=predictor,\n)\n\n# This passes 'population_train' and the\n# peripheral tables (meta, order and trans)\n# to the pipeline.\npipe.check(container.train)\n\npipe.fit(container.train)\n\npipe.score(container.test)\n</code></pre> <p>Technically, you don't actually have to use a <code>Container</code>. You might as well do this (in fact, a <code>Container</code> is just syntactic sugar for this approach):</p> <p><pre><code>pipe.check(\n    population_train,\n    {\"meta\": meta, \"order\": order, \"trans\": trans},\n)\n\npipe.fit(\n    population_train,\n    {\"meta\": meta, \"order\": order, \"trans\": trans},\n)\n\npipe.score(\n    population_test,\n    {\"meta\": meta, \"order\": order, \"trans\": trans},\n)\n</code></pre> Or you could even do this. The order of the peripheral tables can be inferred from the <code>__repr__</code> method of the pipeline, and it is usually in alphabetical order.</p> <pre><code>pipe.check(\n    population_train,\n    [meta, order, trans],\n)\n\npipe.fit(\n    population_train,\n    [meta, order, trans],\n)\n\npipe.score(\n    population_test,\n    [meta, order, trans],\n)\n</code></pre> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def __init__(\n    self,\n    data_model: Optional[DataModel] = None,\n    peripheral: Optional[List[Placeholder]] = None,\n    preprocessors: Optional[\n        Union[\n            CategoryTrimmer,\n            EmailDomain,\n            Imputation,\n            Mapping,\n            Seasonal,\n            Substring,\n            TextFieldSplitter,\n            List[\n                Union[\n                    CategoryTrimmer,\n                    EmailDomain,\n                    Imputation,\n                    Mapping,\n                    Seasonal,\n                    Substring,\n                    TextFieldSplitter,\n                ]\n            ],\n        ],\n    ] = None,\n    feature_learners: Optional[\n        Union[\n            Union[Fastboost, FastProp, Multirel, Relboost, RelMT],\n            List[Union[Fastboost, FastProp, Multirel, Relboost, RelMT]],\n        ]\n    ] = None,\n    feature_selectors: Optional[\n        Union[\n            Union[\n                LinearRegression,\n                LogisticRegression,\n                XGBoostClassifier,\n                XGBoostRegressor,\n                ScaleGBMClassifier,\n                ScaleGBMRegressor,\n            ],\n            List[\n                Union[\n                    LinearRegression,\n                    LogisticRegression,\n                    XGBoostClassifier,\n                    XGBoostRegressor,\n                    ScaleGBMClassifier,\n                    ScaleGBMRegressor,\n                ]\n            ],\n        ],\n    ] = None,\n    predictors: Optional[\n        Union[\n            LinearRegression,\n            LogisticRegression,\n            XGBoostClassifier,\n            XGBoostRegressor,\n            ScaleGBMClassifier,\n            ScaleGBMRegressor,\n            List[\n                Union[\n                    LinearRegression,\n                    LogisticRegression,\n                    XGBoostClassifier,\n                    XGBoostRegressor,\n                    ScaleGBMClassifier,\n                    ScaleGBMRegressor,\n                ]\n            ],\n        ]\n    ] = None,\n    loss_function: Optional[str] = None,\n    tags: Optional[List[str]] = None,\n    include_categorical: bool = False,\n    share_selected_features: float = 0.5,\n) -&gt; None:\n    data_model = data_model or DataModel(\"population\")\n\n    if not isinstance(data_model, DataModel):\n        raise TypeError(\"'data_model' must be a getml.data.DataModel.\")\n\n    peripheral = peripheral or _infer_peripheral(data_model.population)\n\n    preprocessors = preprocessors or []\n\n    feature_learners = feature_learners or []\n\n    feature_selectors = feature_selectors or []\n\n    predictors = predictors or []\n\n    tags = tags or []\n\n    if not isinstance(preprocessors, list):\n        preprocessors = [preprocessors]\n\n    if not isinstance(feature_learners, list):\n        feature_learners = [feature_learners]\n\n    if not isinstance(feature_selectors, list):\n        feature_selectors = [feature_selectors]\n\n    if not isinstance(predictors, list):\n        predictors = [predictors]\n\n    if not isinstance(peripheral, list):\n        peripheral = [peripheral]\n\n    if not isinstance(tags, list):\n        tags = [tags]\n\n    self._id: str = NOT_FITTED\n\n    self.type = \"Pipeline\"\n\n    loss_function = (\n        loss_function\n        or (\n            [fl.loss_function for fl in feature_learners if fl.loss_function]\n            or [\"SquareLoss\"]\n        )[0]\n    )\n\n    feature_learners = [\n        _handle_loss_function(fl, loss_function) for fl in feature_learners\n    ]\n\n    self.data_model = data_model\n    self.feature_learners = feature_learners\n    self.feature_selectors = feature_selectors\n    self.include_categorical = include_categorical\n    self.loss_function = loss_function\n    self.peripheral = peripheral\n    self.predictors = predictors\n    self.preprocessors = preprocessors\n    self.share_selected_features = share_selected_features\n    self.tags = Tags(tags)\n\n    self._metadata: Optional[AllMetadata] = None\n\n    self._scores: Dict[str, Any] = {}\n\n    self._targets: List[str] = []\n\n    setattr(type(self), \"_supported_params\", list(self.__dict__.keys()))\n\n    self._validate()\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.accuracy","title":"accuracy  <code>property</code>","text":"<pre><code>accuracy: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the accuracy of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p> RETURNS DESCRIPTION <code>Union[float, List[float]]</code> <p>The accuracy of the pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.auc","title":"auc  <code>property</code>","text":"<pre><code>auc: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the auc of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p> RETURNS DESCRIPTION <code>Union[float, List[float]]</code> <p>The auc of the pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.columns","title":"columns  <code>property</code>","text":"<pre><code>columns: Columns\n</code></pre> <p><code>Columns</code> object that can be used to handle information about the original columns utilized by the feature learners.</p> RETURNS DESCRIPTION <code>Columns</code> <p>The columns object.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.cross_entropy","title":"cross_entropy  <code>property</code>","text":"<pre><code>cross_entropy: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the cross entropy of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p> RETURNS DESCRIPTION <code>Union[float, List[float]]</code> <p>The cross entropy of the pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.features","title":"features  <code>property</code>","text":"<pre><code>features: Features\n</code></pre> <p><code>Features</code> object that can be used to handle the features generated by the feature learners.</p> RETURNS DESCRIPTION <code>Features</code> <p>The features object.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.fitted","title":"fitted  <code>property</code>","text":"<pre><code>fitted: bool\n</code></pre> <p>Whether the pipeline has already been fitted.</p> RETURNS DESCRIPTION <code>bool</code> <p>Whether the pipeline has already been fitted.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.mae","title":"mae  <code>property</code>","text":"<pre><code>mae: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the mae of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p> RETURNS DESCRIPTION <code>Union[float, List[float]]</code> <p>The mae of the pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.plots","title":"plots  <code>property</code>","text":"<pre><code>plots: Plots\n</code></pre> <p><code>Plots</code> object that can be used to generate plots like an ROC curve or a lift curve.</p> RETURNS DESCRIPTION <code>Plots</code> <p>The plots object.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>ID of the pipeline. This is used to uniquely identify the pipeline on the Engine.</p> RETURNS DESCRIPTION <code>str</code> <p>The ID of the pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.is_classification","title":"is_classification  <code>property</code>","text":"<pre><code>is_classification: bool\n</code></pre> <p>Whether the pipeline can be used for classification problems.</p> RETURNS DESCRIPTION <code>bool</code> <p>Whether the pipeline can be used for classification problems.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.is_regression","title":"is_regression  <code>property</code>","text":"<pre><code>is_regression: bool\n</code></pre> <p>Whether the pipeline can be used for regression problems.</p> RETURNS DESCRIPTION <code>bool</code> <p>Whether the pipeline can be used for regression problems.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.metadata","title":"metadata  <code>property</code>","text":"<pre><code>metadata: Optional[AllMetadata]\n</code></pre> <p>Contains information on the data frames that were passed to .fit(...). The roles contained therein can be directly passed to existing data frames to correctly reassign the roles of existing columns. If the pipeline has not been fitted, this is None.</p> RETURNS DESCRIPTION <code>Optional[AllMetadata]</code> <p>The metadata of the pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Returns the ID of the pipeline. The name property is kept for backward compatibility.</p> RETURNS DESCRIPTION <code>str</code> <p>The ID of the pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.rmse","title":"rmse  <code>property</code>","text":"<pre><code>rmse: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the rmse of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p> RETURNS DESCRIPTION <code>Union[float, List[float]]</code> <p>The rmse of the pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.rsquared","title":"rsquared  <code>property</code>","text":"<pre><code>rsquared: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the rsquared of the latest scoring run (the last time <code>.score()</code> was called) on the pipeline.</p> <p>For programmatic access use <code>metrics</code>.</p> RETURNS DESCRIPTION <code>Union[float, List[float]]</code> <p>The rsquared of the pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.scores","title":"scores  <code>property</code>","text":"<pre><code>scores: Scores\n</code></pre> <p>Contains all scores generated by <code>score</code></p> RETURNS DESCRIPTION <code>Scores</code> <p>A container that holds the scores for the pipeline.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.scored","title":"scored  <code>property</code>","text":"<pre><code>scored: bool\n</code></pre> <p>Whether the pipeline has been scored.</p> RETURNS DESCRIPTION <code>bool</code> <p>Whether the pipeline has been scored.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.tables","title":"tables  <code>property</code>","text":"<pre><code>tables: Tables\n</code></pre> <p><code>Tables</code> object that can be used to handle information about the original tables utilized by the feature learners.</p> RETURNS DESCRIPTION <code>Tables</code> <p>The tables object.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.targets","title":"targets  <code>property</code>","text":"<pre><code>targets: List[str]\n</code></pre> <p>Contains the names of the targets used for this pipeline.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>The names of the targets.</p>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.check","title":"check","text":"<pre><code>check(\n    population_table: Union[DataFrame, View, Subset],\n    peripheral_tables: Optional[\n        Union[\n            Dict[str, Union[DataFrame, View]],\n            Sequence[Union[DataFrame, View]],\n        ]\n    ] = None,\n) -&gt; Optional[Issues]\n</code></pre> <p>Checks the validity of the data model.</p> PARAMETER DESCRIPTION <code>population_table</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable.</p> <p> TYPE: <code>Union[DataFrame, View, Subset]</code> </p> <code>peripheral_tables</code> <p>Additional tables corresponding to the <code>peripheral</code> <code>Placeholder</code> instance variable. If passed as a list, the order needs to match the order of the corresponding placeholders passed to <code>peripheral</code>.</p> <p>If you pass a <code>Subset</code> to <code>population_table</code>, the peripheral tables from that subset will be used. If you use a <code>Container</code>, <code>StarSchema</code> or <code>TimeSeries</code>, that means you are passing a <code>Subset</code>.</p> <p> TYPE: <code>Optional[Union[Dict[str, Union[DataFrame, View]], Sequence[Union[DataFrame, View]]]]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def check(\n    self,\n    population_table: Union[DataFrame, View, data.Subset],\n    peripheral_tables: Optional[\n        Union[\n            Dict[str, Union[DataFrame, View]],\n            Sequence[Union[DataFrame, View]],\n        ]\n    ] = None,\n) -&gt; Optional[Issues]:\n    \"\"\"\n    Checks the validity of the data model.\n\n    Args:\n        population_table:\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable.\n\n        peripheral_tables:\n            Additional tables corresponding to the ``peripheral``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If passed as a list, the order needs to\n            match the order of the corresponding placeholders passed\n            to ``peripheral``.\n\n            If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n            the peripheral tables from that subset will be used. If you use\n            a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n            or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n            a [`Subset`][getml.data.Subset].\n\n    \"\"\"\n\n    if isinstance(population_table, data.Subset):\n        peripheral_tables = population_table.peripheral\n        population_table = population_table.population\n\n    peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n    _check_df_types(population_table, peripheral_tables)\n\n    temp = copy.deepcopy(self)\n\n    temp._send()\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = temp.type + \".check\"\n    cmd[\"name_\"] = temp.id\n\n    cmd[\"peripheral_dfs_\"] = [df._getml_deserialize() for df in peripheral_tables]\n    cmd[\"population_df_\"] = population_table._getml_deserialize()\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            comm.handle_engine_exception(msg)\n        print(\"Checking data model...\")\n        msg = comm.log(sock)\n        if msg != \"Success!\":\n            comm.handle_engine_exception(msg)\n        print()\n        issues = Issues(comm.recv_issues(sock))\n        if len(issues) == 0:\n            print(\"OK.\")\n        else:\n            print(\n                f\"The pipeline check generated {len(issues.info)} \"\n                + f\"issues labeled INFO and {len(issues.warnings)} \"\n                + \"issues labeled WARNING.\"\n            )\n\n    temp.delete()\n\n    return None if len(issues) == 0 else issues\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.delete","title":"delete","text":"<pre><code>delete() -&gt; None\n</code></pre> <p>Deletes the pipeline from the Engine.</p> Warning <p>You can not undo this action!</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def delete(self) -&gt; None:\n    \"\"\"\n    Deletes the pipeline from the Engine.\n\n    Warning:\n        You can not undo this action!\n    \"\"\"\n    self._check_whether_fitted()\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".delete\"\n    cmd[\"name_\"] = self.id\n    cmd[\"mem_only_\"] = False\n\n    comm.send(cmd)\n\n    self._id = NOT_FITTED\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.deploy","title":"deploy","text":"<pre><code>deploy(deploy: bool) -&gt; None\n</code></pre> <p>Allows a fitted pipeline to be addressable via an HTTP request. See deployment for details.</p> PARAMETER DESCRIPTION <code>deploy</code> <p>If <code>True</code>, the deployment of the pipeline will be triggered.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def deploy(self, deploy: bool) -&gt; None:\n    \"\"\"Allows a fitted pipeline to be addressable via an HTTP request.\n    See [deployment][deployment] for details.\n\n    Args:\n        deploy: If `True`, the deployment of the pipeline\n            will be triggered.\n    \"\"\"\n    self._check_whether_fitted()\n\n    if not isinstance(deploy, bool):\n        raise TypeError(\"'deploy' must be of type bool\")\n\n    self._validate()\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".deploy\"\n    cmd[\"name_\"] = self.id\n    cmd[\"deploy_\"] = deploy\n\n    comm.send(cmd)\n\n    self._save()\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.fit","title":"fit","text":"<pre><code>fit(\n    population_table: Union[DataFrame, View, Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n    validation_table: Optional[\n        Union[DataFrame, View, Subset]\n    ] = None,\n    check: bool = True,\n) -&gt; Pipeline\n</code></pre> <p>Trains the feature learning algorithms, feature selectors and predictors.</p> PARAMETER DESCRIPTION <code>population_table</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable.</p> <p> TYPE: <code>Union[DataFrame, View, Subset]</code> </p> <code>peripheral_tables</code> <p>Additional tables corresponding to the <code>peripheral</code> <code>Placeholder</code> instance variable. If passed as a list, the order needs to match the order of the corresponding placeholders passed to <code>peripheral</code>.</p> <p>If you pass a <code>Subset</code> to <code>population_table</code>, the peripheral tables from that subset will be used. If you use a <code>Container</code>, <code>StarSchema</code> or <code>TimeSeries</code>, that means you are passing a <code>Subset</code>.</p> <p> TYPE: <code>Optional[Union[Sequence[Union[DataFrame, View]], Dict[str, Union[DataFrame, View]]]]</code> DEFAULT: <code>None</code> </p> <code>validation_table</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable. If you are passing a subset, that subset must be derived from the same container as population_table.</p> <p>Only used for early stopping in <code>XGBoostClassifier</code> and <code>XGBoostRegressor</code>.</p> <p> TYPE: <code>Optional[Union[DataFrame, View, Subset]]</code> DEFAULT: <code>None</code> </p> <code>check</code> <p>Whether you want to check the data model before fitting. The checks are equivalent to the checks run by <code>check</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Pipeline</code> <p>The fitted pipeline.</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def fit(\n    self,\n    population_table: Union[DataFrame, View, data.Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n    validation_table: Optional[Union[DataFrame, View, data.Subset]] = None,\n    check: bool = True,\n) -&gt; Pipeline:\n    \"\"\"Trains the feature learning algorithms, feature selectors\n    and predictors.\n\n    Args:\n        population_table:\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable.\n\n        peripheral_tables:\n            Additional tables corresponding to the ``peripheral``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If passed as a list, the order needs to\n            match the order of the corresponding placeholders passed\n            to ``peripheral``.\n\n            If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n            the peripheral tables from that subset will be used. If you use\n            a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n            or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n            a [`Subset`][getml.data.Subset].\n\n        validation_table:\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If you are passing a subset, that subset\n            must be derived from the same container as *population_table*.\n\n            Only used for early stopping in [`XGBoostClassifier`][getml.predictors.XGBoostClassifier]\n            and [`XGBoostRegressor`][getml.predictors.XGBoostRegressor].\n\n        check:\n            Whether you want to check the data model before fitting. The checks are\n            equivalent to the checks run by [`check`][getml.Pipeline.check].\n\n    Returns:\n        The fitted pipeline.\n    \"\"\"\n\n    additional_tags = (\n        [\"container-\" + population_table.container_id]\n        if isinstance(population_table, data.Subset)\n        else []\n    )\n\n    if (\n        isinstance(population_table, data.Subset)\n        and isinstance(validation_table, data.Subset)\n        and validation_table.container_id != population_table.container_id\n    ):\n        raise ValueError(\n            \"The subset used for validation must be from the same container \"\n            + \"as the subset used for training.\"\n        )\n\n    if isinstance(population_table, data.Subset):\n        peripheral_tables = population_table.peripheral\n        population_table = population_table.population\n\n    if isinstance(validation_table, data.Subset):\n        validation_table = validation_table.population\n\n    peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n    _check_df_types(population_table, peripheral_tables)\n\n    if check:\n        warnings = self.check(population_table, peripheral_tables)\n        if warnings:\n            print(\"To see the issues in full, run .check() on the pipeline.\")\n            print()\n\n    self._send(additional_tags)\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = self.type + \".fit\"\n    cmd[\"name_\"] = self.id\n\n    cmd[\"peripheral_dfs_\"] = [df._getml_deserialize() for df in peripheral_tables]\n    cmd[\"population_df_\"] = population_table._getml_deserialize()\n\n    if validation_table is not None:\n        cmd[\"validation_df_\"] = validation_table._getml_deserialize()\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n\n        if msg != \"Found!\":\n            comm.handle_engine_exception(msg)\n\n        begin = time.monotonic()\n\n        msg = comm.log(sock)\n\n        end = time.monotonic()\n\n        if \"Trained\" in msg:\n            print()\n            print(msg)\n            _print_time_taken(begin, end, \"Time taken: \")\n        else:\n            comm.handle_engine_exception(msg)\n\n    self._save()\n\n    return self.refresh()\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.predict","title":"predict","text":"<pre><code>predict(\n    population_table: Union[DataFrame, View, Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n    table_name: str = \"\",\n) -&gt; Union[NDArray[float_], None]\n</code></pre> <p>Forecasts on new, unseen data using the trained <code>predictor</code>.</p> <p>Returns the predictions generated by the pipeline based on <code>population_table</code> and <code>peripheral_tables</code> or writes them into a database named <code>table_name</code>.</p> PARAMETER DESCRIPTION <code>population_table</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable.</p> <p> TYPE: <code>Union[DataFrame, View, Subset]</code> </p> <code>peripheral_tables</code> <p>Additional tables corresponding to the <code>peripheral</code> <code>Placeholder</code> instance variable. If passed as a list, the order needs to match the order of the corresponding placeholders passed to <code>peripheral</code>.</p> <p>If you pass a <code>Subset</code> to <code>population_table</code>, the peripheral tables from that subset will be used. If you use a <code>Container</code>, <code>StarSchema</code> or <code>TimeSeries</code>, that means you are passing a <code>Subset</code>.</p> <p> TYPE: <code>Optional[Union[Sequence[Union[DataFrame, View]], Dict[str, Union[DataFrame, View]]]]</code> DEFAULT: <code>None</code> </p> <code>table_name</code> <p>If not an empty string, the resulting predictions will be written into a table in a <code>database</code>. Refer to Unified import interface for further information.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> RETURNS DESCRIPTION <code>Union[NDArray[float_], None]</code> <p>Resulting predictions provided in an array of the (number of rows in <code>population_table</code>, number of targets in <code>population_table</code>).</p> Note <p>Only fitted pipelines (<code>fit</code>) can be used for prediction.</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def predict(\n    self,\n    population_table: Union[DataFrame, View, data.Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n    table_name: str = \"\",\n) -&gt; Union[NDArray[np.float_], None]:\n    \"\"\"Forecasts on new, unseen data using the trained ``predictor``.\n\n    Returns the predictions generated by the pipeline based on\n    `population_table` and `peripheral_tables` or writes them into\n    a database named `table_name`.\n\n    Args:\n        population_table:\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable.\n\n        peripheral_tables:\n            Additional tables corresponding to the ``peripheral``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If passed as a list, the order needs to\n            match the order of the corresponding placeholders passed\n            to ``peripheral``.\n\n            If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n            the peripheral tables from that subset will be used. If you use\n            a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n            or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n            a [`Subset`][getml.data.Subset].\n\n        table_name:\n            If not an empty string, the resulting predictions will\n            be written into a table in a [`database`][getml.database].\n            Refer to [Unified import interface][importing-data-unified-interface] for further information.\n\n    Returns:\n        Resulting predictions provided in an array of the (number of rows in `population_table`, number of targets in `population_table`).\n\n    Note:\n        Only fitted pipelines\n        ([`fit`][getml.Pipeline.fit]) can be used for\n        prediction.\n\n\n    \"\"\"\n\n    self._check_whether_fitted()\n\n    if isinstance(population_table, data.Subset):\n        peripheral_tables = population_table.peripheral\n        population_table = population_table.population\n\n    peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n    _check_df_types(population_table, peripheral_tables)\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be of type str\")\n\n    self._validate()\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".transform\"\n    cmd[\"name_\"] = self.id\n    cmd[\"http_request_\"] = False\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            comm.handle_engine_exception(msg)\n        y_hat = self._transform(\n            peripheral_tables,\n            population_table,\n            sock,\n            predict=True,\n            table_name=table_name,\n        )\n\n    return y_hat\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.refresh","title":"refresh","text":"<pre><code>refresh() -&gt; Pipeline\n</code></pre> <p>Reloads the pipeline from the Engine.</p> <p>This discards all local changes you have made since the last time you called <code>fit</code>.</p> RETURNS DESCRIPTION <code>Pipeline</code> <p>Current instance</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def refresh(self) -&gt; Pipeline:\n    \"\"\"Reloads the pipeline from the Engine.\n\n    This discards all local changes you have made since the\n    last time you called [`fit`][getml.Pipeline.fit].\n\n    Returns:\n            Current instance\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".refresh\"\n    cmd[\"name_\"] = self.id\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n\n    if msg[0] != \"{\":\n        comm.handle_engine_exception(msg)\n\n    json_obj = json.loads(msg)\n\n    self._parse_json_obj(json_obj)\n\n    return self\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.score","title":"score","text":"<pre><code>score(\n    population_table: Union[DataFrame, View, Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n) -&gt; Scores\n</code></pre> <p>Calculates the performance of the <code>predictor</code>.</p> <p>Returns different scores calculated on <code>population_table</code> and <code>peripheral_tables</code>.</p> PARAMETER DESCRIPTION <code>population_table</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable.</p> <p> TYPE: <code>Union[DataFrame, View, Subset]</code> </p> <code>peripheral_tables</code> <p>Additional tables corresponding to the <code>peripheral</code> <code>Placeholder</code> instance variable. If passed as a list, the order needs to match the order of the corresponding placeholders passed to <code>peripheral</code>.</p> <p>If you pass a <code>Subset</code> to <code>population_table</code>, the peripheral tables from that subset will be used. If you use a <code>Container</code>, <code>StarSchema</code> or <code>TimeSeries</code>, that means you are passing a <code>Subset</code>.</p> <p> TYPE: <code>Optional[Union[Sequence[Union[DataFrame, View]], Dict[str, Union[DataFrame, View]]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Scores</code> <p>The scores of the pipeline.</p> Note <p>Only fitted pipelines (<code>fit</code>) can be scored.</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def score(\n    self,\n    population_table: Union[DataFrame, View, data.Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n) -&gt; Scores:\n    \"\"\"Calculates the performance of the ``predictor``.\n\n    Returns different scores calculated on `population_table` and\n    `peripheral_tables`.\n\n    Args:\n        population_table:\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable.\n\n        peripheral_tables:\n            Additional tables corresponding to the ``peripheral``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If passed as a list, the order needs to\n            match the order of the corresponding placeholders passed\n            to ``peripheral``.\n\n            If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n            the peripheral tables from that subset will be used. If you use\n            a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n            or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n            a [`Subset`][getml.data.Subset].\n\n    Returns:\n        The scores of the pipeline.\n\n    Note:\n        Only fitted pipelines\n        ([`fit`][getml.Pipeline.fit]) can be\n        scored.\n\n    \"\"\"\n\n    self._check_whether_fitted()\n\n    if isinstance(population_table, data.Subset):\n        peripheral_tables = population_table.peripheral\n        population_table = population_table.population\n\n    peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n    _check_df_types(population_table, peripheral_tables)\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".transform\"\n    cmd[\"name_\"] = self.id\n    cmd[\"http_request_\"] = False\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n\n        if msg != \"Found!\":\n            comm.handle_engine_exception(msg)\n\n        self._transform(\n            peripheral_tables, population_table, sock, predict=True, score=True\n        )\n\n        msg = comm.recv_string(sock)\n\n        if msg != \"Success!\":\n            comm.handle_engine_exception(msg)\n\n        scores = comm.recv_string(sock)\n\n        scores = json.loads(scores)\n\n    self.refresh()\n\n    self._save()\n\n    return self.scores\n</code></pre>"},{"location":"reference/pipeline/pipeline/#getml.pipeline.Pipeline.transform","title":"transform","text":"<pre><code>transform(\n    population_table: Union[DataFrame, View, Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n    df_name: str = \"\",\n    table_name: str = \"\",\n) -&gt; Union[DataFrame, NDArray[float_], None]\n</code></pre> <p>Translates new data into the trained features.</p> <p>Transforms the data passed in <code>population_table</code> and <code>peripheral_tables</code> into features, which can be inserted into machine learning models.</p> PARAMETER DESCRIPTION <code>population_table</code> <p>Main table containing the target variable(s) and corresponding to the <code>population</code> <code>Placeholder</code> instance variable.</p> <p> TYPE: <code>Union[DataFrame, View, Subset]</code> </p> <code>peripheral_tables</code> <p>Additional tables corresponding to the <code>peripheral</code> <code>Placeholder</code> instance variable. If passed as a list, the order needs to match the order of the corresponding placeholders passed to <code>peripheral</code>.</p> <p>If you pass a <code>Subset</code> to <code>population_table</code>, the peripheral tables from that subset will be used. If you use a <code>Container</code>, <code>StarSchema</code> or <code>TimeSeries</code>, that means you are passing a <code>Subset</code>.</p> <p> TYPE: <code>Optional[Union[Sequence[Union[DataFrame, View]], Dict[str, Union[DataFrame, View]]]]</code> DEFAULT: <code>None</code> </p> <code>df_name</code> <p>If not an empty string, the resulting features will be written into a newly created DataFrame.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>table_name</code> <p>If not an empty string, the resulting features will be written into a table in a <code>database</code>. Refer to Unified import interface for further information.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> RETURNS DESCRIPTION <code>Union[DataFrame, NDArray[float_], None]</code> <p>The features generated by the pipeline.</p> Example <p>By default, <code>transform</code> returns a <code>ndarray</code>: <pre><code>my_features_array = pipe.transform()\n</code></pre> You can also export your features as a <code>DataFrame</code> by providing the <code>df_name</code> argument: <pre><code>my_features_df = pipe.transform(df_name=\"my_features\")\n</code></pre> Or you can write the results directly into a database: <pre><code>getml.database.connect_odbc(...)\npipe.transform(table_name=\"MY_FEATURES\")\n</code></pre></p> Note <p>Only fitted pipelines (<code>fit</code>) can transform data into features.</p> Source code in <code>getml/pipeline/pipeline.py</code> <pre><code>def transform(\n    self,\n    population_table: Union[DataFrame, View, data.Subset],\n    peripheral_tables: Optional[\n        Union[\n            Sequence[Union[DataFrame, View]],\n            Dict[str, Union[DataFrame, View]],\n        ]\n    ] = None,\n    df_name: str = \"\",\n    table_name: str = \"\",\n) -&gt; Union[DataFrame, NDArray[np.float_], None]:\n    \"\"\"Translates new data into the trained features.\n\n    Transforms the data passed in `population_table` and\n    `peripheral_tables` into features, which can be inserted into\n    machine learning models.\n\n\n    Args:\n        population_table:\n            Main table containing the target variable(s) and\n            corresponding to the ``population``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable.\n\n        peripheral_tables:\n            Additional tables corresponding to the ``peripheral``\n            [`Placeholder`][getml.data.Placeholder] instance\n            variable. If passed as a list, the order needs to\n            match the order of the corresponding placeholders passed\n            to ``peripheral``.\n\n            If you pass a [`Subset`][getml.data.Subset] to `population_table`,\n            the peripheral tables from that subset will be used. If you use\n            a [`Container`][getml.data.Container], [`StarSchema`][getml.data.StarSchema]\n            or [`TimeSeries`][getml.data.TimeSeries], that means you are passing\n            a [`Subset`][getml.data.Subset].\n\n        df_name:\n            If not an empty string, the resulting features will be\n            written into a newly created DataFrame.\n\n        table_name:\n            If not an empty string, the resulting features will\n            be written into a table in a [`database`][getml.database].\n            Refer to [Unified import interface][importing-data-unified-interface] for further information.\n\n    Returns:\n        The features generated by the pipeline.\n\n    ??? example\n        By default, `transform` returns a [`ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html):\n        ```python\n        my_features_array = pipe.transform()\n        ```\n        You can also export your features as a [`DataFrame`][getml.DataFrame]\n        by providing the `df_name` argument:\n        ```python\n        my_features_df = pipe.transform(df_name=\"my_features\")\n        ```\n        Or you can write the results directly into a database:\n        ```python\n        getml.database.connect_odbc(...)\n        pipe.transform(table_name=\"MY_FEATURES\")\n        ```\n\n    Note:\n        Only fitted pipelines\n        ([`fit`][getml.Pipeline.fit]) can transform\n        data into features.\n\n    \"\"\"\n\n    self._check_whether_fitted()\n\n    if isinstance(population_table, data.Subset):\n        peripheral_tables = population_table.peripheral\n        population_table = population_table.population\n\n    peripheral_tables = _transform_peripheral(peripheral_tables, self.peripheral)\n\n    _check_df_types(population_table, peripheral_tables)\n\n    self._validate()\n\n    cmd: Dict[str, Any] = {}\n    cmd[\"type_\"] = self.type + \".transform\"\n    cmd[\"name_\"] = self.id\n    cmd[\"http_request_\"] = False\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Found!\":\n            comm.handle_engine_exception(msg)\n        y_hat = self._transform(\n            peripheral_tables,\n            population_table,\n            sock,\n            df_name=df_name,\n            table_name=table_name,\n        )\n\n    if df_name != \"\":\n        return data.DataFrame(name=df_name).refresh()\n\n    return y_hat\n</code></pre>"},{"location":"reference/pipeline/plots/","title":"Plots","text":""},{"location":"reference/pipeline/plots/#getml.pipeline.Plots","title":"getml.pipeline.Plots","text":"<pre><code>Plots(name: str)\n</code></pre> <p>Custom class for handling the plots generated by the pipeline.</p> PARAMETER DESCRIPTION <code>name</code> <p>The id of the pipeline the plots are associated with.</p> <p> TYPE: <code>str</code> </p> Example <pre><code>recall, precision = my_pipeline.plots.precision_recall_curve()\nfpr, tpr = my_pipeline.plots.roc_curve()\n</code></pre> Source code in <code>getml/pipeline/plots.py</code> <pre><code>def __init__(self, name: str) -&gt; None:\n    if not isinstance(name, str):\n        raise ValueError(\"'name' must be a str.\")\n\n    self.name = name\n</code></pre>"},{"location":"reference/pipeline/plots/#getml.pipeline.Plots.lift_curve","title":"lift_curve","text":"<pre><code>lift_curve(target_num: int = 0) -&gt; Tuple[ndarray, ndarray]\n</code></pre> <p>Returns the data for the lift curve, as displayed in the getML Monitor.</p> <p>This requires that you call <code>score</code> first. The data used for the curve will always be the data from the last time you called <code>score</code>.</p> PARAMETER DESCRIPTION <code>target_num</code> <p>Indicates for which target you want to plot the lift curve. (Pipelines can have more than one target.)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>Tuple[ndarray, ndarray]</code> <p>The first array is the proportion of samples, usually displayed on the x-axis. The second array is the lift, usually displayed on the y-axis.</p> Source code in <code>getml/pipeline/plots.py</code> <pre><code>def lift_curve(self, target_num: int = 0) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Returns the data for the lift curve, as displayed in the getML Monitor.\n\n    This requires that you call\n    [`score`][getml.Pipeline.score] first. The data used\n    for the curve will always be the data from the *last* time\n    you called [`score`][getml.Pipeline.score].\n\n    Args:\n        target_num:\n            Indicates for which target you want to plot the lift\n            curve. (Pipelines can have more than one target.)\n\n    Returns:\n        The first array is the proportion of samples, usually displayed on the x-axis.\n            The second array is the lift, usually displayed on the y-axis.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.lift_curve\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"target_num_\"] = target_num\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.handle_engine_exception(msg)\n        msg = comm.recv_string(sock)\n\n    json_obj = json.loads(msg)\n\n    return (np.asarray(json_obj[\"proportion_\"]), np.asarray(json_obj[\"lift_\"]))\n</code></pre>"},{"location":"reference/pipeline/plots/#getml.pipeline.Plots.precision_recall_curve","title":"precision_recall_curve","text":"<pre><code>precision_recall_curve(\n    target_num: int = 0,\n) -&gt; Tuple[ndarray, ndarray]\n</code></pre> <p>Returns the data for the precision-recall curve, as displayed in the getML Monitor.</p> <p>This requires that you call <code>score</code> first. The data used for the curve will always be the data from the last time you called <code>score</code>.</p> PARAMETER DESCRIPTION <code>target_num</code> <p>Indicates for which target you want to plot the lift curve. (Pipelines can have more than one target.)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>Tuple[ndarray, ndarray]</code> <p>The first array is the recall (a.k.a. true positive rate), usually displayed on the x-axis. The second array is the precision, usually displayed on the y-axis.</p> Source code in <code>getml/pipeline/plots.py</code> <pre><code>def precision_recall_curve(\n    self, target_num: int = 0\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Returns the data for the precision-recall curve, as displayed in the getML\n    Monitor.\n\n    This requires that you call\n    [`score`][getml.Pipeline.score] first. The data used\n    for the curve will always be the data from the *last* time\n    you called [`score`][getml.Pipeline.score].\n\n    Args:\n        target_num:\n            Indicates for which target you want to plot the lift\n            curve. (Pipelines can have more than one target.)\n\n    Returns:\n        The first array is the recall (a.k.a. true positive rate), usually displayed on the x-axis.\n            The second array is the precision, usually displayed on the y-axis.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.precision_recall_curve\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"target_num_\"] = target_num\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.handle_engine_exception(msg)\n        msg = comm.recv_string(sock)\n\n    json_obj = json.loads(msg)\n\n    return (np.asarray(json_obj[\"tpr_\"]), np.asarray(json_obj[\"precision_\"]))\n</code></pre>"},{"location":"reference/pipeline/plots/#getml.pipeline.Plots.roc_curve","title":"roc_curve","text":"<pre><code>roc_curve(target_num: int = 0) -&gt; Tuple[ndarray, ndarray]\n</code></pre> <p>Returns the data for the ROC curve, as displayed in the getML Monitor.</p> <p>This requires that you call <code>score</code> first. The data used for the curve will always be the data from the last time you called <code>score</code>.</p> PARAMETER DESCRIPTION <code>target_num</code> <p>Indicates for which target you want to plot the lift curve. (Pipelines can have more than one target.)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>Tuple[ndarray, ndarray]</code> <p>The first array is the false positive rate, usually displayed on the x-axis. The second array is the true positive rate, usually displayed on the y-axis.</p> Source code in <code>getml/pipeline/plots.py</code> <pre><code>def roc_curve(self, target_num: int = 0) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Returns the data for the ROC curve, as displayed in the getML Monitor.\n\n    This requires that you call\n    [`score`][getml.Pipeline.score] first. The data used\n    for the curve will always be the data from the *last* time\n    you called [`score`][getml.Pipeline.score].\n\n    Args:\n        target_num:\n            Indicates for which target you want to plot the lift\n            curve. (Pipelines can have more than one target.)\n\n    Returns:\n        The first array is the false positive rate, usually displayed on the x-axis.\n            The second array is the true positive rate, usually displayed on the y-axis.\n    \"\"\"\n\n    cmd: Dict[str, Any] = {}\n\n    cmd[\"type_\"] = \"Pipeline.roc_curve\"\n    cmd[\"name_\"] = self.name\n\n    cmd[\"target_num_\"] = target_num\n\n    with comm.send_and_get_socket(cmd) as sock:\n        msg = comm.recv_string(sock)\n        if msg != \"Success!\":\n            comm.handle_engine_exception(msg)\n        msg = comm.recv_string(sock)\n\n    json_obj = json.loads(msg)\n\n    return (np.asarray(json_obj[\"fpr_\"]), np.asarray(json_obj[\"tpr_\"]))\n</code></pre>"},{"location":"reference/pipeline/score/","title":"Score","text":""},{"location":"reference/pipeline/score/#getml.pipeline.score","title":"getml.pipeline.score","text":""},{"location":"reference/pipeline/score/#getml.pipeline.score.Score","title":"Score  <code>dataclass</code>","text":"<pre><code>Score(date_time: datetime, set_used: str, target: str)\n</code></pre> <p>               Bases: <code>ABC</code></p>"},{"location":"reference/pipeline/score/#getml.pipeline.score.ClassificationScore","title":"ClassificationScore  <code>dataclass</code>","text":"<pre><code>ClassificationScore(\n    date_time: datetime,\n    set_used: str,\n    target: str,\n    accuracy: float,\n    auc: float,\n    cross_entropy: float,\n)\n</code></pre> <p>               Bases: <code>Score</code></p> <p>Dataclass that holds data of a scoring run for a classification pipeline.</p> PARAMETER DESCRIPTION <code>accuracy</code> <p>The <code>accuracy</code> of the classification.</p> <p> TYPE: <code>float</code> </p> <code>auc</code> <p>The area under the curve: <code>auc</code>.</p> <p> TYPE: <code>float</code> </p> <code>cross_entropy</code> <p>The <code>cross_entropy</code>.</p> <p> TYPE: <code>float</code> </p>"},{"location":"reference/pipeline/score/#getml.pipeline.score.RegressionScore","title":"RegressionScore  <code>dataclass</code>","text":"<pre><code>RegressionScore(\n    date_time: datetime,\n    set_used: str,\n    target: str,\n    mae: float,\n    rmse: float,\n    rsquared: float,\n)\n</code></pre> <p>               Bases: <code>Score</code></p> <p>Dataclass that holds data of a scoring run for a regression pipeline.</p> PARAMETER DESCRIPTION <code>mae</code> <p>The mean absolute error: <code>mae</code></p> <p> TYPE: <code>float</code> </p> <code>rmse</code> <p>The root mean squared error: <code>rmse</code></p> <p> TYPE: <code>float</code> </p> <code>rsquared</code> <p>The squared correlation coefficient: <code>rsquared</code></p> <p> TYPE: <code>float</code> </p>"},{"location":"reference/pipeline/scores_container/","title":"Scores","text":""},{"location":"reference/pipeline/scores_container/#getml.pipeline.Scores","title":"getml.pipeline.Scores","text":"<pre><code>Scores(\n    data: Sequence[Score], latest: Dict[str, List[float]]\n)\n</code></pre> <p>Container which holds the history of all scores associated with a given pipeline. The container supports slicing and is sort- and filterable.</p> PARAMETER DESCRIPTION <code>data</code> <p>A list of <code>Score</code> objects.</p> <p> TYPE: <code>Sequence[Score]</code> </p> <code>latest</code> <p>A dictionary containing the latest scores for each metric.</p> <p> TYPE: <code>Dict[str, List[float]]</code> </p> Source code in <code>getml/pipeline/scores_container.py</code> <pre><code>def __init__(self, data: Sequence[Score], latest: Dict[str, List[float]]) -&gt; None:\n    self._latest = latest\n\n    self.is_classification = all(\n        isinstance(score, ClassificationScore) for score in data\n    )\n\n    self.is_regression = not self.is_classification\n\n    self.data = data\n\n    self.sets_used = [score.set_used for score in data]\n</code></pre>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.Scores.accuracy","title":"accuracy  <code>property</code>","text":"<pre><code>accuracy: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the <code>accuracy</code> from the latest scoring run.</p>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.Scores.auc","title":"auc  <code>property</code>","text":"<pre><code>auc: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the <code>auc</code> from the latest scoring run.</p>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.Scores.cross_entropy","title":"cross_entropy  <code>property</code>","text":"<pre><code>cross_entropy: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the <code>cross entropy</code> from the latest scoring run.</p>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.Scores.mae","title":"mae  <code>property</code>","text":"<pre><code>mae: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the <code>mae</code> from the latest scoring run.</p> RETURNS DESCRIPTION <code>Union[float, List[float]]</code> <p>The mean absolute error.</p>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.Scores.rmse","title":"rmse  <code>property</code>","text":"<pre><code>rmse: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the <code>rmse</code> from the latest scoring run.</p> RETURNS DESCRIPTION <code>Union[float, List[float]]</code> <p>The root mean squared error.</p>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.Scores.rsquared","title":"rsquared  <code>property</code>","text":"<pre><code>rsquared: Union[float, List[float]]\n</code></pre> <p>A convenience wrapper to retrieve the <code>rsquared</code> from the latest scoring run.</p> RETURNS DESCRIPTION <code>Union[float, List[float]]</code> <p>The squared correlation coefficient.</p>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.Scores.filter","title":"filter","text":"<pre><code>filter(conditional: Callable[[Score], bool]) -&gt; Scores\n</code></pre> <p>Filters the scores container.</p> PARAMETER DESCRIPTION <code>conditional</code> <p>A callable that evaluates to a boolean for a given item.</p> <p> TYPE: <code>Callable[[Score], bool]</code> </p> RETURNS DESCRIPTION <code>Scores</code> <p>A container of filtered scores.</p> Example <pre><code>from datetime import datetime, timedelta\none_week_ago = datetime.today() - timedelta(days=7)\nscores_last_week = pipe.scores.filter(lambda score: score.date_time &gt;= one_week_ago)\n</code></pre> Source code in <code>getml/pipeline/scores_container.py</code> <pre><code>def filter(self, conditional: Callable[[Score], bool]) -&gt; Scores:\n    \"\"\"\n    Filters the scores container.\n\n    Args:\n        conditional:\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered scores.\n\n    ??? example\n        ```python\n        from datetime import datetime, timedelta\n        one_week_ago = datetime.today() - timedelta(days=7)\n        scores_last_week = pipe.scores.filter(lambda score: score.date_time &gt;= one_week_ago)\n        ```\n    \"\"\"\n    scores_filtered = [score for score in self.data if conditional(score)]\n\n    return Scores(scores_filtered, self._latest)\n</code></pre>"},{"location":"reference/pipeline/scores_container/#getml.pipeline.Scores.sort","title":"sort","text":"<pre><code>sort(\n    key: Callable[[Score], Union[float, int, str]],\n    descending: bool = False,\n) -&gt; Scores\n</code></pre> <p>Sorts the scores container.</p> PARAMETER DESCRIPTION <code>key</code> <p>A callable that evaluates to a sort key for a given item.</p> <p> TYPE: <code>Callable[[Score], Union[float, int, str]]</code> </p> <code>descending</code> <p>Whether to sort in descending order.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Scores</code> <p>A container of sorted scores.</p> Example <pre><code>by_auc = pipe.scores.sort(key=lambda score: score.auc)\nmost_recent_first = pipe.scores.sort(key=lambda score: score.date_time, descending=True)\n</code></pre> Source code in <code>getml/pipeline/scores_container.py</code> <pre><code>def sort(\n    self, key: Callable[[Score], Union[float, int, str]], descending: bool = False\n) -&gt; Scores:\n    \"\"\"\n    Sorts the scores container.\n\n    Args:\n        key:\n            A callable that evaluates to a sort key for a given item.\n\n        descending:\n            Whether to sort in descending order.\n\n    Returns:\n            A container of sorted scores.\n\n    ??? example\n        ```python\n        by_auc = pipe.scores.sort(key=lambda score: score.auc)\n        most_recent_first = pipe.scores.sort(key=lambda score: score.date_time, descending=True)\n        ```\n    \"\"\"\n\n    scores_sorted = sorted(self.data, key=key, reverse=descending)\n    return Scores(scores_sorted, self._latest)\n</code></pre>"},{"location":"reference/pipeline/sql_code/","title":"SQLCode","text":""},{"location":"reference/pipeline/sql_code/#getml.pipeline.SQLCode","title":"getml.pipeline.SQLCode","text":"<pre><code>SQLCode(\n    code: Sequence[Union[str, SQLString]],\n    dialect: str = sqlite3,\n)\n</code></pre> <p>Custom class for handling the SQL code of the features generated by the pipeline.</p> PARAMETER DESCRIPTION <code>code</code> <p>The SQL code of the features.</p> <p> TYPE: <code>Sequence[Union[str, SQLString]]</code> </p> <code>dialect</code> <p>The SQL dialect used in the code. Default is 'sqlite3'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>sqlite3</code> </p> Example <pre><code>sql_code = my_pipeline.features.to_sql()\n\n# You can access individual features\n# by index.\nfeature_1_1 = sql_code[0]\n\n# You can also access them by name.\nfeature_1_10 = sql_code[\"FEATURE_1_10\"]\n\n# You can also type the name of\n# a table or column to find all\n# features related to that table\n# or column.\nfeatures = sql_code.find(\"SOME_TABLE\")\n\n# HINT: The generated SQL code always\n# escapes table and column names using\n# quotation marks. So if you want exact\n# matching, you can do this:\nfeatures = sql_code.find('\"SOME_TABLE\"')\n</code></pre> Source code in <code>getml/pipeline/sql_code.py</code> <pre><code>def __init__(\n    self,\n    code: Sequence[Union[str, SQLString]],\n    dialect: str = sqlite3,\n) -&gt; None:\n    if not _is_typed_list(code, str):\n        raise TypeError(\"'code' must be a list of str.\")\n\n    self.code = [SQLString(elem) for elem in code]\n\n    self.dialect = dialect\n\n    self.tables = [\n        _edit_table_name(table_name)\n        for table_name in re.findall(_table_pattern(self.dialect), \"\".join(code))\n    ]\n</code></pre>"},{"location":"reference/pipeline/sql_code/#getml.pipeline.SQLCode.find","title":"find","text":"<pre><code>find(keyword: str) -&gt; SQLCode\n</code></pre> <p>Returns the SQLCode for all features containing the keyword.</p> PARAMETER DESCRIPTION <code>keyword</code> <p>The keyword to be found.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>SQLCode</code> <p>The SQL code for all features containing the keyword.</p> Source code in <code>getml/pipeline/sql_code.py</code> <pre><code>def find(self, keyword: str) -&gt; SQLCode:\n    \"\"\"\n    Returns the SQLCode for all features\n    containing the keyword.\n\n    Args:\n        keyword: The keyword to be found.\n\n    Returns:\n        The SQL code for all features containing the keyword.\n    \"\"\"\n    if not isinstance(keyword, str):\n        raise TypeError(\"'keyword' must be a str.\")\n\n    return SQLCode([elem for elem in self.code if keyword in elem], self.dialect)\n</code></pre>"},{"location":"reference/pipeline/sql_code/#getml.pipeline.SQLCode.save","title":"save","text":"<pre><code>save(\n    fname: str, split: bool = True, remove: bool = False\n) -&gt; None\n</code></pre> <p>Saves the SQL code to a file.</p> PARAMETER DESCRIPTION <code>fname</code> <p>The name of the file or folder (if <code>split==True</code>) in which you want to save the features.</p> <p> TYPE: <code>str</code> </p> <code>split</code> <p>If True, the code will be split into multiple files, one for each feature and saved into a folder <code>fname</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>remove</code> <p>If True, the existing SQL files in <code>fname</code> folder generated previously with the save method will be removed.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>getml/pipeline/sql_code.py</code> <pre><code>def save(self, fname: str, split: bool = True, remove: bool = False) -&gt; None:\n    \"\"\"\n    Saves the SQL code to a file.\n\n    Args:\n        fname:\n            The name of the file or folder (if `split==True`)\n            in which you want to save the features.\n\n        split:\n            If True, the code will be split into multiple files, one for\n            each feature and saved into a folder `fname`.\n\n        remove:\n            If True, the existing SQL files in `fname` folder generated\n            previously with the save method will be removed.\n    \"\"\"\n    if not split:\n        with open(fname, \"w\", encoding=\"utf-8\") as sqlfile:\n            sqlfile.write(str(self))\n        return\n\n    directory = Path(fname)\n\n    if directory.exists():\n        iter_dir = os.listdir(fname)\n\n        pattern = r\"^\\d{4}.*\\_.*\\.sql$\"\n\n        exist_files_path = [fp for fp in iter_dir if re.search(pattern, fp)]\n\n        if not remove and exist_files_path:\n            print(f\"The following files already exist in the directory ({fname}):\")\n            for fp in np.sort(exist_files_path):\n                print(fp)\n            print(\"Please set 'remove=True' to remove them.\")\n            return\n\n        if remove and exist_files_path:\n            for fp in exist_files_path:\n                os.remove(fname + \"/\" + fp)\n\n    directory.mkdir(exist_ok=True)\n\n    for index, code in enumerate(self.code, 1):\n        match = re.search(_table_pattern(self.dialect), str(code))\n        name = _edit_table_name(match.group(1).lower()) if match else \"feature\"\n        name = _edit_windows_filename(name).replace(\".\", \"_\").replace(\"`\", \"\")\n        file_path = directory / f\"{index:04d}_{name}.sql\"\n        with open(file_path, \"w\", encoding=\"utf-8\") as sqlfile:\n            sqlfile.write(str(code))\n</code></pre>"},{"location":"reference/pipeline/sql_code/#getml.pipeline.SQLCode.to_str","title":"to_str","text":"<pre><code>to_str() -&gt; str\n</code></pre> <p>Returns a raw string representation of the SQL code.</p> RETURNS DESCRIPTION <code>str</code> <p>A raw string representation of the SQL code.</p> Source code in <code>getml/pipeline/sql_code.py</code> <pre><code>def to_str(self) -&gt; str:\n    \"\"\"\n    Returns a raw string representation of the SQL code.\n\n    Returns:\n        A raw string representation of the SQL code.\n    \"\"\"\n    return str(self)\n</code></pre>"},{"location":"reference/pipeline/sql_string/","title":"Sql string","text":""},{"location":"reference/pipeline/sql_string/#getml.pipeline.sql_string","title":"getml.pipeline.sql_string","text":"<p>Custom str type that holds SQL Source code.</p>"},{"location":"reference/pipeline/sql_string/#getml.pipeline.sql_string.SQLString","title":"SQLString","text":"<p>               Bases: <code>str</code></p> <p>A custom string type that handles the representation of SQL code strings.</p>"},{"location":"reference/pipeline/table/","title":"Table","text":""},{"location":"reference/pipeline/table/#getml.pipeline.table.Table","title":"getml.pipeline.table.Table  <code>dataclass</code>","text":"<pre><code>Table(\n    name: str, importance: float, target: str, marker: str\n)\n</code></pre> <p>A dataclass that holds data about a single table.</p> PARAMETER DESCRIPTION <code>name</code> <p>The name of the table.</p> <p> TYPE: <code>str</code> </p> <code>importance</code> <p>The importance of the table.</p> <p> TYPE: <code>float</code> </p> <code>target</code> <p>The target of the table.</p> <p> TYPE: <code>str</code> </p> <code>marker</code> <p>The marker of the table.</p> <p> TYPE: <code>str</code> </p>"},{"location":"reference/pipeline/tables/","title":"Tables","text":""},{"location":"reference/pipeline/tables/#getml.pipeline.Tables","title":"getml.pipeline.Tables","text":"<pre><code>Tables(\n    targets: Sequence[str],\n    columns: Columns,\n    data: Optional[Sequence[Table]] = None,\n)\n</code></pre> <p>This container holds a pipeline's tables. These tables are build from the columns for which importances can be calculated. The motivation behind this container is to determine which tables are more important than others.</p> <p>Tables can be accessed by name, index or with a NumPy array. The container supports slicing and can be sorted and filtered. Further, the container holds global methods to request tables' importances.</p> PARAMETER DESCRIPTION <code>targets</code> <p>The targets associated with the pipeline.</p> <p> TYPE: <code>Sequence[str]</code> </p> <code>columns</code> <p>The columns with which the tables are built.</p> <p> TYPE: <code>Columns</code> </p> <code>data</code> <p>A list of <code>Table</code> objects.</p> <p> TYPE: <code>Optional[Sequence[Table]]</code> DEFAULT: <code>None</code> </p> Note <p>The container is an iterable. So, in addition to <code>filter</code> you can also use python list comprehensions for filtering.</p> Example <pre><code>all_my_tables = my_pipeline.tables\nfirst_table = my_pipeline.tables[0]\nall_but_last_10_tables = my_pipeline.tables[:-10]\nimportant_tables = [table for table in my_pipeline.tables if table.importance &gt; 0.1]\nnames, importances = my_pipeline.tables.importances()\n</code></pre> Source code in <code>getml/pipeline/tables.py</code> <pre><code>def __init__(\n    self,\n    targets: Sequence[str],\n    columns: Columns,\n    data: Optional[Sequence[Table]] = None,\n) -&gt; None:\n    self._targets = targets\n    self._columns = columns\n\n    if data is not None:\n        self.data = data\n\n    else:\n        self._load_tables()\n\n    if not (targets and columns) and not data:\n        raise ValueError(\n            \"Missing required arguments. Either provide `targets` &amp; \"\n            \"`columns` or else provide `data`.\"\n        )\n</code></pre>"},{"location":"reference/pipeline/tables/#getml.pipeline.Tables.names","title":"names  <code>property</code>","text":"<pre><code>names: List[str]\n</code></pre> <p>Holds the names of a <code>Pipeline</code>'s tables.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>List containing the names.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/tables/#getml.pipeline.Tables.targets","title":"targets  <code>property</code>","text":"<pre><code>targets: List[str]\n</code></pre> <p>Holds the targets of a <code>Pipeline</code>'s tables.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>List containing the names.</p> Note <p>The order corresponds to the current sorting of the container.</p>"},{"location":"reference/pipeline/tables/#getml.pipeline.Tables.filter","title":"filter","text":"<pre><code>filter(conditional: Callable[[Table], bool]) -&gt; Tables\n</code></pre> <p>Filters the tables container.</p> PARAMETER DESCRIPTION <code>conditional</code> <p>A callable that evaluates to a boolean for a given item.</p> <p> TYPE: <code>Callable[[Table], bool]</code> </p> RETURNS DESCRIPTION <code>Tables</code> <p>A container of filtered tables.</p> Example <pre><code>important_tables = my_pipeline.table.filter(lambda table: table.importance &gt; 0.1)\nperipheral_tables = my_pipeline.tables.filter(lambda table: table.marker == \"[PERIPHERAL]\")\n</code></pre> Source code in <code>getml/pipeline/tables.py</code> <pre><code>def filter(self, conditional: Callable[[Table], bool]) -&gt; Tables:\n    \"\"\"\n    Filters the tables container.\n\n    Args:\n        conditional:\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered tables.\n\n    ??? example\n        ```python\n        important_tables = my_pipeline.table.filter(lambda table: table.importance &gt; 0.1)\n        peripheral_tables = my_pipeline.tables.filter(lambda table: table.marker == \"[PERIPHERAL]\")\n        ```\n    \"\"\"\n    tables_filtered = [table for table in self.data if conditional(table)]\n    return self._make_tables(tables_filtered)\n</code></pre>"},{"location":"reference/pipeline/tables/#getml.pipeline.Tables.importances","title":"importances","text":"<pre><code>importances(\n    target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[str_], NDArray[float_]]\n</code></pre> <p>Returns the importances of tables.</p> <p>Table importances are calculated by summing up the importances of the columns belonging to the tables. Each column is assigned an importance value that measures its contribution to the predictive performance. For each target, the importances add up to 1.</p> PARAMETER DESCRIPTION <code>target_num</code> <p>Indicates for which target you want to view the importances. (Pipelines can have more than one target.)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>sort</code> <p>Whether you want the results to be sorted.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>NDArray[str_]</code> <p>The first array contains the names of the tables.</p> <code>NDArray[float_]</code> <p>The second array contains their importances. By definition, all importances add up to 1.</p> Source code in <code>getml/pipeline/tables.py</code> <pre><code>def importances(\n    self, target_num: int = 0, sort: bool = True\n) -&gt; Tuple[NDArray[np.str_], NDArray[np.float_]]:\n    \"\"\"\n    Returns the importances of tables.\n\n    Table importances are calculated by summing up the importances of the\n    columns belonging to the tables. Each column is assigned an importance\n    value that measures its contribution to the predictive performance. For\n    each target, the importances add up to 1.\n\n    Args:\n        target_num:\n            Indicates for which target you want to view the\n            importances. (Pipelines can have more than one target.)\n\n        sort:\n            Whether you want the results to be sorted.\n\n    Returns:\n        The first array contains the names of the tables.\n        The second array contains their importances. By definition, all importances add up to 1.\n    \"\"\"\n\n    target_name = self._targets[target_num]\n\n    names = np.empty(0, dtype=str)\n    importances = np.empty(0, dtype=float)\n\n    for table in self.data:\n        if table.target == target_name:\n            names = np.append(names, table.name)\n            importances = np.append(importances, table.importance)\n\n    if not sort:\n        return names, importances\n\n    indices = np.argsort(importances)[::-1]\n\n    return (names[indices], importances[indices])\n</code></pre>"},{"location":"reference/pipeline/tables/#getml.pipeline.Tables.sort","title":"sort","text":"<pre><code>sort(\n    by: Optional[str] = None,\n    key: Optional[Callable[[Table], Any]] = None,\n    descending: Optional[bool] = None,\n) -&gt; Tables\n</code></pre> <p>Sorts the Tables container. If no arguments are provided the container is sorted by target and name.</p> PARAMETER DESCRIPTION <code>by</code> <p>The name of field to sort by. Possible fields:     - name(s)     - importances(s)</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>key</code> <p>A callable that evaluates to a sort key for a given item.</p> <p> TYPE: <code>Optional[Callable[[Table], Any]]</code> DEFAULT: <code>None</code> </p> <code>descending</code> <p>Whether to sort in descending order.</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tables</code> <p>A container of sorted tables.</p> Example <pre><code>by_importance = my_pipeline.tables.sort(key=lambda table: table.importance)\n</code></pre> Source code in <code>getml/pipeline/tables.py</code> <pre><code>def sort(\n    self,\n    by: Optional[str] = None,\n    key: Optional[Callable[[Table], Any]] = None,\n    descending: Optional[bool] = None,\n) -&gt; Tables:\n    \"\"\"\n    Sorts the Tables container. If no arguments are provided the\n    container is sorted by target and name.\n\n    Args:\n        by:\n            The name of field to sort by. Possible fields:\n                - name(s)\n                - importances(s)\n        key:\n            A callable that evaluates to a sort key for a given item.\n        descending:\n            Whether to sort in descending order.\n\n    Returns:\n            A container of sorted tables.\n\n    ??? example\n        ```python\n        by_importance = my_pipeline.tables.sort(key=lambda table: table.importance)\n        ```\n    \"\"\"\n\n    reverse = False if descending is None else descending\n\n    if (by is not None) and (key is not None):\n        raise ValueError(\"Only one of `by` and `key` can be provided.\")\n\n    if key is not None:\n        tables_sorted = sorted(self.data, key=key, reverse=reverse)\n        return self._make_tables(tables_sorted)\n\n    if by is None:\n        tables_sorted = sorted(\n            self.data, key=lambda table: table.name, reverse=reverse\n        )\n        tables_sorted.sort(key=lambda table: table.target)\n        return self._make_tables(tables_sorted)\n\n    if re.match(pattern=\"names?$\", string=by):\n        tables_sorted = sorted(\n            self.data, key=lambda table: table.name, reverse=reverse\n        )\n        return self._make_tables(tables_sorted)\n\n    if re.match(pattern=\"importances?$\", string=by):\n        reverse = True if descending is None else descending\n        tables_sorted = sorted(\n            self.data, key=lambda table: table.importance, reverse=reverse\n        )\n        return self._make_tables(tables_sorted)\n\n    raise ValueError(f\"Cannot sort by: {by}.\")\n</code></pre>"},{"location":"reference/pipeline/tables/#getml.pipeline.Tables.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas() -&gt; DataFrame\n</code></pre> <p>Returns all information related to the tables in a pandas DataFrame.</p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A pandas DataFrame containing the tables' names, importances, targets and markers.</p> Source code in <code>getml/pipeline/tables.py</code> <pre><code>def to_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns all information related to the tables in a pandas DataFrame.\n\n    Returns:\n        A pandas DataFrame containing the tables' names, importances, targets and markers.\n    \"\"\"\n\n    data_frame = pd.DataFrame()\n\n    for i, table in enumerate(self.data):\n        data_frame.loc[i, \"name\"] = table.name\n        data_frame.loc[i, \"importance\"] = table.importance\n        data_frame.loc[i, \"target\"] = table.target\n        data_frame.loc[i, \"marker\"] = table.marker\n\n    return data_frame\n</code></pre>"},{"location":"reference/predictors/","title":"Predictors","text":""},{"location":"reference/predictors/#getml.predictors","title":"getml.predictors","text":"<p>The predictor classes defined in this module serve two purposes.</p> <p>First, a predictor can be used as a feature selector in <code>Pipeline</code> to only select the best features generated during the automated feature learning and to get rid of any redundancies.</p> <p>Second, by using it as a predictor, it will be trained on the features of the supplied data set and used to predict unknown results.</p> <p>Every time a new data set is passed to the <code>predict</code> method of one of the models, the raw relational data is interpreted in the data model, which was provided during the construction of the model, transformed into features using the trained feature learning algorithm, and, finally, its target will be predicted using the trained predictor.</p> <p>The algorithms can be grouped according to their finesse and whether you want to use them for a classification or regression problem. For memory intensive applications, the getML Enterprise edition offers predictors with memory mapping .</p> simple sophisticated memory intensive regression <code>LinearRegression</code> <code>XGBoostRegressor</code> <code>ScaleGBMRegressor</code> classification <code>LogisticRegression</code> <code>XGBoostClassifier</code> <code>ScaleGBMClassifier</code> Note <p>All predictors need to be passed to <code>Pipeline</code>.</p>"},{"location":"reference/predictors/linear_regression/","title":"LinearRegression","text":""},{"location":"reference/predictors/linear_regression/#getml.predictors.LinearRegression","title":"getml.predictors.LinearRegression  <code>dataclass</code>","text":"<pre><code>LinearRegression(\n    learning_rate: float = 0.9, reg_lambda: float = 1e-10\n)\n</code></pre> <p>               Bases: <code>_Predictor</code></p> <p>Simple predictor for regression problems.</p> <p>Learns a simple linear relationship using ordinary least squares (OLS) regression:</p> \\[ \\hat{y} = w_0 + w_1 * feature_1 + w_2 * feature_2 + ... \\] <p>The weights are optimized by minimizing the squared loss of the predictions \\(\\hat{y}\\) w.r.t. the target \\(y\\).</p> \\[ L(y,\\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i -\\hat{y}_i)^2 \\] <p>Linear regressions can be trained arithmetically or numerically. Training arithmetically is more accurate, but suffers worse scalability.</p> <p>If you decide to pass categorical features to the <code>LinearRegression</code>, it will be trained numerically. Otherwise, it will be trained arithmetically.</p> PARAMETER DESCRIPTION <code>learning_rate</code> <p>The learning rate used for training numerically (only relevant when categorical features are included). Range: (0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.9</code> </p> <code>reg_lambda</code> <p>L2 regularization parameter. Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-10</code> </p>"},{"location":"reference/predictors/linear_regression/#getml.predictors.LinearRegression.validate","title":"validate","text":"<pre><code>validate(params: Optional[dict] = None)\n</code></pre> <p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> PARAMETER DESCRIPTION <code>params</code> <p>A dictionary containing the parameters to validate. If nothing is passed, the default parameters will be validated.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> Example <pre><code>l = getml.predictors.LinearRegression()\nl.learning_rate = 8.1\nl.validate()\n</code></pre> Note <p>This method is called at end of the <code>__init__</code> constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML Engine.</p> Source code in <code>getml/predictors/linear_regression.py</code> <pre><code>def validate(self, params: Optional[dict] = None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params: A dictionary containing\n            the parameters to validate. If nothing is passed,\n            the default parameters will be validated.\n\n    ??? example\n        ```python\n        l = getml.predictors.LinearRegression()\n        l.learning_rate = 8.1\n        l.validate()\n        ```\n\n    Note:\n        This method is called at end of the `__init__` constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML Engine.\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    _validate_linear_model_parameters(params)\n</code></pre>"},{"location":"reference/predictors/logistic_regression/","title":"LogisticRegression","text":""},{"location":"reference/predictors/logistic_regression/#getml.predictors.LogisticRegression","title":"getml.predictors.LogisticRegression  <code>dataclass</code>","text":"<pre><code>LogisticRegression(\n    learning_rate: float = 0.9, reg_lambda: float = 1e-10\n)\n</code></pre> <p>               Bases: <code>_Predictor</code></p> <p>Simple predictor for classification problems.</p> <p>Learns a simple linear relationship using the sigmoid function:</p> \\[ \\hat{y} = \\sigma(w_0 + w_1 * feature_1 + w_2 * feature_2 + ...) \\] <p>\\(\\sigma\\) denotes the sigmoid function:</p> \\[ \\sigma(z) = \\frac{1}{1 + exp(-z)} \\] <p>The weights are optimized by minimizing the cross entropy loss of the predictions \\(\\hat{y}\\) w.r.t. the targets \\(y\\).</p> \\[ L(\\hat{y},y) = - y*\\log \\hat{y} - (1 - y)*\\log(1 - \\hat{y}) \\] <p>Logistic regressions are always trained numerically.</p> <p>If you decide to pass categorical features: <code>annotating_roles_categorical</code> to the <code>LogisticRegression</code>, it will be trained using the Broyden-Fletcher-Goldfarb-Shannon (BFGS) algorithm. Otherwise, it will be trained using adaptive moments (Adam). BFGS is more accurate, but less scalable than Adam.</p> PARAMETER DESCRIPTION <code>learning_rate</code> <p>The learning rate used for the Adaptive Moments algorithm (only relevant when categorical features are included). Range: (0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.9</code> </p> <code>reg_lambda</code> <p>L2 regularization parameter. Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-10</code> </p>"},{"location":"reference/predictors/logistic_regression/#getml.predictors.LogisticRegression.validate","title":"validate","text":"<pre><code>validate(params: Optional[dict] = None)\n</code></pre> <p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> PARAMETER DESCRIPTION <code>params</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <p>Examples:</p> <pre><code>l = getml.predictors.LogisticRegression()\nl.learning_rate = 20\nl.validate()\n</code></pre> Note <p>This method is called at end of the <code>__init__</code> constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML Engine.</p> Source code in <code>getml/predictors/logistic_regression.py</code> <pre><code>def validate(self, params: Optional[dict] = None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params: A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    Examples:\n        ```python\n        l = getml.predictors.LogisticRegression()\n        l.learning_rate = 20\n        l.validate()\n        ```\n\n    Note:\n        This method is called at end of the `__init__` constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML Engine.\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    _validate_linear_model_parameters(params)\n</code></pre>"},{"location":"reference/predictors/scale_gbm_classifier/","title":"ScaleGBMClassifier","text":""},{"location":"reference/predictors/scale_gbm_classifier/#getml.predictors.ScaleGBMClassifier","title":"getml.predictors.ScaleGBMClassifier  <code>dataclass</code>","text":"<pre><code>ScaleGBMClassifier(\n    colsample_bylevel: float = 1.0,\n    colsample_bytree: float = 1.0,\n    early_stopping_rounds: int = 10,\n    gamma: float = 0.0,\n    goss_a: float = 1.0,\n    goss_b: float = 0.0,\n    learning_rate: float = 0.1,\n    max_depth: int = 3,\n    min_child_weights: float = 1.0,\n    n_estimators: int = 100,\n    n_jobs: int = 1,\n    objective: Literal[\n        \"binary:logistic\"\n    ] = \"binary:logistic\",\n    reg_lambda: float = 1.0,\n    seed: int = 5843,\n)\n</code></pre> <p>               Bases: <code>_Predictor</code></p> <p>Standard gradient boosting classifier that fully supports memory mapping    and can be used for datasets that do not fit into memory.</p> <p>Gradient tree boosting trains an ensemble of decision trees by training each tree to predict the prediction error of all previous trees in the ensemble:</p> \\[ \\min_{\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\nabla f_{t,i}; y_i), \\] <p>where \\(\\nabla f_{t,i}\\) is the prediction generated by the newest decision tree for sample \\(i\\) and \\(f_{t-1,i}\\) is the prediction generated by all previous trees, \\(L(...)\\) is the loss function used and \\(y_i\\) is the target we are trying to predict.</p> <p>XGBoost implements this general approach by adding two specific components:</p> <ol> <li> <p>The loss function \\(L(...)\\) is approximated using a Taylor series.</p> </li> <li> <p>The leaves of the decision tree \\(\\nabla f_{t,i}\\) contain weights    that can be regularized.</p> </li> </ol> <p>These weights are calculated as follows:</p> \\[ w_l = -\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda}, \\] <p>where \\(g_i\\) and \\(h_i\\) are the first and second order derivative of \\(L(...)\\) w.r.t. \\(f_{t-1,i}\\), \\(w_l\\) denotes the weight on leaf \\(l\\) and \\(i \\in l\\) denotes all samples on that leaf.</p> <p>\\(\\lambda\\) is the regularization parameter <code>reg_lambda</code>. This hyperparameter can be set by the users or the hyperparameter optimization algorithm to avoid overfitting.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> PARAMETER DESCRIPTION <code>colsample_bylevel</code> <p>Subsample ratio for the columns used, for each level inside a tree.</p> <p>Note that ScaleGBM grows its trees level-by-level, not node-by-node. At each level, a subselection of the features will be randomly picked and the best feature for each split will be chosen. This hyperparameter determines the share of features randomly picked at each level. When set to 1, then now such sampling takes place.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>colsample_bytree</code> <p>Subsample ratio for the columns used, for each tree. This means that for each tree, a subselection of the features will be randomly chosen. This hyperparameter determines the share of features randomly picked for each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>early_stopping_rounds</code> <p>The number of early_stopping_rounds for which we see no improvement on the validation set until we stop the training process.</p> <p>Range: (0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>gamma</code> <p>Minimum loss reduction required for any update to the tree. This means that every potential update will first be evaluated for its improvement to the loss function. If the improvement exceeds gamma, the update will be accepted.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>goss_a</code> <p>Share of the samples with the largest residuals taken for each tree.</p> <p>If <code>goss_a</code> is set to 1, then gradients one-sided sampling is effectively turned off.</p> <p>Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>goss_b</code> <p>Share of the samples that are not in the <code>goss_a</code> percentile of largest residuals randomly sampled for each tree.</p> <p>The sum of <code>goss_a</code> and <code>goss_b</code> cannot exceed 1.</p> <p>Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>learning_rate</code> <p>Learning rate for the gradient boosting algorithm. When a new tree \\(\\nabla f_{t,i}\\) is trained, it will be added to the existing trees \\(f_{t-1,i}\\). Before doing so, it will be multiplied by the learning_rate.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>max_depth</code> <p>Maximum allowed depth of the trees.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>min_child_weights</code> <p>Minimum sum of weights needed in each child node for a split. The idea here is that any leaf should have a minimum number of samples in order to avoid overfitting. This very common form of regularizing decision trees is slightly modified to refer to weights instead of number of samples, but the basic idea is the same.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>n_estimators</code> <p>Number of estimators (trees).</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [10, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>n_jobs</code> <p>Number of parallel threads. When set to zero, then the optimal number of threads will be inferred automatically.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>objective</code> <p>Specify the learning task and the corresponding learning objective.</p> <p>Possible values:</p> <ul> <li><code>binary:logistic</code></li> </ul> <p> TYPE: <code>Literal['binary:logistic']</code> DEFAULT: <code>'binary:logistic'</code> </p> <code>reg_lambda</code> <p>L2 regularization on the weights. Please refer to the introductory remarks to understand how this hyperparameter influences your weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>seed</code> <p>Seed used for random sampling and other random factors.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5843</code> </p>"},{"location":"reference/predictors/scale_gbm_classifier/#getml.predictors.ScaleGBMClassifier.validate","title":"validate","text":"<pre><code>validate(params: Optional[dict] = None)\n</code></pre> <p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> PARAMETER DESCRIPTION <code>params</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> Note <p>This method is called at end of the <code>__init__</code> constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML Engine.</p> Source code in <code>getml/predictors/scale_gbm_classifier.py</code> <pre><code>def validate(self, params: Optional[dict] = None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params: A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    Note:\n        This method is called at end of the `__init__` constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML Engine.\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    unsupported_params = [\n        k for k in params if k not in type(self)._supported_params\n    ]\n\n    if unsupported_params:\n        raise KeyError(\n            \"The following instance variables are not supported \"\n            + f\"in {self.type}: {unsupported_params}\"\n        )\n\n    _validate_scalegbm_parameters(params)\n</code></pre>"},{"location":"reference/predictors/scale_gbm_regressor/","title":"ScaleGBMRegressor","text":""},{"location":"reference/predictors/scale_gbm_regressor/#getml.predictors.ScaleGBMRegressor","title":"getml.predictors.ScaleGBMRegressor  <code>dataclass</code>","text":"<pre><code>ScaleGBMRegressor(\n    colsample_bylevel: float = 1.0,\n    colsample_bytree: float = 1.0,\n    early_stopping_rounds: int = 10,\n    gamma: float = 0.0,\n    goss_a: float = 1.0,\n    goss_b: float = 0.0,\n    learning_rate: float = 0.1,\n    max_depth: int = 3,\n    min_child_weights: float = 1.0,\n    n_estimators: int = 100,\n    n_jobs: int = 1,\n    objective: Literal[\n        \"reg:squarederror\"\n    ] = \"reg:squarederror\",\n    reg_lambda: float = 1.0,\n    seed: int = 5843,\n)\n</code></pre> <p>               Bases: <code>_Predictor</code></p> <p>Standard gradient boosting regressor that fully supports memory mapping    and can be used for datasets that do not fit into memory.</p> <p>Gradient tree boosting trains an ensemble of decision trees by training each tree to predict the prediction error of all previous trees in the ensemble:</p> \\[ \\min_{\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\nabla f_{t,i}; y_i), \\] <p>where \\(\\nabla f_{t,i}\\) is the prediction generated by the newest decision tree for sample \\(i\\) and \\(f_{t-1,i}\\) is the prediction generated by all previous trees, \\(L(...)\\) is the loss function used and \\(y_i\\) is the target we are trying to predict.</p> <p>The regressor implements this general approach by adding two specific components:</p> <ol> <li> <p>The loss function \\(L(...)\\) is approximated using a Taylor series.</p> </li> <li> <p>The leaves of the decision tree \\(\\nabla f_{t,i}\\) contain weights    that can be regularized.</p> </li> </ol> <p>These weights are calculated as follows:</p> \\[ w_l = -\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda}, \\] <p>where \\(g_i\\) and \\(h_i\\) are the first and second order derivative of \\(L(...)\\) w.r.t. \\(f_{t-1,i}\\), \\(w_l\\) denotes the weight on leaf \\(l\\) and \\(i \\in l\\) denotes all samples on that leaf.</p> <p>\\(\\lambda\\) is the regularization parameter <code>reg_lambda</code>. This hyperparameter can be set by the users or the hyperparameter optimization algorithm to avoid overfitting.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> PARAMETER DESCRIPTION <code>colsample_bylevel</code> <p>Subsample ratio for the columns used, for each level inside a tree.</p> <p>Note that ScaleGBM grows its trees level-by-level, not node-by-node. At each level, a subselection of the features will be randomly picked and the best feature for each split will be chosen. This hyperparameter determines the share of features randomly picked at each level. When set to 1, then now such sampling takes place.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>colsample_bytree</code> <p>Subsample ratio for the columns used, for each tree. This means that for each tree, a subselection of the features will be randomly chosen. This hyperparameter determines the share of features randomly picked for each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>early_stopping_rounds</code> <p>The number of early_stopping_rounds for which we see no improvement on the validation set until we stop the training process.</p> <p>Range: (0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>gamma</code> <p>Minimum loss reduction required for any update to the tree. This means that every potential update will first be evaluated for its improvement to the loss function. If the improvement exceeds gamma, the update will be accepted.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>goss_a</code> <p>Share of the samples with the largest residuals taken for each tree.</p> <p>If <code>goss_a</code> is set to 1, then gradients one-sided sampling is effectively turned off.</p> <p>Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>goss_b</code> <p>Share of the samples that are not in the <code>goss_a</code> percentile of largest residuals randomly sampled for each tree.</p> <p>The sum of <code>goss_a</code> and <code>goss_b</code> cannot exceed 1.</p> <p>Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>learning_rate</code> <p>Learning rate for the gradient boosting algorithm. When a new tree \\(\\nabla f_{t,i}\\) is trained, it will be added to the existing trees \\(f_{t-1,i}\\). Before doing so, it will be multiplied by the learning_rate.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>max_depth</code> <p>Maximum allowed depth of the trees.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>min_child_weights</code> <p>Minimum sum of weights needed in each child node for a split. The idea here is that any leaf should have a minimum number of samples in order to avoid overfitting. This very common form of regularizing decision trees is slightly modified to refer to weights instead of number of samples, but the basic idea is the same.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>n_estimators</code> <p>Number of estimators (trees).</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [10, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>n_jobs</code> <p>Number of parallel threads. When set to zero, then the optimal number of threads will be inferred automatically.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>objective</code> <p>Specify the learning task and the corresponding learning objective.</p> <p>Possible values:</p> <ul> <li><code>reg:squarederror</code></li> </ul> <p> TYPE: <code>Literal['reg:squarederror']</code> DEFAULT: <code>'reg:squarederror'</code> </p> <code>reg_lambda</code> <p>L2 regularization on the weights. Please refer to the introductory remarks to understand how this hyperparameter influences your weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>seed</code> <p>Seed used for random sampling and other random factors.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>5843</code> </p>"},{"location":"reference/predictors/scale_gbm_regressor/#getml.predictors.ScaleGBMRegressor.validate","title":"validate","text":"<pre><code>validate(params=None)\n</code></pre> <p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> PARAMETER DESCRIPTION <code>params</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> Note <p>This method is called at end of the <code>__init__</code> constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML Engine.</p> Source code in <code>getml/predictors/scale_gbm_regressor.py</code> <pre><code>def validate(self, params=None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params (dict, optional): A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    Note:\n        This method is called at end of the `__init__` constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML Engine.\n    \"\"\"\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    unsupported_params = [\n        k for k in params if k not in type(self)._supported_params\n    ]\n\n    if unsupported_params:\n        raise KeyError(\n            \"The following instance variables are not supported \"\n            + f\"in {self.type}: {unsupported_params}\"\n        )\n\n    _validate_scalegbm_parameters(params)\n</code></pre>"},{"location":"reference/predictors/xgboost_classifier/","title":"XGBoostClassifier","text":""},{"location":"reference/predictors/xgboost_classifier/#getml.predictors.XGBoostClassifier","title":"getml.predictors.XGBoostClassifier  <code>dataclass</code>","text":"<pre><code>XGBoostClassifier(\n    booster: str = \"gbtree\",\n    colsample_bylevel: float = 1.0,\n    colsample_bytree: float = 1.0,\n    early_stopping_rounds: int = 10,\n    gamma: float = 0.0,\n    learning_rate: float = 0.1,\n    max_delta_step: float = 0.0,\n    max_depth: int = 3,\n    min_child_weights: float = 1.0,\n    n_estimators: int = 100,\n    external_memory: bool = False,\n    normalize_type: str = \"tree\",\n    num_parallel_tree: int = 1,\n    n_jobs: int = 1,\n    objective: Literal[\n        \"reg:logistic\", \"binary:logistic\", \"binary:logitraw\"\n    ] = \"binary:logistic\",\n    one_drop: bool = False,\n    rate_drop: float = 0.0,\n    reg_alpha: float = 0.0,\n    reg_lambda: float = 1.0,\n    sample_type: str = \"uniform\",\n    silent: bool = True,\n    skip_drop: float = 0.0,\n    subsample: float = 1.0,\n)\n</code></pre> <p>               Bases: <code>_Predictor</code></p> <p>Gradient boosting classifier based on xgboost .</p> <p>XGBoost is an implementation of the gradient tree boosting algorithm that is widely recognized for its efficiency and predictive accuracy.</p> <p>Gradient tree boosting trains an ensemble of decision trees by training each tree to predict the prediction error of all previous trees in the ensemble:</p> \\[ \\min_{\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\nabla f_{t,i}; y_i), \\] <p>where \\(\\nabla f_{t,i}\\) is the prediction generated by the newest decision tree for sample \\(i\\) and \\(f_{t-1,i}\\) is the prediction generated by all previous trees, \\(L(...)\\) is the loss function used and \\(y_i\\) is the target we are trying to predict.</p> <p>XGBoost implements this general approach by adding two specific components:</p> <ol> <li> <p>The loss function \\(L(...)\\) is approximated using a Taylor series.</p> </li> <li> <p>The leaves of the decision tree \\(\\nabla f_{t,i}\\) contain weights    that can be regularized.</p> </li> </ol> <p>These weights are calculated as follows:</p> \\[ w_l = -\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda}, \\] <p>where \\(g_i\\) and \\(h_i\\) are the first and second order derivative of \\(L(...)\\) w.r.t. \\(f_{t-1,i}\\), \\(w_l\\) denotes the weight on leaf \\(l\\) and \\(i \\in l\\) denotes all samples on that leaf.</p> <p>\\(\\lambda\\) is the regularization parameter <code>reg_lambda</code>. This hyperparameter can be set by the users or the hyperparameter optimization algorithm to avoid overfitting.</p> PARAMETER DESCRIPTION <code>booster</code> <p>Which base classifier to use.</p> <p>Possible values:</p> <ul> <li><code>gbtree</code>: normal gradient boosted decision trees</li> <li><code>gblinear</code>: uses a linear model instead of decision trees</li> <li>'dart': adds dropout to the standard gradient boosting algorithm.   Please also refer to the remarks on rate_drop for further   explanation on 'dart'.</li> </ul> <p> TYPE: <code>str</code> DEFAULT: <code>'gbtree'</code> </p> <code>colsample_bylevel</code> <p>Subsample ratio for the columns used, for each level inside a tree.</p> <p>Note that XGBoost grows its trees level-by-level, not node-by-node. At each level, a subselection of the features will be randomly picked and the best feature for each split will be chosen. This hyperparameter determines the share of features randomly picked at each level. When set to 1, then now such sampling takes place.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>colsample_bytree</code> <p>Subsample ratio for the columns used, for each tree. This means that for each tree, a subselection of the features will be randomly chosen. This hyperparameter determines the share of features randomly picked for each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>early_stopping_rounds</code> <p>The number of early_stopping_rounds for which we see no improvement on the validation set until we stop the training process.</p> <p>Range: (0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>gamma</code> <p>Minimum loss reduction required for any update to the tree. This means that every potential update will first be evaluated for its improvement to the loss function. If the improvement exceeds gamma, the update will be accepted.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>learning_rate</code> <p>Learning rate for the gradient boosting algorithm. When a new tree \\(\\nabla f_{t,i}\\) is trained, it will be added to the existing trees \\(f_{t-1,i}\\). Before doing so, it will be multiplied by the learning_rate.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>max_delta_step</code> <p>The maximum delta step allowed for the weight estimation of each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \u221e)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>max_depth</code> <p>Maximum allowed depth of the trees.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>min_child_weights</code> <p>Minimum sum of weights needed in each child node for a split. The idea here is that any leaf should have a minimum number of samples in order to avoid overfitting. This very common form of regularizing decision trees is slightly modified to refer to weights instead of number of samples, but the basic idea is the same.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>n_estimators</code> <p>Number of estimators (trees).</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [10, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>external_memory</code> <p>When the in_memory flag of the Engine is set to False, XGBoost can use the external memory functionality. This reduces the memory consumption, but can also affect the quality of the predictions. External memory is deactivated by default and it is recommended to only use external memory for feature selection. When the in_memory flag of the Engine is set to True, (the default value), XGBoost will never use external memory.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>normalize_type</code> <p>This determines how to normalize trees during 'dart'.</p> <p>Possible values:</p> <ul> <li> <p>'tree': a new tree has the same weight as a single   dropped tree.</p> </li> <li> <p>'forest': a new tree has the same weight as a the sum of   all dropped trees.</p> </li> </ul> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'tree'</code> </p> <code>n_jobs</code> <p>Number of parallel threads. When set to zero, then the optimal number of threads will be inferred automatically.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>objective</code> <p>Specify the learning task and the corresponding learning objective.</p> <p>Possible values:</p> <ul> <li><code>reg:logistic</code></li> <li><code>binary:logistic</code></li> <li><code>binary:logitraw</code></li> </ul> <p> TYPE: <code>Literal['reg:logistic', 'binary:logistic', 'binary:logitraw']</code> DEFAULT: <code>'binary:logistic'</code> </p> <code>one_drop</code> <p>If set to True, then at least one tree will always be dropped out. Setting this hyperparameter to true reduces the likelihood of overfitting.</p> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>rate_drop</code> <p>Dropout rate for trees - determines the probability that a tree will be dropped out. Dropout is an algorithm that enjoys considerable popularity in the deep learning community. It means that every node can be randomly removed during training.</p> <p>This approach can also be applied to gradient boosting, where it means that every tree can be randomly removed with a certain probability. Said probability is determined by rate_drop. Dropout for gradient boosting is referred to as the 'dart' algorithm.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>reg_alpha</code> <p>L1 regularization on the weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>reg_lambda</code> <p>L2 regularization on the weights. Please refer to the introductory remarks to understand how this hyperparameter influences your weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>sample_type</code> <p>Possible values:</p> <ul> <li> <p><code>uniform</code>: every tree is equally likely to be dropped   out</p> </li> <li> <p><code>weighted</code>: the dropout probability will be proportional   to a tree's weight</p> </li> </ul> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to <code>dart</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'uniform'</code> </p> <code>silent</code> <p>In silent mode, XGBoost will not print out information on the training progress.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>skip_drop</code> <p>Probability of skipping the dropout during a given iteration. Please also refer to the remarks on rate_drop for further explanation.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <p>Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>subsample</code> <p>Subsample ratio from the training set. This means that for every tree a subselection of samples from the training set will be included into training. Please note that this samples without replacement - the common approach for random forests is to sample with replace.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p>"},{"location":"reference/predictors/xgboost_classifier/#getml.predictors.XGBoostClassifier.validate","title":"validate","text":"<pre><code>validate(params: Optional[dict] = None)\n</code></pre> <p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> PARAMETER DESCRIPTION <code>params</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> Example <pre><code>x = getml.predictors.XGBoostClassifier()\nx.gamma = 200\nx.validate()\n</code></pre> Note <p>This method is called at end of the <code>__init__</code> constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML Engine.</p> Source code in <code>getml/predictors/xgboost_classifier.py</code> <pre><code>def validate(self, params: Optional[dict] = None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params: A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    ??? example\n        ```python\n        x = getml.predictors.XGBoostClassifier()\n        x.gamma = 200\n        x.validate()\n        ```\n\n    Note:\n        This method is called at end of the `__init__` constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML Engine.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    _validate_xgboost_parameters(params)\n\n    # ------------------------------------------------------------\n\n    if params[\"objective\"] not in [\n        \"reg:logistic\",\n        \"binary:logistic\",\n        \"binary:logitraw\",\n    ]:\n        raise ValueError(\n            \"\"\"'objective' supported in XGBoostClassifier\n                             are 'reg:logistic', 'binary:logistic',\n                             and 'binary:logitraw'\"\"\"\n        )\n</code></pre>"},{"location":"reference/predictors/xgboost_regressor/","title":"XGBoostRegressor","text":""},{"location":"reference/predictors/xgboost_regressor/#getml.predictors.XGBoostRegressor","title":"getml.predictors.XGBoostRegressor  <code>dataclass</code>","text":"<pre><code>XGBoostRegressor(\n    booster: str = \"gbtree\",\n    colsample_bylevel: float = 1.0,\n    colsample_bytree: float = 1.0,\n    early_stopping_rounds: int = 10,\n    external_memory: bool = False,\n    gamma: float = 0.0,\n    learning_rate: float = 0.1,\n    max_delta_step: float = 0.0,\n    max_depth: int = 3,\n    min_child_weights: float = 1.0,\n    n_estimators: int = 100,\n    normalize_type: str = \"tree\",\n    num_parallel_tree: int = 1,\n    n_jobs: int = 1,\n    objective: Literal[\n        \"reg:squarederror\", \"reg:tweedie\", \"reg:linear\"\n    ] = \"reg:squarederror\",\n    one_drop: bool = False,\n    rate_drop: float = 0.0,\n    reg_alpha: float = 0.0,\n    reg_lambda: float = 1.0,\n    sample_type: str = \"uniform\",\n    silent: bool = True,\n    skip_drop: float = 0.0,\n    subsample: float = 1.0,\n)\n</code></pre> <p>               Bases: <code>_Predictor</code></p> <p>Gradient boosting regressor based on xgboost .</p> <p>XGBoost is an implementation of the gradient tree boosting algorithm that is widely recognized for its efficiency and predictive accuracy.</p> <p>Gradient tree boosting trains an ensemble of decision trees by training each tree to predict the prediction error of all previous trees in the ensemble:</p> \\[ \\min_{\\nabla f_{t,i}} \\sum_i L(f_{t-1,i} + \\nabla f_{t,i}; y_i), \\] <p>where \\(\\nabla f_{t,i}\\) is the prediction generated by the newest decision tree for sample \\(i\\) and \\(f_{t-1,i}\\) is the prediction generated by all previous trees, \\(L(...)\\) is the loss function used and \\(y_i\\) is the target we are trying to predict.</p> <p>XGBoost implements this general approach by adding two specific components:</p> <ol> <li> <p>The loss function \\(L(...)\\) is approximated using a Taylor series.</p> </li> <li> <p>The leaves of the decision tree \\(\\nabla f_{t,i}\\) contain weights    that can be regularized.</p> </li> </ol> <p>These weights are calculated as follows:</p> \\[ w_l = -\\frac{\\sum_{i \\in l} g_i}{ \\sum_{i \\in l} h_i + \\lambda}, \\] <p>where \\(g_i\\) and \\(h_i\\) are the first and second order derivative of \\(L(...)\\) w.r.t. \\(f_{t-1,i}\\), \\(w_l\\) denotes the weight on leaf \\(l\\) and \\(i \\in l\\) denotes all samples on that leaf.</p> <p>\\(\\lambda\\) is the regularization parameter <code>reg_lambda</code>. This hyperparameter can be set by the users or the hyperparameter optimization algorithm to avoid overfitting.</p> PARAMETER DESCRIPTION <code>booster</code> <p>Which base classifier to use.</p> <p>Possible values:</p> <ul> <li><code>gbtree</code>: normal gradient boosted decision trees</li> <li><code>gblinear</code>: uses a linear model instead of decision trees</li> <li>'dart': adds dropout to the standard gradient boosting algorithm.   Please also refer to the remarks on rate_drop for further   explanation on <code>dart</code>.</li> </ul> <p> TYPE: <code>str</code> DEFAULT: <code>'gbtree'</code> </p> <code>colsample_bylevel</code> <p>Subsample ratio for the columns used, for each level inside a tree.</p> <p>Note that XGBoost grows its trees level-by-level, not node-by-node. At each level, a subselection of the features will be randomly picked and the best feature for each split will be chosen. This hyperparameter determines the share of features randomly picked at each level. When set to 1, then now such sampling takes place.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>colsample_bytree</code> <p>Subsample ratio for the columns used, for each tree. This means that for each tree, a subselection of the features will be randomly chosen. This hyperparameter determines the share of features randomly picked for each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>early_stopping_rounds</code> <p>The number of early_stopping_rounds for which we see no improvement on the validation set until we stop the training process.</p> <p>Range: (0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>external_memory</code> <p>When the in_memory flag of the Engine is set to False, XGBoost can use the external memory functionality. This reduces the memory consumption, but can also affect the quality of the predictions. External memory is deactivated by default and it is recommended to only use external memory for feature selection. When the in_memory flag of the Engine is set to True, (the default value), XGBoost will never use external memory.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>gamma</code> <p>Minimum loss reduction required for any update to the tree. This means that every potential update will first be evaluated for its improvement to the loss function. If the improvement exceeds gamma, the update will be accepted.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>learning_rate</code> <p>Learning rate for the gradient boosting algorithm. When a new tree \\(\\nabla f_{t,i}\\) is trained, it will be added to the existing trees \\(f_{t-1,i}\\). Before doing so, it will be multiplied by the learning_rate.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>max_delta_step</code> <p>The maximum delta step allowed for the weight estimation of each tree.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \u221e)</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>max_depth</code> <p>Maximum allowed depth of the trees.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>min_child_weights</code> <p>Minimum sum of weights needed in each child node for a split. The idea here is that any leaf should have a minimum number of samples in order to avoid overfitting. This very common form of regularizing decision trees is slightly modified to refer to weights instead of number of samples, but the basic idea is the same.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>n_estimators</code> <p>Number of estimators (trees).</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [10, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>normalize_type</code> <p>This determines how to normalize trees during 'dart'.</p> <p>Possible values:</p> <ul> <li> <p><code>tree</code>: a new tree has the same weight as a single   dropped tree.</p> </li> <li> <p><code>forest</code>: a new tree has the same weight as the sum of   all dropped trees.</p> </li> </ul> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to <code>dart</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'tree'</code> </p> <code>n_jobs</code> <p>Number of parallel threads. When set to zero, then the optimal number of threads will be inferred automatically.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>objective</code> <p>Specify the learning task and the corresponding learning objective.</p> <p>Possible values:</p> <ul> <li><code>reg:squarederror</code></li> <li><code>reg:tweedie</code></li> <li>'reg:linear'</li> </ul> <p> TYPE: <code>Literal['reg:squarederror', 'reg:tweedie', 'reg:linear']</code> DEFAULT: <code>'reg:squarederror'</code> </p> <code>one_drop</code> <p>If set to True, then at least one tree will always be dropped out. Setting this hyperparameter to true reduces the likelihood of overfitting.</p> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>rate_drop</code> <p>Dropout rate for trees - determines the probability that a tree will be dropped out. Dropout is an algorithm that enjoys considerable popularity in the deep learning community. It means that every node can be randomly removed during training.</p> <p>This approach can also be applied to gradient boosting, where it means that every tree can be randomly removed with a certain probability. Said probability is determined by rate_drop. Dropout for gradient boosting is referred to as the 'dart' algorithm.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Will be ignored if <code>booster</code> is not set to <code>dart</code>.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>reg_alpha</code> <p>L1 regularization on the weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>reg_lambda</code> <p>L2 regularization on the weights. Please refer to the introductory remarks to understand how this hyperparameter influences your weights.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: [0, \u221e]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>sample_type</code> <p>Possible values:</p> <ul> <li> <p><code>uniform</code>: every tree is equally likely to be dropped   out</p> </li> <li> <p><code>weighted</code>: the dropout probability will be proportional   to a tree's weight</p> </li> </ul> <p>Please also refer to the remarks on rate_drop for further explanation.</p> <p>Will be ignored if <code>booster</code> is not set to 'dart'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'uniform'</code> </p> <code>silent</code> <p>In silent mode, XGBoost will not print out information on the training progress.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>skip_drop</code> <p>Probability of skipping the dropout during a given iteration. Please also refer to the remarks on rate_drop for further explanation.</p> <p>Increasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Will be ignored if <code>booster</code> is not set to <code>dart</code>.</p> <p>Range: [0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>subsample</code> <p>Subsample ratio from the training set. This means that for every tree a subselection of samples from the training set will be included into training. Please note that this samples without replacement - the common approach for random forests is to sample with replace.</p> <p>Decreasing this hyperparameter reduces the likelihood of overfitting.</p> <p>Range: (0, 1]</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p>"},{"location":"reference/predictors/xgboost_regressor/#getml.predictors.XGBoostRegressor.validate","title":"validate","text":"<pre><code>validate(params: Optional[dict] = None)\n</code></pre> <p>Checks both the types and the values of all instance variables and raises an exception if something is off.</p> PARAMETER DESCRIPTION <code>params</code> <p>A dictionary containing the parameters to validate. If not is passed, the own parameters will be validated.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> Example <pre><code>x = getml.predictors.XGBoostRegressor()\nx.gamma = 200\nx.validate()\n</code></pre> Note <p>This method is called at end of the <code>__init__</code> constructor and every time before the predictor - or a class holding it as an instance variable - is sent to the getML Engine.</p> Source code in <code>getml/predictors/xgboost_regressor.py</code> <pre><code>def validate(self, params: Optional[dict] = None):\n    \"\"\"Checks both the types and the values of all instance\n    variables and raises an exception if something is off.\n\n    Args:\n        params: A dictionary containing\n            the parameters to validate. If not is passed,\n            the own parameters will be validated.\n\n    ??? example\n        ```python\n        x = getml.predictors.XGBoostRegressor()\n        x.gamma = 200\n        x.validate()\n        ```\n\n    Note:\n        This method is called at end of the `__init__` constructor\n        and every time before the predictor - or a class holding\n        it as an instance variable - is sent to the getML Engine.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if params is None:\n        params = self.__dict__\n    else:\n        params = {**self.__dict__, **params}\n\n    if not isinstance(params, dict):\n        raise ValueError(\"params must be None or a dictionary!\")\n\n    _validate_xgboost_parameters(params)\n\n    # ------------------------------------------------------------\n\n    if params[\"objective\"] not in [\"reg:squarederror\", \"reg:tweedie\", \"reg:linear\"]:\n        raise ValueError(\n            \"\"\"'objective' supported in XGBoostRegressor\n                             are 'reg:squarederror', 'reg:tweedie',\n                             and 'reg:linear'\"\"\"\n        )\n</code></pre>"},{"location":"reference/preprocessors/","title":"Preprocessors","text":""},{"location":"reference/preprocessors/#getml.preprocessors","title":"getml.preprocessors","text":"<p>Contains routines for preprocessing data frames.</p>"},{"location":"reference/preprocessors/#getml.preprocessors.CategoryTrimmer","title":"CategoryTrimmer  <code>dataclass</code>","text":"<pre><code>CategoryTrimmer(\n    max_num_categories: int = 999, min_freq: int = 30\n)\n</code></pre> <p>               Bases: <code>_Preprocessor</code></p> <p>Reduces the cardinality of high-cardinality categorical columns.</p> PARAMETER DESCRIPTION <code>max_num_categories</code> <p>The maximum cardinality allowed. If the cardinality is higher than that only the most frequent categories will be kept, all others will be trimmed.</p> <p> TYPE: <code>int</code> DEFAULT: <code>999</code> </p> <code>min_freq</code> <p>The minimum frequency required for a category to be included.</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> Example <pre><code>category_trimmer = getml.preprocessors.CategoryTrimmer()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[category_trimmer],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre>"},{"location":"reference/preprocessors/#getml.preprocessors.EmailDomain","title":"EmailDomain  <code>dataclass</code>","text":"<pre><code>EmailDomain()\n</code></pre> <p>               Bases: <code>_Preprocessor</code></p> <p>The EmailDomain preprocessor extracts the domain from e-mail addresses.</p> <p>For instance, if the e-mail address is 'some.guy@domain.com', the preprocessor will automatically extract '@domain.com'.</p> <p>The preprocessor will be applied to all <code>text</code> columns that were assigned one of the <code>subroles</code> <code>include.email</code> or <code>only.email</code>.</p> <p>It is recommended that you assign <code>only.email</code>, because it is unlikely that the e-mail address itself is interesting.</p> Example <pre><code>my_data_frame.set_subroles(\"email\", getml.data.subroles.only.email)\n\ndomain = getml.preprocessors.EmailDomain()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[domain],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre>"},{"location":"reference/preprocessors/#getml.preprocessors.Imputation","title":"Imputation  <code>dataclass</code>","text":"<pre><code>Imputation(add_dummies: bool = False)\n</code></pre> <p>               Bases: <code>_Preprocessor</code></p> <p>The Imputation preprocessor replaces all NULL values in numerical columns with the mean of the remaining columns.</p> <p>Optionally, it can additionally add a dummy column that signifies whether the original value was imputed.</p> PARAMETER DESCRIPTION <code>add_dummies</code> <p>Whether you want to add dummy variables that signify whether the original value was imputed.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Example <pre><code>imputation = getml.preprocessors.Imputation()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[imputation],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre>"},{"location":"reference/preprocessors/#getml.preprocessors.Mapping","title":"Mapping  <code>dataclass</code>","text":"<pre><code>Mapping(\n    aggregation: Iterable[\n        MappingAggregations\n    ] = MAPPING.default,\n    min_freq: int = 30,\n    multithreading: bool = True,\n)\n</code></pre> <p>               Bases: <code>_Preprocessor</code></p> <p>A mapping preprocessor maps categorical values, discrete values and individual words in a text field to numerical values. These numerical values are retrieved by aggregating targets in the relational neighbourhood.</p> <p>You are particularly encouraged to use the mapping preprocessor in combination with <code>FastProp</code>.</p> <p>Refer to the User guide for more information.</p> Enterprise edition <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> ATTRIBUTE DESCRIPTION <code>agg_sets</code> <p>It is a class variable holding the available aggregation sets for the mapping preprocessor. Value: <code>MAPPING</code>.</p> <p> TYPE: <code>MappingAggregationsSets</code> </p> PARAMETER DESCRIPTION <code>aggregation</code> <p>The aggregation function to use over the targets.</p> <p>Must be an aggregation supported by Mapping preprocessor (<code>MAPPING_AGGREGATIONS</code>).</p> <p> TYPE: <code>Iterable[MappingAggregations]</code> DEFAULT: <code>default</code> </p> <code>min_freq</code> <p>The minimum number of targets required for a value to be included in the mapping. Range: [0, \u221e]</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> <code>multithreading</code> <p>Whether you want to apply multithreading.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Example <pre><code>mapping = getml.preprocessors.Mapping()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[mapping],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre>"},{"location":"reference/preprocessors/#getml.preprocessors.Seasonal","title":"Seasonal  <code>dataclass</code>","text":"<pre><code>Seasonal(\n    disable_year: bool = False,\n    disable_month: bool = False,\n    disable_weekday: bool = False,\n    disable_hour: bool = False,\n    disable_minute: bool = False,\n)\n</code></pre> <p>               Bases: <code>_Preprocessor</code></p> <p>The Seasonal preprocessor extracts seasonal data from time stamps.</p> <p>The preprocessor automatically iterates through all time stamps in any data frame and extracts seasonal parameters.</p> <p>These include:</p> <ul> <li>year</li> <li>month</li> <li>weekday</li> <li>hour</li> <li>minute</li> </ul> <p>The algorithm also evaluates the potential usefulness of any extracted seasonal parameter. Parameters that are unlikely to be useful are not included.</p> PARAMETER DESCRIPTION <code>disable_year</code> <p>Prevents the Seasonal preprocessor from extracting the year from time stamps.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>disable_month</code> <p>Prevents the Seasonal preprocessor from extracting the month from time stamps.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>disable_weekday</code> <p>Prevents the Seasonal preprocessor from extracting the weekday from time stamps.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>disable_hour</code> <p>Prevents the Seasonal preprocessor from extracting the hour from time stamps.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>disable_minute</code> <p>Prevents the Seasonal preprocessor from extracting the minute from time stamps.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Example <pre><code>seasonal = getml.preprocessors.Seasonal()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[seasonal],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre>"},{"location":"reference/preprocessors/#getml.preprocessors.Substring","title":"Substring  <code>dataclass</code>","text":"<pre><code>Substring(begin: int, length: int, unit: str = '')\n</code></pre> <p>               Bases: <code>_Preprocessor</code></p> <p>The Substring preprocessor extracts substrings from categorical columns and unused string columns.</p> <p>The preprocessor will be applied to all <code>categorical</code> and <code>text</code> columns that were assigned one of the <code>subroles</code> <code>include.substring</code> or <code>only.substring</code>.</p> <p>To further limit the scope of a substring preprocessor, you can also assign a unit.</p> PARAMETER DESCRIPTION <code>begin</code> <p>Index of the beginning of the substring (starting from 0).</p> <p> TYPE: <code>int</code> </p> <code>length</code> <p>The length of the substring.</p> <p> TYPE: <code>int</code> </p> <code>unit</code> <p>The unit of all columns to which the preprocessor should be applied. These columns must also have the subrole substring.</p> <p>If it is left empty, then the preprocessor will be applied to all columns with the subrole <code>include.substring</code> or <code>only.substring</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> Example <pre><code>my_df.set_subroles(\"col1\", getml.data.subroles.include.substring)\n\nmy_df.set_subroles(\"col2\", getml.data.subroles.include.substring)\nmy_df.set_unit(\"col2\", \"substr14\")\n\n# Will be applied to col1 and col2\nsubstr13 = getml.preprocessors.Substring(0, 3)\n\n# Will only be applied to col2\nsubstr14 = getml.preprocessors.Substring(0, 3, \"substr14\")\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[substr13],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre>"},{"location":"reference/preprocessors/#getml.preprocessors.TextFieldSplitter","title":"TextFieldSplitter  <code>dataclass</code>","text":"<pre><code>TextFieldSplitter()\n</code></pre> <p>               Bases: <code>_Preprocessor</code></p> <p>A TextFieldSplitter splits columns with role <code>text</code> into relational bag-of-words representations to allow the feature learners to learn patterns based on the prescence of certain words within the text fields.</p> <p>Text fields will be split on a whitespace or any of the following characters:</p> <p><pre><code>; , . ! ? - | \" \\t \\v \\f \\r \\n % ' ( ) [ ] { }\n</code></pre> Refer to the User Guide for more information.</p> Example <pre><code>text_field_splitter = getml.preprocessors.TextFieldSplitter()\n\npipe = getml.Pipeline(\n    population=population_placeholder,\n    peripheral=[order_placeholder, trans_placeholder],\n    preprocessors=[text_field_splitter],\n    feature_learners=[feature_learner_1, feature_learner_2],\n    feature_selectors=feature_selector,\n    predictors=predictor,\n    share_selected_features=0.5\n)\n</code></pre>"},{"location":"reference/project/","title":"Project Management","text":""},{"location":"reference/project/#getml.project","title":"getml.project","text":"<p>This module helps you handle your current project.</p>"},{"location":"reference/project/#getml.project.attrs.load","title":"load","text":"<pre><code>load(bundle: str, name: Optional[str] = None) -&gt; None\n</code></pre> <p>Loads a project from a bundle and connects to it.</p> PARAMETER DESCRIPTION <code>bundle</code> <p>The <code>.getml</code> bundle file to load.</p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>A name for the project contained in the bundle. If None, the name will be extracted from the bundle.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/project/attrs.py</code> <pre><code>@module_function\ndef load(bundle: str, name: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Loads a project from a bundle and connects to it.\n\n    Args:\n        bundle: The `.getml` bundle file to load.\n\n        name: A name for the project contained in the bundle.\n            If None, the name will be extracted from the bundle.\n    \"\"\"\n    return comm._load_project(bundle, name)\n</code></pre>"},{"location":"reference/project/#getml.project.attrs.delete","title":"delete","text":"<pre><code>delete() -&gt; None\n</code></pre> <p>Deletes the currently connected project. All related pipelines, data frames and hyperopts will be irretrievably deleted.</p> Source code in <code>getml/project/attrs.py</code> <pre><code>@module_function\ndef delete() -&gt; None:\n    \"\"\"\n    Deletes the currently connected project. All related pipelines,\n    data frames and hyperopts will be irretrievably deleted.\n    \"\"\"\n    comm._delete_project(_name())\n</code></pre>"},{"location":"reference/project/#getml.project.attrs.restart","title":"restart","text":"<pre><code>restart() -&gt; None\n</code></pre> <p>Suspends and then relaunches the currently connected project. This will kill all jobs currently running on that process.</p> Source code in <code>getml/project/attrs.py</code> <pre><code>@module_function\ndef restart() -&gt; None:\n    \"\"\"\n    Suspends and then relaunches the currently connected project.\n    This will kill all jobs currently running on that process.\n    \"\"\"\n    comm._set_project(_name(), restart=True)\n</code></pre>"},{"location":"reference/project/#getml.project.attrs.save","title":"save","text":"<pre><code>save(\n    filename: Optional[Union[PathLike, str]] = None,\n    target_dir: Optional[str] = None,\n    replace: bool = False,\n) -&gt; None\n</code></pre> <p>Saves the currently connected project to disk.</p> PARAMETER DESCRIPTION <code>filename</code> <p>The name of the <code>.getml</code> bundle file</p> <p> TYPE: <code>Optional[Union[PathLike, str]]</code> DEFAULT: <code>None</code> </p> <code>replace</code> <p>Whether to replace an existing bundle.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Deprecated <p>1.5: The <code>target_dir</code> argument is deprecated.</p> Source code in <code>getml/project/attrs.py</code> <pre><code>@module_function\ndef save(\n    filename: Optional[Union[PathLike, str]] = None,\n    target_dir: Optional[str] = None,\n    replace: bool = False,\n) -&gt; None:\n    \"\"\"\n    Saves the currently connected project to disk.\n\n    Args:\n        filename: The name of the `.getml` bundle file\n\n        replace: Whether to replace an existing bundle.\n\n    Deprecated:\n        1.5: The `target_dir` argument is deprecated.\n    \"\"\"\n    if target_dir is not None:\n        warnings.warn(\n            \"The target_dir argument is deprecated. Use filename with a path instead.\",\n            DeprecationWarning,\n        )\n        bundle_name = filename if filename else f\"{_name()}.getml\"\n        filename = Path(target_dir) / bundle_name\n    return comm._save_project(_name(), filename, replace)\n</code></pre>"},{"location":"reference/project/#getml.project.attrs.suspend","title":"suspend","text":"<pre><code>suspend() -&gt; None\n</code></pre> <p>Suspends the currently connected project.</p> Source code in <code>getml/project/attrs.py</code> <pre><code>@module_function\ndef suspend() -&gt; None:\n    \"\"\"\n    Suspends the currently connected project.\n    \"\"\"\n    return comm._suspend_project(_name())\n</code></pre>"},{"location":"reference/project/#getml.project.attrs.switch","title":"switch","text":"<pre><code>switch(name: str) -&gt; None\n</code></pre> <p>Creates a new project or loads an existing one.</p> <p>If there is no project called <code>name</code> present on the Engine, a new one will be created. See the User guide for more information.</p> PARAMETER DESCRIPTION <code>name</code> <p>Name of the new project.</p> <p> TYPE: <code>str</code> </p> Source code in <code>getml/project/attrs.py</code> <pre><code>@module_function\ndef switch(name: str) -&gt; None:\n    \"\"\"Creates a new project or loads an existing one.\n\n    If there is no project called `name` present on the Engine, a new one will\n    be created. See the [User guide][project-management] for more\n    information.\n\n    Args:\n        name: Name of the new project.\n    \"\"\"\n    comm._set_project(name)\n</code></pre>"},{"location":"reference/project/#data_frames","title":"data_frames","text":"<p><pre><code>getml.project.data_frames\n</code></pre> An instance of getml.project.DataFrames.</p> <p></p>"},{"location":"reference/project/#hyperopts","title":"hyperopts","text":"<p><pre><code>getml.project.hyperopts\n</code></pre> An instance of getml.project.Hyperopts.</p> <p></p>"},{"location":"reference/project/#pipelines","title":"pipelines","text":"<p><pre><code>getml.project.pipelines\n</code></pre> An instance of getml.project.Pipelines.</p> <p></p>"},{"location":"reference/project/#name","title":"name","text":"<p><pre><code>getml.project.name\n</code></pre> Holds the name of the current project.</p>"},{"location":"reference/project/data_frames/","title":"DataFrames","text":""},{"location":"reference/project/data_frames/#getml.project.DataFrames","title":"getml.project.DataFrames","text":"<pre><code>DataFrames(data=None)\n</code></pre> <p>Container which holds all data frames associated with the running project that are currently stored in memory. The container supports slicing and is sort- and filterable.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def __init__(self, data=None):\n    self._in_memory = list_data_frames()[\"in_memory\"]\n    self._on_disk = list_data_frames()[\"on_disk\"]\n\n    if data is None:\n        self.data = [load_data_frame(name) for name in self._in_memory]\n    else:\n        self.data = data\n</code></pre>"},{"location":"reference/project/data_frames/#getml.project.DataFrames.in_memory","title":"in_memory  <code>property</code>","text":"<pre><code>in_memory: List[str]\n</code></pre> <p>Returns the names of all data frames currently in memory.</p> RETURNS DESCRIPTION <code>List[str]</code> <p>The names of all data frames currently in memory.</p>"},{"location":"reference/project/data_frames/#getml.project.DataFrames.on_disk","title":"on_disk  <code>property</code>","text":"<pre><code>on_disk: List[str]\n</code></pre> <p>Returns the names of all data frames stored in the project folder.</p>"},{"location":"reference/project/data_frames/#getml.project.DataFrames.delete","title":"delete","text":"<pre><code>delete() -&gt; None\n</code></pre> <p>Deletes all data frames in the current project.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def delete(self) -&gt; None:\n    \"\"\"\n    Deletes all data frames in the current project.\n    \"\"\"\n\n    for name in self.on_disk:\n        DataFrame(name).delete()\n</code></pre>"},{"location":"reference/project/data_frames/#getml.project.DataFrames.filter","title":"filter","text":"<pre><code>filter(conditional: Callable) -&gt; DataFrames\n</code></pre> <p>Filters the data frames container.</p> PARAMETER DESCRIPTION <code>conditional</code> <p>A callable that evaluates to a boolean for a given item.</p> <p> TYPE: <code>Callable</code> </p> RETURNS DESCRIPTION <code>DataFrames</code> <p>A container of filtered data frames.</p> Example <pre><code>big_frames = getml.project.data_frames.filter(lambda frame: frame.memory_usage &gt; 1000)\n</code></pre> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def filter(self, conditional: Callable) -&gt; DataFrames:\n    \"\"\"\n    Filters the data frames container.\n\n    Args:\n        conditional:\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered data frames.\n\n    ??? example\n        ```python\n        big_frames = getml.project.data_frames.filter(lambda frame: frame.memory_usage &gt; 1000)\n        ```\n    \"\"\"\n\n    dfs_filtered = [df for df in self.data if conditional(df)]\n    return DataFrames(data=dfs_filtered)\n</code></pre>"},{"location":"reference/project/data_frames/#getml.project.DataFrames.load","title":"load","text":"<pre><code>load() -&gt; None\n</code></pre> <p>Loads all data frames stored in the project folder to memory.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"\n    Loads all data frames stored in the project folder to memory.\n    \"\"\"\n\n    for df in self.on_disk:\n        if df not in self.in_memory:\n            self.data.append(load_data_frame(df))\n</code></pre>"},{"location":"reference/project/data_frames/#getml.project.DataFrames.retrieve","title":"retrieve","text":"<pre><code>retrieve()\n</code></pre> <p>Retrieve a dict of all data frames in memory.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def retrieve(self):\n    \"\"\"\n    Retrieve a dict of all data frames in memory.\n    \"\"\"\n\n    return {df.name: df for df in self.data}\n</code></pre>"},{"location":"reference/project/data_frames/#getml.project.DataFrames.save","title":"save","text":"<pre><code>save() -&gt; None\n</code></pre> <p>Saves all data frames currently in memory to disk.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def save(self) -&gt; None:\n    \"\"\"\n    Saves all data frames currently in memory to disk.\n    \"\"\"\n\n    for df in self.data:\n        df.save()\n</code></pre>"},{"location":"reference/project/data_frames/#getml.project.DataFrames.sort","title":"sort","text":"<pre><code>sort(key: Callable, descending: bool = False) -&gt; DataFrames\n</code></pre> <p>Sorts the data frames container.</p> PARAMETER DESCRIPTION <code>key</code> <p>A callable that evaluates to a sort key for a given item.</p> <p> TYPE: <code>Callable</code> </p> <code>descending</code> <p>Whether to sort in descending order.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>DataFrames</code> <p>A container of sorted data frames.</p> Example <pre><code>by_num_rows = getml.project.data_frames.sort(lambda frame: frame.nrows())\n</code></pre> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def sort(self, key: Callable, descending: bool = False) -&gt; DataFrames:\n    \"\"\"\n    Sorts the data frames container.\n\n    Args:\n        key:\n            A callable that evaluates to a sort key for a given item.\n\n        descending:\n            Whether to sort in descending order.\n\n    Returns:\n            A container of sorted data frames.\n\n    ??? example\n        ```python\n        by_num_rows = getml.project.data_frames.sort(lambda frame: frame.nrows())\n        ```\n    \"\"\"\n\n    dfs_sorted = sorted(self.data, key=key, reverse=descending)\n    return DataFrames(data=dfs_sorted)\n</code></pre>"},{"location":"reference/project/data_frames/#getml.project.DataFrames.unload","title":"unload","text":"<pre><code>unload() -&gt; None\n</code></pre> <p>Unloads all data frames in the current project from memory.</p> Source code in <code>getml/project/containers/data_frames.py</code> <pre><code>def unload(self) -&gt; None:\n    \"\"\"\n    Unloads all data frames in the current project from memory.\n    \"\"\"\n\n    for name in self.on_disk:\n        DataFrame(name).unload()\n</code></pre>"},{"location":"reference/project/hyperopts/","title":"Hyperopts","text":""},{"location":"reference/project/hyperopts/#getml.project.Hyperopts","title":"getml.project.Hyperopts","text":"<pre><code>Hyperopts(data=None)\n</code></pre> <p>Container which holds all hyperopts associated with the currently running project. The container supports slicing and is sort- and filterable.</p> Source code in <code>getml/project/containers/hyperopts.py</code> <pre><code>def __init__(self, data=None):\n    self.ids = list_hyperopts()\n\n    if data is None:\n        self.data = [load_hyperopt(id) for id in self.ids]\n    else:\n        self.data = data\n</code></pre>"},{"location":"reference/project/hyperopts/#getml.project.Hyperopts.filter","title":"filter","text":"<pre><code>filter(conditional: Callable) -&gt; Hyperopts\n</code></pre> <p>Filters the hyperopts container.</p> PARAMETER DESCRIPTION <code>conditional</code> <p>A callable that evaluates to a boolean for a given item.</p> <p> TYPE: <code>Callable</code> </p> RETURNS DESCRIPTION <code>Hyperopts</code> <p>A container of filtered hyperopts.</p> Example <pre><code>gaussian_hyperopts = getml.project.hyperopts.filter(lamda hyp: \"Gaussian\" in hyp.type)\n</code></pre> Source code in <code>getml/project/containers/hyperopts.py</code> <pre><code>def filter(self, conditional: Callable) -&gt; Hyperopts:\n    \"\"\"\n    Filters the hyperopts container.\n\n    Args:\n        conditional:\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered hyperopts.\n\n    ??? example\n        ```python\n        gaussian_hyperopts = getml.project.hyperopts.filter(lamda hyp: \"Gaussian\" in hyp.type)\n        ```\n    \"\"\"\n    hyperopts_filtered = [\n        hyperopt for hyperopt in self.data if conditional(hyperopt)\n    ]\n    return Hyperopts(data=hyperopts_filtered)\n</code></pre>"},{"location":"reference/project/hyperopts/#getml.project.Hyperopts.sort","title":"sort","text":"<pre><code>sort(key: Callable, descending: bool = False) -&gt; Hyperopts\n</code></pre> <p>Sorts the hyperopts container.</p> PARAMETER DESCRIPTION <code>key</code> <p>A callable that evaluates to a sort key for a given item.</p> <p> TYPE: <code>Callable</code> </p> <code>descending</code> <p>Whether to sort in descending order.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Hyperopts</code> <p>A container of sorted hyperopts.</p> Example <pre><code>by_type = getml.project.hyperopt.sort(lambda hyp: hyp.type)\n</code></pre> Source code in <code>getml/project/containers/hyperopts.py</code> <pre><code>def sort(self, key: Callable, descending: bool = False) -&gt; Hyperopts:\n    \"\"\"\n    Sorts the hyperopts container.\n\n    Args:\n        key:\n            A callable that evaluates to a sort key for a given item.\n\n        descending:\n            Whether to sort in descending order.\n\n    Returns:\n            A container of sorted hyperopts.\n\n    ??? example\n        ```python\n        by_type = getml.project.hyperopt.sort(lambda hyp: hyp.type)\n        ```\n    \"\"\"\n    hyperopts_sorted = sorted(self.data, key=key, reverse=descending)\n    return Hyperopts(data=hyperopts_sorted)\n</code></pre>"},{"location":"reference/project/pipelines/","title":"Pipelines","text":""},{"location":"reference/project/pipelines/#getml.project.Pipelines","title":"getml.project.Pipelines","text":"<pre><code>Pipelines(data=None)\n</code></pre> <p>Container which holds all pipelines associated with the currently running project. The container supports slicing and is sort- and filterable.</p> Example <p>Show the first 10 pipelines belonging to the current project: <pre><code>getml.project.pipelines[:10]\n</code></pre> You can use nested list comprehensions to retrieve a scoring history for your project: <pre><code>import matplotlib.pyplot as plt\n\nhyperopt_scores = [(score.date_time, score.mae) for pipe in getml.project.pipelines\n                      for score in pipe.scores[\"data_test\"]\n                      if \"hyperopt\" in pipe.tags]\n\nfig, ax = plt.subplots()\nax.bar(*zip(*hyperopt_scores))\n</code></pre></p> Source code in <code>getml/project/containers/pipelines.py</code> <pre><code>def __init__(self, data=None):\n    self.ids = list_pipelines()\n\n    if data is None:\n        self.data = _refresh_all()\n    else:\n        self.data = data\n</code></pre>"},{"location":"reference/project/pipelines/#getml.project.Pipelines.sort","title":"sort","text":"<pre><code>sort(key: Callable, descending: bool = False) -&gt; Pipelines\n</code></pre> <p>Sorts the pipelines container.</p> PARAMETER DESCRIPTION <code>key</code> <p>A callable that evaluates to a sort key for a given item.</p> <p> TYPE: <code>Callable</code> </p> <code>descending</code> <p>Whether to sort in descending order.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Pipelines</code> <p>A container of sorted pipelines.</p> Example <pre><code>by_auc = getml.project.pipelines.sort(key=lambda pipe: pipe.auc)\nby_fl = getml.project.pipelines.sort(key=lambda pipe: pipe.feature_learners[0].type)\n</code></pre> Source code in <code>getml/project/containers/pipelines.py</code> <pre><code>def sort(self, key: Callable, descending: bool = False) -&gt; Pipelines:\n    \"\"\"\n    Sorts the pipelines container.\n\n    Args:\n        key:\n            A callable that evaluates to a sort key for a given item.\n\n        descending:\n            Whether to sort in descending order.\n\n    Returns:\n            A container of sorted pipelines.\n\n    ??? example\n        ```python\n        by_auc = getml.project.pipelines.sort(key=lambda pipe: pipe.auc)\n        by_fl = getml.project.pipelines.sort(key=lambda pipe: pipe.feature_learners[0].type)\n        ```\n    \"\"\"\n    pipelines_sorted = sorted(self.data, key=key, reverse=descending)\n    return Pipelines(data=pipelines_sorted)\n</code></pre>"},{"location":"reference/project/pipelines/#getml.project.Pipelines.filter","title":"filter","text":"<pre><code>filter(conditional: Callable) -&gt; Pipelines\n</code></pre> <p>Filters the pipelines container.</p> PARAMETER DESCRIPTION <code>conditional</code> <p>A callable that evaluates to a boolean for a given item.</p> <p> TYPE: <code>Callable</code> </p> RETURNS DESCRIPTION <code>Pipelines</code> <p>A container of filtered pipelines.</p> Example <pre><code>pipelines_with_tags = getml.project.pipelines.filter(lambda pipe: len(pipe.tags) &gt; 0)\naccurate_pipes = getml.project.pipelines.filter(lambda pipe: all(acc &gt; 0.9 for acc in pipe.accuracy))\n</code></pre> Source code in <code>getml/project/containers/pipelines.py</code> <pre><code>def filter(self, conditional: Callable) -&gt; Pipelines:\n    \"\"\"\n    Filters the pipelines container.\n\n    Args:\n        conditional:\n            A callable that evaluates to a boolean for a given item.\n\n    Returns:\n            A container of filtered pipelines.\n\n    ??? example\n        ```python\n        pipelines_with_tags = getml.project.pipelines.filter(lambda pipe: len(pipe.tags) &gt; 0)\n        accurate_pipes = getml.project.pipelines.filter(lambda pipe: all(acc &gt; 0.9 for acc in pipe.accuracy))\n        ```\n    \"\"\"\n    pipelines_filtered = [\n        pipeline for pipeline in self.data if conditional(pipeline)\n    ]\n\n    return Pipelines(data=pipelines_filtered)\n</code></pre>"},{"location":"reference/sqlite3/","title":"Sqlite3","text":""},{"location":"reference/sqlite3/#getml.sqlite3","title":"getml.sqlite3","text":"<p>This module contains wrappers around sqlite3 and related utility functions, which enable you to productionize pipelines using only sqlite3 and Python, fully based on open-source code.</p> <p>This requires SQLite version 3.33.0 or above. To check the sqlite3 version of your Python distribution, do the following:</p> <pre><code>import sqlite3\nsqlite3.sqlite_version\n</code></pre> Example <p>For our example we will assume that you want to productionize the CORA project.</p> <p>First, we want to transpile the features into SQL code, like this:</p> <p><pre><code># Set targets to False, if you want an inference pipeline.\npipe1.features.to_sql(targets=True).save(\"cora\")\n</code></pre> This transpiles the features learned by pipe1 into a set of SQLite3 scripts ready to be executed. These scripts are contained in a folder called \"cora\".</p> <p>We also assume that you have the three tables needed for the CORA project in the form of pandas.DataFrames (other data sources are possible).</p> <p>We want to create a new sqlite3 connection and then read in the data: <pre><code>conn = getml.sqlite3.connect(\"cora.db\")\n\ngetml.sqlite3.read_pandas(\n    conn, table_name=\"cites\", data_frame=cites, if_exists=\"replace\")\n\ngetml.sqlite3.read_pandas(\n    conn, table_name=\"content\", data_frame=content, if_exists=\"replace\")\n\ngetml.sqlite3.read_pandas(\n    conn, table_name=\"paper\", data_frame=paper, if_exists=\"replace\")\n</code></pre> Now we can execute the scripts we have just created: <pre><code>conn = getml.sqlite3.execute(conn, \"cora\")\n</code></pre> The transpiled pipeline will always create a table called \"FEATURES\", which contain the features. Here is how we retrieve them: <pre><code>features = getml.sqlite3.to_pandas(conn, \"FEATURES\")\n</code></pre> Now you have created your features in a pandas DataFrame ready to be inserted into your favorite machine learning library.</p> <p>To build stable data science pipelines, it is often a good idea to ensure type safety by hard-coding your table schema. You can use the sniff... methods to do that: <pre><code>getml.sqlite3.sniff_pandas(\"cites\", cites)\n</code></pre> This will generate SQLite3 code that creates the \"cites\" table. You can hard-code that into your pipeline. This will ensure that the data always have the correct types, avoiding awkward problems in the future.</p>"},{"location":"reference/sqlite3/#getml.sqlite3.connect.connect","title":"connect","text":"<pre><code>connect(database: str) -&gt; Connection\n</code></pre> <p>Generates a new sqlite3 connection.</p> <p>This connection contains all customized aggregations and transformation functions needed to execute the SQL pipeline generated by getML. Other than that it behaves just like a normal sqlite3 connection from the Python standard library.</p> PARAMETER DESCRIPTION <code>database</code> <p>Filename of the database. Use ':memory:' to create an in-memory database.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Connection</code> <p>A new sqlite3 connection with all custom functions and aggregations registered.</p> Source code in <code>getml/sqlite3/connect.py</code> <pre><code>def connect(database: str) -&gt; sqlite3.Connection:\n    \"\"\"\n    Generates a new sqlite3 connection.\n\n    This connection contains all customized aggregations\n    and transformation functions needed to execute the\n    SQL pipeline generated by getML. Other than that\n    it behaves just like a normal sqlite3 connection from\n    the Python standard library.\n\n    Args:\n        database:\n            Filename of the database. Use ':memory:' to\n            create an in-memory database.\n\n    Returns:\n        A new sqlite3 connection with all custom\n            functions and aggregations registered.\n    \"\"\"\n\n    if not isinstance(database, str):\n        raise TypeError(\"'database' must be of type str\")\n\n    if sqlite3.sqlite_version &lt; \"3.33.0\":\n        raise ValueError(\n            \"getML requires SQLite version 3.33.0 or above. Found version \"\n            + sqlite3.sqlite_version\n            + \". Please upgrade Python and/or the Python sqlite3 package.\"\n        )\n\n    conn = sqlite3.connect(database)\n\n    conn.create_function(\"contains\", 2, _contains)\n    conn.create_function(\"email_domain\", 1, _email_domain)\n    conn.create_function(\"get_word\", 2, _get_word)\n    conn.create_function(\"num_words\", 1, _num_words)\n\n    conn.create_aggregate(\"COUNT_ABOVE_MEAN\", 1, _CountAboveMean)  # type: ignore\n    conn.create_aggregate(\"COUNT_BELOW_MEAN\", 1, _CountBelowMean)  # type: ignore\n    conn.create_aggregate(\"COUNT_DISTINCT_OVER_COUNT\", 1, _CountDistinctOverCount)  # type: ignore\n    conn.create_aggregate(\"EWMA_1S\", 2, _EWMA1S)  # type: ignore\n    conn.create_aggregate(\"EWMA_1M\", 2, _EWMA1M)  # type: ignore\n    conn.create_aggregate(\"EWMA_1H\", 2, _EWMA1H)  # type: ignore\n    conn.create_aggregate(\"EWMA_1D\", 2, _EWMA1D)  # type: ignore\n    conn.create_aggregate(\"EWMA_7D\", 2, _EWMA7D)  # type: ignore\n    conn.create_aggregate(\"EWMA_30D\", 2, _EWMA30D)  # type: ignore\n    conn.create_aggregate(\"EWMA_90D\", 2, _EWMA90D)  # type: ignore\n    conn.create_aggregate(\"EWMA_365D\", 2, _EWMA365D)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_1S\", 2, _EWMATrend1S)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_1M\", 2, _EWMATrend1M)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_1H\", 2, _EWMATrend1H)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_1D\", 2, _EWMATrend1D)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_7D\", 2, _EWMATrend7D)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_30D\", 2, _EWMATrend30D)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_90D\", 2, _EWMATrend90D)  # type: ignore\n    conn.create_aggregate(\"EWMA_TREND_365D\", 2, _EWMATrend365D)  # type: ignore\n    conn.create_aggregate(\"FIRST\", 2, _First)  # type: ignore\n    conn.create_aggregate(\"KURTOSIS\", 1, _Kurtosis)\n    conn.create_aggregate(\"LAST\", 2, _Last)  # type: ignore\n    conn.create_aggregate(\"MEDIAN\", 1, _Median)\n    conn.create_aggregate(\"MODE\", 1, _Mode)\n    conn.create_aggregate(\"NUM_MAX\", 1, _NumMax)  # type: ignore\n    conn.create_aggregate(\"NUM_MIN\", 1, _NumMin)  # type: ignore\n    conn.create_aggregate(\"Q1\", 1, _Q1)  # type: ignore\n    conn.create_aggregate(\"Q5\", 1, _Q5)  # type: ignore\n    conn.create_aggregate(\"Q10\", 1, _Q10)  # type: ignore\n    conn.create_aggregate(\"Q25\", 1, _Q25)  # type: ignore\n    conn.create_aggregate(\"Q75\", 1, _Q75)  # type: ignore\n    conn.create_aggregate(\"Q90\", 1, _Q90)  # type: ignore\n    conn.create_aggregate(\"Q95\", 1, _Q95)  # type: ignore\n    conn.create_aggregate(\"Q99\", 1, _Q99)  # type: ignore\n    conn.create_aggregate(\"SKEW\", 1, _Skew)\n    conn.create_aggregate(\"STDDEV\", 1, _Stddev)\n    conn.create_aggregate(\"TIME_SINCE_FIRST_MAXIMUM\", 2, _TimeSinceFirstMaximum)  # type: ignore\n    conn.create_aggregate(\"TIME_SINCE_FIRST_MINIMUM\", 2, _TimeSinceFirstMinimum)  # type: ignore\n    conn.create_aggregate(\"TIME_SINCE_LAST_MAXIMUM\", 2, _TimeSinceLastMaximum)  # type: ignore\n    conn.create_aggregate(\"TIME_SINCE_LAST_MINIMUM\", 2, _TimeSinceLastMinimum)  # type: ignore\n    conn.create_aggregate(\"TREND\", 2, _Trend)  # type: ignore\n    conn.create_aggregate(\"VAR\", 1, _Var)\n    conn.create_aggregate(\"VARIATION_COEFFICIENT\", 1, _VariationCoefficient)\n\n    return conn\n</code></pre>"},{"location":"reference/sqlite3/#getml.sqlite3.execute.execute","title":"execute","text":"<pre><code>execute(conn: Connection, fname: str) -&gt; None\n</code></pre> <p>Executes an SQL script or several SQL scripts on SQLite3.</p> PARAMETER DESCRIPTION <code>conn</code> <p>A sqlite3 connection created by <code>connect</code>.</p> <p> TYPE: <code>Connection</code> </p> <code>fname</code> <p>The names of the SQL script or a folder containing SQL scripts. If you decide to pass a folder, the SQL scripts must have the ending '.sql'.</p> <p> TYPE: <code>str</code> </p> Source code in <code>getml/sqlite3/execute.py</code> <pre><code>def execute(conn: sqlite3.Connection, fname: str) -&gt; None:\n    \"\"\"\n    Executes an SQL script or several SQL scripts on SQLite3.\n\n    Args:\n        conn:\n            A sqlite3 connection created by [`connect`][getml.sqlite3.connect.connect].\n\n        fname:\n            The names of the SQL script or a folder containing SQL scripts.\n            If you decide to pass a folder, the SQL scripts must have the ending '.sql'.\n    \"\"\"\n    # ------------------------------------------------------------\n\n    if not isinstance(conn, sqlite3.Connection):\n        raise TypeError(\"'conn' must be an sqlite3.Connection object\")\n\n    if not isinstance(fname, str):\n        raise TypeError(\"'fname' must be of type str\")\n\n    # ------------------------------------------------------------\n\n    # Store temporary object in-memory.\n    conn.execute(\"PRAGMA temp_store=2;\")\n\n    if os.path.isdir(fname):\n        scripts = _retrieve_scripts(fname, \".sql\")\n        for script in scripts:\n            execute(conn, script)\n        return\n\n    _log(\"Executing \" + fname + \"...\")\n\n    with open(fname, encoding=\"utf-8\") as sqlfile:\n        queries = sqlfile.read().split(\";\")\n\n    for query in queries:\n        conn.execute(query + \";\")\n\n    conn.commit()\n</code></pre>"},{"location":"reference/sqlite3/#getml.sqlite3.read_csv.read_csv","title":"read_csv","text":"<pre><code>read_csv(\n    conn: Connection,\n    fnames: Union[str, List[str]],\n    table_name: str,\n    header: bool = True,\n    if_exists: str = \"append\",\n    quotechar: str = '\"',\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n) -&gt; None\n</code></pre> <p>Reads a list of CSV files and writes them into an sqlite3 table.</p> PARAMETER DESCRIPTION <code>conn</code> <p>A sqlite3 connection created by <code>connect</code>.</p> <p> TYPE: <code>Connection</code> </p> <code>fnames</code> <p>The names of the CSV files.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>table_name</code> <p>The name of the table to write to.</p> <p> TYPE: <code>str</code> </p> <code>header</code> <p>Whether the csv file contains a header. If True, the first line is skipped and column names are inferred accordingly.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>quotechar</code> <p>The string escape character.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'\"'</code> </p> <code>if_exists</code> <p>How to behave if the table already exists:</p> <ul> <li>'fail': Raise a ValueError.</li> <li>'replace': Drop the table before inserting new values.</li> <li>'append': Insert new values to the existing table.</li> </ul> <p> TYPE: <code>str</code> DEFAULT: <code>'append'</code> </p> <code>sep</code> <p>The field separator.</p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>skip</code> <p>The number of lines to skip (before a possible header)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>colnames</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you can explicitly pass them. If you pass colnames, it is assumed that the CSV files do not contain a header, thus overriding the 'header' variable.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>getml/sqlite3/read_csv.py</code> <pre><code>def read_csv(\n    conn: sqlite3.Connection,\n    fnames: Union[str, List[str]],\n    table_name: str,\n    header: bool = True,\n    if_exists: str = \"append\",\n    quotechar: str = '\"',\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n) -&gt; None:\n    \"\"\"\n    Reads a list of CSV files and writes them into an sqlite3 table.\n\n    Args:\n        conn:\n            A sqlite3 connection created by [`connect`][getml.sqlite3.connect.connect].\n\n        fnames:\n            The names of the CSV files.\n\n        table_name:\n            The name of the table to write to.\n\n        header:\n            Whether the csv file contains a header. If True, the first line\n            is skipped and column names are inferred accordingly.\n\n        quotechar:\n            The string escape character.\n\n        if_exists:\n            How to behave if the table already exists:\n\n            - 'fail': Raise a ValueError.\n            - 'replace': Drop the table before inserting new values.\n            - 'append': Insert new values to the existing table.\n\n        sep:\n            The field separator.\n\n        skip:\n            The number of lines to skip (before a possible header)\n\n        colnames:\n            The first line of a CSV file\n            usually contains the column names. When this is not the case, you can\n            explicitly pass them. If you pass colnames, it is assumed that the\n            CSV files do not contain a header, thus overriding the 'header' variable.\n    \"\"\"\n    # ------------------------------------------------------------\n\n    if not isinstance(fnames, list):\n        fnames = [fnames]\n\n    # ------------------------------------------------------------\n\n    if not isinstance(conn, sqlite3.Connection):\n        raise TypeError(\"'conn' must be an sqlite3.Connection object\")\n\n    if not _is_non_empty_typed_list(fnames, str):\n        raise TypeError(\"'fnames' must be a string or a non-empty list of strings\")\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be a string\")\n\n    if not isinstance(header, bool):\n        raise TypeError(\"'header' must be a bool\")\n\n    if not isinstance(quotechar, str):\n        raise TypeError(\"'quotechar' must be a str\")\n\n    if not isinstance(if_exists, str):\n        raise TypeError(\"'if_exists' must be a str\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be a str\")\n\n    if not isinstance(skip, int):\n        raise TypeError(\"'skip' must be an int\")\n\n    if colnames is not None and not _is_typed_list(colnames, str):\n        raise TypeError(\"'colnames' must be a list of strings or None\")\n\n    # ------------------------------------------------------------\n\n    schema = sniff_csv(\n        fnames=fnames,\n        table_name=table_name,\n        header=header,\n        quotechar=quotechar,\n        sep=sep,\n        skip=skip,\n        colnames=colnames,\n    )\n\n    _create_table(conn, table_name, schema, if_exists)\n\n    for fname in fnames:\n        _log(\"Loading '\" + fname + \"' into '\" + table_name + \"'...\")\n        data = _read_csv_file(fname, sep, quotechar, header and not colnames, skip)\n        read_list(conn, table_name, data)\n</code></pre>"},{"location":"reference/sqlite3/#getml.sqlite3.read_list.read_list","title":"read_list","text":"<pre><code>read_list(\n    conn: Connection, table_name: str, data: List[List[Any]]\n) -&gt; None\n</code></pre> <p>Reads data into an sqlite3 table.</p> PARAMETER DESCRIPTION <code>conn</code> <p>A sqlite3 connection created by <code>connect</code>.</p> <p> TYPE: <code>Connection</code> </p> <code>table_name</code> <p>The name of the table to write to.</p> <p> TYPE: <code>str</code> </p> <code>data</code> <p>The data to insert into the table. Every list represents one row to be read into the table.</p> <p> TYPE: <code>List[List[Any]]</code> </p> Source code in <code>getml/sqlite3/read_list.py</code> <pre><code>def read_list(conn: sqlite3.Connection, table_name: str, data: List[List[Any]]) -&gt; None:\n    \"\"\"\n    Reads data into an sqlite3 table.\n\n    Args:\n        conn:\n            A sqlite3 connection created by [`connect`][getml.sqlite3.connect.connect].\n\n        table_name:\n            The name of the table to write to.\n\n        data:\n            The data to insert into the table.\n            Every list represents one row to be read into the table.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(conn, sqlite3.Connection):\n        raise TypeError(\"'conn' must be an sqlite3.Connection object\")\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be a string\")\n\n    if not isinstance(data, list):\n        raise TypeError(\"'data' must be a list of lists\")\n\n    # ------------------------------------------------------------\n\n    ncols = _get_num_columns(conn, table_name)\n    old_length = len(data)\n    data = [line for line in data if len(line) == ncols]\n    placeholders = \"(\" + \",\".join([\"?\"] * ncols) + \")\"\n    query = 'INSERT INTO \"' + table_name + '\" VALUES ' + placeholders\n    conn.executemany(query, data)\n    conn.commit()\n    _log(\n        \"Read \"\n        + str(len(data))\n        + \" lines. \"\n        + str(old_length - len(data))\n        + \" invalid lines.\"\n    )\n</code></pre>"},{"location":"reference/sqlite3/#getml.sqlite3.read_pandas.read_pandas","title":"read_pandas","text":"<pre><code>read_pandas(\n    conn: Connection,\n    table_name: str,\n    data_frame: DataFrame,\n    if_exists: Literal[\n        \"fail\", \"replace\", \"append\"\n    ] = \"append\",\n) -&gt; None\n</code></pre> <p>Loads a pandas.DataFrame into SQLite3.</p> PARAMETER DESCRIPTION <code>conn</code> <p>A sqlite3 connection created by <code>connect</code>.</p> <p> TYPE: <code>Connection</code> </p> <code>table_name</code> <p>The name of the table to write to.</p> <p> TYPE: <code>str</code> </p> <code>data_frame</code> <p>The pandas.DataFrame to read into the table. The column names must match the column names of the target table in the SQLite3 database, but their order is not important.</p> <p> TYPE: <code>DataFrame</code> </p> <code>if_exists</code> <p>How to behave if the table already exists:</p> <ul> <li>'fail': Raise a ValueError.</li> <li>'replace': Drop the table before inserting new values.</li> <li>'append': Insert new values into the existing table.</li> </ul> <p> TYPE: <code>Literal['fail', 'replace', 'append']</code> DEFAULT: <code>'append'</code> </p> Source code in <code>getml/sqlite3/read_pandas.py</code> <pre><code>def read_pandas(\n    conn: sqlite3.Connection,\n    table_name: str,\n    data_frame: pd.DataFrame,\n    if_exists: Literal[\"fail\", \"replace\", \"append\"] = \"append\",\n) -&gt; None:\n    \"\"\"\n    Loads a pandas.DataFrame into SQLite3.\n\n    Args:\n        conn:\n            A sqlite3 connection created by [`connect`][getml.sqlite3.connect.connect].\n\n        table_name:\n            The name of the table to write to.\n\n        data_frame:\n            The pandas.DataFrame to read\n            into the table. The column names must match the column\n            names of the target table in the SQLite3 database, but\n            their order is not important.\n\n        if_exists:\n            How to behave if the table already exists:\n\n            - 'fail': Raise a ValueError.\n            - 'replace': Drop the table before inserting new values.\n            - 'append': Insert new values into the existing table.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(conn, sqlite3.Connection):\n        raise TypeError(\"'conn' must be an sqlite3.Connection object\")\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be a str\")\n\n    if not isinstance(data_frame, pd.DataFrame):\n        raise TypeError(\"'data_frame' must be a pandas.DataFrame\")\n\n    if not isinstance(if_exists, str):\n        raise TypeError(\"'if_exists' must be a str\")\n\n    # ------------------------------------------------------------\n\n    _log(\"Loading pandas.DataFrame into '\" + table_name + \"'...\")\n\n    schema = sniff_pandas(table_name, data_frame)\n\n    _create_table(conn, table_name, schema, if_exists)\n\n    colnames = _get_colnames(conn, table_name)\n    data = data_frame[colnames].values.tolist()\n    data = [\n        [\n            field\n            if isinstance(field, (numbers.Number, str)) or field is None\n            else str(field)\n            for field in row\n        ]\n        for row in data\n    ]\n    read_list(conn, table_name, data)\n</code></pre>"},{"location":"reference/sqlite3/#getml.sqlite3.sniff_csv.sniff_csv","title":"sniff_csv","text":"<pre><code>sniff_csv(\n    fnames: Union[str, List[str]],\n    table_name: str,\n    header: bool = True,\n    num_lines_sniffed: int = 1000,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n) -&gt; str\n</code></pre> <p>Sniffs a list of csv files.</p> PARAMETER DESCRIPTION <code>fnames</code> <p>The list of CSV file names to be read.</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> <code>table_name</code> <p>Name of the table in which the data is to be inserted.</p> <p> TYPE: <code>str</code> </p> <code>header</code> <p>Whether the csv file contains a header. If True, the first line is skipped and column names are inferred accordingly.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>num_lines_sniffed</code> <p>Number of lines analyzed by the sniffer.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>quotechar</code> <p>The character used to wrap strings.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'\"'</code> </p> <code>sep</code> <p>The separator used for separating fields.</p> <p> TYPE: <code>str</code> DEFAULT: <code>','</code> </p> <code>skip</code> <p>Number of lines to skip at the beginning of each file.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>colnames</code> <p>The first line of a CSV file usually contains the column names. When this is not the case, you can explicitly pass them. If you pass colnames, it is assumed that the CSV files do not contain a header, thus overriding the 'header' variable.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Appropriate <code>CREATE TABLE</code> statement.</p> Source code in <code>getml/sqlite3/sniff_csv.py</code> <pre><code>def sniff_csv(\n    fnames: Union[str, List[str]],\n    table_name: str,\n    header: bool = True,\n    num_lines_sniffed: int = 1000,\n    quotechar: str = '\"',\n    sep: str = \",\",\n    skip: int = 0,\n    colnames: Optional[List[str]] = None,\n) -&gt; str:\n    \"\"\"\n    Sniffs a list of csv files.\n\n    Args:\n        fnames:\n            The list of CSV file names to be read.\n\n        table_name:\n            Name of the table in which the data is to be inserted.\n\n        header:\n            Whether the csv file contains a header. If True, the first line\n            is skipped and column names are inferred accordingly.\n\n        num_lines_sniffed:\n            Number of lines analyzed by the sniffer.\n\n        quotechar:\n            The character used to wrap strings.\n\n        sep:\n            The separator used for separating fields.\n\n        skip:\n            Number of lines to skip at the beginning of each\n            file.\n\n        colnames:\n            The first line of a CSV file\n            usually contains the column names. When this is not the case, you can\n            explicitly pass them. If you pass colnames, it is assumed that the\n            CSV files do not contain a header, thus overriding the 'header' variable.\n\n    Returns:\n            Appropriate `CREATE TABLE` statement.\n    \"\"\"\n\n    # ------------------------------------------------------------\n\n    if not isinstance(fnames, list):\n        fnames = [fnames]\n\n    # ------------------------------------------------------------\n\n    if not _is_non_empty_typed_list(fnames, str):\n        raise TypeError(\"'fnames' must be a string or a non-empty list of strings\")\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be a string\")\n\n    if not isinstance(header, bool):\n        raise TypeError(\"'header' must be a bool\")\n\n    if not isinstance(num_lines_sniffed, int):\n        raise TypeError(\"'num_lines_sniffed' must be a int\")\n\n    if not isinstance(quotechar, str):\n        raise TypeError(\"'quotechar' must be a str\")\n\n    if not isinstance(sep, str):\n        raise TypeError(\"'sep' must be a str\")\n\n    if not isinstance(skip, int):\n        raise TypeError(\"'skip' must be an int\")\n\n    if colnames is not None and not _is_typed_list(colnames, str):\n        raise TypeError(\"'colnames' must be a list of strings or None\")\n\n    # ------------------------------------------------------------\n\n    header_lines = 0 if header and not colnames else None\n\n    def read(fname):\n        return pd.read_csv(\n            fname,\n            nrows=num_lines_sniffed,\n            header=header_lines,\n            sep=sep,\n            quotechar=quotechar,\n            skiprows=skip,\n            names=colnames,\n        )\n\n    data_frames = [read(fname) for fname in fnames]\n\n    merged = pd.concat(data_frames, join=\"inner\")\n\n    return sniff_pandas(table_name, merged)\n</code></pre>"},{"location":"reference/sqlite3/#getml.sqlite3.sniff_pandas.sniff_pandas","title":"sniff_pandas","text":"<pre><code>sniff_pandas(table_name: str, data_frame: DataFrame) -&gt; str\n</code></pre> <p>Sniffs a pandas data frame.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table in which the data is to be inserted.</p> <p> TYPE: <code>str</code> </p> <code>data_frame</code> <p>The pandas.DataFrame to read into the table.</p> <p> TYPE: <code>DataFrame</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Appropriate <code>CREATE TABLE</code> statement.</p> Source code in <code>getml/sqlite3/sniff_pandas.py</code> <pre><code>def sniff_pandas(table_name: str, data_frame: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Sniffs a pandas data frame.\n\n    Args:\n        table_name:\n            Name of the table in which the data is to be inserted.\n\n        data_frame:\n            The pandas.DataFrame to read into the table.\n\n    Returns:\n            Appropriate `CREATE TABLE` statement.\n    \"\"\"\n    # ------------------------------------------------------------\n\n    if not isinstance(table_name, str):\n        raise TypeError(\"'table_name' must be a str\")\n\n    if not isinstance(data_frame, pd.DataFrame):\n        raise TypeError(\"'data_frame' must be a pandas.DataFrame\")\n\n    # ------------------------------------------------------------\n\n    colnames = data_frame.columns\n    coltypes = data_frame.dtypes\n\n    sql_types: Dict[str, List[str]] = {\"INTEGER\": [], \"REAL\": [], \"TEXT\": []}\n\n    for cname, ctype in zip(colnames, coltypes):\n        if _is_int_type(ctype):\n            sql_types[\"INTEGER\"].append(cname)\n            continue\n        if _is_numerical_type_numpy(ctype):\n            sql_types[\"REAL\"].append(cname)\n        else:\n            sql_types[\"TEXT\"].append(cname)\n\n    return _generate_schema(table_name, sql_types)\n</code></pre>"},{"location":"reference/sqlite3/#getml.sqlite3.to_list.to_list","title":"to_list","text":"<pre><code>to_list(\n    conn: Connection, query: str\n) -&gt; Tuple[List[str], List[list]]\n</code></pre> <p>Transforms a query or table into a list of lists. Returns a tuple which contains the column names and the actual data.</p> PARAMETER DESCRIPTION <code>conn</code> <p>A sqlite3 connection created by <code>connect</code>.</p> <p> TYPE: <code>Connection</code> </p> <code>query</code> <p>The query used to get the table. You can also pass the name of the table, in which case the entire table will be imported.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Tuple[List[str], List[list]]</code> <p>The column names and the data as a list of lists.</p> Source code in <code>getml/sqlite3/to_list.py</code> <pre><code>def to_list(conn: sqlite3.Connection, query: str) -&gt; Tuple[List[str], List[list]]:\n    \"\"\"\n    Transforms a query or table into a list of lists. Returns\n    a tuple which contains the column names and the actual data.\n\n    Args:\n        conn:\n            A sqlite3 connection created by [`connect`][getml.sqlite3.connect.connect].\n\n        query:\n            The query used to get the table. You can also\n            pass the name of the table, in which case the entire\n            table will be imported.\n\n    Returns:\n            The column names and the data as a list of lists.\n    \"\"\"\n    # ------------------------------------------------------------\n\n    if not isinstance(conn, sqlite3.Connection):\n        raise TypeError(\"'conn' must be an sqlite3.Connection object\")\n\n    if not isinstance(query, str):\n        raise TypeError(\"'query' must be a str\")\n\n    # ------------------------------------------------------------\n\n    query = _handle_query(query)\n    cursor = conn.execute(query)\n    colnames = [description[0] for description in cursor.description]\n    data = cursor.fetchall()\n    return colnames, data\n</code></pre>"},{"location":"reference/sqlite3/#getml.sqlite3.to_pandas.to_pandas","title":"to_pandas","text":"<pre><code>to_pandas(conn: Connection, query: str) -&gt; DataFrame\n</code></pre> <p>Returns a table as a pandas.DataFrame.</p> PARAMETER DESCRIPTION <code>conn</code> <p>A sqlite3 connection created by <code>connect</code>.</p> <p> TYPE: <code>Connection</code> </p> <code>query</code> <p>The query used to get the table. You can also pass the name of the table, in which case the entire table will be imported.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>The table as a pandas.DataFrame.</p> Source code in <code>getml/sqlite3/to_pandas.py</code> <pre><code>def to_pandas(conn: sqlite3.Connection, query: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns a table as a pandas.DataFrame.\n\n    Args:\n        conn:\n            A sqlite3 connection created by [`connect`][getml.sqlite3.connect.connect].\n\n        query:\n            The query used to get the table. You can also\n            pass the name of the table, in which case the entire\n            table will be imported.\n\n    Returns:\n            The table as a pandas.DataFrame.\n    \"\"\"\n    # ------------------------------------------------------------\n\n    if not isinstance(conn, sqlite3.Connection):\n        raise TypeError(\"'conn' must be an sqlite3.Connection object\")\n\n    if not isinstance(query, str):\n        raise TypeError(\"'query' must be a str\")\n\n    # ------------------------------------------------------------\n\n    colnames, data = to_list(conn, query)\n    data_frame = pd.DataFrame(data)\n    data_frame.columns = colnames\n    return data_frame\n</code></pre>"},{"location":"user_guide/","title":"User Guide","text":""},{"location":"user_guide/#user-guide","title":"User Guide","text":"<p>The User Guide provides a comprehensive overview of getML's features and capabilities.  Conveniently, it is divided in three levels of detail:</p> <ul> <li> <p> Quick start | 5 min</p> <p>Get a rapid introduction to getML\u2019s feature engineering through a simple example</p> <p>Go to Quick Start</p> </li> <li> <p> Walkthrough | 2 hours</p> <p>Dive into a detailed demonstration to get familiar with fundamentals of getML</p> <p>Go to Walkthrough</p> </li> <li> <p> Concepts | In-depth</p> <p>Explore the thorough reference of theoretical concepts underpinning getML </p> <p>Go to Concepts</p> </li> </ul>"},{"location":"user_guide/concepts/","title":"Concepts","text":""},{"location":"user_guide/concepts/#concepts","title":"Concepts","text":"<p>Designed to align with the typical workflow of a Data Science project, this concept section  offers detailed insights into key concepts, supported by references to the comprehensive  Python API documentation. Its goal is to equip you with all the necessary information to effectively  use getML for your projects, ensuring a seamless and productive experience.</p>"},{"location":"user_guide/concepts/#getml-suite","title":"getML Suite","text":"<p>This section introduces the core components of the getML Suite: the Engine, Monitor,  and Python API. It explains how these elements integrate to support your data science  projects effectively.</p>"},{"location":"user_guide/concepts/#managing-projects","title":"Managing Projects","text":"<p>Learn to manage your projects within the getML Engine. This section covers the  functionalities of the project module in the Python API and how to orchestrate project  activities seamlessly.</p>"},{"location":"user_guide/concepts/#importing-data","title":"Importing Data","text":"<p>Discover how to import data from a variety of sources using the Unified Import Interface.  This section covers importing data from nine different sources, providing a comprehensive guide.</p>"},{"location":"user_guide/concepts/#annotating-data","title":"Annotating Data","text":"<p>Understand the importance of data annotation in automated relational feature engineering.  This section explains the different roles in data annotation and their appropriate usage.</p>"},{"location":"user_guide/concepts/#data-model","title":"Data Model","text":"<p>Explore essential data modeling concepts, including the distinction between population and  placeholder tables. Learn to leverage high-level abstractions like the Star Schema,  Snowflake Schema, and Time Series to simplify your data modeling process.</p>"},{"location":"user_guide/concepts/#preprocessing","title":"Preprocessing","text":"<p>Learn how getML's built-in preprocessing functionalities, such as Mapping and Imputation,  can streamline the often labor-intensive task of data preprocessing.</p>"},{"location":"user_guide/concepts/#feature-engineering","title":"Feature Engineering","text":"<p>Feature Engineering is at the heart of getML. This section delves into its objectives and introduces the feature learning algorithms of getML: FastProp, Fastboost,  MultiRel, Relboost, and RelMT.</p>"},{"location":"user_guide/concepts/#predicting","title":"Predicting","text":"<p>Explore the six built-in predictors of getML and learn how they integrate into the  overall getML pipeline to facilitate efficient predictions.</p>"},{"location":"user_guide/concepts/#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<p>Although default parameters generally yield robust results, this section outlines how to  enhance model performance through getML\u2019s straightforward hyperparameter optimization  routines.</p>"},{"location":"user_guide/concepts/#deployment","title":"Deployment","text":"<p>Deploy results and pipelines without the need for external libraries.  This section explains the deployment process using built-in getML functionalities.</p>"},{"location":"user_guide/concepts/annotating_data/","title":"Annotating data","text":""},{"location":"user_guide/concepts/annotating_data/#annotating-data_1","title":"Annotating data","text":"<p>After you have imported your data into the getML Engine, there is one more step to undertake before you can start learning features: You need to assign a role to each column. Why is that?</p> <p>First, the general structure of the individual data frames is needed to construct the relational data model. This is done by assigning the roles join key and time stamp. The former defines the columns that are used to join different data frames, the latter ensures that only rows in a reasonable time frame are taken into account (otherwise there might be data leaks).</p> <p>Second, you need to tell the feature learning algorithm how to interpret the individual columns for it to construct sophisticated features. That is why we need the roles numerical, categorical, and target. You can also assign units to each column in a Data Frame.</p> <p>This chapter contains detailed information on the individual roles and units.</p>"},{"location":"user_guide/concepts/annotating_data/#in-short","title":"In short","text":"<p>When building the data model, you should keep the following things in mind:</p> <ul> <li>Every <code>DataFrame</code> in a data model needs to have at least one    column (<code>columns</code>) with the role join key.</li> <li>In case you have time series data, the role time stamp has to be used to prevent data leaks (refer to data model time series for details).</li> </ul> <p>When learning features, please keep the following things in mind:</p> <ul> <li>Only <code>columns</code> with roles of categorical, numerical, and time stamp will be used by the feature learning algorithm for aggregations or conditions, unless you explicitly tell it to aggregate target columns as well (refer to <code>allow_lagged_target</code> in <code>join()</code>).</li> <li>Columns are only compared with each other if they have the same unit.</li> <li>If you want to make sure that a column is only used for comparison, you can set <code>comparison_only</code> (refer to annotating units). Time stamps are automatically set to <code>comparison_only</code>.</li> </ul> <p></p>"},{"location":"user_guide/concepts/annotating_data/#roles","title":"Roles","text":"<p>Roles determine if and how <code>columns</code> are handled during the construction of the data model and how they are interpreted by the Feature Learning Algorithm. The following roles are available in getML:</p> Role Class Included in FL Algorithm <code>categorical</code> <code>StringColumn</code> yes <code>numerical</code> <code>FloatColumn</code> yes <code>text</code> <code>StringColumn</code> yes <code>time_stamp</code> <code>FloatColumn</code> yes <code>join_key</code> <code>StringColumn</code> no <code>target</code> <code>FloatColumn</code> not by default <code>unused_float</code> <code>FloatColumn</code> no <code>unused_string</code> <code>StringColumn</code> no <p>Note</p> <p>When constructing a <code>DataFrame</code> via the class methods <code>from_csv</code>, <code>from_pandas</code>, <code>from_db</code>, and <code>from_json</code>, all <code>columns</code> will have either the role unused float or unused string. Unused columns will be ignored by the feature learning and machine learning (ML) algorithms.</p>"},{"location":"user_guide/concepts/annotating_data/#example","title":"Example","text":"<pre><code>import pandas as pd\ndata_df = dict(\n    animal=[\"hawk\", \"parrot\", \"goose\"],\n    votes=[12341, 5127, 65311],\n    weight=[12.14, 12.6, 11.92],\n    animal_id=[123, 512, 671],\n    date=[\"2019-05-02\", \"2019-02-28\", \"2018-12-24\"]\n)\npandas_df = pd.DataFrame(data=data_df)\ngetml_df = getml.data.DataFrame.from_pandas(pandas_df, name='animal elections')\n\ngetml_df\n# Output:\n# | votes        | weight       | animal_id    | animal        | date          |\n# | unused float | unused float | unused float | unused string | unused string |\n# ------------------------------------------------------------------------------\n# | 12341        | 12.14        | 123          | hawk          | 2019-05-02    |\n# | 5127         | 12.6         | 512          | parrot        | 2019-02-28    |\n# | 65311        | 11.92        | 671          | goose         | 2018-12-24    |\n</code></pre> <p>To make use of the imported data, you have to tell getML how you intend to use each column by assigning a role (<code>roles</code>). This is done by using the <code>set_role</code> method of the <code>DataFrame</code>. Each column must have exactly one role. If you wish to use a column in two different roles, you have to add it twice and assign each copy a different role.</p> <pre><code>getml_df.set_role(['animal_id'], getml.data.roles.join_key)\ngetml_df.set_role(['animal'], getml.data.roles.categorical)\ngetml_df.set_role(['votes', 'weight'], getml.data.roles.numerical)\ngetml_df.set_role(['date'], getml.data.roles.time_stamp)    \ngetml_df\n# Output:\n# | date                        | animal_id | animal      | votes     | weight    |\n# | time stamp                  | join key  | categorical | numerical | numerical |\n# ---------------------------------------------------------------------------------\n# | 2019-05-02T00:00:00.000000Z | 123       | hawk        | 12341     | 12.14     |\n# | 2019-02-28T00:00:00.000000Z | 512       | parrot      | 5127      | 12.6      |\n# | 2018-12-24T00:00:00.000000Z | 671       | goose       | 65311     | 11.92     |\n</code></pre> <p>Note</p> <p>When assigning new roles to existing columns, you might notice that some of these calls are completed in an instance while others might take a considerable amount of time. What's happening here? A column's role also determines its type. When you set a new role, an implicit type conversion might take place.</p>"},{"location":"user_guide/concepts/annotating_data/#a-note-on-reproducibility-and-efficiency","title":"A note on reproducibility and efficiency","text":"<p>When building a stable pipeline you want to deploy in a productive environment, the flexible default behavior of the import interface might be more of an obstacle. For instance, CSV files are not type-safe. A column that was interpreted as a float column for one set of files might be interpreted as a string column for a different set of files. This obviously has implications for the stability of your pipeline. Therefore, it might be a good idea to hard-code column roles.</p> <p>In the getML Python API, you can bypass the default deduction of the role of each column by providing a dictionary mapping each column name to a role in the import interface.</p> <p><pre><code>roles = {\"categorical\": [\"colname1\", \"colname2\"], \"target\": [\"colname3\"]}\ndf_expd = getml.data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"',\n    roles=roles,\n    ignore=True\n)\n</code></pre> If the <code>ignore</code> argument is set to <code>True</code>, any columns missing in the dictionary won't be imported at all.</p> <p>If you feel that writing the roles by hand is too tedious, you can use <code>dry</code>: If you call the import interface while setting the <code>dry</code> argument to <code>True</code>, no data is read. Instead, the default roles of all columns will be returned as a dictionary. You can store, alter, and hard-code this dictionary into your stable pipeline.</p> <p><pre><code>roles = getml.data.DataFrame.from_csv(\n    fnames=[\"file1.csv\", \"file2.csv\"],\n    name=\"MY DATA FRAME\",\n    sep=';',\n    quotechar='\"',\n    dry=True                                     \n)\n</code></pre> Even if your data source is type safe, setting roles is still a good idea because it is also more efficient. Using <code>set_role()</code> creates a deep copy of the original column and might perform an implicit type conversion. If you already know where you want your data to end up, it might be a good idea to set roles in advance. </p>"},{"location":"user_guide/concepts/annotating_data/#join-key","title":"Join key","text":"<p><code>Join keys</code> are required to establish a relation between two <code>DataFrame</code> objects. Please refer to section Data Model for details.</p> <p>The content of this column is allowed to contain NULL values. NULL values won't be matched to anything, not even to NULL values in other join keys.</p> <p>Columns of role <code>join key</code> will not be aggregated by the feature learning algorithm or used for conditions.</p> <p></p>"},{"location":"user_guide/concepts/annotating_data/#time-stamp","title":"Time stamp","text":"<p>This role is used to prevent data leaks. When you join one table onto another, you usually want to make sure that no data from the future is used. Time stamps can be used to limit your joins.</p> <p>In addition, the feature learning algorithm can aggregate time stamps or use them for conditions. However, they will not be compared to fixed values unless you explicitly change their units. This means that conditions like this are not possible by default:</p> <p><pre><code>WHERE time_stamp &gt; some_fixed_date\n</code></pre> Instead, time stamps will always be compared to other time stamps:</p> <p><pre><code>WHERE time_stamp1 - time_stamp2 &gt; some_value\n</code></pre> This is because it is unlikely that comparing time stamps to a fixed date performs well out-of-sample.</p> <p>When assigning the role time stamp to a column that is currently a  <code>StringColumn</code>,  you need to specify the format of this string. You can do so by using  the <code>time_formats</code> argument of <code>set_role()</code>. You can pass a list of time formats that is used to try to interpret the input strings. Possible format options are</p> <ul> <li>%w - abbreviated weekday (Mon, Tue, ...)</li> <li>%W - full weekday (Monday, Tuesday, ...)</li> <li>%b - abbreviated month (Jan, Feb, ...)</li> <li>%B - full month (January, February, ...)</li> <li>%d - zero-padded day of month (01 .. 31)</li> <li>%e - day of month (1 .. 31)</li> <li>%f - space-padded day of month ( 1 .. 31)</li> <li>%m - zero-padded month (01 .. 12)</li> <li>%n - month (1 .. 12)</li> <li>%o - space-padded month ( 1 .. 12)</li> <li>%y - year without century (70)</li> <li>%Y - year with century (1970)</li> <li>%H - hour (00 .. 23)</li> <li>%h - hour (00 .. 12)</li> <li>%a - am/pm</li> <li>%A - AM/PM</li> <li>%M - minute (00 .. 59)</li> <li>%S - second (00 .. 59)</li> <li>%s - seconds and microseconds (equivalent to %S.%F)</li> <li>%i - millisecond (000 .. 999)</li> <li>%c - centisecond (0 .. 9)</li> <li>%F - fractional seconds/microseconds (000000 - 999999)</li> <li>%z - time zone differential in ISO 8601 format (Z or +NN.NN)</li> <li>%Z - time zone differential in RFC format (GMT or +NNNN)</li> <li>%% - percent sign </li> </ul> <p>If none of the formats works, the getML Engine will try to interpret the time stamps as numerical values. If this fails, the time stamp will be set to NULL.</p>"},{"location":"user_guide/concepts/annotating_data/#example_1","title":"Example","text":"<pre><code>data_df = dict(\ndate1=[getml.data.time.days(365), getml.data.time.days(366), getml.data.time.days(367)],\ndate2=['1971-01-01', '1971-01-02', '1971-01-03'],\ndate3=['1|1|71', '1|2|71', '1|3|71'],\n)\ndf = getml.data.DataFrame.from_dict(data_df, name='dates')\ndf.set_role(['date1', 'date2', 'date3'], getml.data.roles.time_stamp, time_formats=['%Y-%m-%d', '%n|%e|%y'])\ndf\n# | date1                       | date2                       | date3                       |\n# | time stamp                  | time stamp                  | time stamp                  |\n# -------------------------------------------------------------------------------------------\n# | 1971-01-01T00:00:00.000000Z | 1971-01-01T00:00:00.000000Z | 1971-01-01T00:00:00.000000Z |\n# | 1971-01-02T00:00:00.000000Z | 1971-01-02T00:00:00.000000Z | 1971-01-02T00:00:00.000000Z |\n# | 1971-01-03T00:00:00.000000Z | 1971-01-03T00:00:00.000000Z | 1971-01-03T00:00:00.000000Z |\n</code></pre> <p>Note</p> <p>getML time stamps are actually floats expressing the number of seconds since  UNIX time (1970-01-01T00:00:00).</p> <p></p>"},{"location":"user_guide/concepts/annotating_data/#target","title":"Target","text":"<p>The associated <code>columns</code> contain the variables we want to predict. They are not used by the feature learning algorithm unless we explicitly tell it to do so (refer to <code>allow_lagged_target</code> in <code>join()</code>). However, they are such an important part of the analysis that the population table is required to contain at least one of them (refer to data model tables).</p> <p>Note</p> <p>The content of the target columns needs to be numerical. For classification problems, target variables can only assume the values 0 or 1. Target variables can never be <code>NULL</code>.</p> <p></p>"},{"location":"user_guide/concepts/annotating_data/#numerical","title":"Numerical","text":"<p>This role tells the getML Engine to include the associated <code>FloatColumn</code> during feature learning.</p> <p>It should be used for all data with an inherent ordering, regardless of whether it is sampled from a continuous quantity, like passed time or the total amount of rainfall, or a discrete one, like the number of sugary mulberries one has eaten since lunch. </p>"},{"location":"user_guide/concepts/annotating_data/#categorical","title":"Categorical","text":"<p>This role tells the getML Engine to include the associated <code>StringColumn</code> during feature learning.</p> <p>It should be used for all data with no inherent ordering, even if the categories are encoded as integers instead of strings. </p>"},{"location":"user_guide/concepts/annotating_data/#text","title":"Text","text":"<p>getML provides the role <code>text</code> to annotate free form text fields within relational data structures. getML deals with columns of role <code>text</code> through one of two approaches: Text fields can either be integrated into features by learning conditions based on the mere presence (or absence) of certain words in those text fields (the default) or they can be split into a relational bag-of-words representation by means of the <code>TextFieldSplitter</code> preprocessor. For more information on getML's handling of text fields, refer to the Preprocessing section </p>"},{"location":"user_guide/concepts/annotating_data/#unused_float","title":"Unused_float","text":"<p>Marks a <code>FloatColumn</code> as unused.</p> <p>The associated columns will be neither used for the data model nor by the feature learning algorithms and predictors. </p>"},{"location":"user_guide/concepts/annotating_data/#unused_string","title":"Unused_string","text":"<p>Marks a <code>StringColumn</code> as unused.</p> <p>The associated columns will be neither used for the data model nor by the feature learning algorithms and predictors. </p>"},{"location":"user_guide/concepts/annotating_data/#units","title":"Units","text":"<p>By default, all columns of role categorical or numerical will only be compared to fixed values.</p> <p><pre><code>...\nWHERE numerical_column &gt; some_value \nOR categorical_column == 'some string'\n...\n</code></pre> If you want the feature learning algorithms to compare these columns with each other (like in the snippet below),  you have to explicitly set a unit. </p> <p><pre><code>...\nWHERE numerical_column1 - numerical_column2 &gt; some_value\nOR categorical_column1 != categorical_column2\n...\n</code></pre> Using <code>set_unit()</code> you can set the unit of a column to an arbitrary, non-empty string. If it matches the string of another column, both of them will be compared by the getML Engine. Please note that a column can not have more than one unit.</p> <p>There are occasions where only a pairwise comparison of columns but not a comparison with fixed values is useful. To cope with this problem, you can set the <code>comparison_only</code> flag in <code>set_unit()</code>.</p> <p>Note</p> <p>Note that time stamps are used for comparison only by default. The feature  learning algorithm will not compare them to a fixed date, because it is very unlikely that such a feature would perform well out-of-sample.</p>"},{"location":"user_guide/concepts/data_model/","title":"Data model","text":""},{"location":"user_guide/concepts/data_model/#data-model_1","title":"Data model","text":"<p>Defining the data model is a crucial step before training one of getML's <code>Pipeline</code>s. You typically deal with this step after having imported your data and specified the roles of each column.</p> <p>When working with getML, the raw data usually comes in the form of relational data. That means the information relevant for a prediction is spread over several tables. The data model is the definition of the relationships between all of them.</p> <p>Most relational machine learning problems can be represented in the form of a star schema, in which case you can use the <code>StarSchema</code> abstraction. If your dataset is a time series, you can use the <code>TimeSeries</code> abstraction. </p>"},{"location":"user_guide/concepts/data_model/#tables","title":"Tables","text":"<p>Note</p> <ul> <li>When defining the data model, we distinguish between a population table and one or more peripheral tables.</li> <li>In the context of this tutorial, we will use the term \"table\" as a catch-all for <code>DataFrame</code>s and <code>View</code>s.</li> </ul> <p></p>"},{"location":"user_guide/concepts/data_model/#the-population-table","title":"The population table","text":"<p>The population table is the main table of the analysis. It defines the statistical population of your machine learning problem and contains the target variable(s), which we want to predict. Furthermore, the table usually also contains one or more columns with the role join_key. These are keys used to establish a relationship \u2013 also called joins \u2013 with one or more peripheral tables.</p> <p>The example below contains the population table of a customer churn analysis. The target variable is <code>churn</code> \u2013 whether a person stops using the services and products of a company. It also contains the information whether or not a given customer has churned after a certain reference date. The join key <code>customer_id</code> is used to establish relations with a peripheral table. Additionally, the date the customer joined the company is contained in the column <code>date_joined</code>, which we have assigned the role time_stamp.</p> <p></p> <p></p>"},{"location":"user_guide/concepts/data_model/#peripheral-tables","title":"Peripheral tables","text":"<p>Peripheral tables contain additional information relevant for the prediction of the target variable in the population table. Each of them is related to the latter (or another peripheral table, refer to the snowflake schema) via a join_key.</p> <p>The images below represent two peripheral tables that could be used for our customer churn analysis. One table represents complaints a customer made with a certain agent, and the other represents the transactions the customer made using their account.</p> <p></p> <p></p>"},{"location":"user_guide/concepts/data_model/#placeholders","title":"Placeholders","text":"<p>In getML, <code>Placeholder</code>s are used to construct the <code>DataModel</code>. They are abstract representations of <code>DataFrame</code>s or <code>View</code>s and the relationships among each other, but do not contain any data themselves.</p> <p>The idea behind the placeholder concept is that they allow constructing an abstract data model without any reference to an actual dataset. This data model serves as input for the <code>Pipeline</code>. Later on, the <code>feature_learning</code> algorithms can be trained and applied on any dataset that follows this data model.</p> <p>More information on how to construct placeholders and build a data model can be found in the API documentation for <code>Placeholder</code> and <code>DataModel</code>. </p>"},{"location":"user_guide/concepts/data_model/#joins","title":"Joins","text":"<p>Joins are used to establish relationships between placeholders. To join two placeholders, the data frames used to derive them should both have at least one join_key. The joining itself is done using the <code>join()</code> method (follow the link for examples).</p> <p>All columns corresponding to time stamps have to be given the role time_stamp, and one of them in both the population and peripheral  table is usually passed to the <code>join()</code> method. This  approach ensures that no information from the future is considered during training by including only those rows of the peripheral table in the join operation for which the time stamp of the corresponding row in the population table is either the same or more recent.</p> <p></p>"},{"location":"user_guide/concepts/data_model/#data-schemata","title":"Data schemata","text":"<p>After creating placeholders for all data frames in an analysis, we are ready to create the actual data schema. A data schema is a certain way of assembling population and peripheral tables.</p>"},{"location":"user_guide/concepts/data_model/#the-star-schema","title":"The star schema","text":"<p>The <code>StarSchema</code> is the simplest way of establishing relations between the population and the peripheral tables, sufficient for the majority of data science projects.</p> <p>In the star schema, the population table is surrounded by any number of peripheral tables, all joined via a certain join key. However, no joins between peripheral tables are allowed.</p> <p>Because this is a very popular schema in many machine learning problems on relational data, getML contains a special class for these sorts of problems: <code>StarSchema</code>.</p> <p>The population table and two peripheral tables introduced in Tables can be arranged in a star schema like this:</p> <p></p> <p></p>"},{"location":"user_guide/concepts/data_model/#the-snowflake-schema","title":"The snowflake schema","text":"<p>In some cases, the star schema is not enough to represent the complexity of a dataset. This is where the snowflake schema comes in: In a snowflake schema, peripheral tables can have peripheral tables of their own.</p> <p>Assume that in the customer churn analysis shown earlier, there is an additional table containing information about the calls a certain agent made in customer service. It can be joined to the <code>COMPLAINTS</code> table using the key <code>agent_id</code>.</p> <p></p> <p>To model snowflake schemata, you need to use the <code>DataModel</code> and <code>Container</code> classes.</p> <p></p>"},{"location":"user_guide/concepts/data_model/#time-series","title":"Time series","text":"<p>Time series can be handled by a self-join. In addition, some extra parameters and considerations are required when building features based on time stamps. </p>"},{"location":"user_guide/concepts/data_model/#self-joining-a-single-table","title":"Self-joining a single table","text":"<p>If you are dealing with a classical (multivariate) time series and all your data is contained in a single table, all the concepts covered so far still apply. You just have to perform a so-called self-join by providing your table as both the population and peripheral table and join them.</p> <p>The process works as follows: Whenever a row in the population table - a single measurement - is taken, it will be combined with all the content of the peripheral table - the same time series - for which the time stamps are smaller than the one in the line we picked. A familiar term for this is a \"rolling window\".</p> <p>You can also use the <code>TimeSeries</code> abstraction, which abstracts away the self-join. In this case, you do not have to think about self-joins too much.</p>"},{"location":"user_guide/concepts/data_model/#horizon-and-memory","title":"Horizon and Memory","text":"<p>Crucial concepts of time series analysis are horizon and memory. In the context of getML's time series analysis, horizon is defined as a point forecast. That means the prediction of the target variable at the point as far in the future as defined by the horizon.   </p> <p>Memory, on the other hand is the time duration into the past, that is considered when making a prediction. The memory is used to define the time window of data entering the model between the past and now. The horizon defines the point in the future that the predictions is being made for.   </p>"},{"location":"user_guide/concepts/data_model/#time_stamps-and-on","title":"<code>time_stamps</code> and <code>on</code>","text":"<p>Two parameters in the time series signature determine how the self join is carried out. The <code>time_stamps</code> parameter defines what column is the underlying time dimension to which memory and horizon are applied to. The chosen column must also be of role time_stamp.  </p> <p><code>on</code> simply provides an extra handle to control, what subset of the data is part of any given time series. For example if you have a time series of sales data, you might want to only consider the sales data of a certain product category. In this case you would specify the <code>on</code> parameter to be the column containing the product category.</p> <p>Tip</p> <p>If you assign a column to the <code>on</code> parameter, then this column will not enter the model as a predictor. If you have reason to believe that this column is relevant to the model (i.e. the actual product category), duplicate that column in advance and assign the duplicate to the <code>on</code> parameter. (see class method <code>add()</code>)</p>"},{"location":"user_guide/concepts/data_model/#lagged-target-and-horizon","title":"Lagged Target and horizon","text":"<p>Another useful parameter in time series analysis is <code>lagged_target</code>. This boolean controls whether the target variable is used as a predictor. Including the target variable as a predictor can be useful in time series analysis, when at time of prediction, the target variable up until and including now is known. In turn, this means lagged target variables are only permissible if the target variable is predicted for some when in the future. That is, the horizon must be assigned a positive value.</p>"},{"location":"user_guide/concepts/data_model/#features-based-on-time-stamps","title":"Features based on time stamps","text":"<p>The getML Engine is able to automatically generate features based on aggregations over time windows. Both the length of the time window and the aggregation itself will be determined by the feature learning algorithm. The only requirement is to provide the temporal resolution your time series is sampled with in the <code>delta_t</code> parameter in any feature learning algorithm.</p>"},{"location":"user_guide/concepts/deployment/","title":"Deployment","text":"<p>The results of the feature learning and the prediction can be retrieved in different ways and formats.</p> <p>Transpiling pipelines</p> <p>Using <code>SQLCode.save()</code>, you can transpile Pipelines to SQL code, which can be used without any proprietary components.</p> <p>Returning Python objects</p> <p>Using the <code>Pipeline.transform</code> and <code>Pipeline.predict</code> methods of a trained <code>Pipeline</code>, you can access both the features and the predictions as <code>numpy.ndarray</code> via the Python API.</p> <p>Writing into a database</p> <p>You can also write both features and prediction results back into a new table of the connected database by providing the <code>table_name</code> argument in the <code>Pipeline.transform</code> and <code>Pipeline.predict</code> methods. Please refer to the unified import interface for information on how to connect to a database.</p> <p>Responding to a HTTP POST request</p> <p>The getML Suite contains HTTP endpoints to post new data via a JSON string and retrieve either the resulting features or the predictions.</p>"},{"location":"user_guide/concepts/deployment/#batch-prediction","title":"Batch prediction","text":"<p>Batch prediction pipelines are the most common way of productionizing machine learning pipelines on relational data. These pipelines are usually set to run regularly (once a month, once a week, once a day...) to create a batch of predictions on the newest data. They are typically inserted into a Docker container and scheduled using tools like Jenkins and/or Airflow.</p> <p>If you are looking for a pure Python, 100% open-source way to productionize getML's <code>Pipeline</code>s, you can transpile all the features into sqlite3 code. sqlite3 is part of the Python standard library, and you can use getML's 100% open source and pure Python <code>sqlite3</code> which provides some useful extra functionality not included in Python's standard library.</p>"},{"location":"user_guide/concepts/deployment/#http-endpoints","title":"HTTP Endpoints","text":"<p>As soon as you have trained a pipeline, whitelisted it for external access using its  <code>deploy</code> method, and configured the getML Monitor  for remote access, you can transform new data into </p> <p>features or make predictions on them using these endpoints:</p> <ul> <li>Transform endpoint: <code>http://localhost:1709/transform/PIPELINE_NAME</code></li> <li>Predict endpoint: <code>http://localhost:1709/predict/PIPELINE_NAME</code></li> </ul> <p>To each of them, you must send a POST request containing the new data as a JSON string in a specific request format.</p> <p>Note</p> <p>For testing and developing purposes, you can also use the HTTP port of the Monitor to query the endpoints. Note that this is only possible within the same host. The corresponding syntax is  http://localhost:1709/predict/PIPELINE_NAME</p> <p></p>"},{"location":"user_guide/concepts/deployment/#request-format","title":"Request Format","text":"<p>In all POST requests to the endpoints, a JSON string with the following syntax has to be provided in the body:</p> <pre><code>{\n  \"peripheral\": [{\n    \"column_1\": [],\n    \"column_2\": []\n  },{\n    \"column_1\": [],\n    \"column_2\": []\n  }],\n  \"population\": {\n    \"column_1\": [],\n    \"column_2\": []\n  }\n}\n</code></pre> <p>It has to have exactly two keys in the top level called <code>population</code> and <code>peripheral</code>. These will contain the new input data.</p> <p>The order of the columns is irrelevant. They will be matched according to their names. However, the order of the individual peripheral tables is very important and has to exactly match the order the corresponding Placeholder have been provided in the constructor of <code>pipeline</code>.</p> <p>In our example above, we could post a JSON string like this:</p> <pre><code>{\n  \"peripheral\": [{\n    \"column_01\": [2.4, 3.0, 1.2, 1.4, 2.2],\n    \"join_key\": [\"0\", \"0\", \"0\", \"0\", \"0\"],\n    \"time_stamp\": [0.1, 0.2, 0.3, 0.4, 0.8]\n  }],\n  \"population\": {\n    \"column_01\": [2.2, 3.2],\n    \"join_key\": [\"0\", \"0\"],\n    \"time_stamp\": [0.65, 0.81]\n  }\n}\n</code></pre>"},{"location":"user_guide/concepts/deployment/#time-stamp-formats-in-requests","title":"Time stamp formats in requests","text":"<p>You might have noticed that the time stamps in the example above have been passed as numerical values and not as their string representations shown in the beginning. Both ways are supported by the getML Monitor. But if you choose to pass the string representation, you also have to specify the particular format in order for the getML Engine to interpret your data properly.</p> <pre><code>{\n  \"peripheral\": [{\n    \"column_01\": [2.4, 3.0, 1.2, 1.4, 2.2],\n    \"join_key\": [\"0\", \"0\", \"0\", \"0\", \"0\"],\n    \"time_stamp\": [\"2010-01-01 00:15:00\", \"2010-01-01 08:00:00\", \"2010-01-01 09:30:00\", \"2010-01-01 13:00:00\", \"2010-01-01 23:35:00\"]\n  }],\n  \"population\": {\n    \"column_01\": [2.2, 3.2],\n    \"join_key\": [\"0\", \"0\"],\n    \"time_stamp\": [\"2010-01-01 12:30:00\", \"2010-01-01 23:30:00\"]\n  },\n  \"timeFormats\": [\"%Y-%m-%d %H:%M:%S\"]\n}\n</code></pre> <p>All special characters available for specifying the format of the time stamps are listed and described in e.g. <code>read_csv()</code>.</p>"},{"location":"user_guide/concepts/deployment/#using-an-existing-dataframe","title":"Using an existing <code>DataFrame</code>","text":"<p>You can also use a <code>DataFrame</code> that already  exists on the getML Engine:</p> <pre><code>{\n  \"peripheral\": [{\n    \"df\": \"peripheral_table\"\n  }],\n  \"population\": {\n    \"column_01\": [2.2, 3.2],\n    \"join_key\": [\"0\", \"0\"],\n    \"time_stamp\": [0.65, 0.81]\n  }\n}\n</code></pre>"},{"location":"user_guide/concepts/deployment/#using-data-from-a-database","title":"Using data from a database","text":"<p>You can also read the data from the connected database (see unified import interface)  by passing an arbitrary query to the <code>query</code> key:</p> <pre><code>{\n  \"peripheral\": [{\n    \"query\": \"SELECT * FROM PERIPHERAL WHERE join_key = '0';\"\n  }],\n  \"population\": {\n    \"column_01\": [2.2, 3.2],\n    \"join_key\": [\"0\", \"0\"],\n    \"time_stamp\": [0.65, 0.81]\n  }\n}\n</code></pre> <p></p>"},{"location":"user_guide/concepts/deployment/#transform-endpoint","title":"Transform Endpoint","text":"<p>The transform endpoint returns the generated features.</p> <p>http://localhost:1709/transform/PIPELINE_NAME</p> <p>Such an HTTP request can be sent in many languages. For illustration purposes, we will use the command line tool <code>curl</code>, which comes preinstalled on both Linux and macOS. Also, we will use the HTTP port via localhost (only possible for terminals running on the same machine as the getML Monitor) for better reproducibility.</p> <p><pre><code>curl --header \"Content-Type: application/json\"           \\\n     --request POST                                      \\\n     --data '{\"peripheral\":[{\"column_01\":[2.4,3.0,1.2,1.4,2.2],\"join_key\":[\"0\",\"0\",\"0\",\"0\",\"0\"],\"time_stamp\":[0.1,0.2,0.3,0.4,0.8]}],\"population\":{\"column_01\":[2.2,3.2],\"join_key\":[\"0\",\"0\"],\"time_stamp\":[0.65,0.81]}}' \\\n     http://localhost:1709/transform/PIPELINE_NAME\n</code></pre> </p>"},{"location":"user_guide/concepts/deployment/#predict-endpoint","title":"Predict Endpoint","text":"<p>When using getML as an end-to-end data science pipeline, you can use the predict endpoint to upload new, unseen data and receive the resulting predictions as a response via HTTP.</p> <p>http://localhost:1709/predict/PIPELINE_NAME</p> <p>Such an HTTP request can be sent in many languages. For illustration purposes, we will use the command line tool <code>curl</code>, which comes preinstalled on both Linux and macOS. Also, we will use the HTTP port via localhost (only possible for terminals running on the same machine as the getML Monitor) for better reproducibility.</p> <pre><code>curl --header \"Content-Type: application/json\"           \\\n     --request POST                                      \\\n     --data '{\"peripheral\":[{\"column_01\":[2.4,3.0,1.2,1.4,2.2],\"join_key\":[\"0\",\"0\",\"0\",\"0\",\"0\"],\"time_stamp\":[0.1,0.2,0.3,0.4,0.8]}],\"population\":{\"column_01\":[2.2,3.2],\"join_key\":[\"0\",\"0\"],\"time_stamp\":[0.65,0.81]}}' \\\n     http://localhost:1709/predict/PIPELINE_NAME\n</code></pre>"},{"location":"user_guide/concepts/feature_engineering/","title":"Feature engineering","text":""},{"location":"user_guide/concepts/feature_engineering/#feature-engineering_1","title":"Feature engineering","text":"<p>The deep learning revolution has enabled automated feature engineering for images and sound data. Yet, for relational data and classical time series analysis, feature engineering is still done by hand or using very simple brute force methods. Our mission is to change that.</p> <p>The automation of feature engineering on relational data and time series is at the heart of the getML Suite. There are other libraries that implement feature engineering tools on top of frameworks like <code>data.tables</code> in R, <code>pandas</code> in Python, or <code>Apache Spark</code>. In essence, they all use a brute force approach: Generate a large number of features, then use some sort of feature selection routine to pick a small subselection of them.</p> <p>getML has chosen another path: Our highly efficient feature learning algorithms produce features that are far more advanced than what manual feature engineering could achieve or what could be accomplished using simple brute force approaches.</p>"},{"location":"user_guide/concepts/feature_engineering/#definition","title":"Definition","text":"<p>Feature engineering is the process of constructing variables, so-called features, from a dataset. These features are used as the input for machine learning algorithms. In most real-world datasets, the raw data is spread over multiple tables and the task is to bring these tables together and construct features based on their relationships. These features are stored in a flat feature table. In other words, feature engineering is the operation of merging and aggregating a relational data model into a flat (feature) table. From an academic point of view, most machine learning algorithms used nowadays can be classified as propositional learners. The process of creating flat attribute-value representations from relational data through simple rules or aggregation functions therefore is called propositionalization.</p> <p>Usually, feature engineering is done manually, by using brute force approaches or domain knowledge. This process is sometimes also referred to as data wrangling. In any case it is a tedious, time-consuming, and error-prone process. Manual feature engineering is often done by writing scripts in languages like Python, R, or SQL.</p> <p>Note</p> <p>Unfortunately, the term feature engineering is ambiguous. More often than not, feature engineering is meant to describe numerical transformations or encoding techniques on a single table. The definition used above assumes that the raw data comes in relational form, which is true for almost all real-world datasets.</p> <p></p>"},{"location":"user_guide/concepts/feature_engineering/#feature-learning-vs-propositionalization","title":"Feature learning vs. propositionalization","text":"<p>We follow academia and classify techniques that use simple, merely unconditional transformations (like aggregations) to construct flat (attribute-value) representations as propositionalization approaches, while we classify algorithms which directly learn from relational data structures as feature learning. Here, we pick up a term coined in the deep learning context, where complex relationships are equally learned from raw input data.</p> <p>getML provides a framework capable of automatically extracting useful and meaningful features from a relational data model by finding the best merge and aggregate operations. In fact, the relationships between the target and the original data is learned through one of getML's feature learning algorithms.</p> <p></p>"},{"location":"user_guide/concepts/feature_engineering/#design-principles","title":"Design principles","text":"<p>The general procedure for feature learning on relational data and time series using getML looks like this:</p> <p>The only required input is a relational data schema. In particular, there needs to be some sort of target variable(s), which shall be predicted. For time series, the schema would typically be a self-join. In addition to this general information on the data schema, the intended usage of the variables has to be provided by setting the roles of the corresponding columns. How to setup a data scheme is described in data model.</p> <p>Features are often of the type (illustrated in pseudo SQL-like syntax): <pre><code>COUNT the number of `transactions` within the last X `days`\n</code></pre> where \\(X\\) is some sort of fixed numerical value. getML's algorithms do identify appropriate values for \\(X\\) automatically and there is no need for you to provide them by hand.</p> <p>Features can also take the form of:</p> <p><pre><code>COUNT the number of `transactions` for which the `transaction type` is \u2018type_1\u2019 OR \u2018type_2\u2019 OR \u2019type_3\u2019 OR \u2026\n</code></pre> getML's algorithms also find appropriate conditions based on categorical data without any input from the user.</p> <p>The feature learning algorithms can handle combinations of conditions too. So, features of the form:</p> <p><pre><code>SOME_AGGREGATION( over some column ) WHERE ( condition_1 AND\ncondition_2 ) OR ( condition_3 AND condition_4 ) OR condition_5\n</code></pre> will be engineered automatically as well. Again, no input from the user is required.</p> <p>To increase transparency relating to the created features, they can be expressed in SQL code. Even though automatically generated features will always be less intuitive than hand-crafted ones and could be quite complex, we want the user to get an understanding of what is going on.</p> <p></p>"},{"location":"user_guide/concepts/feature_engineering/#algorithms","title":"Algorithms","text":"<p>getML contains five powerful feature learning algorithms:</p> <ul> <li><code>FastProp</code></li> <li><code>Multirel</code></li> <li><code>Relboost</code></li> <li><code>Fastboost</code></li> <li><code>RelMT</code></li> </ul> <p></p>"},{"location":"user_guide/concepts/feature_engineering/#fastprop","title":"FastProp","text":"<p><code>FastProp</code> is getML's take on propositionalization. It is a fast and efficient implementation utilizing aggregations-based operations, which transform a relational data structure to a flat table. FastProp allows for the really fast generation of a substantial number of features based on simple (unconditional) aggregations.</p> <p>A typical FastProp feature looks like this:</p> <pre><code>CREATE TABLE FEATURE_1 AS\nSELECT MAX( t2.column ) AS feature_1,\n      t1.rowid AS \"rownum\"\nFROM \"population\" t1\nLEFT JOIN \"peripheral\" t2\nON t1.join_key = t2.join_key\nWHERE t2.time_stamp &lt;= t1.time_stamp\nORDER BY t2.time_stamp\nGROUP BY t1.rownum,\n         t1.join_key,\n         t1.time_stamp;\n</code></pre> <p>Note</p> <p>It is recommended that you combine FastProp with mappings.</p> <p>You may notice that such a feature looks pretty similar to the Multirel feature below. And indeed, FastProp shares some of its <code>aggregations</code> with Multirel. FastProp features, however, are usually much simpler because they lack the complex conditions learned by getML's other algorithms (the <code>WHERE</code> statement in the SQL representation). FastProp is an excellent choice in an exploration phase of a data science project and delivers decent results out of the box in many cases. </p> <p></p>"},{"location":"user_guide/concepts/feature_engineering/#multirel","title":"Multirel","text":"<p>Enterprise edition</p> <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> <p>Simply speaking, <code>Multirel</code> is a more efficient variation of Multi-relational Decision Tree Learning (MRDTL). The core idea is to minimize redundancies in the original algorithm by incremental updates. We then combined our improved version of MRDTL with ensemble learning methods.</p> <p>MRDTL is a strain of academic literature that was particularly popular in the early 2000s. It is based on a greedy, tree-like approach:</p> <ul> <li>Define some sort of objective function that evaluates the quality of your feature as it relates to the target variable(s).</li> <li>Pick an aggregation and some column to be aggregated.</li> <li>Try out different conditions. Keep the one that generates the greatest improvement of your objective. Repeat until no improvement can be found or some sort of stopping criterion is reached.</li> </ul> <p>The reason this approach has never really taken off outside of academia is that an efficient implementation is far from trivial. Most papers on MRDTL implement the algorithm on top of an existing relational database system, like MySQL.</p> <p>The main problem with trying to implement something like this on top of an existing database is that it requires many redundant operations. Consider a feature like:</p> <p><pre><code>COUNT the number of `transactions` in the last X `days`\n</code></pre> As we iterate through different values for the threshold \\(X\\), we are forced to repeat the same operations on the same data over and over again. Tasks like this bring traditional database engines to their knees.</p> <p>The core idea of getML's Multirel algorithm is to minimize redundancies through <code>incremental updates</code>. To allow for incremental updates and maximal efficiency, we developed a database Engine from scratch in C++. When we evaluate a feature like:</p> <pre><code>COUNT the number of `transactions` in the last 90 `days`\n</code></pre> <p>and</p> <pre><code>COUNT the number of `transactions` in the last 91 `days`\n</code></pre> <p>there is very little change in between. Multirel only recalculates what has changed and keeps everything else untouched. Therefore, it needs two ingredients that can be incrementally updated: An objective function and the actual aggregation(s).</p> <p>Our first ingredient is an objective function that must be suited for incremental updates. When we move from 90 to 91 days, presumably only very few lines in the population table actually change. We do not need to recalculate the entire table. In practice, most commonly used objective functions are fine and this is not much of a limitation. However, there are some, like rank correlation, that cannot be used.</p> <p>The second ingredient, the aggregations, must allow for incremental updates too. This part is a bit harder, so let us elaborate: Let\u2019s say we have a match between the population table that contains our targets and another table (or a self-join). This match happens to be between the two thresholds 90 and 91 days. As we move from 90 to 91 days, we have to update our aggregation for that match. For maximum efficiency, this needs also to be done incrementally. That means we do not want to recalculate the entire aggregation for all matches that it aggregates - instead just for the one match in between the two thresholds.</p> <p>We want to also support the <code>AND</code> and <code>OR</code> combinations of conditions. Therefore, it is possible that a match was not included in the aggregation before, but becomes part of it as we move the threshold. It is also possible that the match was included in the aggregation, but now it isn\u2019t anymore.</p> <p>For an aggregation like <code>COUNT</code>,  incremental updates are straightforward. If the match was not included but now it is, then increment by 1. If it was included but isn't anymore, then decrement by 1.</p> <p>Things are more tricky for aggregations like <code>MAX</code>,  <code>MEDIAN</code>, or  <code>COUNT_DISTINCT</code>. For instance,  whereas  incrementing <code>MAX</code> is easy, decrementing it  is hard. If the match used to be included and is in fact the maximum value, we now have to find the next biggest match. And we have to find it quickly - ideally iterating through a set of thresholds should take linear time in the number of matches. To make it even more complicated, some cross-joins might result in a lot of matches, so any data structures that have non-trivial memory overhead are a no-go.</p> <p>Everything so far has shed light on how we train one feature. But in practice, we want more than one. So, how do we do that? Since we are using a tree-based algorithm anyway, we are able to harness the power of ensemble learning algorithms that have been shown to work very well with non-relational decision trees, namely bagging and gradient boosting.</p> <p>With bagging, we just sample randomly from our population table. We train a feature on that sample and then pick a different random sample to train the next feature.</p> <p>With gradient boosting, we calculate the pseudo-residuals of our previously trained features. We then train features that predict these pseudo-residuals. This procedure guarantees that new features are targeted and compensate the weaknesses of older ones.</p> <p>Transpiled to SQL, a typical feature generated by Multirel looks like this:</p> <p></p> <pre><code>CREATE TABLE FEATURE_1 AS\nSELECT COUNT( * ) AS feature_1,\n       t1.join_key,\n       t1.time_stamp\nFROM (\n     SELECT * ,\n            ROW_NUMBER() OVER ( ORDER BY join_key, time_stamp ASC ) AS rownum\n     FROM POPULATION\n) t1\nLEFT JOIN PERIPHERAL t2\nON t1.join_key = t2.join_key\nWHERE (\n   ( t1.time_stamp - t2.time_stamp &lt;= 0.499624 )\n) AND t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.rownum,\n         t1.join_key,\n         t1.time_stamp;\n</code></pre> <p>Further information can be found in the API documentation for <code>Multirel</code>.</p> <p></p>"},{"location":"user_guide/concepts/feature_engineering/#relboost","title":"Relboost","text":"<p>Enterprise edition</p> <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> <p><code>Relboost</code> is a generalization of the gradient boosting algorithm. More specifically, it generalizes the xgboost implementation to relational learning.</p> <p>The main difference between Relboost and Multirel is that Multirel aggregates columns, whereas Relboost aggregates learnable weights.</p> <p>Relboost addresses a problem with Multirel that is related to computational complexity theory: In Multirel, every column can be aggregated and/or used for generating a condition. That means that the number of possible features is \\(\\mathcal{O}(n^2)\\) in the number of columns in the original tables. As a result, having twice as many columns will lead to a search space that is four times as large (in reality, it is a bit more complicated than that, but the basic point is true).</p> <p>Any computer scientist or applied mathematician will tell you that \\(\\mathcal{O}(n^2)\\) is a problem. If you have tables with many columns, it might turn out to be a problem. Of course, this issue is not specific to Multirel: It is a very fundamental problem that you would also have, if you were to write your features by hand or use brute force.</p> <p>Relboost offers a way out of this dilemma: Because Relboost aggregates learnable weights and columns will only be used for conditions, but not for aggregation. So, now the search space is \\(\\mathcal{O}(n)\\) in the number of columns in the original tables - much better.</p> <p>This might seem very theoretical, but it has considerable implications: From our experience with real-world data in various projects, we know that Relboost usually outperforms Multirel in terms of predictive accuracy and training time.</p> <p>However, these advantages come at a price: First, the features generated by Relboost are less intuitive. They are further away from what you might write by hand, even though they can still be expressed as SQL code. Second, it is more difficult to apply Relboost to multiple targets, because Relboost has to learn separate rules and weights for each target.</p> <p>Expressed as SQL code, a typical feature generated by Relboost looks like this:</p> <p></p> <p><pre><code>CREATE TABLE FEATURE_1 AS\nSELECT SUM(\nCASE\n     WHEN ( t1.time_stamp - t2.time_stamp &gt; 0.499624 ) THEN 0.0\n     WHEN ( t1.time_stamp - t2.time_stamp &lt;= 0.499624 OR t1.time_stamp IS NULL OR t2.time_stamp IS NULL ) THEN 1.0\n     ELSE NULL\nEND\n) AS feature_1,\n     t1.join_key,\n     t1.time_stamp\nFROM (\n     SELECT *,\n            ROW_NUMBER() OVER ( ORDER BY join_key, time_stamp ASC ) AS rownum\n     FROM POPULATION\n) t1\nLEFT JOIN PERIPHERAL t2\nON t1.join_key = t2.join_key\nWHERE t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.rownum,\n         t1.join_key,\n         t1.time_stamp;\n</code></pre> Further information can be found in the API documentation for <code>Relboost</code>.</p> <p></p>"},{"location":"user_guide/concepts/feature_engineering/#fastboost","title":"Fastboost","text":"<p>Enterprise edition</p> <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> <p>While both are generalizations of the gradient boosting algorithm, the main difference  between <code>Fastboost</code>  and <code>Relboost</code> is that Fastboost uses a simpler algorithm and is therefore much faster  and more scalable. Fastboost is particularly suitable for large datasets or datasets  with many cross-joins. However, from a statistical point of view, the Relboost  algorithm is theoretically more sound.</p> <p>Note</p> <ul> <li>For datasets with many columns, Fastboost can even outperform FastProp in terms of speed.</li> <li>Unlike the other algorithms, Fastboost makes extensive use of memory mapping, so you will need free disc space to use it.</li> </ul> <p>The features generated by Fastboost are indistinguishable from Relboost when  expressed as SQL code. Much like Relboost features, the features generated by  Fastboost are less intuitive. They are further away from what you might write by  hand, even though they can still be expressed as SQL code. Also, much like Relboost,  it is more difficult to apply Fastboost to multiple targets, because Fastboost has  to learn separate rules and weights for each target.</p> <p></p>"},{"location":"user_guide/concepts/feature_engineering/#relmt","title":"RelMT","text":"<p>Enterprise edition</p> <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> <p><code>RelMT</code> is a generalization of linear model trees to relational data. Linear model trees are decision trees with a linear model at each leaf, resulting in a hybrid model that combines the strengths of linear models (like interpretability or the ability to capture linear relationships) with those of tree-based algorithms (like good performance or the ability to capture nonlinear relationships).</p> <p>RelMT features are particularly well-suited for time-series applications because time series often carry autoregressive structures, which can be approximated well by linear models. Think that this month's revenue can usually be modeled particularly well as a (linear) function of last month's revenue and so on. Purely tree-based models often struggle to learn such relationships because they have to fit a piecewise-constant model by predicting the average of all observations associated with each leaf. Thus, it can require a vast amount of splits to approximate a linear relationship.</p> <p>Here is a typical RelMT feature:</p> <p><pre><code>CREATE TABLE FEATURE_1 AS\nSELECT SUM(\nCASE\n    WHEN ( t1.time_stamp - t2.time_stamp &gt; 0.499624 ) THEN\nCOALESCE( t1.time_stamp - julianday( '1970-01-01' ) - 17202.004, 0.0 ) * -122.121 + COALESCE( t2.column - 3301.156, 0.0 ) * -0.003 \n    WHEN ( t1.time_stamp - t2.time_stamp &lt;= 0.499624 OR t1.time_stamp IS NULL OR t2.time_stamp IS NULL ) THEN\nCOALESCE( t1.time_stamp - julianday( '1970-01-01' ) - 17202.004, 0.0 ) * 3.654 + COALESCE( t2.column - 3301.156, 0.0 ) * -1.824 + -8.720\n     ELSE NULL\nEND\n) AS feature_1,\n     t1.join_key,\n     t1.time_stamp\nFROM (\n     SELECT *,\n            ROW_NUMBER() OVER ( ORDER BY join_key, time_stamp ASC ) AS rownum\n     FROM POPULATION\n) t1\nLEFT JOIN PERIPHERAL t2\nON t1.join_key = t2.join_key\nWHERE t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.rownum,\n         t1.join_key,\n         t1.time_stamp;\n</code></pre> RelMT features share some characteristics with Relboost features: Compare the example feature to the Relboost feature above. Both algorithms generate splits based on a combination of conditions (the <code>WHEN</code> part of the <code>CASE WHEN</code> statement above). But while Relboost learns weights for its leaves (the <code>THEN</code> part of the <code>CASE WHEN</code> statement), RelMT learns a linear model, allowing for linear combinations between columns from the population table and columns of a certain peripheral table.</p>"},{"location":"user_guide/concepts/getml_suite/","title":"getML Suite","text":""},{"location":"user_guide/concepts/getml_suite/#the-getml-suite","title":"The getML Suite","text":"<p>The getML ecosystem comprises three fundamental components:</p> <ul> <li>getML Engine</li> <li>Python API</li> <li>getML Monitor</li> </ul> <p></p>"},{"location":"user_guide/concepts/getml_suite/#engine_1","title":"Engine","text":"<p>Written in C++, the getML Engine is the core of the Suite and does all the heavy lifting. It is responsible for data management, feature engineering, and machine learning.</p>"},{"location":"user_guide/concepts/getml_suite/#starting-the-engine","title":"Starting the Engine","text":"<p>Depending on the method used for the installation of getML Suite, the Engine can be started by executing:</p> <ul> <li><code>getml.engine.launch()</code> in the Python API for the pip-based installation</li> <li><code>./getML</code> in terminal for CLI-based installation</li> <li><code>docker compose up</code> in terminal for docker-based installation</li> </ul> <p>Follow the links to learn more about each method. </p>"},{"location":"user_guide/concepts/getml_suite/#shutting-down-the-engine","title":"Shutting down the Engine","text":"<p>Depending on how you started the Engine, there are different ways to shut it down:</p> <ul> <li>In the Python API: <code>getml.engine.shutdown()</code></li> <li>On command-line interface (CLI): Press <code>Ctrl-C</code> or run <code>getML -stop</code></li> <li>For a docker container: Press <code>Ctrl-C</code></li> <li>Click the ' Shutdown' tab in the sidebar of the monitor (Enterprise edition).</li> </ul>"},{"location":"user_guide/concepts/getml_suite/#logging","title":"Logging","text":"<p>The Engine keeps a log about what it is currently doing.</p> <p>The easiest way to view the log is to click the '&lt;&gt; Log' tab in the sidebar of the getML Monitor. The Engine will also output its log to the command line when it is started using the command-line interface.</p> <p></p>"},{"location":"user_guide/concepts/getml_suite/#python-api","title":"Python API","text":"<p>Control the Engine with the getML Python API, which provides handlers to the objects in the Engine and all other necessary tools for end-to-end data science projects. For an in-depth read about its individual classes and methods, check out the Python API documentation.</p> <p>Note</p> <ul> <li>The classes in the Python API act as handles to objects in the getML Engine.</li> <li>When you connect to or create a project:<ul> <li>The API establishes a socket connection to the Engine through a determined port.</li> <li>All subsequent commands are sent to the Engine via this connection.</li> </ul> </li> </ul>"},{"location":"user_guide/concepts/getml_suite/#setup-new-project","title":"Setup new project","text":"<p>Set a project in the getML Engine using <code>set_project()</code>.</p> <pre><code>import getml\ngetml.engine.launch()\ngetml.engine.set_project(\"test\")\n</code></pre> <p>Note</p> <p>If the project name does not match an existing project, a new one will be created.</p>"},{"location":"user_guide/concepts/getml_suite/#managing-projects","title":"Managing projects","text":"<p>To get a list of all available projects, use <code>list_projects()</code>. To remove an entire project, use <code>delete_project()</code>.</p> <pre><code>getml.engine.list_projects()\ngetml.engine.delete_project(\"test\")\n</code></pre> <p>For more information, refer to the Managing projects section.</p> <p></p>"},{"location":"user_guide/concepts/getml_suite/#dataframes","title":"DataFrames","text":"<p>Create a <code>DataFrame</code> by calling for example:</p> <pre><code>data = getml.data.DataFrame.from_csv(\n    \"path/to/my/data.csv\", \n    \"my_data\"\n)\n</code></pre> <p>This creates a data frame object in the getML Engine, imports the provided data, and returns a handler to the object as a <code>DataFrame</code> in the Python API.</p> <p>Note</p> <p>There are many other methods to create a <code>DataFrame</code>, including <code>from_db()</code>, <code>from_json()</code>, or <code>from_pandas()</code>. For a full list of available methods, refer to the Importing data section.</p> <p>Synchronization</p> <p>When you apply any method, like <code>add()</code>, the changes will be automatically reflected in both the Engine and Python. Under the hood, the Python API sends a command to create a new column to the getML Engine. The moment the Engine is done, it informs the Python API and the latter triggers the <code>refresh()</code> method to update the Python handler.</p> <p>Saving</p> <p>Warning</p> <p>DataFrames are never saved automatically and never loaded automatically. All unsaved changes to a <code>DataFrame</code> will be lost when restarting the Engine. </p> <p>To get a list of all your current data_frames, access the container via:</p> <pre><code>getml.project.data_frames\n#or\ngetml.data.list_data_frames()\n</code></pre> <p>You can save a specific data frame to disk using <code>.save()</code> method on the <code>DataFrame</code>:</p> <pre><code># by index\ngetml.project.data_frames[0].save()\n# by name\ngetml.project.data_frames[\"my_data\"].save()\n</code></pre> <p>To save all data frames associated with the current project, use the <code>.save()</code> method on the <code>Container</code>:</p> <pre><code>getml.project.data_frames.save()\n</code></pre> <p>Loading</p> <p>To load a specific <code>DataFrame</code>, use <code>load_data_frame()</code> or <code>DataFrame().load()</code>:</p> <pre><code>df = getml.data.load_data_frame(\"my_data\")\n# Forces the API to load the version stored on disk over the one held in memory\ndf = getml.data.DataFrame(\"my_data\").load()\n</code></pre> <p>Use <code>.load()</code> on the <code>Container</code> to load all data frames associated with the current project:</p> <pre><code>getml.project.data_frames.load()\n</code></pre> <p>Note</p> <p>If a <code>DataFrame</code> is already available in memory (for example \"my_data\" from above), <code>load_data_frame()</code> will return a handle to that data frame. If no such <code>DataFrame</code> is held in memory, the function will try to load the data frame from disk and then return a handle. If that is unsuccessful, an exception is thrown.</p>"},{"location":"user_guide/concepts/getml_suite/#pipelines","title":"Pipelines","text":"<p>The lifecycle of a <code>Pipeline</code> is straightforward and streamlined by the getML Engine, which automatically saves all changes made to a pipeline and loads all pipelines within a project. Pipelines are created within the Python API using constructors, where they are defined by a set of hyperparameters.</p> <p>Note</p> <p>The actual weights of the machine learning algorithms are stored exclusively in the getML Engine and are not transferred to the Python API.</p> <p>Any changes made through methods such as <code>fit()</code> are automatically updated in both the Engine and the Python API.</p> <p>By using <code>set_project()</code>, you can load an existing project, and all associated pipelines will be automatically loaded into memory. To view all pipelines in the current project, access the Pipelines container via <code>getml.project.Pipelines</code>.</p> <p>The function <code>list_pipelines()</code> lists all available pipelines within a project: <pre><code>getml.pipeline.list_pipelines()\n</code></pre></p> <p>To create a corresponding handle in the Python API, use the <code>load()</code> function: <pre><code>pipe = getml.pipeline.load(NAME_OF_THE_PIPELINE)\n</code></pre></p> <p></p>"},{"location":"user_guide/concepts/getml_suite/#monitor_1","title":"Monitor","text":"<p>Enterprise edition</p> <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> <p>The Monitor provides information on the data imported into the Engine, as well as on the trained pipelines and their performance. It is written in Go and compiled into a binary separate from the getML Engine.</p> <p>Accessing the Monitor</p> <p>The Monitor runs on the same machine as the Engine, using sockets for communication. By default, it opens an HTTP port (1709) for browser access. To view the Monitor, enter the following address in your browser's navigation bar:</p> <p>http://localhost:1709</p> <p>Please note, the HTTP port is only accessible from within the host machine running the getML Suite.</p> <p>The main purpose of the Monitor is to provide visual feedback to support your data science projects.</p> <p>Tip</p> <p>If you experience issues opening the Monitor, try the following steps:</p> <ul> <li>Manually shut down and restart the Engine using <code>getml.engine.shutdown()</code> and <code>getml.engine.launch()</code>.</li> <li>Kill the associated background process in the terminal and restart the Engine.</li> <li>Close all tabs and windows where the Monitor was previously running and try again.</li> </ul> <p>To get started, head over to the installation instructions.</p>"},{"location":"user_guide/concepts/hyperopt/","title":"Hyperparameter optimization","text":""},{"location":"user_guide/concepts/hyperopt/#hyperparameter-optimization","title":"Hyperparameter optimization","text":"<p>Enterprise edition</p> <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> <p>In the sections on feature engineering and predicting, we learned how to train both the feature learning algorithm and the  machine learning algorithm used for prediction in the getML Engine. However, there  are lots of parameters involved. <code>Multirel</code>,  <code>Relboost</code>, <code>RelMT</code>,  <code>FastProp</code>,  <code>LinearRegression</code>,  <code>LogisticRegression</code>,  <code>XGBoostClassifier</code>,  and <code>XGBoostRegressor</code> all have their own  settings. That is why you might want to use hyperparameter optimization.</p> <p>The most relevant parameters of these classes can be chosen to constitute individual  dimensions of a parameter space. For each parameter, a lower and upper bound has to  be provided and the hyperparameter optimization will search the space within these  bounds. This will be done iteratively by drawing a specific parameter combination, overwriting the corresponding parameters in a base pipeline, and then fitting and scoring it. The algorithm used to draw from the parameter space is represented by the different classes of <code>hyperopt</code>.</p> <p>While <code>RandomSearch</code> and <code>LatinHypercubeSearch</code> are purely statistical approaches, <code>GaussianHyperparameterSearch</code> uses prior knowledge obtained from evaluations of previous parameter combinations to select the next one.</p>"},{"location":"user_guide/concepts/hyperopt/#tuning-routines","title":"Tuning routines","text":"<p>The easiest way to conduct a hyperparameter optimization in getML are the tuning routines <code>tune_feature_learners()</code> and <code>tune_predictors()</code>. They roughly work as follows:</p> <ul> <li> <p>They begin with a base pipeline, in which the default parameters for the feature learner or the predictor are used.</p> </li> <li> <p>They then proceed by optimizing 2 or 3 parameters at a time using a <code>GaussianHyperparameterSearch</code>. If the best pipeline outperforms the base pipeline, the best pipeline becomes the new base pipeline.</p> </li> <li> <p>Taking the base pipeline from the previous steps, the tuning routine then optimizes the next 2 or 3 hyperparameters. If the best pipeline from that hyperparameter optimization outperforms the current base pipeline, that pipeline becomes the new base pipeline.</p> </li> <li> <p>These steps are then repeated for more hyperparameters.</p> </li> </ul> <p>The following tables list the tuning recipes and hyperparameter subspaces for each step:</p>"},{"location":"user_guide/concepts/hyperopt/#tuning-recipes-for-predictors","title":"Tuning recipes for predictors","text":"Predictor Stage Hyperparameter Subspace <code>LinearRegression</code>; <code>LogisticRegression</code> 1 (base parameters) reg_lambda [1E-11, 100] learning_rate [0.5, 0.99] <code>XGBoostClassifier</code>; <code>XGBoostRegressor</code> 1 (base parameters) learning_rate [0.05, 0.3] 2 (tree parameters) max_depth [1, 15] min_child_weights [1, 6] gamma [0, 5] 3 (sampling parameters) colsample_bytree [0.75, 0.9] subsample [0.75, 0.9] 4 (regularization parameters) reg_alpha [0, 5] reg_lambda [0, 10]"},{"location":"user_guide/concepts/hyperopt/#tuning-recipes-for-feature-learners","title":"Tuning recipes for feature learners","text":"Feature Learner Stage Hyperparameter Subspace <code>FastProp</code> 1 (base parameters) num_features [50, 500] n_most_frequent [0, 20] <code>Multirel</code> 1 (base parameters) num_features [10, 50] shrinkage [0, 0.3] 2 (tree parameters) max_length [0, 10] min_num_samples [1, 500] 3 (regularization parameters) share_aggregations [0.1, 0.5] <code>Relboost</code> 1 (base parameters) num_features [10, 50] shrinkage [0, 0.3] 2 (tree parameters) max_length [0, 10] min_num_samples [1, 500] 3 (regularization parameters) share_aggregations [0.1, 0.5] <code>RelMT</code> 1 (base parameters) num_features [10, 50] shrinkage [0, 0.3] 2 (tree parameters) max_depth [1, 8] min_num_samples [1, 500] 3 (regularization parameters) reg_lambda [0, 0.0001] <p>The advantage of the tuning routines is that they provide a convenient out-of-the-box experience for hyperparameter tuning. For most use cases, it is sufficient to tune the XGBoost predictor.</p> <p>More advanced users can rely on the more low-level hyperparameter optimization routines.</p>"},{"location":"user_guide/concepts/hyperopt/#random-search","title":"Random search","text":"<p>A <code>RandomSearch</code> draws random hyperparameter sets from the hyperparameter space.</p>"},{"location":"user_guide/concepts/hyperopt/#latin-hypercube-search","title":"Latin hypercube search","text":"<p>A <code>LatinHypercubeSearch</code> draws almost random hyperparameter sets from the hyperparameter space, but ensures that they are sufficiently different from each other.</p>"},{"location":"user_guide/concepts/hyperopt/#gaussian-hyperparameter-search","title":"Gaussian hyperparameter search","text":"<p>A <code>GaussianHyperparameterSearch</code> works like this:</p> <ul> <li> <p>It begins with a burn-in phase, usually about 70% to 90% of all iterations. During that burn-in phase, the hyperparameter space is sampled more or less at random, using either a random search or a latin hypercube search. You can control this phase using <code>ratio_iter</code> and <code>surrogate_burn_in_algorithm</code>.</p> </li> <li> <p>Once enough information has been collected, it fits a Gaussian process on the hyperparameters with the score we want to maximize or minimize as the predicted variable. Note that the Gaussian process has hyperparameters itself, which are also optimized. You can control this phase using <code>gaussian_kernel</code>, <code>gaussian_optimization_algorithm</code>, <code>gaussian_optimization_burn_in_algorithm</code>, and <code>gaussian_optimization_burn_ins</code>.</p> </li> <li> <p>It then uses the Gaussian process to predict the expected information (EI). The EI is a measure of how much additional information it might get from evaluating a particular point in the hyperparameter space. The expected information is to be maximized. The point in the hyperparameter space with the maximum expected information is the next point that is actually evaluated (meaning a new pipeline with these hyperparameters is trained). You can control this phase using <code>optimization_algorithm</code>, <code>optimization_burn_ins</code>, and <code>optimization_burn_in_algorithm</code>.</p> </li> </ul> <p>Note</p> <p>In a nutshell, the <code>GaussianHyperparameterSearch</code> behaves like human data scientists:</p> <ul> <li>At first, it picks random hyperparameter combinations.</li> <li>Once it has gained a better understanding of the hyperparameter space, it starts evaluating hyperparameter combinations that are particularly interesting.</li> </ul>"},{"location":"user_guide/concepts/importing_data/","title":"Importing Data","text":""},{"location":"user_guide/concepts/importing_data/#importing-data_1","title":"Importing data","text":"<p>Before being able to analyze and process your data using the getML Suite, you have to import it into the Engine. At the end of this step, you will have your data in data frame objects in the getML Engine and will be ready to annotate them.</p> <p>Note</p> <p>If you have imported your data into the Engine before and want to restore it, refer to Python API: DataFrame</p> <p></p>"},{"location":"user_guide/concepts/importing_data/#unified-import-interface","title":"Unified import interface","text":"<p>The getML Python API provides a unified import interface requiring similar arguments and resulting in the same output format, regardless of the data source.</p> <p>You can use one of the dedicated class methods (e.g. <code>from_csv()</code>)  to construct a data frame object in the getML Engine, fill it with the provided data, and retrieve  a <code>DataFrame</code> handle in the Python API.</p> <p>Example</p> <p>Example demonstrating the use of <code>from_csv()</code> <pre><code>data = getml.data.DataFrame.from_csv(\n    \"path/to/my/data.csv\", \n    \"my_data\"\n)\n</code></pre></p> <p>If you already have a data frame object in place, you can use the methods of the corresponding  <code>DataFrame</code> handle (e.g. <code>read_csv()</code>)  to either replace its content with new data or append to it.</p> <p>All those functions also have their counterparts for exporting (e.g. <code>to_csv()</code>).</p>"},{"location":"user_guide/concepts/importing_data/#data-frames","title":"Data Frames","text":"<p>The resulting <code>DataFrame</code> instance in the Python API represents a handle to the corresponding data frame object in the getML Engine. The mapping between the two is done based on the name of the object, which has to be unique. Similarly, the names of  the <code>columns</code> are required to be unique within the data frame they are associated with.</p>"},{"location":"user_guide/concepts/importing_data/#handling-of-null-values","title":"Handling of NULL values","text":"<p>Unfortunately, data sources often  contain missing or corrupt data - also called NULL values. getML is able to work with missing values except for the target variable, which must not contain any NULL values (because having NULL targets does not make any sense). Please refer to the section on  join keys for details about their handling during the construction of the data model.</p> <p>During import, a NULL value is automatically inserted at all occurrences of the strings \"nan\", \"None\", \"NA\", or an empty string as well as at all occurrences of <code>None</code> and <code>NaN</code>.</p>"},{"location":"user_guide/concepts/importing_data/#import-formats","title":"Import Formats","text":""},{"location":"user_guide/concepts/importing_data/#csv_1","title":"CSV","text":"<p>The fastest way to import data into the getML Engine is to read it directly from CSV files.</p> <p>Import from CSV</p> <p>Using the <code>from_csv()</code> class method, you can create a new <code>DataFrame</code> based on a table stored in the provided file(s). The <code>read_csv()</code> method will replace the content of the current <code>DataFrame</code> instance or append further rows.</p> <p>Export to CSV</p> <p>In addition to reading data from a CSV file, you can also write an existing <code>DataFrame</code> back into one using <code>to_csv()</code>.</p> <p></p>"},{"location":"user_guide/concepts/importing_data/#pandas_1","title":"Pandas","text":"<p>Pandas is one of the key packages used in most data science projects done in Python. The associated import interface is one of the slowest, but you can harness the good data exploration and manipulation capabilities of this Python package.</p> <p>Import from Pandas</p> <p>Using the <code>DataFrame.from_pandas()</code> class method, you can create a new <code>DataFrame</code> based on the provided <code>pandas.DataFrame</code>. The <code>read_pandas()</code> method will replace the content of the current <code>DataFrame</code> instance or append further rows.</p> <p>Export to Pandas</p> <p>In addition to reading data from a <code>pandas.DataFrame</code>, you can also write an existing <code>DataFrame</code> back into a <code>pandas.DataFrame</code> using <code>DataFrame.to_pandas()</code>. </p> <p>Note</p> <p>Due to the way data is stored within the getML Engine, the dtypes of the original <code>pandas.DataFrame</code> cannot be restored properly and there might be inconsistencies in the order of microseconds being introduced into timestamps.</p> <p></p>"},{"location":"user_guide/concepts/importing_data/#json_1","title":"JSON","text":"<p>A convenient but slow way to import data into the getML Engine via its Python API.</p> <p>Import from JSON</p> <p>Using the <code>from_json()</code> class method, you can create a new <code>DataFrame</code> based on a JSON string. The <code>read_json()</code> method will replace the content of the current <code>DataFrame</code> instance or append further rows.</p> <p>Export to JSON</p> <p>In addition to reading data from a JSON string, you can also convert an existing <code>DataFrame</code> into one using <code>to_json()</code>.</p> <p></p>"},{"location":"user_guide/concepts/importing_data/#sqlite3_1","title":"SQLite3","text":"<p>SQLite3 is a popular in-memory database. It is faster than classical relational databases like PostgreSQL but less stable under massive parallel access. It requires all contained datasets to be loaded into memory, which might use up too much RAM, especially for large datasets.</p> <p>As with all other databases in the unified import interface of the getML Python API,  you first need to connect to it using <code>connect_sqlite3()</code>.</p> <p>Import from SQLite3</p> <p>By selecting an existing table from your database in the <code>DataFrame.from_db()</code> class method, you can create a new <code>DataFrame</code> containing all its data. Alternatively, you can use the <code>read_db()</code> and <code>read_query()</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p> <p>Export to SQLite3</p> <p>You can also write your results back into the SQLite3 database. By providing a name for the destination table in <code>transform()</code>, the features generated from your raw data will be written back. Passing it into <code>predict()</code> generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p> <p></p>"},{"location":"user_guide/concepts/importing_data/#mysql_1","title":"MySQL","text":"<p>MySQL is one of the most popular databases in use today. It  can be connected to the getML Engine using the function <code>connect_mysql()</code>. But first, make sure your database is running, you have the corresponding hostname, port as well as your user name and password ready, and you can reach it from via your command line.</p> <p>If you are unsure which port or socket your MySQL is running on, you can start the <code>mysql</code> command line interface <pre><code>$ mysql\n</code></pre></p> <p>Once inside the MySQL interface, use the following queries to get the required insights: <pre><code>&gt; SELECT @@port;\n\n&gt; SELECT @@socket;\n</code></pre></p> <p>Import from MySQL</p> <p>By selecting an existing table of your database in the <code>DataFrame.from_db()</code> class method, you can create a new <code>DataFrame</code> containing all its data. Alternatively, you can use the <code>read_db()</code> and <code>read_query()</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p> <p>Export to MySQL</p> <p>You can also write your results back into the MySQL database. By providing a name for the destination table in <code>transform()</code>, the features generated from your raw data will be written back. Passing it into <code>predict()</code> generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p> <p></p>"},{"location":"user_guide/concepts/importing_data/#mariadb_1","title":"MariaDB","text":"<p>MariaDB is a popular open source fork of MySQL. It can be connected to the getML Engine using the function <code>connect_mariadb()</code>. But first, make sure your database is running, you have the corresponding hostname, port as well as your username and password ready, and you can reach it from your command line.</p> <p>If you are unsure which port or socket your MariaDB is running on, you can start the <code>mysql</code> command line interface </p> <pre><code>$ mysql\n</code></pre> <p>Once inside the MariaDB interface, use the following queries to get the required insights:</p> <pre><code>MariaDB [(none)]&gt; SELECT @@port;\n\nMariaDB [(none)]&gt; SELECT @@socket;\n</code></pre> <p>Import from MariaDB</p> <p>By selecting an existing table of your database in the <code>DataFrame.from_db()</code> class method, you can create a new <code>DataFrame</code> containing all its data. Alternatively, you can use the <code>read_db()</code> and <code>read_query()</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p> <p>Export to MariaDB</p> <p>You can also write your results back into the MariaDB database. By providing a name for the destination table in <code>transform()</code>, the features generated from your raw data will be written back. Passing it into <code>predict()</code> generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p> <p></p>"},{"location":"user_guide/concepts/importing_data/#postgresql_1","title":"PostgreSQL","text":"<p>PostgreSQL is a powerful and well-established open  source database system. It can be connected to the getML Engine using the function <code>connect_postgres()</code>. Make sure your database is running, you have the corresponding hostname, port, user name, and password ready, and you can reach it from your command line.</p> <p>Import from PostgreSQL</p> <p>By selecting an existing table from your database in the <code>DataFrame.from_db()</code> class method, you can create a new <code>DataFrame</code> containing all its data. Alternatively, you can use the <code>read_db()</code> and <code>read_query()</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p> <p>Export to PostgreSQL</p> <p>You can also write your results back into the PostgreSQL database. If you provide a name for the destination table in <code>transform()</code>, the features generated from your raw data will be written back. Passing it into <code>predict()</code> generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p> <p></p>"},{"location":"user_guide/concepts/importing_data/#greenplum_1","title":"Greenplum","text":"<p>Enterprise edition</p> <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> <p>Greenplum is an open source database system maintained by Pivotal Software, Inc. It can be connected to the getML Engine using the function <code>connect_greenplum()</code>. But first, make sure your database is running, you have the corresponding hostname, port as well as your user name and password ready, and you can reach it from your command line.</p> <p>Import from Greenplum</p> <p>By selecting an existing table of your database in the <code>from_db()</code> class method, you can create a new <code>DataFrame</code> containing all its data. Alternatively, you can use the <code>read_db()</code> and <code>read_query()</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p> <p>Export to Greenplum</p> <p>You can also write your results back into the Greenplum database. By providing a name for the destination table in <code>Pipeline.transform()</code>, the features generated from your raw data will be written back. Passing it into <code>Pipeline.predict()</code> generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p> <p></p>"},{"location":"user_guide/concepts/importing_data/#odbc_1","title":"ODBC","text":"<p>Enterprise edition</p> <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> <p>ODBC (Open Database Connectivity) is an API specification for connecting software programming language to a database, developed by Microsoft.</p> <p>In a nutshell, it works like this:</p> <ul> <li>Any database of relevance has an ODBC driver that translates calls from the ODBC API into a format the database can understand, returning the query results in a format understood by the ODBC API.</li> <li>To connect getML or other software to a database using ODBC, you first need to install the ODBC driver provided by your database vendor.</li> <li>In theory, ODBC drivers should translate queries from the SQL 99 standard into the SQL dialect, but this is often ignored in practice. Also, not all ODBC drivers support all ODBC calls.</li> </ul> <p>With getML, native APIs are preferred for connecting to relational databases. ODBC is used when native APIs are not feasible due to licensing or other restrictions, especially for connecting to proprietary databases like Oracle, Microsoft SQL Server, or IBM DB2.</p> <p>ODBC is pre-installed on modern Windows operating systems, while Linux and macOS can use open-source implementations like unixODBC and iODBC, with getML using unixODBC.</p> <p>An example: Microsoft SQL Server</p> <p>To connect to Microsoft SQL Server using ODBC:</p> <ol> <li>If you do not have a Microsoft SQL Server instance, you can download a trial or development version.</li> <li>Download the ODBC driver for SQL Server.</li> <li>Configure the ODBC driver. Many drivers provide customized scripts for this, so manual configuration might not be necessary.</li> </ol> <p>For Linux and macOS, create a <code>.odbc.ini</code> file in your home directory with the following contents:</p> <p><pre><code>[ANY-NAME-YOU-WANT]\nDriver = /opt/microsoft/msodbcsql17/lib64/libmsodbcsql-17.5.so.2.1\nServer = 123.45.67.890\nPort = 1433\nUser = YOUR-USERNAME\nPassword = YOUR-PASSWORD\nDatabase = YOUR-DATABASE\nLanguage = us_english\nNeedODBCTypesOnly = 1\n</code></pre> On Docker, you can make appropriate changes to the Dockerfile and then rerun <code>./setup.sh</code> or <code>bash setup.sh</code>.</p> <p>You will need to set the following parameters:</p> <ul> <li>The first line is the server name or data source name. You can use this name to tell getML that this is the server you want to connect to.</li> <li>The Driver is the location of the ODBC driver you have just downloaded. The location or file name might be different on your system.</li> <li>The Server is the IP address of the server. If the server is on the same machine as getML, just write \"localhost\".</li> <li>The Port is the port on which to connect the server. The default port for SQL Server is 1433.</li> <li>User and Password are the user name and password that allow access to the server.</li> <li>The Database is the database inside the server you want to connect to.</li> </ul> <p>You can now connect getML to the database:</p> <pre><code>getml.database.connect_odbc(\n    server_name=\"ANY-NAME-YOU-WANT\",\n    user=\"YOUR-USERNAME\",\n    password=\"YOUR-PASSWORD\",\n    escape_chars=\"[]\")\n</code></pre> <p>Important: Always pass escape_chars</p> <p>Earlier we mentioned that ODBC drivers are supposed to translate standard SQL queries into the specific SQL dialects, but this requirement is often ignored.</p> <p>A typical issue is escape characters, needed when the names of your schemas, tables, or columns are SQL keywords, like the loans dataset containing a table named ORDER.</p> <p>To avoid this problem, you can envelop the schema, table, and column names in escape characters.  <pre><code>SELECT \"some_column\" FROM \"SOME_SCHEMA\".\"SOME_TABLE\";\n</code></pre> getML always uses escape characters for its automatically generated queries.</p> <p>The SQL standard requires that the quotation mark (\") be used as the escape character. However, many SQL dialects do not follow this requirement, e.g., SQL Server uses \"[]\":</p> <p><pre><code>SELECT [some_column] FROM [SOME_SCHEMA].[SOME_TABLE];\n</code></pre> MySQL and MariaDB work like this: <pre><code>SELECT `some_column` FROM `SOME_SCHEMA`.`SOME_TABLE`;\n</code></pre> To avoid frustration, determine your server's escape characters and explicitly pass  them to <code>connect_odbc()</code>.</p> <p>Import data using ODBC</p> <p>By selecting an existing table from your database in the <code>DataFrame.from_db()</code> class method, you can create a new <code>DataFrame</code> containing all its data. Alternatively, you can use the <code>read_db()</code> and <code>read_query()</code> methods to replace the content of the current <code>DataFrame</code> instance or append further rows based on either a table or a specific query.</p> <p>Export data using ODBC</p> <p>You can also write your results back into the database using ODBC. When you provide a name for the destination table in <code>transform()</code>, the features generated from your raw data will be written back. Passing it into <code>predict()</code> generates predictions of the target variables to new, unseen data and stores the result into the corresponding table.</p>"},{"location":"user_guide/concepts/predicting/","title":"Predicting","text":""},{"location":"user_guide/concepts/predicting/#predicting_1","title":"Predicting","text":"<p>Now that you know how to engineer a flat table of features, you are ready to make predictions of the target variable(s).</p>"},{"location":"user_guide/concepts/predicting/#using-getml","title":"Using getML","text":"<p>getML comes with six built-in machine learning predictors:</p> <ul> <li><code>LinearRegression</code></li> <li><code>LogisticRegression</code></li> <li><code>XGBoostClassifier</code></li> <li><code>XGBoostRegressor</code></li> <li><code>ScaleGBMClassifier</code></li> <li><code>ScaleGBMRegressor</code></li> </ul> <p>Enterprise edition</p> <p><code>ScaleGBMClassifier</code> and <code>ScaleGBMRegressor</code> are exclusive to the Enterprise edition and are not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> <p>Using one of them in your analysis is very simple. Just pass one as the <code>predictor</code> argument to either <code>Pipeline</code> on initialization. As a list, more than one predictor can be passed to the pipeline.</p> <pre><code>feature_learner1 = getml.feature_learners.Relboost()\n\nfeature_learner2 = getml.feature_learners.Multirel()\n\npredictor = getml.predictors.XGBoostRegressor()\n\npipe = getml.pipeline.Pipeline(\n    data_model=data_model,\n    peripheral=peripheral_placeholder,\n    feature_learners=[feature_learner1, feature_learner2],\n    predictors=predictor,\n)\n</code></pre> <p>When you call <code>fit()</code> on a pipeline, the entire pipeline will be trained.</p> <p>Note</p> <p>The time estimation for training a pipeline is a rough estimate. Occasionally, the training time can be significantly longer than the estimate. But the pipeline never silently crashes. Given enough time, computations always finish.</p> <p>Note that <code>Pipeline</code> comes with dependency tracking. That means it can figure out on its own what has changed and what needs to be trained again.</p> <pre><code>feature_learner1 = getml.feature_learners.Relboost()\n\nfeature_learner2 = getml.feature_learners.Multirel()\n\npredictor = getml.predictors.XGBoostRegressor()\n\npipe = getml.pipeline.Pipeline(\n    data_model=data_model,\n    population=population_placeholder,\n    peripheral=peripheral_placeholder,\n    feature_learners=[feature_learner1, feature_learner2],\n    predictors=predictor \n)\n\npipe.fit(...)\n\npipe.predictors[0].n_estimators = 50\n\n# Only the predictor has changed,\n# so only the predictor will be refitted.\npipe.fit(...)\n</code></pre> <p>To score the performance of your prediction on a test dataset, the getML models come with a <code>score()</code> method. The available metrics are documented in <code>metrics</code>.</p> <p>To use a trained model, including both the trained features and the predictor, to make predictions on new, unseen data, call the <code>predict()</code> method of your model.</p>"},{"location":"user_guide/concepts/predicting/#using-external-software","title":"Using external software","text":"<p>In our experience, the most relevant contribution to making accurate predictions are the generated features. Before trying to tweak your analysis by using sophisticated prediction algorithms and tuning their hyperparameters, we recommend tuning the hyperparameters of your <code>Multirel</code> or <code>Relboost</code> instead. You can do so either by hand or using getML's automated hyperparameter optimization.</p> <p>If you wish to use external predictors, you can transform new data, which is compliant with your relational data model, to a flat feature table using the <code>transform()</code> method of your pipeline.</p>"},{"location":"user_guide/concepts/preprocessing/","title":"Preprocessing","text":""},{"location":"user_guide/concepts/preprocessing/#preprocessing_1","title":"Preprocessing","text":"<p>As preprocessing, we categorize operations on data frames that are not directly related to the relational data model. While feature learning and <code>propositionalization</code> deal with relational data structures and result in a single-table representation thereof, we categorize all operations that work on single tables as preprocessing. This includes numerical transformations, encoding techniques, or alternative representations.</p> <p>getML's preprocessors allow you to extract domains from email addresses (<code>EmailDomain</code>), impute missing values (<code>Imputation</code>), map categorical columns to a continuous representation (<code>Mapping</code>), extract seasonal components from time stamps (<code>Seasonal</code>), extract sub strings from string-based columns (<code>Substring</code>) and split up <code>text</code> columns (<code>TextFieldSplitter</code>). Preprocessing operations in getML are very efficient and happen really fast. In fact, most of the time you won't even notice the presence of a preprocessor in your pipeline. getML's preprocessors operate on an abstract level without polluting your original data, are evaluated lazily and their set-up requires minimal effort.</p> <p>Here is a small example that shows the <code>Seasonal</code> preprocessor in action. <pre><code>import getml\n\ngetml.project.switch(\"seasonal\")\n\ntraffic = getml.datasets.load_interstate94()\n\n# traffic explicitly holds seasonal components (hour, day, month, ...)\n# extracted from column ds; we copy traffic and delete all those components\ntraffic2 = traffic.drop([\"hour\", \"weekday\", \"day\", \"month\", \"year\"])\n\nstart_test = getml.data.time.datetime(2018, 3, 14)\n\nsplit = getml.data.split.time(\n    population=traffic,\n    test=start_test,\n    time_stamp=\"ds\",\n)\n\ntime_series1 = getml.data.TimeSeries(\n    population=traffic,\n    split=split,\n    time_stamps=\"ds\",\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.days(7),\n    lagged_targets=True,\n)\n\ntime_series2 = getml.data.TimeSeries(\n    population=traffic2,\n    split=split,\n    time_stamps=\"ds\",\n    horizon=getml.data.time.hours(1),\n    memory=getml.data.time.days(7),\n    lagged_targets=True,\n)\n\nfast_prop = getml.feature_learning.FastProp(\n    loss_function=getml.feature_learning.loss_function.SquareLoss)\n\npipe1 = getml.pipeline.Pipeline(\n    data_model=time_series1.data_model,\n    feature_learners=[fast_prop],\n    predictors=[getml.predictors.XGBoostRegressor()]\n)\n\npipe2 = getml.pipeline.Pipeline(\n    data_model=time_series2.data_model,\n    preprocessors=[getml.preprocessors.Seasonal()],\n    feature_learners=[fast_prop],\n    predictors=[getml.predictors.XGBoostRegressor()]\n)\n\n# pipe1 includes no preprocessor but receives the data frame with the components\npipe1.fit(time_series1.train)\n\n# pipe2 includes the preprocessor; receives data w/o components\npipe2.fit(time_series2.train)\n\nmonth_based1 = pipe1.features.filter(lambda feat: \"month\" in feat.sql)\nmonth_based2 = pipe2.features.filter(\n    lambda feat: \"COUNT( DISTINCT t2.\\\"strftime('%m'\" in feat.sql\n)\n\nprint(month_based1[1].sql)\n# Output:\n# DROP TABLE IF EXISTS \"FEATURE_1_10\";\n# \n# CREATE TABLE \"FEATURE_1_10\" AS\n# SELECT COUNT( t2.\"month\"  ) - COUNT( DISTINCT t2.\"month\" ) AS \"feature_1_10\",\n#     t1.rowid AS \"rownum\"\n# FROM \"POPULATION__STAGING_TABLE_1\" t1\n# LEFT JOIN \"POPULATION__STAGING_TABLE_2\" t2\n# ON 1 = 1\n# WHERE t2.\"ds, '+1.000000 hours'\" &lt;= t1.\"ds\"\n# AND ( t2.\"ds, '+7.041667 days'\" &gt; t1.\"ds\" OR t2.\"ds, '+7.041667 days'\" IS NULL )\n# GROUP BY t1.rowid;\n\nprint(month_based2[0].sql)\n# Output:\n# DROP TABLE IF EXISTS \"FEATURE_1_5\";\n# \n# CREATE TABLE \"FEATURE_1_5\" AS\n# SELECT COUNT( t2.\"strftime('%m', ds )\"  ) - COUNT( DISTINCT t2.\"strftime('%m', ds )\" ) AS \"feature_1_5\",\n#     t1.rowid AS \"rownum\"\n# FROM \"POPULATION__STAGING_TABLE_1\" t1\n# LEFT JOIN \"POPULATION__STAGING_TABLE_2\" t2\n# ON 1 = 1\n# WHERE t2.\"ds, '+1.000000 hours'\" &lt;= t1.\"ds\"\n# AND ( t2.\"ds, '+7.041667 days'\" &gt; t1.\"ds\" OR t2.\"ds, '+7.041667 days'\" IS NULL )\n# GROUP BY t1.rowid;\n</code></pre></p> <p>If you compare both of the features above, you will notice they are exactly the same: <code>COUNT - COUNT(DISTINCT)</code> on the month component conditional on the time-based restrictions introduced through memory and horizon.</p> <p>Pipelines can include more than one preprocessor.</p> <p>While most of getML's preprocessors are straightforward, two of them deserve a more detailed introduction: <code>Mapping</code> and <code>TextFieldSplitter</code>. </p>"},{"location":"user_guide/concepts/preprocessing/#mappings","title":"Mappings","text":"<p>Enterprise edition</p> <p>This feature is exclusive to the Enterprise edition and is not available in the Community edition. Discover the benefits of the Enterprise edition and compare their features.</p> <p>For licensing information and technical support, please contact us.</p> <p><code>Mappings</code> are an alternative representation for categorical columns, text columns, and (quasi-categorical) discrete-numerical columns. Each discrete value (category) of a categorical column is mapped to a continuous spectrum by calculating the average target value for the subset of all rows belonging to the respective category. For columns from peripheral tables, the average target values are propagated back by traversing the relational structure.</p> <p>Mappings are a simple and interpretable alternative representation for categorical data. By introducing a continuous representation, mappings allow getML's feature learning algorithms to apply arbitrary aggregations to categorical columns. Further, mappings enable huge gains in efficiency when learning patterns from categorical data. You can control the extent mappings are utilized by specifying the minimum number of matching rows required for categories that constitutes a mapping through the <code>min_freq</code> parameter.</p> <p>Here is an example mapping from the CORA notebook: <pre><code> DROP TABLE IF EXISTS \"CATEGORICAL_MAPPING_1_1_1\";\n CREATE TABLE \"CATEGORICAL_MAPPING_1_1_1\"(key TEXT NOT NULL PRIMARY KEY, value NUMERIC);\n INSERT INTO \"CATEGORICAL_MAPPING_1_1_1\"(key, value)\n VALUES('Case_Based', 0.7109826589595376),\n       ('Rule_Learning', 0.07368421052631578),\n       ('Reinforcement_Learning', 0.0576923076923077),\n       ('Theory', 0.0547945205479452),\n       ('Genetic_Algorithms', 0.03157894736842105),\n       ('Neural_Networks', 0.02088772845953003),\n       ('Probabilistic_Methods', 0.01293103448275862);\n</code></pre> Inspecting the actual values, it's highly likely, that this mapping stems from a feature learned by a sub learner targeting the label \"Case_Based\". In addition to the trivial case, we can see that the next closed neighboring category is the \"Rule_Learning\" category, to which 7.3 % of the papers citing the target papers are categorized. </p>"},{"location":"user_guide/concepts/preprocessing/#handling-of-free-form-text","title":"Handling of free form text","text":"<p>getML provides the role <code>text</code> to annotate free form text fields within relational data structures. Learning from <code>text</code> columns works as follows: First, for each of the <code>text</code> columns, a vocabulary is built by taking into account the feature learner's text mining specific hyperparameter <code>vocab_size</code>. If a text field contains words that belong to the vocabulary, getML deals with columns of role <code>text</code> through one of two approaches: Text fields can either be integrated into features by learning conditions based on the mere presence (or absence) of certain words in those text fields (the default) or they can be split into a relational bag-of-words representation by means of the <code>TextFieldSplitter</code> preprocessor. Opting for the second approach is as easy as adding the <code>TextFieldSplitter</code> to the list of <code>preprocessors</code> on your <code>Pipeline</code>. The resulting bag of words can be viewed as another one-to-many relationship within our data model where each row holding a text field is related to n peripheral rows (n is the number of words in the text field). Consider the following example, where the text field is split into a relational bag of words.</p>"},{"location":"user_guide/concepts/preprocessing/#one-row-of-a-table-with-a-text-field","title":"One row of a table with a text field","text":"rownum text field 52 The quick brown fox jumps over the lazy dog"},{"location":"user_guide/concepts/preprocessing/#the-implicit-peripheral-table-that-results-from-splitting","title":"The (implicit) peripheral table that results from splitting","text":"rownum words 52 the 52 quick 52 brown 52 fox 52 jumps 52 over 52 the 52 lazy 52 dog <p>As text fields now present another relation, getML's feature learning algorithms are able to learn structural logic from text fields' contents by applying aggregations over the resulting bag of words itself (<code>COUNT WHERE words IN ('quick', 'jumps')</code>). Further, by utilizing mappings, any aggregation applicable to a (mapped) categorical column can be applied to bag-of-words mappings as well.</p> <p>Note that the splitting of text fields can be computationally expensive. If performance suffers too much, you may resort to the default behavior by removing the <code>TextFieldSplitter</code> from your <code>Pipeline</code>.</p>"},{"location":"user_guide/concepts/project_management/","title":"Managing projects","text":""},{"location":"user_guide/concepts/project_management/#managing-projects","title":"Managing Projects","text":"<p>When working with getML, all data is organized into projects. These projects are managed through the <code>getml.project</code> module.</p>"},{"location":"user_guide/concepts/project_management/#relationship-between-projects-and-engine-processes","title":"Relationship between Projects and Engine Processes","text":"<p>Each project is linked to a specific instance of the getML Engine, which runs as a global process, independent of your Python session. This setup allows multiple users to share a single getML instance and work on different projects. When switching projects using <code>getml.project.switch()</code>, the Python API spawns a new process and connects to it, while the current project's process moves to the background (unless explicitly suspended via <code>getml.project.suspend()</code>). You can also work on multiple projects simultaneously from different Python sessions, which is particularly useful when using Jupyter Lab to manage multiple notebooks and Python kernels concurrently.</p> <p>To load an existing project or create a new one, use <code>getml.engine.set_project()</code>.</p> <p>To shut down the Engine process associated with the current project, call <code>getml.project.suspend()</code>. Suspending a project flushes the Engine's memory, and any unsaved changes to data frames are lost (refer to Dataframes for details). All pipelines of the new project are automatically loaded into memory. You can retrieve all your project's pipelines through <code>getml.project.pipelines</code>. </p> <p>Projects can be deleted by calling <code>getml.engine.delete_project()</code> to delete a project by name or <code>getml.project.delete()</code> to suspend and delete the currently loaded project.</p>"},{"location":"user_guide/concepts/project_management/#managing-data-using-projects","title":"Managing Data Using Projects","text":"<p>Each project has its own folder in <code>~/.getML/getml-VERSION/projects</code> (for Linux and macOS) where all its data and pipelines are stored. On Windows, the projects folder is located in the same directory as <code>getML.exe</code>. These folders can be easily shared between different getML instances and even across different operating systems. However, individual pipelines or data frames cannot be simply copied to another project folder as they are tied to the project. Projects can be bundled, exported, and imported.</p>"},{"location":"user_guide/concepts/project_management/#using-the-project-module-to-manage-your-project","title":"Using the Project Module to Manage Your Project","text":"<p>The <code>getml.project</code> module is the entry point for managing your projects. From here, you can query project-specific data (<code>data_frames</code>, <code>hyperopts</code>, <code>pipelines</code>), manage the state of the current project (<code>delete()</code>, <code>restart()</code>, <code>switch()</code>, <code>suspend()</code>), and import or export projects as <code>.getml</code> bundles to disk (<code>load()</code>, <code>save()</code>).</p>"},{"location":"user_guide/quick_start/","title":"Quick start","text":""},{"location":"user_guide/quick_start/#quick-start","title":"Quick start","text":"<p>getML is an innovative tool for the end-to-end automation of data science projects. It covers everything from convenient data loading procedures to the deployment of trained models.</p> <p>Most notably, getML includes advanced algorithms for automated feature engineering (feature learning) on relational data and time series. Feature engineering on relational data is defined as the creation of a flat table by merging and aggregating data. It is sometimes also referred to as data wrangling. Feature engineering is necessary if your data is distributed over more than one data table.</p> <p>Automated feature engineering</p> <ul> <li>Saves up to 90% of the time spent on a data science project</li> <li>Increases the prediction accuracy over manual feature engineering</li> </ul> <p>Andrew Ng, Professor at Stanford University and Co-founder of Google Brain described manual feature engineering as follows:</p> <p>Coming up with features is difficult, time-consuming, requires expert knowledge. \"Applied machine learning\" is basically feature engineering.</p> <p>The main purpose of getML is to automate this \"difficult, time-consuming\" process as much as possible.</p> <p>getML comes with a high-performance Engine written in C++ and an intuitive Python API. Completing a data science project with getML consists of eight simple steps.</p>"},{"location":"user_guide/quick_start/#1-launch-the-engine","title":"1. Launch the Engine","text":"<pre><code>import getml\n\ngetml.engine.launch()\ngetml.engine.set_project('one_minute_to_getml')\n</code></pre>"},{"location":"user_guide/quick_start/#2-load-the-data-into-the-engine","title":"2. Load the data into the Engine","text":"<pre><code>df_population = getml.data.DataFrame.from_csv('data_population.csv',\n            name='population_table')\ndf_peripheral = getml.data.DataFrame.from_csv('data_peripheral.csv',\n            name='peripheral_table')\n</code></pre>"},{"location":"user_guide/quick_start/#3-annotate-the-data","title":"3. Annotate the data","text":"<pre><code>df_population.set_role(cols='target', role=getml.data.role.target)\ndf_population.set_role(cols='join_key', role=getml.data.role.join_key)\n</code></pre>"},{"location":"user_guide/quick_start/#4-define-the-data-model","title":"4. Define the data model","text":"<pre><code>dm = getml.data.DataModel(population=df_population.to_placeholder())\ndm.add(df_peripheral.to_placeholder())\ndm.population.join(\n   dm.peripheral,\n   on=\"join_key\",\n)\n</code></pre>"},{"location":"user_guide/quick_start/#5-train-the-feature-learning-algorithm-and-the-predictor","title":"5. Train the feature learning algorithm and the predictor","text":"<pre><code>pipe = getml.pipeline.Pipeline(\n    data_model=dm,\n    feature_learners=getml.feature_learning.FastProp(),\n    predictors=getml.predictors.LinearRegression()\n)\npipe.fit(\n    population=df_population,\n    peripheral=[df_peripheral]\n)\n</code></pre>"},{"location":"user_guide/quick_start/#6-evaluate","title":"6. Evaluate","text":"<pre><code>pipe.score(\n    population=df_population_unseen,\n    peripheral=[df_peripheral_unseen]\n)\n</code></pre>"},{"location":"user_guide/quick_start/#7-predict","title":"7. Predict","text":"<pre><code>pipe.predict(\n    population=df_population_unseen,\n    peripheral=[df_peripheral_unseen]\n)\n</code></pre>"},{"location":"user_guide/quick_start/#8-deploy","title":"8. Deploy","text":"<pre><code># Allow the pipeline to respond to HTTP requests\npipe.deploy(True)\n</code></pre> <p>Check out the rest of this documentation to find out how getML achieves top performance on real-world data science projects with many tables and complex data schemes.</p>"},{"location":"user_guide/walkthrough/","title":"Walkthrough","text":""},{"location":"user_guide/walkthrough/#walkthrough","title":"Walkthrough","text":"<p>In this walkthrough, you will learn about the basic concepts of getML. You will tackle a simple problem using the Python API in order to gain a technical understanding of the benefits of getML. More specifically, you will learn how to do the following:</p> <ol> <li>Start a new project</li> <li>Define a data model</li> <li>Building a pipeline</li> <li>Working with a pipeline</li> </ol> <p>The guide is applicable to both the Enterprise and the Community editions of getML. The differences between the two are highlighted here.</p> <p>You have not installed getML on your machine yet? Before you get started, head over to the installation instructions.</p>"},{"location":"user_guide/walkthrough/#introduction","title":"Introduction","text":"<p>Automated machine learning (AutoML) has attracted a great deal of attention in recent years. The goal is to simplify the application of traditional machine learning methods to real-world business problems by automating key steps of a data science project, such as feature extraction, model selection, and hyperparameter optimization. With AutoML, data scientists are able to develop and compare dozens of models, gain insights, generate predictions, and solve more business problems in less time.</p> <p>While it is often claimed that AutoML covers the complete workflow of a data science project - from the raw data set to the deployable machine learning models - current solutions have one major drawback: They cannot handle real world business data. This data typically comes in the form of relational data. The relevant information is scattered over a multitude of tables that are related via so-called join keys. In order to start an AutoML pipeline, a flat feature table has to be created from the raw relational data by hand. This step is called feature engineering and is a tedious and error-prone process that accounts for up to 90% of the time in a data science project.</p> <p></p> <p>getML adds automated feature engineering on relational data and time series to AutoML. The getML algorithms Multirel, Relboost, and RelMT find the right aggregations and subconditions needed to construct meaningful features from the raw relational data. This is done by performing a sophisticated, gradient-boosting-based heuristic. In doing so, getML brings the vision of end-to-end automation of machine learning within reach for the first time. Note that getML also includes automated model deployment via a HTTP endpoint or database connectors.</p> <p>All functionality of getML is implemented in the so-called getML Engine. It is written in C++ to achieve the highest performance and efficiency possible and is responsible for all the heavy lifting. The getML Python API acts as a bridge to communicate with the Engine.  In addition, the getML Monitor, a graphical web interface available in the Enterprise edition, provides you with an overview of your current projects and pipelines.</p> <p>In this walkthrough you will learn the basic steps and commands to tackle your data science projects using the Python API. For illustration purpose we will show how an example data set would have been dealt with using classical data science tools. In contrast, we demonstrate on the same example data set how the most tedious part of a data science project - merging and aggregating a relation data set - is automated using getML. By the end of this tutorial you are ready to tackle your own use cases with getML and dive deeper into our software using a variety of follow-up material.</p>"},{"location":"user_guide/walkthrough/#starting-a-new-project","title":"Starting a new project","text":"<p>After you\u2019ve successfully installed getML, you can begin by executing the following in a jupyter-notebook:</p> <p><pre><code>import getml \nprint(f\"getML API version: {getml.__version__}\\n\")\ngetml.engine.launch() #not needed in docker based installations\n\n\"\"\"\nLaunched the getML Engine. The log output will be stored in\n/home/xxxxx/.getML/logs/xxxxxxxxxxxxxx.log.\n\"\"\"\n</code></pre> This will import the getML Python API, launch the Engine, and (in the Enterprise edition) the Monitor.</p> <p>The getML Monitor, available in the Enterprise edition, is the frontend to the Engine. It should open automatically by launching the Engine. In case it does not, visit http://localhost:1709/ to open it. From now on, the entire analysis is run from Python.</p> <p>The entry-point for your project is the <code>getml.project</code> module. From here, you can start projects and control running projects. Further, you have access to all project-specific entities, and you can export a project as a <code>.getml</code> bundle to disk or load a <code>.getml</code> bundle from disk. To see the running projects, you can execute:</p> <pre><code>getml.project\n\"\"\"\nCannot reach the getML Engine. Please make sure you have set a project.\nTo set: `getml.engine.set_project(...)`\nAvailable projects:\n\"\"\"\n</code></pre> <p>This message tells us that we have no running Engine instance because we have not set a project. So, we follow the advice and create a new project. All datasets and models belonging to a project will be stored in <code>~/.getML/projects</code>.</p> <p><pre><code>getml.engine.set_project(\"getting_started\")\n\"\"\"\nConnected to project 'getting_started'\n\"\"\"\n</code></pre> Now, when you check the current projects:</p> <pre><code>getml.project\n\"\"\"\nCurrent project:\n\ngetting_started\n\"\"\"\n</code></pre>"},{"location":"user_guide/walkthrough/#data-set","title":"Data Set","text":"<p>The data set used in this tutorial consists of 2 tables: (I) the so-called population table represents the entities we want to make a prediction about in the analysis and (II) the peripheral table contains additional information and is related to the population table via a join key. Such a data set could appear, for example, in a customer churn analysis where each row in the population table represents a customer and each row in the peripheral table represents a transaction. It could also be part of a predictive maintenance campaign where each row in the population table corresponds to a particular machine in a production line and each row in the peripheral table to a measurement from a certain sensor.</p> <p>In this guide, however, we do not assume any particular use case. After all, getML is applicable to a wide range of problems from different domains. Domain specific examples can be found in the notebooks.</p> <pre><code>population_table, peripheral_table = getml.datasets.make_numerical(\n     n_rows_population=500,\n     n_rows_peripheral=100000,\n     random_state=1709\n)\nprint(\"Data Frames\")\nprint(getml.project.data_frames)\n\nprint(\"Population table\")\nprint(population_table)\n\n\"\"\"\nData Frames\n    name                        rows     columns   memory usage\n0   numerical_peripheral_1709   100000         3           2.00 MB\n1   numerical_population_1709      500         4           0.01 MB\n\nPopulation table\nName   time_stamp                    join_key   targets   column_01\nRole   time_stamp                    join_key    target   numerical\nUnits   time stamp, comparison only\n    0   1970-01-01 00:00:00.470834           0       101     -0.6295\n    1   1970-01-01 00:00:00.899782           1        88     -0.9622\n    2   1970-01-01 00:00:00.085734           2        17      0.7326\n    3   1970-01-01 00:00:00.365223           3        74     -0.4627\n    4   1970-01-01 00:00:00.442957           4        96     -0.8374\n        ...                                ...       ...     ...\n  495   1970-01-01 00:00:00.945288         495        93      0.4998\n  496   1970-01-01 00:00:00.518100         496       101     -0.4657\n  497   1970-01-01 00:00:00.312872         497        59      0.9932\n  498   1970-01-01 00:00:00.973845         498        92      0.1197\n  499   1970-01-01 00:00:00.688690         499       101     -0.1274\n\n\n  500 rows x 4 columns\n  memory usage: 0.01 MB\n  type: getml.DataFrame\n\"\"\"\n</code></pre> <p>The population table contains 4 columns. The column called <code>column_01</code> contains a random numerical value. The next column, <code>targets</code>, is the one we want to predict in the analysis. To this end, we also need to use the information from the peripheral table.</p> <p>The relationship between the population and peripheral table is established using the <code>join_key</code> and <code>time_stamp</code> columns: Join keys are used to connect one or more rows from one table with one or more rows from the other table. Time stamps are used to limit these joins by enforcing causality and thus ensuring that no data from the future is used during the training.</p> <p>In the peripheral table, <code>columns_01</code> also contains a random numerical value. The population table and the peripheral table have a one-to-many relationship via <code>join_key</code>. This means that one row in the population table is associated with many rows in the peripheral table. In order to use the information from the peripheral table, we need to merge the many rows corresponding to one entry in the population table into so-called features. This is done using certain aggregations.</p> <p></p> <p>For example, such an aggregation could be the sum of all values in <code>column_01</code>. We could also apply a subcondition, like taking only values into account that fall into a certain time range with respect to the entry in the population table. In SQL code such a feature would look like this:</p> <pre><code>SELECT COUNT( * )\nFROM POPULATION t1\nLEFT JOIN PERIPHERAL t2\nON t1.join_key = t2.join_key\nWHERE (\n  ( t1.time_stamp - t2.time_stamp &lt;= TIME_WINDOW )\n) AND t2.time_stamp &lt;= t1.time_stamp\nGROUP BY t1.join_key,\n    t1.time_stamp;\n</code></pre> <p>Unfortunately, neither the right aggregation nor the right subconditions are clear a priori. The feature that allows us to predict the target best could very well be e.g.\u00a0the average of all values in <code>column_01</code> that fall below a certain threshold, or something completely different. If you were to tackle this problem with classical machine learning tools, you would have to write many SQL features by hand and find the best ones in a trial-and-error-like fashion. At best, you could apply some domain knowledge that guides you towards the right direction. This approach, however, bears two major disadvantages that prevent you from finding the best-performing features.</p> <ol> <li>You might not have sufficient domain knowledge.</li> <li>You might not have sufficient resources for such a time-consuming, tedious, and error-prone process.</li> </ol> <p>This is where getML comes in. It finds the correct features for you - automatically. You do not need to manually merge and aggregate tables in order to get started with a data science project. In addition, getML uses the derived features in a classical AutoML setting to easily make predictions with established and well-performing algorithms. This means getML provides an end-to-end solution starting from the relational data to a trained ML-model. How this is done via the getML Python API is demonstrated in the following.</p>"},{"location":"user_guide/walkthrough/#defining-the-data-model","title":"Defining the data model","text":"<p>Most machine learning problems on relational data can be expressed as a simple star schema. This example is no exception, so we will use the predefined <code>StarSchema</code> class.</p> <pre><code>split = getml.data.split.random(train=0.8, test=0.2)\n\nstar_schema = getml.data.StarSchema(\n    population=population_table, alias=\"population\", split=split)\n\nstar_schema.join(peripheral_table,\n                 alias=\"peripheral\",\n                 on=\"join_key\",\n                 time_stamps=\"time_stamp\",\n)\n</code></pre>"},{"location":"user_guide/walkthrough/#building-a-pipeline","title":"Building a pipeline","text":"<p>Now we can define the feature learner. Additionally, you can alter some hyperparameters like the number of features you want to train or the list of aggregations to select from when building features.</p> <p><pre><code>fastprop = getml.feature_learning.FastProp(\n     num_features=5,\n     aggregation=[\n         getml.feature_learning.aggregations.COUNT,\n         getml.feature_learning.aggregations.SUM\n     ],\n)\n</code></pre> getML bundles the sequential operations of a data science project (preprocessing, feature engineering, and predicting) into <code>Pipeline</code> objects. In addition to the <code>Placeholders</code> representing the <code>DataFrames</code> you also have to provide a feature learner (from <code>getml.feature_learning</code>) and a predictor (from <code>getml.predictors</code>).</p> <pre><code>pipe = getml.pipeline.Pipeline(\n     data_model=star_schema.data_model,\n     feature_learners=[fastprop],\n     predictors=[getml.predictors.LinearRegression()],\n)\n</code></pre> <p>Note</p> <p>For the sake of demonstration, we have chosen a narrow search field in aggregation space by only letting FastProp use <code>COUNT</code> and <code>SUM</code>, we use a simple <code>LinearRegression</code> and construct only 5 different features. In real world projects you have little reason to artificially restrict your aggregation set and would use something more straightforward like <code>aggregation=getml.feature_learning.aggregations.FASTPROP.default</code>, you would construct at least 100 features and you could consider using a more powerful predictor to get results significantly better than what we will achieve here.</p>"},{"location":"user_guide/walkthrough/#working-with-a-pipeline","title":"Working with a pipeline","text":"<p>Now, that we have defined a <code>Pipeline</code>, we can let getML do the heavy lifting of your typical data science project. With a well-defined <code>Pipeline</code>, you can, i.a.:</p> <ul> <li><code>fit()</code> the pipeline, to learn the logic behind your features (also referred to as training);</li> <li><code>score()</code> the pipeline to evaluate its performance on unseen data;</li> <li><code>transform()</code> the pipeline and materialize the learned logic into concrete (numerical) features;</li> <li><code>predict()</code> the <code>target</code>s for unseen data;</li> <li><code>deploy()</code> the pipeline to an http endpoint.</li> </ul>"},{"location":"user_guide/walkthrough/#training","title":"Training","text":"<p>When fitting the model, we pass the handlers to the actual data residing in the getML Engine \u2013 the <code>DataFrame</code>s.</p> <pre><code>pipe.fit(star_schema.train)\n\n\"\"\"\nChecking data model...\nStaging...  100% \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u2022 0:00:00 \u2022 0:00:00\nChecking... 100% \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u2022 0:00:00 \u2022 0:00:00\n\nOK.\nStaging...                                 100% \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u2022 0:00:00 \u2022 0:00:00\nFastProp: Trying 5 features...             100% \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u2022 0:00:00 \u2022 0:00:00\nFastProp: Building features...             100% \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u2022 0:00:00 \u2022 0:00:00\nLinearRegression: Training as predictor... 100% \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u2022 0:00:00 \u2022 0:00:00\n\nTrained pipeline.\nTime taken: 0h:0m:0.023915\n\nPipeline(data_model='population',\n         feature_learners=['FastProp'],\n         feature_selectors=[],\n         include_categorical=False,\n         loss_function='SquareLoss',\n         peripheral=['peripheral'],\n         predictors=['LinearRegression'],\n         preprocessors=[],\n         share_selected_features=0.5,\n         tags=['container-cjCqZq'])\n\"\"\"\n</code></pre> <p>That\u2019s it. The features learned by <code>FastProp</code> as well as the <code>LinearRegression</code> are now trained on our data set.</p>"},{"location":"user_guide/walkthrough/#scoring","title":"Scoring","text":"<p>We can also score our algorithms on the test set.</p> <pre><code>pipe.score(star_schema.test)\n\n\"\"\"\nStaging...                     100% \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u2022 0:00:00 \u2022 0:00:00\nPreprocessing...               100% \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u2022 0:00:00 \u2022 0:00:00\nFastProp: Building features... 100% \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u2022 0:00:00 \u2022 0:00:00\n\n    date time             set used   target        mae      rmse   rsquared\n0   2024-08-08 12:14:11   train      targets    3.3721    4.1891     0.9853\n1   2024-08-08 12:21:35   test       targets    3.7548    4.7093     0.981 \n\"\"\"\n</code></pre> <p>Our model is able to predict the target variable in the newly generated data set pretty accurately. Though, the enterprise feature learner <code>Multirel</code> performs even better here with R<sup>2</sup> of 0.9995 and MAE and RMSE of 0.07079 and 0.1638 respectively.</p>"},{"location":"user_guide/walkthrough/#making-predictions","title":"Making predictions","text":"<p>Let\u2019s simulate the arrival of unseen data and generate another population table. The data model is already defined and the pipeline trained. All we need to do is to add data to a <code>Container</code> and pass it to the existing <code>pipe</code> object. </p> <pre><code>population_table_unseen, peripheral_table_unseen = getml.datasets.make_numerical(\n    n_rows_population=200,\n    n_rows_peripheral=8000,\n    random_state=1711,\n)\n\ncontainer_unseen = getml.data.Container(population_table_unseen)\n\ncontainer_unseen.add(peripheral=peripheral_table_unseen)\n\nyhat = pipe.predict(container_unseen.full)\n\nprint(yhat[:10])\n\n\"\"\"\nStaging...                     100% \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u2022 0:00:00 \u2022 0:00:00\nPreprocessing...               100% \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u2022 0:00:00 \u2022 0:00:00\nFastProp: Building features... 100% \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u2022 0:00:00 \u2022 0:00:00\n\n&gt;&gt;&gt; print(yhat[:10])\n[[ 4.16876676]\n [17.32933   ]\n [26.62467516]\n [-5.30655759]\n [27.4984785 ]\n [21.48631811]\n [18.16896219]\n [ 5.2784719 ]\n [20.5992354 ]\n [26.20538556]]\n \"\"\"\n</code></pre>"},{"location":"user_guide/walkthrough/#extracting-features","title":"Extracting features","text":"<p>Of course, you can also transform a specific data set into the corresponding features in order to insert them into another machine learning algorithm.</p> <pre><code>features = pipe.transform(container_unseen.full)\nprint(features)\n\n\"\"\"\nStaging...                     100% \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u2022 0:00:00 \u2022 0:00:00\nPreprocessing...               100% \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u2022 0:00:00 \u2022 0:00:00\nFastProp: Building features... 100% \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 \u2022 0:00:00 \u2022 0:00:00\n\n[[-7.14232213e-01  2.39745475e-01  2.62855261e-01  1.28462060e-02\n   5.00000000e+00 -3.18568319e-01]\n [-1.17601634e-01  3.42472663e+00  3.61423201e+00  3.24305583e-02\n   1.40000000e+01  3.94656676e-01]\n [-2.48645436e+00  1.27495266e+01  1.33228011e+01  1.99520872e-02\n   3.60000000e+01  1.24700392e-01]\n ...\n [ 9.55124379e-01  9.16437833e-01  9.40897830e-01  2.73040074e-02\n   8.00000000e+00 -7.49963688e-01]\n [-3.56023429e+00  3.37346772e+00  2.11562428e+00  2.53698895e-02\n   1.50000000e+01 -7.27880243e-01]\n [ 2.72804029e-02  2.87302783e-02  5.36035230e-02  2.77103542e-02\n   2.00000000e+00 -3.53700424e-01]]\n\"\"\"\n</code></pre> <p>If you want to see a SQL transpilation of a feature's logic, you can do so by clicking on the feature in the Monitor (Enterprise edition only) or by inspecting the sql attribute on a feature. A <code>Pipeline</code>'s features are held by the <code>Features</code> container. For example, to inspect the SQL code of the feature with the highest importance, run: <pre><code>pipe.features.sort(key=lambda feature: feature.importance, descending = True)[0].sql\n</code></pre></p> <p>That should return something like this:</p> <pre><code>DROP TABLE IF EXISTS \"FEATURE_1_5\";\n\nCREATE TABLE \"FEATURE_1_5\" AS\nSELECT COUNT( * ) AS \"feature_1_5\",\n     t1.rowid AS rownum\nFROM \"POPULATION__STAGING_TABLE_1\" t1\nINNER JOIN \"PERIPHERAL__STAGING_TABLE_2\" t2\nON t1.\"join_key\" = t2.\"join_key\"\nWHERE t2.\"time_stamp\" &lt;= t1.\"time_stamp\"\nGROUP BY t1.rowid;\n</code></pre> <p>This very much resembles the ad hoc definition we tried in the beginning. The correct aggregation to use on this data set is <code>COUNT</code>. getML extracted this definition completely autonomously.</p>"},{"location":"user_guide/walkthrough/#next-steps","title":"Next steps","text":"<p>This guide has shown you the very basics of getML. Starting with raw data, you have completed a full project including feature engineering and linear regression using an automated end-to-end pipeline. The most tedious part of this process - finding the right aggregations and subconditions to construct a feature table from the relational data model - was also included in this pipeline.</p> <p>But there\u2019s more! Examples show application of getML on real world data sets.</p> <p>Also, don\u2019t hesitate to contact us with your feedback.</p>"}]}